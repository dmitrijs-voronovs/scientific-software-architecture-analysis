quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Performance,eadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:7657,Optimiz,Optimize,7657,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"eadInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 : INFO: TaskReport: stage=0, partition=38854, attempt=0, peakBytes=65536, peakBytesReadable=64.00 KiB, chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:208) ~[scala-library-2.12.15.jar:?]; 	at is.hail.io.StreamBlockInputBuffer.readBlock(InputBuffers.scala:552) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:384) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.readInt(InputBuffers.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:2853,concurren,concurrent,2853,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,2,['concurren'],['concurrent']
Performance,"ease notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/janus/releases"">janus's releases</a>.</em></p>; <blockquote>; <h2>janus 1.0.0 release</h2>; <ul>; <li>Dropped Python 3.6 support</li>; <li>Janus is marked as stable, no API changes was made for years</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/janus/blob/master/CHANGES.rst"">janus's changelog</a>.</em></p>; <blockquote>; <h2>1.0.0 (2021-12-17)</h2>; <ul>; <li>Drop Python 3.6 support</li>; </ul>; <h2>0.7.0 (2021-11-24)</h2>; <ul>; <li>Add SyncQueue and AsyncQueue Protocols to provide type hints for sync and async queues <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/374"">#374</a></li>; </ul>; <h2>0.6.2 (2021-10-24)</h2>; <ul>; <li>Fix Python 3.10 compatibility <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/358"">#358</a></li>; </ul>; <h2>0.6.1 (2020-10-26)</h2>; <ul>; <li>; <p>Raise RuntimeError on queue.join() after queue closing. <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/295"">#295</a></p>; </li>; <li>; <p>Replace <code>timeout</code> type from <code>Optional[int]</code> to <code>Optional[float]</code> <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/267"">#267</a></p>; </li>; </ul>; <h2>0.6.0 (2020-10-10)</h2>; <ul>; <li>; <p>Drop Python 3.5, the minimal supported version is Python 3.6</p>; </li>; <li>; <p>Support Python 3.9</p>; </li>; <li>; <p>Refomat with <code>black</code></p>; </li>; </ul>; <h2>0.5.0 (2020-04-23)</h2>; <ul>; <li>Remove explicit loop arguments and forbid creating queues outside event loops <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/246"">#246</a></li>; </ul>; <h2>0.4.0 (2018-07-28)</h2>; <ul>; <li>; <p>Add <code>py.typed</code> macro <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/89"">#89</a></p>; </li>; <li>; <p>Drop python 3.4 support ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11466:1151,queue,queue,1151,https://hail.is,https://github.com/hail-is/hail/pull/11466,1,['queue'],['queue']
Performance,"eators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:1705,perform,performance,1705,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['perform'],['performance']
Performance,"eb_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,991"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; ```. Recall that pod creation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the database and k8s. Eventually this sequence of log messages appears three times in quick succession. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6617:4950,concurren,concurrent,4950,https://hail.is,https://github.com/hail-is/hail/issues/6617,1,['concurren'],['concurrent']
Performance,"ec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collectin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4844,cache,cached,4844,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"ect(pca_rows.locus.contig, pca_rows.file_row_idx); contig_row_list = pca_rows.collect(); print('finished collecting'); contig_reformed = [(x['contig'], x['file_row_idx']) for x in contig_row_list]; print('reformed'); from collections import defaultdict; contig_row_dict = defaultdict(list); for k, v in contig_reformed:; contig_row_dict[k].append(v); print('dictionary created'). with hl.hadoop_open(contig_row_dict_location, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953:3351,load,loadings,3351,https://hail.is,https://github.com/hail-is/hail/issues/3953,1,['load'],['loadings']
Performance,"ectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:2486,Load,LoadVCF,2486,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"ecute(local_jar_location, self.scratch, self.log_file, self.jar_url, self.argv); File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/rwalters-hail-tmp/o?name=merged_round2_sumstats.fix_lowconf.mt/entries/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:1547,concurren,concurrent,1547,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,ecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6007,Optimiz,Optimize,6007,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,ecuteWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHand,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218809,concurren,concurrent,218809,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,ecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.cl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218734,concurren,concurrent,218734,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ecutor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 8.0 in stage 0.0 (TID 8, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:109497,concurren,concurrent,109497,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,ecutor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:99393,concurren,concurrent,99393,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ecutor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6123,Load,LoadVCF,6123,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,ecutor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:4,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9717,Load,LoadVCF,9717,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"ed PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3100,cache,cached,3100,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,ed(Interpret.scala:58); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$foldConstants$1(FoldConstants.scala:47); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.Loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7066,Optimiz,Optimize,7066,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['Optimize']
Performance,"ed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:1485,perform,performance,1485,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['perform'],['performance']
Performance,"ed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:1264,perform,performance,1264,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['perform'],['performance']
Performance,"edge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the original data. Alex Kotlar: Would we want to associate an instance of a PType with a single region?. Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hold a region. Then a lazily decoding PType can hold a (uniquely owned) region to decode into, invisibly to callers. Patrick Schultz: In which case I don't think the region argument to loadElement would be needed. Alex Kotlar:; In which case I don't think the region argument to loadElement would be needed; agreed. Tim Poterba: ok, I think I'm convinced.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826:1914,perform,performance,1914,https://hail.is,https://github.com/hail-is/hail/issues/7826,4,"['load', 'perform']","['load', 'loadElement', 'performance']"
Performance,"edit:. Never mind about the proposal. I think copy is useful, for instance in LoadPlink:. ```scala; val (sampleInfo, signature) = LoadPlink.parseFam(fam, ffConfig, hc.sFS). val nameMap = Map(""id"" -> ""s""); val saSignature = signature.copy(fields = signature.fields.map(f => f.copy(name = nameMap.getOrElse(f.name, f.name)))); ```. Here if I return a non-canonical PStruct from LoadPlink, I may want to have a single method that would allow me to construct a new instance, using the same class as that used to construct signature.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7703#issuecomment-567074778:78,Load,LoadPlink,78,https://hail.is,https://github.com/hail-is/hail/issues/7703#issuecomment-567074778,3,['Load'],['LoadPlink']
Performance,"eduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_clie",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6616:7990,concurren,concurrent,7990,https://hail.is,https://github.com/hail-is/hail/issues/6616,2,['concurren'],['concurrent']
Performance,"ef execute(self, ir: BaseIR, timed: bool = False) -> Any:; 217 try:; --> 218 return super().execute(ir, timed); 219 except Exception as err:; 220 if self._copy_log_on_error:. File /usr/local/lib/python3.11/dist-packages/hail/backend/backend.py:190, in Backend.execute(self, ir, timed); 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; --> 190 raise e.maybe_user_error(ir) from None; 191 if ir.typ == tvoid:; 192 value = None. File /usr/local/lib/python3.11/dist-packages/hail/backend/backend.py:188, in Backend.execute(self, ir, timed); 186 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 187 try:; --> 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; 190 raise e.maybe_user_error(ir) from None. File /usr/local/lib/python3.11/dist-packages/hail/backend/py4j_backend.py:221, in Py4JBackend._rpc(self, action, payload); 219 if resp.status_code >= 400:; 220 error_json = orjson.loads(resp.content); --> 221 raise fatal_error_from_java_error_triplet(; 222 error_json['short'], error_json['expanded'], error_json['error_id']; 223 ); 224 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: IllegalArgumentException: requirement failed. Java stack trace:; java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.expr.ir.lowering.TableStage.repartitionNoShuffle(LowerTableIR.scala:357); 	at is.hail.rvd.AbstractRVDSpec$.$anonfun$readZippedLowered$12(AbstractRVDSpec.scala:207); 	at is.hail.expr.ir.TableNativeZippedReader.lower(TableIR.scala:2042); 	at is.hail.expr.ir.TableReader.lower(TableIR.scala:663); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1061); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1050); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2242); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14529:5889,load,loads,5889,https://hail.is,https://github.com/hail-is/hail/issues/14529,1,['load'],['loads']
Performance,"ef=""https://redirect.github.com/python-pillow/Pillow/issues/7619"">#7619</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Import plugins relative to the module <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7576"">#7576</a> [<a href=""https://github.com/deliangyang""><code>@​deliangyang</code></a>]</li>; <li>Translate encoder error codes to strings; deprecate <code>ImageFile.raise_oserror()</code> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7609"">#7609</a> [<a href=""https://github.com/bgilbert""><code>@​bgilbert</code></a>]</li>; <li>Updated readthedocs to latest version of Python <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7611"">#7611</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Support reading BC4U and DX10 BC1 images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/6486"">#6486</a> [<a href=""https://github.com/REDxEYE""><code>@​REDxEYE</code></a>]</li>; <li>Optimize ImageStat.Stat.extrema <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7593"">#7593</a> [<a href=""https://github.com/florath""><code>@​florath</code></a>]</li>; <li>Handle pathlib.Path in FreeTypeFont <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7578"">#7578</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use list comprehensions to create transformed lists <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7597"">#7597</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; <li>Added support for reading DX10 BC4 DDS images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7603"">#7603</a> [<a href=""https://github.com/sambvfx""><code>@​sambvfx</code></a>]</li>; <li>Optimized ImageStat.Stat.count <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7599"">#7599</a> [<a href=""https://github.com/florath""><code>@​florath</code></a>]</li>; <li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:7208,Optimiz,Optimize,7208,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['Optimiz'],['Optimize']
Performance,efficiently load compressed VCFs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4:12,load,load,12,https://hail.is,https://github.com/hail-is/hail/issues/4,1,['load'],['load']
Performance,egatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:10470,Load,LoadMatrix,10470,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,elationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 3.795ms self 0.010ms children 3.785ms %children 99.73%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.737ms self 0.071ms children 3.665ms %children 98.09%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:133718,Optimiz,OptimizePass,133718,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,elationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 4.599ms self 0.009ms children 4.590ms %children 99.81%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.531ms self 0.089ms children 4.442ms %children 98.03%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:149505,Optimiz,OptimizePass,149505,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,elationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 8.774ms self 0.014ms children 8.760ms %children 99.84%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.039ms self 0.039ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 8.695ms self 0.088ms children 8.607ms %children 98.99%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:118000,Optimiz,OptimizePass,118000,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"eleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to differentiate between k8 outages and lack of user requests. I also want to be able to quickly present a potential investor aggregate data, for notebook and all other manner of hail services: for notebook case: # notebooks created, length a notebook was used, how many notebooks were shared with others (I think this could be a useful feature, even if ""sharing"" meant taking a user-opt-in text dump that could be used to re-create a notebook, rather than connecting to that user's notebook), how many times a user re-visited a notebook, what our fullfillmen",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:2079,Perform,Performance,2079,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290,1,['Perform'],['Performance']
Performance,elf 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply tota,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:184136,Optimiz,OptimizePass,184136,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,elf 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.023ms self 0.023ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply tota,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:191161,Optimiz,OptimizePass,191161,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,elf 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.038ms self 0.029ms children 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.189ms self 0.189ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply tota,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:17871,Optimiz,OptimizePass,17871,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,elf 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.044ms self 0.035ms children 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.193ms self 0.193ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply tota,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:10887,Optimiz,OptimizePass,10887,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,elocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:60); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1476); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:574); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:563); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:288); 	at is.hail.services.package$.retryTransientErrors(package.scala:163); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:286); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:26); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:239); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:235); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/wes-bipolar-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:6631,concurren,concurrent,6631,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['concurren'],['concurrent']
Performance,"ember of the ""getting things done"" community than the; ""modern C++"" community. I have seen std::unique_ptr used in practice, and it was a bad experience.; And I stand by my contention that it doesn't solve any of the hard problems (whereas shared_ptr; very much does). Now I realize that people writing books about C++ write a good deal about move semantics and; unique_ptr. My interpretation is that there's a lot of writing about it because it involves concepts; which simply don't occur in any other commonly-used languages, and as such it requires a; good deal of explanation and justification because it's peculiar and unfamiliar. I suggest that; other languages haven't invented this concept because it's a) confusing and b) not particularly; useful. There's one really good thing you get from move semantics: the ability to resize a std::vector<T>; or std::unordered_map<T> without constructing deep copies of each T object. In the cases; where that's useful, it's very useful for optimizing performance without totally bypassing all your; abstraction mechanisms. The other ways people attempt to exploit move semantics are IMO; just a bad idea: if you want to pass around a large expensive-to-create object, then do it the; Java way by putting it on the heap and passing around some kind of reference, and *everyone* can understand it, not just experts in modern C++. Another angle on this debate would be to look at some open-source C++ projects and see how; often they actually use unique_ptr and/or std::move. My guess is that it's much less common; in practice than you might think from reading books about C++, because the overlap between; ""object ownership is passed around"" and ""... but we always know precisely who has ownership""; is not a very big part of the design space - compared to a whole lot of ""always owned by the object which created it"" and ""used in several places at once and we don't know who will be the last to drop it"". [Update: the LLVM codebase, including tests, is ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3718#issuecomment-396683489:1027,optimiz,optimizing,1027,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396683489,4,"['optimiz', 'perform']","['optimizing', 'performance']"
Performance,"ement 0 (Ref __iruid_514))))))\n (Let __iruid_515\n (MakeTuple (0)\n (ResultOp 0 (Collect (CollectStateSig +PInt32))))\n (MakeStruct\n (idx (GetTupleElement 0 (Ref __iruid_515)))))))))\n2022-11-15 20:30:18.206 root: INFO: optimize optimize: compileLowerer, after LowerArrayAggsToRunAggs: after: IR size 56:\n(MakeTuple (0)\n (Let __iruid_532\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (CollectStateSig +PInt32)) ()))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32))))\n (Let __iruid_533\n (CollectDistributedArray\n table_aggregate_singlestage __iruid_534\n __iruid_535\n (ToStream False\n (Literal Array[Struct{start:Int32,end:Int32}]\n <literal value>))\n (MakeStruct (__iruid_458 (Ref __iruid_532)))\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0\n (GetField __iruid_458 (Ref __iruid_535)))))\n (StreamFor __iruid_536\n (StreamMap __iruid_537\n (StreamRange -1 True\n (GetField start (Ref __iruid_534))\n (GetField end (Ref __iruid_534))\n (I32 1))\n (MakeStruct (idx (Ref __iruid_537))))\n (Begin\n (SeqOp 0 (Collect (CollectStateSig +PInt32))\n ((GetField idx (Ref __iruid_536)))))))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32))))\n (NA String))\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0 (Ref __iruid_532))))\n (StreamFor __iruid_538\n (ToStream True (Ref __iruid_533))\n (Begin\n (CombOpValue 0 (Collect (CollectStateSig +PInt32))\n (GetTupleElement 0 (Ref __iruid_538))))))\n (Let __iruid_539\n (MakeTuple (0)\n (ResultOp 0 (Collect (CollectStateSig +PInt32))))\n (MakeStruct\n (idx (GetTupleElement 0 (Ref __iruid_539)))))))))\n2022-11-15 20:30:18.222 root: INFO: instruction count: 3: __C1122HailClassLoaderContainer.<init>\n2022-11-15 20:30:18.222 root: INFO: instruction count: 3: __C1122HailClassLoaderContainer.<clinit>\n2022-11-15 20:30:18.222 root",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:11064,optimiz,optimize,11064,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,4,['optimiz'],['optimize']
Performance,"emitPackEncoder now supports requested type. I see a small (few percent) increase in performance in import_vcf/write on a gVCF file. Still, getting rid of a RVB is always a good change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5606:85,perform,performance,85,https://hail.is,https://github.com/hail-is/hail/pull/5606,1,['perform'],['performance']
Performance,emory.scala:96); 	at is.hail.annotations.Region.allocate(Region.scala:332); 	at __C35collect_distributed_array.__m61split_ToArray(Unknown Source); 	at __C35collect_distributed_array.__m54split_StreamFor(Unknown Source); 	at __C35collect_distributed_array.__m49begin_group_0(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$2(BackendUtils.scala:31); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$1(BackendUtils.scala:30); 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:142); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.hail.backend.service.Worker$.main(Worker.scala:142); 	at is.hail.backend.service.Main$.main(Main.scala:32); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. and when set to 8 cores/highmem took a couple minutes but finished successfully,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573:2661,concurren,concurrent,2661,https://hail.is,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573,6,['concurren'],['concurrent']
Performance,"emoving batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitute for this at the moment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10376:1635,perform,performance,1635,https://hail.is,https://github.com/hail-is/hail/pull/10376,2,['perform'],['performance']
Performance,"empt=0, peakBytes=656384, peakBytesReadable=641.00 KiB, chunks requested=4, cache hits=2; 2023-09-22 19:11:12.656 : INFO: RegionPool: FREE: 641.0K allocated (257.0K blocks / 384.0K chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.ob",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:9919,concurren,concurrent,9919,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,en 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.EmitContext.analyze total 0.106ms self 0.106ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:219480,Optimiz,OptimizePass,219480,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,en 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.InlineApplyIR total 0.057ms self 0.006ms children 0.051ms %children 89.57%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.InlineApplyIR/is.hail.expr.ir.lowering.CompilableIR total,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:203507,Optimiz,OptimizePass,203507,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,en 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass total 0.157ms self 0.005ms children 0.152ms %children 96.93%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass/is.hail.expr.ir,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:211459,Optimiz,OptimizePass,211459,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,en 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.070ms self 0.070ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:198855,Optimiz,OptimizePass,198855,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,en 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:183423,Optimiz,Optimize,183423,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,en 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.023ms self 0.023ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:190448,Optimiz,Optimize,190448,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,en 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.038ms self 0.029ms children 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.189ms self 0.189ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:17158,Optimiz,Optimize,17158,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,en 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.044ms self 0.035ms children 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.193ms self 0.193ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:10174,Optimiz,Optimize,10174,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,en 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.526ms self 0.526ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:5883,Optimiz,Optimize,5883,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,en 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:6971,Optimiz,Optimize,6971,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"en users install hail, or hail could remove usage of setuptools & its associated modules (`pkg_resources`) at runtime, as some other projects have done: https://github.com/TDAmeritrade/stumpy/issues/950. At a glance, the cleanest thing to do here may be to move off of the deprecated `pkg_resources` and to the recommended `importlib` if it has what you need: https://setuptools.pypa.io/en/latest/pkg_resources.html. I also have to admit that I discovered this while playing around with hail on a Raspberry Pi 4, so it is possible that something else broken caused this failure, but I believe I understand what's happening. Here's my full `pip freeze` for reference:. ```; (venv) (py312) alex@rpi400:~/hail $ pip freeze; aiodns==2.0.0; aiohttp==3.9.3; aiosignal==1.3.1; attrs==23.2.0; avro==1.11.3; azure-common==1.1.28; azure-core==1.30.1; azure-identity==1.15.0; azure-mgmt-core==1.4.0; azure-mgmt-storage==20.1.0; azure-storage-blob==12.19.1; bokeh==3.4.0; boto3==1.34.73; botocore==1.34.73; cachetools==5.3.3; certifi==2024.2.2; cffi==1.16.0; charset-normalizer==3.3.2; click==8.1.7; commonmark==0.9.1; contourpy==1.2.0; cryptography==42.0.5; decorator==4.4.2; Deprecated==1.2.14; dill==0.3.8; frozenlist==1.4.1; google-auth==2.29.0; google-auth-oauthlib==0.8.0; hail==0.2.128; humanize==1.1.0; idna==3.6; isodate==0.6.1; janus==1.0.0; Jinja2==3.1.3; jmespath==1.0.1; jproperties==2.1.1; MarkupSafe==2.1.5; msal==1.28.0; msal-extensions==1.1.0; msrest==0.7.1; multidict==6.0.5; nest-asyncio==1.6.0; numpy==1.26.4; oauthlib==3.2.2; orjson==3.9.11; packaging==24.0; pandas==2.2.1; parsimonious==0.10.0; pillow==10.2.0; plotly==5.20.0; portalocker==2.8.2; protobuf==3.20.2; py4j==0.10.9.5; pyasn1==0.6.0; pyasn1_modules==0.4.0; pycares==4.4.0; pycparser==2.21; Pygments==2.17.2; PyJWT==2.8.0; pyspark==3.3.4; python-dateutil==2.9.0.post0; python-json-logger==2.0.7; pytz==2024.1; PyYAML==6.0.1; regex==2023.12.25; requests==2.31.0; requests-oauthlib==2.0.0; rich==12.6.0; rsa==4.9; s3transfer==0.10.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14428:1967,cache,cachetools,1967,https://hail.is,https://github.com/hail-is/hail/issues/14428,1,['cache'],['cachetools']
Performance,"end$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 8.0 in stage 0.0 (TID 8, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:109436,concurren,concurrent,109436,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,end$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:99332,concurren,concurrent,99332,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"end.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.111ms self 0.111ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/PruneDeadFields, iteration: 0 total 9.943ms self 9.943ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 1 total 0.414ms self 0.414ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 1 total 0.021ms self 0.021ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1 total 0.384ms self 0.005ms children 0.379ms %children 98.72%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 1 total 0.103ms self 0.103ms children 0.000ms %children 0.00%; timing is.hail.backend.Backend",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:5791,Optimiz,Optimize,5791,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,endHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.032ms self 0.028ms children 0.003ms %children 10.95%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:174399,Optimiz,Optimize,174399,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,endHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.056ms self 0.056ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.050ms self 0.041ms children 0.010ms %children 19.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:31082,Optimiz,Optimize,31082,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"enerated/C2.method3 instruction count: 252; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.<init> instruction count: 3; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.apply instruction count: 28; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.apply instruction count: 12; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method1 instruction count: 100; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method2 instruction count: 106; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method3 instruction count: 252; 2019-01-22 13:11:48 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:50 root: INFO: Index reader cache queries: 1168; 2019-01-22 13:11:50 root: INFO: Index reader cache hit rate: 0.747431506849315; 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,varid:String},entry:Struct{GT:Call,GP:+Array[+Float64],dosage:+Float64}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (TableMapRows; (TableMapGlobals; (CastMatrixToTable ""the ent",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:27464,optimiz,optimize,27464,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['optimiz'],['optimize']
Performance,"ent call last):; File ""/opt/conda/default/lib/python3.6/site-packages/luigi/worker.py"", line 199, in run; new_deps = self._run_get_new_deps(); File ""/opt/conda/default/lib/python3.6/site-packages/luigi/worker.py"", line 141, in _run_get_new_deps; task_gen = self.task.run(); File ""/tmp/c7e0443c47b54e91b295e2bff7b554b9/seqr_loading.py"", line 54, in run; self.read_vcf_write_mt(); File ""/tmp/c7e0443c47b54e91b295e2bff7b554b9/seqr_loading.py"", line 84, in read_vcf_write_mt; mt.write(self.output().path, stage_locally=True, overwrite=True); File ""<decorator-gen-1092>"", line 2, in write; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 2529, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowerin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:48470,load,loads,48470,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['load'],['loads']
Performance,"ent/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleClient` objects around to access old data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:1099,concurren,concurrent,1099,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052,1,['concurren'],['concurrent']
Performance,"eption in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundExcepti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1337,load,loadClass,1337,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460,1,['load'],['loadClass']
Performance,eption: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/rwalters-hail-tmp/o?name=merged_round2_sumstats.fix_lowconf.mt/entries/rows/parts/part-15801-2fde3786-67cb-42ed-8aac-f900cfcc4c00&uploadType=resumable&upload_id=ADPycduMEzX6d_uX4CiP6_XItJKmP8UnUnYBfyPoselMbyLUkxs1wDLPnxWl5gXr5LnBaVntYR_i7jchyxgVsRb_5PknvcCIcfDJ; chunkOffset: 16777216; chunkLength: 0; localOffset: 0; remoteOffset: 16777216; lastChunk: false. at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:270); at is.hail.io.fs.FSPositionedOutputStream.write(FS.scala:218); at java.io.DataOutputStream.write(DataOutputStream.java:107); at is.hail.utils.richUtils.ByteTrackingOutputStream.write(ByteTrackingOutputStream.scala:19); at is.hail.io.StreamBlockOutputBuffer.writeBlock(OutputBuffers.scala:293); at is.hail.io.LZ4OutputBlockBuffer.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:3273,concurren,concurrent,3273,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:7817,concurren,concurrent,7817,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['concurren'],['concurrent']
Performance,eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:5264,concurren,concurrent,5264,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['concurren'],['concurrent']
Performance,"eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:11287,concurren,concurrent,11287,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['concurren'],['concurrent']
Performance,"er file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505))). In the Kudu logs, I'm seeing tons of:. W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS; a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet; 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable:; CreateTablet request on kudu.tserver.TabletServerAdminService from; 69.173.65.227:42904 dropped due to backpressure. The service queue is; full; it has 50 items. Suggestions on how to proceed? Should I increase the service queue size?. —; You are receiving this because you authored the thread.; Reply to this email directly or view it on GitHub; https://github.com/broadinstitute/hail/pull/242#issuecomment-208516279",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208722298:2648,queue,queue,2648,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298,2,['queue'],['queue']
Performance,"er#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.513ms self 0.513ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.120ms self 0.120ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 0 total 0.796ms self 0.796ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.111ms self 0.111ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/PruneDeadFields, iteration: 0 total 9.943ms self 9.943ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 1 total 0.414ms self 0.414ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 1 total 0.021ms self 0.021ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHand",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:4954,Optimiz,Optimize,4954,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,"er#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.306ms self 0.306ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 1 total 0.025ms self 0.025ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.151ms self 0.151ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/PruneDeadFields, iteration: 1 total 0.714ms self 0.714ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Verify total 0.022ms self 0.022ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.TypeCheck.apply total 0.124ms self 0.124ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEv",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:8097,Optimiz,Optimize,8097,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,er.java:174); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:134); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/566:3100,concurren,concurrent,3100,https://hail.is,https://github.com/hail-is/hail/issues/566,2,['concurren'],['concurrent']
Performance,er.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11275,Load,LoadVCF,11275,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,erContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:182); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 ShutdownHookManager: INFO: Shutdown hook called; 2019-01-22 13:12:06 ShutdownHookManager: INFO: Deleting directory /tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/pyspark-9e6e1242-84dd-4fa1-8e8a-210dac6042fb; 2019-01-22 13:12:06 ShutdownHookManager: INFO: Deleting directory /tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:222435,concurren,concurrent,222435,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,8,['concurren'],['concurrent']
Performance,erator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneri,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:18351,concurren,concurrent,18351,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['concurren'],['concurrent']
Performance,erator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$ex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:8797,concurren,concurrent,8797,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['concurren'],['concurrent']
Performance,eredInputStream.java:271); E 	at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.init(GzipCompressorInputStream.java:185); E 	at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.<init>(GzipCompressorInputStream.java:168); E 	at is.hail.io.fs.GZipCompressionCodec$.makeInputStream(FS.scala:125); E 	at is.hail.io.fs.FS.open(FS.scala:563); E 	at is.hail.io.fs.FS.open$(FS.scala:560); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.fs.FS.open(FS.scala:578); E 	at is.hail.io.fs.FS.open$(FS.scala:577); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.fs.FS.open(FS.scala:572); E 	at is.hail.io.fs.FS.open$(FS.scala:571); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.fs.FS.open(FS.scala:569); E 	at is.hail.io.fs.FS.open$(FS.scala:569); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.index.IndexReader$.readTypes(IndexReader.scala:65); E 	at is.hail.io.bgen.LoadBgen$.$anonfun$getBgenFileMetadata$2(LoadBgen.scala:208); E 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); E 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); E 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); E 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198); E 	at is.hail.io.bgen.LoadBgen$.getBgenFileMetadata(LoadBgen.scala:207); E 	at is.hail.io.bgen.MatrixBGENReader$.apply(LoadBgen.scala:387); E 	at is.hail.io.bgen.MatrixBGENReader$.fromJValue(LoadBgen.scala:365); E 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:116); E 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1815); E 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001:2566,Load,LoadBgen,2566,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001,1,['Load'],['LoadBgen']
Performance,erence genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2267,Load,LoadPlink,2267,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,eric.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NegativeArraySizeException; at java.util.Arrays.copyOf(Arrays.java:3236); at is.hail.annotations.Region.ensure(Region.scala:139); at is.hail.annotations.Region.allocate(Region.scala:152); at is.hail.annotations.Region.allocate(Region.scala:159); at is.hail.annotations.RegionValueBuilder.allocateRoot(RegionValueBuilder.scala:73); at is.hail.annotations.RegionValueBuilder.startBaseStruct(RegionValueBuilder.scala:92); at is.hail.annotations.RegionValueBuilder.startStruct(RegionValueBuilder.scala:115); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:740); ... 49 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:16076,concurren,concurrent,16076,https://hail.is,https://github.com/hail-is/hail/issues/3507,4,"['Load', 'concurren']","['LoadVCF', 'concurrent']"
Performance,ering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.257ms self 0.257ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.074ms self 0.035ms children 0.040ms %children 53.33%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:96194,Optimiz,OptimizePass,96194,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.264ms self 0.264ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.073ms self 0.037ms children 0.036ms %children 49.79%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:110632,Optimiz,OptimizePass,110632,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.318ms self 0.318ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.089ms self 0.043ms children 0.047ms %children 52.21%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:75015,Optimiz,OptimizePass,75015,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.476ms self 0.476ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.138ms self 0.089ms children 0.049ms %children 35.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:52089,Optimiz,OptimizePass,52089,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.055ms self 0.055ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.391ms self 0.391ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.098ms self 0.049ms children 0.049ms %children 50.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:60617,Optimiz,OptimizePass,60617,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.148ms self 0.148ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 1.317ms self 1.317ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.144ms self 0.075ms children 0.069ms %children 48.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:43561,Optimiz,OptimizePass,43561,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"ertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11328 25772 (MethodStmtX INVOKEINTERFACE is/hail/io/OutputBuffer.writeByte (B)Vinterface; 11329 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2354null Lis/hail/io/OutputBuffer;; 11330 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11331 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 11332 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;))); ```. # Pervasiveness. There is absolutely nothing about this bug that is whole-stage-codegen-specific, but I suspect the much larger single IRs compiled in whole stage code generation made it exponentially more likely for this corner case to occur. I imagine it would be possible to con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:5896,Load,LoadX,5896,https://hail.is,https://github.com/hail-is/hail/pull/11328,2,['Load'],['LoadX']
Performance,ervices.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.close(ServiceCacheableFS.scala:46); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 	at java.io.ObjectOutputStream$BlockDataOutputStream.close(ObjectOutputStream.java:1828); 	at java.io.ObjectOutputStream.close(ObjectOutputStream.java:742); 	at is.hail.utils.package$.using(package.scala:658); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$2(ServiceBackend.scala:119); 	at is.hail.backend.service.ServiceBackend$$Lambda$2192/1687163734.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$1(ServiceBackend.scala:119); 	at is.hail.backend.service.ServiceBackend$$Lambda$2187/1214727901.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:5473,concurren,concurrent,5473,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,10,['concurren'],['concurrent']
Performance,es-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-c,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:37454,cache,cached,37454,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"es: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:3343,Perform,Performance,3343,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['Perform'],['Performance']
Performance,"es</summary>; <p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>; <blockquote>; <h2>10.3.0</h2>; <p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/10.3.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/10.3.0.html</a></p>; <h2>Changes</h2>; <ul>; <li>CVE-2024-28219: Use strncpy to avoid buffer overflow <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7928"">#7928</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; <li>Use <code>functools.lru_cache</code> for <code>hopper()</code> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7912"">#7912</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; <li>Raise ValueError if seeking to greater than offset-sized integer in TIFF <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7883"">#7883</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Improve speed of loading QOI images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7925"">#7925</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Added RGB to I;16N conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7920"">#7920</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Add --report argument to <strong>main</strong>.py to omit supported formats <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7818"">#7818</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>; <li>Added RGB to I;16, I;16L and I;16B conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7918"">#7918</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix editable installation with custom build backend and configuration options <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7658"">#7658</a> [<a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:1125,load,loading,1125,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['load'],['loading']
Performance,"eserialized (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2153"">#2153</a>) (<a href=""https://github.com/googleapis/java-storage/commit/68ad8e7357097e3dd161c2ab5f7a42a060a3702c"">68ad8e7</a>)</li>; <li>Update grpc default metadata projection to include acl same as json (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2150"">#2150</a>) (<a href=""https://github.com/googleapis/java-storage/commit/330e795040592e5df22d44fb5216ad7cf2448e81"">330e795</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.14.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2151"">#2151</a>) (<a href=""https://github.com/googleapis/java-storage/commit/eba8b6a235919a27d1f6dadf770140c7d143aa1a"">eba8b6a</a>)</li>; </ul>; <h2>v2.25.0</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.24.0...v2.25.0"">2.25.0</a> (2023-07-24)</h2>; <h3>Features</h3>; <ul>; <li>BlobWriteChannelV2 - same throughput less GC (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2110"">#2110</a>) (<a href=""https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b"">1b52a10</a>)</li>; <li>Update Storage.createFrom(BlobInfo, Path) to have 150% higher throughput (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2059"">#2059</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4c2f44e28a1ff19ffb2a02e3cefc062a1dd98fdc"">4c2f44e</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>Update BlobWriteChannelV2 to properly carry forward offset after incremental flush (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2125"">#2125</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c099a2f4f8ea9afa6953270876653916b021fd9f"">c099a2f</a>)</li>; <li>Update GrpcStorageImpl.createFrom(BlobInfo, Path) to use RewindableContent (<a href=""https://redirect.github.com/googleapis/java-storage/issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:3393,throughput,throughput,3393,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['throughput'],['throughput']
Performance,"esn't prevent PR tests from pushing to the `cache` tag. This change just makes the tests run by the CI-in-the-PR not overwrite the cache. Every image build for a PR (which is tested by default namespace CI) will still overwrite the cache tag. AFAICT, this; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/foo; ```; Will use as a cache source the `latest` tag in the `gcr.io/hail-vdc/foo` repository. It is *not* sufficient for an image to be present in the repository and untagged or with a different tag from `latest`. In particular, every push to the `cache` tag prevents us from using other images even though they are in the registry! For example, I pushed two images to `cache`:. ```; (base) # gcloud container images list-tags gcr.io/hail-vdc/dktest; DIGEST TAGS TIMESTAMP; fb551d9bdb94 2022-06-10T14:16:39; afb4c5ad2d7b cache,latest 2022-06-10T14:15:55; ```. If I rebuild [1] the most recently pushed image with; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/dktest:cache; ```; it succeeds in getting the cache. If I rebuild the other image with the same import-cache, it does not see that the (untagged) image is already there! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800:1276,cache,cache,1276,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800,4,['cache'],['cache']
Performance,"et `TypeError: an integer is required (got type bytes)` immediately upon importing hail. A full transcript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1090,cache,cached,1090,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,ethod.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5499,Load,LoadPlink,5499,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,ethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:10424,Load,LoadMatrix,10424,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,ethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5572,Load,LoadPlink,5572,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,etting 'es.nodes.wan.only'; 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:247); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 6.0.0; 	at org.elasticsearch.hadoop.util.EsMajorVersion.parse(EsMajorVersion.java:79); 	at org.elasticsearch.hadoop.rest.RestClient.remoteEsVersion(RestClient.java:613); 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:240); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4138:7756,concurren,concurrent,7756,https://hail.is,https://github.com/hail-is/hail/issues/4138,2,['concurren'],['concurrent']
Performance,"euse and cacheing/locking. a) Even with whole-stage codegen, there's a possibility that during development a user; will be tweaking a query in ways which don't change all the stages. And in that case the re-use; would give hits on some stages. The plan is not really to aim at structuring things to get a; high level of re-use, but just to opportunistically exploit re-use which happens to occur -; e.g. if the early stages of an analysis involve reading an existing file and filtering in various; ways, then that may not be changed at all by changes to what happen in the real analysis; after the filtering. And this is also influenced by the medium-term goal of having a Hail service; which (amongst other things) can do simple analyses on small data in under ten seconds - in; that realm compilation time could become a critical factor as a serial bottleneck. [The place where persistent cacheing of compiled files helps most of all is in testing,; where you really are running the exact same queries over and over again on the same; small datasets, and in many cases after making small changes which only affect a few; of the queries]. b) We may actually have some version of the locking problem even if we don't try to reuse the files -; since we have several workers on a node, and possibly a master as well, all needing to put code; into a file (or wait for someone else to populate the file) so that they can load it. Depending on ; precisely how Spark manages things (which I wouldn't want to depend on too much anyway). In fact it's essential that all the workers share the same DLL file, because otherwise they'd be; trying to load multiple DLL's defining the same symbols. That aspect of it could be handled by; putting the files into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could manage it with nothing but a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385:918,cache,cacheing,918,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385,2,['cache'],['cacheing']
Performance,eway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at org.apache.hadoop.fs.Path.initialize(Path.java:205); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHado,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:5348,concurren,concurrent,5348,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,"ex;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:13750,load,loading,13750,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['load'],['loading']
Performance,"execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.367ms self 0.367ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.127ms self 0.127ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardRelationalLets, iteration: 0 total 0.030ms self 0.030ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/PruneDeadFields, iteration: 0 total 0.572ms self 0.572ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Verify total 0.022ms self 0.022ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.TypeCheck.apply total 0.144ms self 0.144ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:13749,Optimiz,Optimize,13749,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.NormalizeNames.apply total 0.154ms self 0.154ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply total 4.179ms self 1.415ms children 2.764ms %children 66.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 1.490ms self 0.008ms children 1.482ms %children 99.45%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.013ms self 0.013ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.460ms self 0.066ms children 1.394ms %children 95.49%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.654ms self 0.654ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.Lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:193524,Optimiz,OptimizePass,193524,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,expectation maximization. Optimization loops.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4732#issuecomment-597125254:26,Optimiz,Optimization,26,https://hail.is,https://github.com/hail-is/hail/issues/4732#issuecomment-597125254,1,['Optimiz'],['Optimization']
Performance,"expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ExtractIntervalFilters, iteration: 0 total 0.016ms self 0.016ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0 total 0.363ms self 0.004ms children 0.358ms %children 98.82%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.358ms self 0.358ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/Simplify, iteration: 0 total 0.077ms self 0.077ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0 total 0.659ms self 0.292ms children 0.367ms %children 55.66%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.367ms self 0.367ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.127ms self 0.127ms children 0.000ms %c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:12253,Optimiz,Optimize,12253,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.600ms self 0.007ms children 0.594ms %children 98.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.575ms self 0.049ms children 0.527ms %children 91.53%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:180251,Optimiz,Optimize,180251,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.601ms self 0.006ms children 0.595ms %children 99.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.577ms self 0.041ms children 0.536ms %children 92.90%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:187276,Optimiz,Optimize,187276,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 1.157ms self 0.010ms children 1.147ms %children 99.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.096ms self 0.044ms children 1.052ms %children 96.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:13986,Optimiz,Optimize,13986,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerMatrixToTablePass/is.hail.expr.ir.lowering.MatrixLoweredToTable total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 1.157ms self 0.010ms children 1.147ms %children 99.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.096ms self 0.044ms children 1.052ms %children 96.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:13648,Optimiz,OptimizePass,13648,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.525ms self 0.004ms children 0.521ms %children 99.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.503ms self 0.061ms children 0.443ms %children 87.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:214517,Optimiz,Optimize,214517,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.518ms self 0.004ms children 0.514ms %children 99.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.496ms self 0.040ms children 0.456ms %children 91.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.228ms self 0.228ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:206496,Optimiz,Optimize,206496,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 12.144ms self 12.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 9.513ms self 3.957ms children 5.556ms %children 58.40%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 4.091ms self 4.091ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:41429,Optimiz,Optimize,41429,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.621ms self 0.621ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.245ms self 0.245ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPip,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:158353,Optimiz,Optimize,158353,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.650ms self 0.650ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.254ms self 0.254ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPip,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:142566,Optimiz,Optimize,142566,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.283ms self 4.283ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.308ms self 0.308ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPip,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:126848,Optimiz,Optimize,126848,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerMatrixToTablePass/is.hail.expr.ir.lowering.AnyIR total 0.027ms self 0.027ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerMatrixToTablePass/is.hail.expr.ir.lowering.MatrixLoweredToTable total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 1.157ms self 0.010ms children 1.147ms %children 99.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.096ms self 0.044ms children 1.052ms %children 96.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.259ms self 0.259ms childr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:13369,Optimiz,OptimizePass,13369,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.184ms self 0.184ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.228ms self 1.353ms children 0.875ms %children 39.29%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.340ms self 0.340ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:153836,Optimiz,Optimize,153836,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.195ms self 0.195ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.409ms self 0.648ms children 0.762ms %children 54.04%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.463ms self 0.463ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:138049,Optimiz,Optimize,138049,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.445ms self 0.445ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.252ms self 0.765ms children 1.487ms %children 66.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.505ms self 0.505ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:122331,Optimiz,Optimize,122331,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 31.509ms self 1.445ms children 30.064ms %children 95.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.344ms self 0.344ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 29.693ms self 9.419ms children 20.274ms %children 68.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:3021,Optimiz,Optimize,3021,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,extRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:6010,concurren,concurrent,6010,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['concurren'],['concurrent']
Performance,"extRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.12-13681278eb89; Error summary: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; ```. --------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:12578,concurren,concurrent,12578,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['concurren'],['concurrent']
Performance,"f any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733:1189,load,load,1189,https://hail.is,https://github.com/hail-is/hail/issues/4733,1,['load'],['load']
Performance,"f.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011161/48459558-9f645c80-e798-11e8-94db-0faa2e44e985.png).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2744,Load,LoadVCF,2744,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,f41a81995c53a28e375c26b.jar.jar:?]; 	at is.hail.relocated.org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:71) ~[gs:__hail-query-daaf463550_jars_4c60fddb171a52c21f41a81995c53a28e375c26b.jar.jar:?]; 	at is.hail.backend.service.ServiceBackendAPI$.$anonfun$main$5(ServiceBackend.scala:467) ~[gs:__hail-query-daaf463550_jars_4c60fddb171a52c21f41a81995c53a28e375c26b.jar.jar:?]; 	at is.hail.utils.package$.using(package.scala:673) ~[gs:__hail-query-daaf463550_jars_4c60fddb171a52c21f41a81995c53a28e375c26b.jar.jar:?]; 	at is.hail.backend.service.ServiceBackendAPI$.main(ServiceBackend.scala:467) ~[gs:__hail-query-daaf463550_jars_4c60fddb171a52c21f41a81995c53a28e375c26b.jar.jar:?]; 	at is.hail.backend.service.Main$.main(Main.scala:10) ~[gs:__hail-query-daaf463550_jars_4c60fddb171a52c21f41a81995c53a28e375c26b.jar.jar:?]; 	at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-daaf463550_jars_4c60fddb171a52c21f41a81995c53a28e375c26b.jar.jar:?]; 	... 12 more; 2024-11-05 02:43:37.787 JVMEntryway: ERROR: Exception encountered in QoB cancel thread.; org.newsclub.net.unix.SocketClosedException: Not open; 	at org.newsclub.net.unix.AFCore.validFdOrException(AFCore.java:90) ~[jvm-entryway.jar:?]; 	at org.newsclub.net.unix.AFSocketImpl$AFInputStreamImpl.read(AFSocketImpl.java:510) ~[jvm-entryway.jar:?]; 	at java.io.DataInputStream.readInt(DataInputStream.java:393) ~[?:?]; 	at is.hail.JVMEntryway$2.run(JVMEntryway.java:136) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:7312,concurren,concurrent,7312,https://hail.is,https://github.com/hail-is/hail/issues/14749,6,['concurren'],['concurrent']
Performance,"f=""https://github.com/damemi""><code>@​damemi</code></a>) [SIG Scheduling and Testing]</li>; <li>Kubelet should reject pods whose OS doesn't match the node's OS label. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105292"">kubernetes/kubernetes#105292</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>) [SIG Apps and Node]</li>; <li>Kubelet: turn the KubeletConfiguration v1beta1 <code>ResolverConfig</code> field from a <code>string</code> to <code>*string</code>. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104624"">kubernetes/kubernetes#104624</a>, <a href=""https://github.com/Haleygo""><code>@​Haleygo</code></a>)</li>; <li>Kubernetes is now built using go 1.17. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/103692"">kubernetes/kubernetes#103692</a>, <a href=""https://github.com/justaugustus""><code>@​justaugustus</code></a>)</li>; <li>Performs strict server side schema validation requests via the <code>fieldValidation=[Strict,Warn,Ignore]</code>. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105916"">kubernetes/kubernetes#105916</a>, <a href=""https://github.com/kevindelgado""><code>@​kevindelgado</code></a>)</li>; <li>Promote <code>IPv6DualStack</code> feature to stable.; Controller Manager flags for the node IPAM controller have slightly changed:; <ol>; <li>When configuring a dual-stack cluster, the user must specify both <code>--node-cidr-mask-size-ipv4</code> and <code>--node-cidr-mask-size-ipv6</code> to set the per-node IP mask sizes, instead of the previous <code>--node-cidr-mask-size</code> flag.</li>; <li>The <code>--node-cidr-mask-size</code> flag is mutually exclusive with <code>--node-cidr-mask-size-ipv4</code> and <code>--node-cidr-mask-size-ipv6</code>.</li>; <li>Single-stack clusters do not need to change, but may choose to use the more specific flags. Users can use either the older <code>--node-cidr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:10068,Perform,Performs,10068,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['Perform'],['Performs']
Performance,fNZv/jHpWQ6lemx/out; 2024-11-05 02:43:37.202 JVMEntryway: INFO: Yielding control to the QoB Job.; 2024-11-05 02:43:37.206 ServiceBackendAPI$: INFO: BatchClient allocated.; 2024-11-05 02:43:37.207 ServiceBackendAPI$: INFO: BatchConfig parsed.; 2024-11-05 02:43:37.209 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2024-11-05 02:43:37.783 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]; 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]; 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; Caused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String length (20013488) exceeds the maximum length (20000000); 	at com.fasterxml.jackson.core.StreamReadConstraints.validateStringLength(StreamReadConstraints.java:324) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.ReadConstrainedTextBuffer.validateStringLength(ReadConstrainedTextBuffer.java:27) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.TextBuffer.finishCurrentSegment(TextBuffer.java:939) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:2766,concurren,concurrent,2766,https://hail.is,https://github.com/hail-is/hail/issues/14749,1,['concurren'],['concurrent']
Performance,factoring out gencode gtf load,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8264:26,load,load,26,https://hail.is,https://github.com/hail-is/hail/pull/8264,1,['load'],['load']
Performance,failing loading table with 3K columns,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4153:8,load,loading,8,https://hail.is,https://github.com/hail-is/hail/issues/4153,1,['load'],['loading']
Performance,"faultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:3310,concurren,concurrent,3310,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,ffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:3340,concurren,concurrent,3340,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['concurren'],['concurrent']
Performance,ffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:10166,concurren,concurrent,10166,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['concurren'],['concurrent']
Performance,"fficient for the cache to work. (I was wrong, see below!). AFAICT, this change doesn't prevent PR tests from pushing to the `cache` tag. This change just makes the tests run by the CI-in-the-PR not overwrite the cache. Every image build for a PR (which is tested by default namespace CI) will still overwrite the cache tag. AFAICT, this; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/foo; ```; Will use as a cache source the `latest` tag in the `gcr.io/hail-vdc/foo` repository. It is *not* sufficient for an image to be present in the repository and untagged or with a different tag from `latest`. In particular, every push to the `cache` tag prevents us from using other images even though they are in the registry! For example, I pushed two images to `cache`:. ```; (base) # gcloud container images list-tags gcr.io/hail-vdc/dktest; DIGEST TAGS TIMESTAMP; fb551d9bdb94 2022-06-10T14:16:39; afb4c5ad2d7b cache,latest 2022-06-10T14:15:55; ```. If I rebuild [1] the most recently pushed image with; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/dktest:cache; ```; it succeeds in getting the cache. If I rebuild the other image with the same import-cache, it does not see that the (untagged) image is already there! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800:1229,cache,cache,1229,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800,2,['cache'],['cache']
Performance,figurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1516); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373); at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); at is.hail.io.fs.HadoopFS.is$hail$io$fs$HadoopFS$$_fileSystem(HadoopFS.scala:157); at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:244); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:226); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:225); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:31,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:3954,Cache,Cache,3954,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['Cache'],['Cache']
Performance,fix --cache-from in docker build,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5670:6,cache,cache-from,6,https://hail.is,https://github.com/hail-is/hail/pull/5670,1,['cache'],['cache-from']
Performance,fix docker image cacheing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4356:17,cache,cacheing,17,https://hail.is,https://github.com/hail-is/hail/pull/4356,1,['cache'],['cacheing']
Performance,fix loadconda,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5239:4,load,loadconda,4,https://hail.is,https://github.com/hail-is/hail/pull/5239,1,['load'],['loadconda']
Performance,fix localref load and store operations to use VarInsnNode instead of …,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2566:13,load,load,13,https://hail.is,https://github.com/hail-is/hail/pull/2566,1,['load'],['load']
Performance,fix optimizer to clean up mt.field.show(),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3966:4,optimiz,optimizer,4,https://hail.is,https://github.com/hail-is/hail/pull/3966,1,['optimiz'],['optimizer']
Performance,fixed problem in LoadVCF where if sample IDs in header1 is subset of …,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2247:17,Load,LoadVCF,17,https://hail.is,https://github.com/hail-is/hail/pull/2247,1,['Load'],['LoadVCF']
Performance,fixed reuse code bug in loadElement in SJavaArray,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10902:24,load,loadElement,24,https://hail.is,https://github.com/hail-is/hail/pull/10902,1,['load'],['loadElement']
Performance,"fixes #13407. CHANGELOG: Resolves #13407 in which uses of `union_rows` could reduce parallelism to one partition resulting in severely degraded performance. TableUnion was always collapsing to a single partition when the key was empty. This adds a special case handling, which just concatenates partitions. The body of the resulting TableStage is a little hacky: it does a StreamMultiMerge, but where exactly one input stream is non-empty. I think that should have fine performance, and I didn’t see any simpler ways to do it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13414:144,perform,performance,144,https://hail.is,https://github.com/hail-is/hail/pull/13414,2,['perform'],['performance']
Performance,"fixes #7418. The main change here is fixing #7418 by making nodes which perform aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7479:72,perform,perform,72,https://hail.is,https://github.com/hail-is/hail/pull/7479,2,['perform'],"['perform', 'performs']"
Performance,flect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:6938,Load,LoadVCF,6938,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Load'],['LoadVCF']
Performance,flection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: is.hail.asm4s.AsmFunction2; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:14046,load,loadClass,14046,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['loadClass']
Performance,flectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at org.apache.hadoop.fs.Path.initialize(Path.java:205); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleH,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:5286,concurren,concurrent,5286,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,flectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:12183,concurren,concurrent,12183,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['concurren'],['concurrent']
Performance,fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5682,Load,LoadMatrix,5682,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"for better IR generation. This change the (`key_struct`, `value_struct`) arguments to a more general (`row`, `new_keys`) which allows individual methods more flexibility in how the row is constructed. @tpoterba this fixes the specific case in #4001, but I'm not sure if you wanted to keep that open as a general issue or just open issues with other cases as we test them? With this change I'm getting:. ```; 2018-07-30 18:11:23 root: INFO: optimize: before:; (TableMapRows (idx) 1; (TableRange 5 2); (InsertFields; (Ref Struct{idx:Int32} row); (x; (I32 5)))); 2018-07-30 18:11:23 root: INFO: optimize: after:; (TableMapRows (idx) 1; (TableRange 5 2); (InsertFields; (Ref Struct{idx:Int32} row); (x; (I32 5)))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4042:440,optimiz,optimize,440,https://hail.is,https://github.com/hail-is/hail/pull/4042,2,['optimiz'],['optimize']
Performance,"for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the origina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826:1266,load,load,1266,https://hail.is,https://github.com/hail-is/hail/issues/7826,2,['load'],"['load', 'loading']"
Performance,"force count isn't the same thing as count without optimization -- force count will optimize, but with a top-level TableLiteral node.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5015#issuecomment-448764451:50,optimiz,optimization,50,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448764451,2,['optimiz'],"['optimization', 'optimize']"
Performance,found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRPar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2194,Load,LoadPlink,2194,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,fqr-9h24; <p>Changed</p>; <pre><code>; - Explicit check the key for ECAlgorithm by @estin in https://github.com/jpadilla/pyjwt/pull/713; - Raise DeprecationWarning for jwt.decode(verify=...) by @akx in https://github.com/jpadilla/pyjwt/pull/742. Fixed; ~~~~~. - Don't use implicit optionals by @rekyungmin in https://github.com/jpadilla/pyjwt/pull/705; - documentation fix: show correct scope for decode_complete() by @sseering in https://github.com/jpadilla/pyjwt/pull/661; - fix: Update copyright information by @kkirsche in https://github.com/jpadilla/pyjwt/pull/729; - Don't mutate options dictionary in .decode_complete() by @akx in https://github.com/jpadilla/pyjwt/pull/743. Added; ~~~~~. - Add support for Python 3.10 by @hugovk in https://github.com/jpadilla/pyjwt/pull/699; - api_jwk: Add PyJWKSet.__getitem__ by @woodruffw in https://github.com/jpadilla/pyjwt/pull/725; - Update usage.rst by @guneybilen in https://github.com/jpadilla/pyjwt/pull/727; - Docs: mention performance reasons for reusing RSAPrivateKey when encoding by @dmahr1 in https://github.com/jpadilla/pyjwt/pull/734; - Fixed typo in usage.rst by @israelabraham in https://github.com/jpadilla/pyjwt/pull/738; - Add detached payload support for JWS encoding and decoding by @fviard in https://github.com/jpadilla/pyjwt/pull/723; - Replace various string interpolations with f-strings by @akx in https://github.com/jpadilla/pyjwt/pull/744; - Update CHANGELOG.rst by @hipertracker in https://github.com/jpadilla/pyjwt/pull/751. `v2.3.0 &amp;lt;https://github.com/jpadilla/pyjwt/compare/2.2.0...2.3.0&amp;gt;`__; -----------------------------------------------------------------------. Fixed; ~~~~~. - Revert &amp;quot;Remove arbitrary kwargs.&amp;quot; `[#701](https://github.com/jpadilla/pyjwt/issues/701) &amp;lt;https://github.com/jpadilla/pyjwt/pull/701&amp;gt;`__. Added; ~~~~~. - Add exception chaining `[#702](https://github.com/jpadilla/pyjwt/issues/702) &amp;lt;https://github.com/jpadilla/pyjwt/pull/702&amp;gt;`__.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11866:10344,perform,performance,10344,https://hail.is,https://github.com/hail-is/hail/pull/11866,1,['perform'],['performance']
Performance,"from @zaczap; ```; 2018-02-21 15:30:31 Hail: INFO: interval filter loaded 89 of 1274 partitions; Traceback (most recent call last):; File ""/tmp/85fbcd66-b973-4c5c-9216-ef00b9a7f3a7/temp_vep.py"", line 34, in <module>; vds = hl.filter_intervals(vds, intervals); File ""<decorator-gen-718>"", line 2, in filter_intervals; File ""/tmp/85fbcd66-b973-4c5c-9216-ef00b9a7f3a7/hail-devel-5b95158ed055.zip/hail/utils/java.py"", line 198, in handle_py4j; hail.utils.java.FatalError: requirement failed. Java stack trace:; scala.Predef$.require(Predef.scala:212); at is.hail.rvd.OrderedRVDPartitioner.<init>(OrderedRVDPartitioner.scala:28); at is.hail.rvd.OrderedRVDPartitioner.copy(OrderedRVDPartitioner.scala:98); at is.hail.rvd.OrderedRVD.filterIntervals(OrderedRVD.scala:270); at is.hail.methods.FilterIntervals$.apply(FilterIntervals.scala:23); at is.hail.methods.FilterIntervals$.apply(FilterIntervals.scala:18); at is.hail.methods.FilterIntervals.apply(FilterIntervals.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); Hail version: devel-5b95158; Error summary: requirement failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [85fbcd66-b973-4c5c-9216-ef00b9a7f3a7] entered state [ERROR] while waiting for [DONE].; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2949:67,load,loaded,67,https://hail.is,https://github.com/hail-is/hail/issues/2949,1,['load'],['loaded']
Performance,"front_end loads them from batch2.front_end:. > setup_aiohttp_jinja2(app, 'batch.front_end'). added tests to check the ui pages load",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7270:10,load,loads,10,https://hail.is,https://github.com/hail-is/hail/pull/7270,2,['load'],"['load', 'loads']"
Performance,fs.FS.open(FS.scala:572); E 	at is.hail.io.fs.FS.open$(FS.scala:571); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.fs.FS.open(FS.scala:569); E 	at is.hail.io.fs.FS.open$(FS.scala:569); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.index.IndexReader$.readTypes(IndexReader.scala:65); E 	at is.hail.io.bgen.LoadBgen$.$anonfun$getBgenFileMetadata$2(LoadBgen.scala:208); E 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); E 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); E 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); E 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198); E 	at is.hail.io.bgen.LoadBgen$.getBgenFileMetadata(LoadBgen.scala:207); E 	at is.hail.io.bgen.MatrixBGENReader$.apply(LoadBgen.scala:387); E 	at is.hail.io.bgen.MatrixBGENReader$.fromJValue(LoadBgen.scala:365); E 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:116); E 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1815); E 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1738); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:2164); E 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:2136); E 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:2164); E 	at is.hail.backend.Backend.$anonfun$matrixTableType$2(Backend.scala:186); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); E 	at is.hail.utils.package$.using(package.scala:664); E 	at is.hail.backend.Execu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001:3216,Load,LoadBgen,3216,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001,1,['Load'],['LoadBgen']
Performance,fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.58-3f304aae6ce2; Error summary: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:6319,concurren,concurrentGlobInternal,6319,https://hail.is,https://github.com/hail-is/hail/issues/9607,6,['concurren'],"['concurrent', 'concurrentGlobInternal']"
Performance,fter disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215295,concurren,concurrent,215295,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,fun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5618,Load,LoadMatrixParser,5618,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrixParser']
Performance,"g JUnit 4 Tests using TestNG (Krishnan Mahadevan); Fixed: GITHUB-2847: Deprecate support for running JUnit tests (Krishnan Mahadevan); Fixed: GITHUB-2844: Deprecate support for running Spock Tests (Krishnan Mahadevan); Fixed: GITHUB-550: Weird <a href=""https://github.com/BeforeMethod""><code>@​BeforeMethod</code></a> and <a href=""https://github.com/AfterMethod""><code>@​AfterMethod</code></a> behaviour with dependsOnMethods (Krishnan Mahadevan); Fixed: GITHUB-893: TestNG should provide an Api which allow to find all dependent of a specific test (Krishnan Mahadevan); New: Added .yml file extension for yaml suite files, previously only .yaml was allowed for yaml (Steven Jubb); Fixed: GITHUB-141: regular expression in &quot;dependsOnMethods&quot; does not work (Krishnan Mahadevan); Fixed: GITHUB-2770: FileAlreadyExistsException when report is generated (melloware); Fixed: GITHUB-2825: Programmatically Loading TestNG Suite from JAR File Fails to Delete Temporary Copy of Suite File (Steven Jubb); Fixed: GITHUB-2818: Add configuration key for callback discrepancy behavior (Krishnan Mahadevan); Fixed: GITHUB-2819: Ability to retry a data provider in case of failures (Krishnan Mahadevan); Fixed: GITHUB-2308: StringIndexOutOfBoundsException in findClassesInPackage - Surefire/Maven - JDK 11 fails (Krishnan Mahadevan); Fixed: GITHUB:2788: TestResult.isSuccess() is TRUE when test fails due to expectedExceptions (Krishnan Mahadevan); Fixed: GITHUB-2800: Running Test Classes with Inherited <a href=""https://github.com/Factory""><code>@​Factory</code></a> and <a href=""https://github.com/DataProvider""><code>@​DataProvider</code></a> Annotated Non-Static Methods Fail (Krishnan Mahadevan); New: Ability to provide custom error message for assertThrows\expectThrows methods (Anatolii Yuzhakov); Fixed: GITHUB-2780: Use SpotBugs instead of abandoned FindBugs; Fixed: GITHUB-2801: JUnitReportReporter is too slow; Fixed: GITHUB-2807: buildStackTrace should be fail-safe (Sergey Chernov); Fixed: G",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:10988,Load,Loading,10988,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['Load'],['Loading']
Performance,"g builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/backend/backend.py"", line 108, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/BROAD.MIT.EDU/cvittal/.local/opt/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/utils/java.py"", line 221, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus). Java ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:1556,cache,cache,1556,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['cache'],['cache']
Performance,"g cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6775,cache,cached,6775,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,g python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39885,cache,cached,39885,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,g s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB); Collecting xyzservices==2023.7.0; Using cached xyzservices-2023.7.0-py3-none-any.whl (56 kB); Collecting yarl==1.9.2; Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a06270766887328bdb51066eccff2cd696a6; Stored in directory: /home/hadoop/.cache/pip/wheels/e3/a2/1e/5c1be0865f4170a89de34e0a798f32f674a7eaf63a93272c7f; Successfully built avro; Instal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:40655,cache,cached,40655,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"g. And this is also influenced by the medium-term goal of having a Hail service; which (amongst other things) can do simple analyses on small data in under ten seconds - in; that realm compilation time could become a critical factor as a serial bottleneck. [The place where persistent cacheing of compiled files helps most of all is in testing,; where you really are running the exact same queries over and over again on the same; small datasets, and in many cases after making small changes which only affect a few; of the queries]. b) We may actually have some version of the locking problem even if we don't try to reuse the files -; since we have several workers on a node, and possibly a master as well, all needing to put code; into a file (or wait for someone else to populate the file) so that they can load it. Depending on ; precisely how Spark manages things (which I wouldn't want to depend on too much anyway). In fact it's essential that all the workers share the same DLL file, because otherwise they'd be; trying to load multiple DLL's defining the same symbols. That aspect of it could be handled by; putting the files into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could manage it with nothing but atomic file-create; and atomic-rename, but that didn't quite pan out). As for writing LLVM IR, it can definitely be done, because that's what Endeca/Oracle did. But there; was such a huge learning curve that only 3 people ever did it successfully (I wasn't one of them),; and debugging seemed very unpleasant and slow. [It was also a masterful achievement in ; job-security-through-obscurity, because no-one in management was going to mess with the; two people who wrote it - until the whole project got canned]. ... and in the time I was there, the Endeca/Oracle stuff wasn't distributed, which could be an",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385:1665,load,load,1665,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385,2,['load'],['load']
Performance,g.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.340ms self 0.340ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.490ms self 0.490ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:155297,Optimiz,OptimizePass,155297,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,g.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.463ms self 0.463ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.058ms self 0.058ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.240ms self 0.240ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:139510,Optimiz,OptimizePass,139510,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,g.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.505ms self 0.505ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.938ms self 0.938ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:123792,Optimiz,OptimizePass,123792,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"g.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.jav",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1084,load,loadClass,1084,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460,1,['load'],['loadClass']
Performance,"g.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6097,Load,LoadVCF,6097,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"g=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9] tools_3.5.1 digest_0.6.18 jsonlite_1.5 tibble_1.4.2 ; [13] nlme_3.1-137 lattice_0.20-35 pkgconfig_2.0.2 rlang_0.2.2 ; [17] shiny_1.1.0 DBI_1.0.0 rstudioapi_0.8 bindrcpp_0.2.2 ; [21] withr_2.1.2 dplyr_0.7.7 httr_1.3.1 sparklyr_0.9.2 ; [25] rappdirs_0.3.1 htmlwidgets_1.3 rprojroot_1.3-2 grid_3.5.1 ; [29] tidyselect_0.2.5 glue_1.3.0 forge_0.1.0 R6_2.3.0 ; [33] purrr_0.2.5 tidyr_0.8.1 magrittr_1.5 backports_1.1.2 ; [37] promises_1.0.1 htmltools_0.3.6 assertthat_0.2.0 mime_0.6 ; [41] xtable_1.8-3 httpuv_1.4.5 openssl_1.0.2 lazyeval_0.2.1 ; [45] broom_0.5.0 crayon_1.3.4 ; [1] 32; ```. How are you telling sparklyr where the Spark jars are? I used `install_spark`. It looks like the latest sparklyr is 0.9.2. What does `sessionInfo()` print for you?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:1838,load,loaded,1838,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977,1,['load'],['loaded']
Performance,"gene_variant|MODIFIER|WASH7P|ENSG00000227232|Transcript|ENST00000488147|unprocessed_pseudogene|||||||||||2206|-1||HGNC|HGNC:38034|||||||||||||||||||||||||||;DP=3;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1	GT:AD:DP:GQ:PL	./.:.:.:.:.	0/1:1,2:3:37:77,0,37; ```. Getting this error message:; ```; INFO: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) running SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.5; SparkUI available at http://seqr-loading-cluster-m.c.seqr-project.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-914bd8a10ca2; LOGGING: writing to /tmp/c7e0443c47b54e91b295e2bff7b554b9/hail-20200405-1408-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'decrease_running_resources': <bound method TaskStatusReporter.decrease_running_resources of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'param_kwargs': {'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'reference_ht_path': 'gs://seqr-reference-data/GRCh38/all_reference_data/combined_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:36767,load,loading-cluster-m,36767,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['load'],['loading-cluster-m']
Performance,"generated/C0.apply instruction count: 12; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method1 instruction count: 112; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method2 instruction count: 82; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method3 instruction count: 252; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.<init> instruction count: 3; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.apply instruction count: 28; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.apply instruction count: 12; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.method1 instruction count: 100; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.method2 instruction count: 106; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.method3 instruction count: 252; 2019-01-22 13:11:45 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:46 root: INFO: Index reader cache queries: 4; 2019-01-22 13:11:46 root: INFO: Index reader cache hit rate: 0.25; 2019-01-22 13:11:46 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.185:44946) with ID 9; 2019-01-22 13:11:46 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q01.scc.bu.edu:38527 with 21.2 GB RAM, BlockManagerId(9, scc-q01.scc.bu.edu, 38527, None); 2019-01-22 13:11:46 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.193:42726) with ID 3; 2019-01-22 13:11:46 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q09.scc.bu.edu:41872 with 21.2 GB RAM, BlockManagerId(3, scc-q09.scc.bu.edu, 41872, None); 2019-01-22 13:11:47 Hail: INFO: Number of BGEN files parsed: 1; 2019-01-22 13:11:47 Hail: INFO: Number of samples in BGEN files: 487409; 2019-01-22 13:11:47 Hail: INFO: Number of variants across all BGEN files: 1261158; 2019-01-22 13:11:48 MemorySto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:24569,cache,cache,24569,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,4,['cache'],['cache']
Performance,gex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB); Collecting xyzservices==2023.7.0; Using cached xyzservices-2023.7.0-py3-none-any.whl (56 kB); Collecting yarl==1.9.2; Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:40232,cache,cached,40232,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10007"">#10007</a>: Drop <code>setuptools</code></li>; </ul>; <h2>Features added</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9075"">#9075</a>: autodoc: Add a config variable :confval:<code>autodoc_typehints_format</code>; to suppress the leading module names of typehints of function signatures (ex.; <code>io.StringIO</code> -&gt; <code>StringIO</code>)</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9831"">#9831</a>: Autosummary now documents only the members specified in a module's; <code>__all__</code> attribute if :confval:<code>autosummary_ignore_module_all</code> is set to; <code>False</code>. The default behaviour is unchanged. Autogen also now supports; this behavior with the <code>--respect-module-all</code> switch.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9555"">#9555</a>: autosummary: Improve error messages on failure to load target object</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9800"">#9800</a>: extlinks: Emit warning if a hardcoded link is replaceable; by an extlink, suggesting a replacement.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9961"">#9961</a>: html: Support nested <!-- raw HTML omitted --> HTML elements in other HTML builders</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10013"">#10013</a>: html: Allow to change the loading method of JS via <code>loading_method</code>; parameter for :meth:<code>Sphinx.add_js_file()</code></li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9551"">#9551</a>: html search: &quot;Hide Search Matches&quot; link removes &quot;highlight&quot; parameter; from URL</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9815"">#9815</a>: html theme: Wrap sidebar components in div t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11522:1922,load,load,1922,https://hail.is,https://github.com/hail-is/hail/pull/11522,2,['load'],['load']
Performance,gle-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36358,cache,cached,36358,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"gle_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4680,cache,cached,4680,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"gory.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2251,concurren,concurrent,2251,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,2,['concurren'],['concurrent']
Performance,"h MakeArray?. I have some thoughts on that, but I think it would require a bigger change to how we represent streams in the IR. One idea is to put stream producers and stream consumers on equal footing. Right now, a stream consumer is a node with a stream child that produces a non-stream value, e.g. `ArrayFor(stream, name, body)`. We could let stream consumers be values themselves, `ArrayFor(name, body)`, with other nodes for connecting producers with consumers, e.g. `RunStream(stream, ArrayFor(name, body))`. Then I think we would need two `MakeStream`s, one that can be unrolled, and one that can be zipped. The former just directly takes a consumer and produces a value (and can choose to duplicate the consumer code or wrap it in a method). The later constructs a stream producer, which can be freely used in zips, etc. It would definitely require some thought how to optimize such a representation (is there a normal form?). > I'm not sure I see that. If you don't duplicate the consumer, the push code is the same. The question is, do you use a variable + switch to track where you are in the MakeStream, or do you use the program counter via labels, where you get the latter from the former by code duplication. I think the easiest way to see it is to think about how you would zip two `MakeStream`s, both of which represent their state using the program counter. You could do it by statically fusing them into a single `MakeStream`, but what if it was something more complicated like `Zip(Map(MakeStream(...), ...), Filter(MakeStream(...), ...))`? You can't represent a compound state with a single program counter. At a higher level, you can only do the optimization of unrolling the `MakeStream` loop if that is the outermost loop in the generated code. A producer wanting to create the outermost loop is what I mean by a push stream. By contrast, to be able to zip arbitrary streams, you want the consumer to create the outermost loop, and on each iteration ""pull"" from each producer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-591962972:1745,optimiz,optimization,1745,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-591962972,1,['optimiz'],['optimization']
Performance,"h is essentially a simplified, staged version of `import_vcf`. I benchmarked the change with:; ```; In [2]: %%time ; ...: import hail as hl ; ...: m = hl.import_matrix_table('/tmp/foo.tsv.gz', ; ...: row_fields={'f0': hl.tstr}, ; ...: no_header=True, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6987:1443,perform,performance,1443,https://hail.is,https://github.com/hail-is/hail/pull/6987,1,['perform'],['performance']
Performance,"h non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:19); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:19); 	at is.hail.utils.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:5096,Cache,CacheDir,5096,https://hail.is,https://github.com/hail-is/hail/issues/14513,4,['Cache'],['CacheDir']
Performance,"h non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: L",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:2478,Cache,CacheDir,2478,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Cache'],['CacheDir']
Performance,"h the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter for loops, but I've since changed my mind. I think loops will have hard region management no matter what. > What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. As a side note, @iitalics stream emitter can handle streams of multiple values fine. Effectively, you can make a `Stream[(Code[A], Code[B], Code[C])]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:3443,perform,performance,3443,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407,2,['perform'],['performance']
Performance,h=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5659:1917,cache,cache-tests,1917,https://hail.is,https://github.com/hail-is/hail/issues/5659,1,['cache'],['cache-tests']
Performance,hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.Reflection,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5391,optimiz,optimize,5391,https://hail.is,https://github.com/hail-is/hail/issues/6458,2,"['Optimiz', 'optimiz']","['Optimize', 'optimize']"
Performance,hail.backend.BackendUtils.$anonfun$collectDArray$16(BackendUtils.scala:91); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$12(Worker.scala:167); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$11(Worker.scala:166); 	at is.hail.backend.service.Worker$.$anonfun$main$11$adapted(Worker.scala:164); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.service.Worker$.main(Worker.scala:164); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	... 11 more. Logs; Main; Log ; 2023-09-24 17:23:30.055 JVMEntryway: ERROR: Exception encountered in QoB cancel thread.; org.newsclub.net.unix.SocketClosedException: Not open; 	at org.newsclub.net.unix.AFCore.validFdOrException(AFCore.java:90) ~[jvm-entryway.jar:?]; 	at org.newsclub.net.unix.AFSocketImpl$AFInputStreamImpl.read(AFSocketImpl.java:510) ~[jvm-entryway.jar:?]; 	at java.io.DataInputStream.readInt(DataInputStream.java:388) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$2.run(JVMEntryway.java:136) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888:4926,concurren,concurrent,4926,https://hail.is,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888,6,['concurren'],['concurrent']
Performance,"hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$4(ServiceBackend.scala:664); at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$3(ServiceBackend.scala:650); at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:822); at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:447); at is.hail.backend.service.Main$.main(Main.scala:15); at is.hail.backend.service.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.wri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:6488,concurren,concurrent,6488,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['concurren'],['concurrent']
Performance,"hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardRelationalLets, iteration: 0 total 0.030ms self 0.030ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/PruneDeadFields, iteration: 0 total 0.572ms self 0.572ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Verify total 0.022ms self 0.022ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.TypeCheck.apply total 0.144ms self 0.144ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LiftRelationalValuesToRelationalLets total 1.135ms self 0.009ms children 1.126ms %children 99.21%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LiftRelationalValuesToRelationalLets/Verify total 0.027ms self 0.027ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LiftRelationalValuesToRelationalLets/Transform total 1.069ms",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:14337,Optimiz,Optimize,14337,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,"hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0 total 0.659ms self 0.292ms children 0.367ms %children 55.66%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.367ms self 0.367ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.127ms self 0.127ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardRelationalLets, iteration: 0 total 0.030ms self 0.030ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/PruneDeadFields, iteration: 0 total 0.572ms self 0.572ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Verify total 0.022ms self 0.022ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:13450,Optimiz,Optimize,13450,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 47.463ms self 0.201ms children 47.262ms %children 99.58%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 6.134ms self 6.134ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.135ms self 0.135ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:38601,Optimiz,OptimizePass,38601,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_matrix_ir$2(SparkBackend.scala:689); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:69); 	at is.hail.utils.package$.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6485,Load,LoadPlink,6485,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 0; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 		at is.hail.io.fs.FSPositionedOutputStream.write(FS.scala:219); 		at java.io.DataOutputStream.write(DataOutputStream.java:107); 		a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:7880,concurren,concurrent,7880,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['concurren'],['concurrent']
Performance,hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:220); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:220); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:2718,concurren,concurrent,2718,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['concurren'],['concurrent']
Performance,"hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:220); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:220); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-e6de08e; Error summary: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [3c5f402fed564ccd85257c0919d4bffb] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""pyhail.py"", line 128, in <module>; main(args, pass_through_args); File ""pyhail.py"", line 109, in main; subprocess.check_output(job); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 573, in check_output; raise CalledProcessError(retcode, cmd, output=output); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', '/Users/gtiao/gnomad_qc/hail/sample_qc/assign_subpops.py', '--cluster', 'gt1', '--files=gs:/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:7237,concurren,concurrent,7237,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['concurren'],['concurrent']
Performance,hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.lang.NullPointerException: null; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionPutTask.call(JsonResumableSessionPutTask.java:201); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:81); 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:99); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.put(JsonResumableSession.java:68); 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedWritableByteChannel.internal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:5676,concurren,concurrent,5676,https://hail.is,https://github.com/hail-is/hail/issues/13937,1,['concurren'],['concurrent']
Performance,"hail/hail/python/hail/table.py"", line 1292, in _ascii_str; rows, has_more, dtype = self.data(); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1276, in data; rows, has_more = t._take_n(self.n); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1423, in _take_n; rows = self.take(n + 1); File ""<decorator-gen-1095>"", line 2, in take; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 2087, in take; return self.head(n).collect(_localize); File ""<decorator-gen-1089>"", line 2, in collect; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1886, in collect; return Env.backend().execute(e._ir); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/Users/konradk/programs/spark-2.4.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchElementException: key not found: 1; [...]; java.util.NoSuchElementException: key not found: 1; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.MapLike$class.apply(MapLike.scala:141); at scala.collection.AbstractMap.apply(Map.scala:59); at is.hail.types.encoded.EBaseStruct.fieldType(EBaseStruct.scala:34); at is.hail.types.encoded.EBaseStruct$$anonfun$8.apply(EBaseStruct.scala:84); at is.hail.types.encoded.EBaseStruct$$anonfun$8.apply(EBaseStruct.scala:83); at scala.collection.Tr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9016:4205,load,loads,4205,https://hail.is,https://github.com/hail-is/hail/issues/9016,1,['load'],['loads']
Performance,"handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0 total 2.553ms self 0.011ms children 2.543ms %children 99.59%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 2.543ms self 2.543ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 0 total 8.675ms self 8.675ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0 total 4.506ms self 3.993ms children 0.513ms %children 11.40%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.513ms self 0.513ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.120ms self 0.120ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 0 total 0.796ms self 0.796ms children 0.000ms %children 0.00%; timing is.hail.backend.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:3800,Optimiz,Optimize,3800,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,"handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1 total 0.384ms self 0.005ms children 0.379ms %children 98.72%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 1 total 0.103ms self 0.103ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1 total 0.579ms self 0.273ms children 0.306ms %children 52.84%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.306ms self 0.306ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 1 total 0.025ms self 0.025ms children 0.000ms %children 0.00%; timing is.hail.backend.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:6943,Optimiz,Optimize,6943,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,"hanged. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/102534"">kubernetes/kubernetes#102534</a>, <a href=""https://github.com/wangyysde""><code>@​wangyysde</code></a>) [SIG API Machinery, Apps, Auth, Autoscaling and Testing]</li>; <li>Ephemeral containers graduated to beta and are now available by default. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105405"">kubernetes/kubernetes#105405</a>, <a href=""https://github.com/verb""><code>@​verb</code></a>)</li>; <li>Fix kube-proxy regression on UDP services because the logic to detect stale connections was not considering if the endpoint was ready. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106163"">kubernetes/kubernetes#106163</a>, <a href=""https://github.com/aojea""><code>@​aojea</code></a>) [SIG API Machinery, Apps, Architecture, Auth, Autoscaling, CLI, Cloud Provider, Contributor Experience, Instrumentation, Network, Node, Release, Scalability, Scheduling, Storage, Testing and Windows]</li>; <li>If a conflict occurs when creating an object with <code>generateName</code>, the server now returns an &quot;AlreadyExists&quot; error with a retry option. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104699"">kubernetes/kubernetes#104699</a>, <a href=""https://github.com/vincepri""><code>@​vincepri</code></a>)</li>; <li>Implement support for recovering from volume expansion failures (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106154"">kubernetes/kubernetes#106154</a>, <a href=""https://github.com/gnufied""><code>@​gnufied</code></a>) [SIG API Machinery, Apps and Storage]</li>; <li>In kubelet, log verbosity and flush frequency can also be configured via the configuration file and not just via command line flags. In other commands (kube-apiserver, kube-controller-manager), the flags are listed in the &quot;Logs flags&quot; group and not under &quot;Global&quot; or &quot;Misc&quot;. The",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:4143,Scalab,Scalability,4143,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['Scalab'],['Scalability']
Performance,"hangelog</strong>: <a href=""https://github.com/kjd/idna/compare/v3.6...v3.7"">https://github.com/kjd/idna/compare/v3.6...v3.7</a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/kjd/idna/blob/master/HISTORY.rst"">idna's changelog</a>.</em></p>; <blockquote>; <p>3.7 (2024-04-11); ++++++++++++++++</p>; <ul>; <li>Fix issue where specially crafted inputs to encode() could; take exceptionally long amount of time to process. [CVE-2024-3651]</li>; </ul>; <p>Thanks to Guido Vranken for reporting the issue.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/kjd/idna/commit/1d365e17e10d72d0b7876316fc7b9ca0eebdd38d""><code>1d365e1</code></a> Release v3.7</li>; <li><a href=""https://github.com/kjd/idna/commit/c1b3154939907fab67c5754346afaebe165ce8e6""><code>c1b3154</code></a> Merge pull request <a href=""https://redirect.github.com/kjd/idna/issues/172"">#172</a> from kjd/optimize-contextj</li>; <li><a href=""https://github.com/kjd/idna/commit/0394ec76ff022813e770ba1fd89658790ea35623""><code>0394ec7</code></a> Merge branch 'master' into optimize-contextj</li>; <li><a href=""https://github.com/kjd/idna/commit/cd58a23173d2b0a40b95ee680baf3e59e8d33966""><code>cd58a23</code></a> Merge pull request <a href=""https://redirect.github.com/kjd/idna/issues/152"">#152</a> from elliotwutingfeng/dev</li>; <li><a href=""https://github.com/kjd/idna/commit/5beb28b9dd77912c0dd656d8b0fdba3eb80222e7""><code>5beb28b</code></a> More efficient resolution of joiner contexts</li>; <li><a href=""https://github.com/kjd/idna/commit/1b121483ed04d9576a1291758f537e1318cddc8b""><code>1b12148</code></a> Update ossf/scorecard-action to v2.3.1</li>; <li><a href=""https://github.com/kjd/idna/commit/d516b874c3388047934938a500c7488d52c4e067""><code>d516b87</code></a> Update Github actions/checkout to v4</li>; <li><a href=""https://github.com/kjd/idna/commit/c095c75943413c75ebf8ac74179757031b7f80b7""><code>c0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14464:1476,optimiz,optimize-contextj,1476,https://hail.is,https://github.com/hail-is/hail/pull/14464,7,['optimiz'],['optimize-contextj']
Performance,hannelHandlerContext.fireChannelRead(AbstractCt io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoot io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.javawork.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.nettyRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:37860,concurren,concurrent,37860,https://hail.is,https://github.com/hail-is/hail/issues/8106,2,['concurren'],['concurrent']
Performance,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3731,load,load,3731,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,2,['load'],['load']
Performance,"hat the dataframe created by hail maintains reference to hail objects and pandas is attempting to recreate these objects when unpickling. I suspect this is not intentional. ```python; # Hail environment; vat_simplified_file = os.path.join(bucket, 'vat.ht'); gwas = hl.read_table(gwas_results_file_no_sex_chr); vat = hl.read_table(vat_simplified_file); gwas = gwas.filter(gwas.p_value <= 1e-4); combined = gwas.join(vat, how='left'); combined_pandas = combined.to_pandas(). gwas_pandas_file = os.path.join(bucket, 'gwas_results.pkl'); combined_pandas.to_pickle(gwas_pandas_file); ```. ```python; # Non hail environment without pyspark; combined_pandas = pd.read_pickle(gwas_pandas_file). ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 216 # expected ""IO[bytes]""; --> 217 return pickle.load(handles.handle) # type: ignore[arg-type]; 218 except excs_to_catch:. /opt/conda/lib/python3.7/site-packages/hail/__init__.py in <module>; 32 # E402 module level import not at top of file; ---> 33 from .table import Table, GroupedTable, asc, desc # noqa: E402; 34 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402. /opt/conda/lib/python3.7/site-packages/hail/table.py in <module>; 4 import numpy as np; ----> 5 import pyspark; 6 from typing import Optional, Dict, Callable, Sequence. ModuleNotFoundError: No module named 'pyspark'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_233/4275665471.py in <module>; ----> 1 combined_pandas = pd.read_pickle(gwas_pandas_file). /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 220 # ""No module named 'pandas.core.sparse.series'""; 221 # ""Can't get attribute '__nat_unpickle'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004:1652,load,load,1652,https://hail.is,https://github.com/hail-is/hail/issues/14004,1,['load'],['load']
Performance,"he mutation is sufficiently well localized, and this is a common compiler pattern. This touches a lot of lines, but the high level changes are:; * Remove the `refMap` from the `IRParserEnvironment`, and remove all code that modifies the typing environment from the parser. Nodes like `Ref` that need a type from the environment get types set to `null`, to be filled in after parsing.; * Add `annotateTypes` pass to fill in type annotations from the environment. This is currently written in the parser, and always called after parsing. This means for the moment we can only parse type correct IR. But in the future we could move this to a separate stage of the compilation pipeline.; * Move typechecking logic on relational nodes from the constructors to a `typecheck` method, which is called from the `TypeCheck` pass.; * Make the `typ` field on IR nodes consistently lazy (or a def when the type is a child's type without modification). Before we sometimes did this for performance, but it's now required to avoid querying children's types during construction.; * Make types mutable on `AggSignature` and `ComparisonOp`, so they can be filled in after parsing.; * Ensure that the structure in `Binds.scala` satisfies the following invariant: to construct the typing environment of child `i`, we only need the types of children with index less than `i`. This was almost always satisfied already, and allows us to use the generic binds infrastucture in the pass to annotate types (where when visiting child `i`, we can only query types of already visited children).; * Change the text representation of `TailLoop`/`Recur` to move the explicit type annotation from `Recur` to `TailLoop`. This is necessary to comply with the previous point. It's also consistent with `Ref`, where types of references are inferred from the environment.; * Add explicit types in the text representation of `TableFilterIntervals` and `MatrixFilterIntervals`, where the types were needed during parsing and we can no longe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13990:3003,perform,performance,3003,https://hail.is,https://github.com/hail-is/hail/pull/13990,1,['perform'],['performance']
Performance,he.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(C,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19260,Load,LoadVCF,19260,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"he.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) t org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.sparkapache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.sparkitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDDputeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MaD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpointla:90) at org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:4cala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$ailureException: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/t sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.javannel(UnixFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:40rage.BlockManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.rator$$anon$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.sparkler.processFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHetty.channel.AbstractChannelHandlerConte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:7294,concurren,concurrent,7294,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,"he.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. ### Error No. 2; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in _repr_html_(self); 2524 ; 2525 def _repr_html_(self):; -> 2526 s = self.table_show._repr_html_(); 2527 if self.displayed_n_cols != self.actual_n_cols:; 2528 s += '<p style=""background: #fdd; padding: 0.4em;"">'. /usr/local/lib/python3.6/site-packages/hail/table.py in _repr_html_(self); 1256 ; 1257 def _repr_html_(self):; -> 1258 return self._html_str(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:26030,concurren,concurrent,26030,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['concurren'],['concurrent']
Performance,he.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.scala:1443); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.scala:1443),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:19689,concurren,concurrent,19689,https://hail.is,https://github.com/hail-is/hail/issues/7044,2,['concurren'],['concurrent']
Performance,he.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.scala:1443); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:7966,concurren,concurrent,7966,https://hail.is,https://github.com/hail-is/hail/issues/7044,2,['concurren'],['concurrent']
Performance,"he.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13085,Load,LoadVCF,13085,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,hed MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycpar,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:37534,cache,cached,37534,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"hed Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 PyYAML-5.4.1 aiohttp-3.7.4 aiohttp-session-2.7.0 async-timeout-3.0.1 asyncinit-0.2.4 attrs-20.3.0 backcall-0.2.0 bokeh-1.4.0 cachetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:7560,cache,cachetools,7560,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cachetools']
Performance,"hed tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting mul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2543,cache,cached,2543,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"hed task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1); 2018-10-09 14:46:42 TaskSchedulerImpl: INFO: Removed TaskSet 4.0, whose tasks have all completed, from pool ; 2018-10-09 14:46:42 DAGScheduler: INFO: ResultStage 4 (collect at utils.scala:197) finished in 0.007 s; 2018-10-09 14:46:42 DAGScheduler: INFO: Job 2 finished: collect at utils.scala:197, took 0.053572 s; 2018-10-09 14:46:42 CodeGenerator: INFO: Code generated in 5.955541 ms; 2018-10-09 14:46:42 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:42 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table7e606a8b83f4` AS `zzz1`; WHERE (0 = 1); 2018-10-09 14:46:42 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:42 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table7e606a8b83f4`; 2018-10-09 14:46:43 root: INFO: optimize: before:; (TableCount; (TableKeyBy () False; (TableLiteral))); 2018-10-09 14:46:43 root: INFO: optimize: after:; (TableCount; (TableLiteral)); 2018-10-09 14:46:43 SparkContext: INFO: Starting job: fold at RVD.scala:361; 2018-10-09 14:46:43 DAGScheduler: INFO: Got job 3 (fold at RVD.scala:361) with 1 output partitions; 2018-10-09 14:46:43 DAGScheduler: INFO: Final stage: ResultStage 5 (fold at RVD.scala:361); 2018-10-09 14:46:43 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 14:46:43 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 14:46:43 DAGScheduler: INFO: Submitting ResultStage 5 (MapPartitionsRDD[28] at mapPartitions at ContextRDD.scala:137), which has no missing parents; 2018-10-09 14:46:43 MemoryStore: INFO: Block broadcast_5 stored as values in memory (estimated size 19.8 KB, free 366.2 MB); 2018-10-09 14:46:43 MemoryStore: INFO: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.1 KB, free 366.2 MB); 2018-10-09 14:46:43 BlockManagerInfo: INFO: Added broadcast_5_piece0 in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:46355,optimiz,optimize,46355,https://hail.is,https://github.com/hail-is/hail/issues/4513,2,['optimiz'],['optimize']
Performance,"hed, and the cached result is returned for n + 1 calls. References; 1. https://reactjs.org/docs/react-api.html#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, des",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:11633,load,load,11633,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['load'],['load']
Performance,hedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(Mo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215224,concurren,concurrent,215224,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"hen injecting esbuild helpers (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8110"">#8110</a>) (<a href=""https://github.com/vitejs/vite/commit/e5556ab"">e5556ab</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8110"">#8110</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.8 (2022-05-04)<!-- raw HTML omitted --></h2>; <ul>; <li>fix: inline js and css paths for virtual html (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7993"">#7993</a>) (<a href=""https://github.com/vitejs/vite/commit/d49e3fb"">d49e3fb</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7993"">#7993</a></li>; <li>fix: only handle merge ssr.noExternal (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8003"">#8003</a>) (<a href=""https://github.com/vitejs/vite/commit/642d65b"">642d65b</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8003"">#8003</a></li>; <li>fix: optimized processing folder renaming in win (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7939"">#7939</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8019"">#8019</a>) (<a href=""https://github.com/vitejs/vite/commit/e5fe1c6"">e5fe1c6</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7939"">#7939</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8019"">#8019</a></li>; <li>fix(css): do not clean id when passing to postcss (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7822"">#7822</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7827"">#7827</a>) (<a href=""https://github.com/vitejs/vite/commit/72f17f8"">72f17f8</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7822"">#7822</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7827"">#7827</a></li>; <li>fix(css): var in image-set (<a href=""https:/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12142:9140,optimiz,optimized,9140,https://hail.is,https://github.com/hail-is/hail/pull/12142,2,['optimiz'],['optimized']
Performance,high level comment -- I'm not sure that the Requiredness class is as ergonomic as I'd like for use in several places in the optimizer/compiler. Can you PR that without the inference stuff so we can look at it independently?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8602#issuecomment-617920311:124,optimiz,optimizer,124,https://hail.is,https://github.com/hail-is/hail/pull/8602#issuecomment-617920311,1,['optimiz'],['optimizer']
Performance,"high level comment -- this is an infrastructure change that will hurt performance. We need to know by how much, and probably add benchmarks that target the specific cases where the added work will hurt most. If things get noticeably slower, we're probably going to need to rework streams so that per-row stream code doesn't incur memory management overhead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-661867074:70,perform,performance,70,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661867074,1,['perform'],['performance']
Performance,hildren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.154ms self 0.069ms children 0.085ms %children 55.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRela,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:172106,Optimiz,Optimize,172106,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,hildren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.031ms self 0.031ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.346ms self 0.121ms children 0.225ms %children 65.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.154ms self 0.154ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRela,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:28789,Optimiz,Optimize,28789,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,hildren 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.092ms self 0.092ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowerin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:176071,Optimiz,OptimizePass,176071,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,hildren 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.201ms self 0.201ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowerin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:32754,Optimiz,OptimizePass,32754,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"hinx/0.8.2/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.1...0.8.2"">https://github.com/spatialaudio/nbsphinx/compare/0.8.1...0.8.2</a></p>; <p>Version 0.8.1 -- 2021-01-18 -- PyPI__ -- diff__</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/beb8fd6f6c5af0af8679b25454e5fa190e8c6f46""><code>beb8fd6</code></a> Release 0.8.8</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/efd08d8f777dcf7f435625b4a5726e240b5e2937""><code>efd08d8</code></a> DOC: mention prerequisites in CONTRIBUTING</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/948ec64ff8e9854e050b9ce603f70a40b0c34169""><code>948ec64</code></a> DOC: enable the sphinx_codeautolink extension (+ intersphinx)</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/01cc76e2d56e563841e339d96a37d811d2a35211""><code>01cc76e</code></a> Insert autolink-concat directive into notebooks if sphinx_codeautolink is loaded</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/9a9ee2d0a6f593703771152827428fae0f791ec5""><code>9a9ee2d</code></a> support text builder</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/403060082b92ccbe2adcf15dbd4fdaef65707304""><code>4030600</code></a> Replace leading/trailing empty code lines with &lt;br/&gt;</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/44829f13d18e676917fc6e7b7ae62d4bac625cd7""><code>44829f1</code></a> Escape &lt;/script&gt; tags in JSON data</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/b34f3e41e9fbf821950ad9e7a0c2a1f2689aae1e""><code>b34f3e4</code></a> CSS: update for hiding copy button in prompts</li>; <li><a href=""https://github.com/spatialaudio/nbsphinx/commit/8bd255757ade0d072e5e13c9da706e002d6fa050""><code>8bd2557</code></a> Change internal representation of _BROKEN_THUMBNAIL</li>; <li><a href=""https://github.com/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11477:5887,load,loaded,5887,https://hail.is,https://github.com/hail-is/hail/pull/11477,1,['load'],['loaded']
Performance,hmm; ```; $ make publish-python-dill && make mirror-dockerhub-images; DOCKER_PREFIX=us-docker.pkg.dev/hail-vdc/hail bash python-dill/push.sh; + for version in 3.8 3.8-slim 3.9 3.9-slim 3.10 3.10-slim 3.11 3.11-slim; + public=hailgenetics/python-dill:3.8; + DOCKER_BUILDKIT=1; + docker build --build-arg PYTHON_VERSION=3.8 --file Dockerfile.out --build-arg BUILDKIT_INLINE_CACHE=1 --tag hailgenetics/python-dill:3.8 .; [+] Building 0.0s (2/2) FINISHED docker:desktop-linux; => [internal] load build definition from Dockerfile.out 0.0s; => => transferring dockerfile: 2B 0.0s; => [internal] load .dockerignore 0.0s; => => transferring context: 2B 0.0s; ERROR: failed to solve: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount969738803/Dockerfile.out: no such file or directory; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13310#issuecomment-1652511856:487,load,load,487,https://hail.is,https://github.com/hail-is/hail/pull/13310#issuecomment-1652511856,2,['load'],['load']
Performance,hodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:7020,Load,LoadVCF,7020,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Load'],['LoadVCF']
Performance,home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core=,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32201,cache,cached,32201,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"hon3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1285,cache,cached,1285,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,hreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9818,Load,LoadVCF,9818,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,hreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4351,Load,LoadVCF,4351,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,"href=""https://github.com/aio-libs/janus/releases"">janus's releases</a>.</em></p>; <blockquote>; <h2>janus 1.0.0 release</h2>; <ul>; <li>Dropped Python 3.6 support</li>; <li>Janus is marked as stable, no API changes was made for years</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/janus/blob/master/CHANGES.rst"">janus's changelog</a>.</em></p>; <blockquote>; <h2>1.0.0 (2021-12-17)</h2>; <ul>; <li>Drop Python 3.6 support</li>; </ul>; <h2>0.7.0 (2021-11-24)</h2>; <ul>; <li>Add SyncQueue and AsyncQueue Protocols to provide type hints for sync and async queues <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/374"">#374</a></li>; </ul>; <h2>0.6.2 (2021-10-24)</h2>; <ul>; <li>Fix Python 3.10 compatibility <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/358"">#358</a></li>; </ul>; <h2>0.6.1 (2020-10-26)</h2>; <ul>; <li>; <p>Raise RuntimeError on queue.join() after queue closing. <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/295"">#295</a></p>; </li>; <li>; <p>Replace <code>timeout</code> type from <code>Optional[int]</code> to <code>Optional[float]</code> <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/267"">#267</a></p>; </li>; </ul>; <h2>0.6.0 (2020-10-10)</h2>; <ul>; <li>; <p>Drop Python 3.5, the minimal supported version is Python 3.6</p>; </li>; <li>; <p>Support Python 3.9</p>; </li>; <li>; <p>Refomat with <code>black</code></p>; </li>; </ul>; <h2>0.5.0 (2020-04-23)</h2>; <ul>; <li>Remove explicit loop arguments and forbid creating queues outside event loops <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/246"">#246</a></li>; </ul>; <h2>0.4.0 (2018-07-28)</h2>; <ul>; <li>; <p>Add <code>py.typed</code> macro <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/89"">#89</a></p>; </li>; <li>; <p>Drop python 3.4 support and fix minimal version python3.5.3 <a href=""h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11466:1170,queue,queue,1170,https://hail.is,https://github.com/hail-is/hail/pull/11466,1,['queue'],['queue']
Performance,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:2164,cache,caches,2164,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040,5,"['bottleneck', 'cache', 'optimiz']","['bottleneck', 'cache', 'caches', 'optimization']"
Performance,"http://34.207.246.132/ for the demo. The copy in the ""About"" section should have a once-over @mkveerapen in particular, and @danking, @cseed, @tpoterba. Future work will provide a multi-column, easier to read version of About, and a linking section with a beautiful code-snippet-containing introduction to Hail (demonstrating file import). I leave these as future tasks, because enough changes are present in this PR. Summary of changes:; * Refined homepage styles, ensured navbar matches to pixel between docs and hail.is root (surprisingly difficult); * Improved mobile styles, especially mobile nav menu (much smoother animation, larger, easier to click on links); * Optimized icon sizes (50KB -> 3.3KB); * Removed all use of bootstrap on hail.is/*.html pages (bootstrap remains on docs, future pr).; * Removed index.md contents. The markdown format is pretty limited. To have a richly-marked up site with consistent styling, the syntax it provides is not enough. The solution is either to add html to index.md, or just write html in a the index.xslt file. I chose the latter, because it's simpler. Future reorganization should simplify this and docs further, though I think I still recommend NextJS and the build system that provides.; * Added threeR115.min.js. This is regrettably large, but doesn't impact page rendering performance in any meaningful way, because it is loaded after all html content (and is cached after the first visit). Future work can go to webgl directly, potentially. There is also ongoing work by the ThreeJS maintainers to allow tree-shaking and smaller builds.; * Added heavily modified fork of VantaJS. Because we are not using something like NextJS, there is no package manager to rely on, so I just checked the file in manually (I have this in a separate repo, we can use that if preferred). License is in line with a note about modifications. Vanta performs very, very poorly (order of 40% CPU usage, old/slow ways of observing whether animated element is in view, u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8634:670,Optimiz,Optimized,670,https://hail.is,https://github.com/hail-is/hail/pull/8634,1,['Optimiz'],['Optimized']
Performance,"https://github-redirect.dependabot.com/psf/requests/issues/5851"">#5851</a>)</p>; </li>; <li>; <p>Fixed handling for <code>AttributeError</code> when calculating length of files obtained; by <code>Tarfile.extractfile()</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5239"">#5239</a>)</p>; </li>; <li>; <p>Fixed urllib3 exception leak, wrapping <code>urllib3.exceptions.InvalidHeader</code> with; <code>requests.exceptions.InvalidHeader</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5914"">#5914</a>)</p>; </li>; <li>; <p>Fixed bug where two Host headers were sent for chunked requests. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5391"">#5391</a>)</p>; </li>; <li>; <p>Fixed regression in Requests 2.26.0 where <code>Proxy-Authorization</code> was; incorrectly stripped from all requests sent with <code>Session.send</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5924"">#5924</a>)</p>; </li>; <li>; <p>Fixed performance regression in 2.26.0 for hosts with a large number of; proxies available in the environment. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5924"">#5924</a>)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/psf/requests/blob/main/HISTORY.md"">requests's changelog</a>.</em></p>; <blockquote>; <h2>2.27.1 (2022-01-05)</h2>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed parsing issue that resulted in the <code>auth</code> component being; dropped from proxy URLs. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6028"">#6028</a>)</li>; </ul>; <h2>2.27.0 (2022-01-03)</h2>; <p><strong>Improvements</strong></p>; <ul>; <li>; <p>Officially added support for Python 3.10. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5928"">#5928</a>)</p>; </li>; <li>; <p>Added a <cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11528:3123,perform,performance,3123,https://hail.is,https://github.com/hail-is/hail/pull/11528,2,['perform'],['performance']
Performance,"https://github-redirect.dependabot.com/psf/requests/issues/5851"">#5851</a>)</p>; </li>; <li>; <p>Fixed handling for <code>AttributeError</code> when calculating length of files obtained; by <code>Tarfile.extractfile()</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5239"">#5239</a>)</p>; </li>; <li>; <p>Fixed urllib3 exception leak, wrapping <code>urllib3.exceptions.InvalidHeader</code> with; <code>requests.exceptions.InvalidHeader</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5914"">#5914</a>)</p>; </li>; <li>; <p>Fixed bug where two Host headers were sent for chunked requests. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5391"">#5391</a>)</p>; </li>; <li>; <p>Fixed regression in Requests 2.26.0 where <code>Proxy-Authorization</code> was; incorrectly stripped from all requests sent with <code>Session.send</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5924"">#5924</a>)</p>; </li>; <li>; <p>Fixed performance regression in 2.26.0 for hosts with a large number of; proxies available in the environment. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5924"">#5924</a>)</p>; </li>; <li>; <p>Fixed idna exception leak, wrapping <code>UnicodeError</code> with; <code>requests.exceptions.InvalidURL</code> for URLs with a leading dot (.) in the; domain. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5414"">#5414</a>)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/psf/requests/commit/31a89d9c8463c3394ca00f408f4b86d814421a09""><code>31a89d9</code></a> v2.27.1</li>; <li><a href=""https://github.com/psf/requests/commit/8fa9724398c4f44090997ff430a1dd3e935a9057""><code>8fa9724</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/psf/requests/issues/6028"">#6028</a> from nateprewitt/prox_aut",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11528:6234,perform,performance,6234,https://hail.is,https://github.com/hail-is/hail/pull/11528,2,['perform'],['performance']
Performance,"https://redirect.github.com/python-pillow/Pillow/issues/7888"">#7888</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Support FITS images with GZIP_1 compression <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7894"">#7894</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use I;16 mode for 9-bit JPEG 2000 images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7900"">#7900</a> [<a href=""https://github.com/scaramallion""><code>@​scaramallion</code></a>]</li>; <li>Raise ValueError if kmeans is negative <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7891"">#7891</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Remove TIFF tag OSUBFILETYPE when saving using libtiff <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7893"">#7893</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Raise ValueError for negative values when loading P1-P3 PPM images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7882"">#7882</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Added reading of JPEG2000 palettes <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7870"">#7870</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Added alpha_quality argument when saving WebP images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7872"">#7872</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed joined corners for ImageDraw rounded_rectangle() non-integer dimensions <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7881"">#7881</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed Python and NumPy pinning on Cygwin <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7880"">#7880</a> [<a href=""https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:3648,load,loading,3648,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['load'],['loading']
Performance,"hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01/17 20:59:27 Localization script execution complete.; 2024/01/17 20:59:38 Done localization.; 2024/01/17 20:59:39 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash hailgenetics/hail@sha256:3f22576793ce3161893aed2bd40949b1fc822d2b7e6517dc0ac993b62badaff8 /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; 24/01/17 20:59:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.3.2; SparkUI available at http://523bc6a27b69:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-bb535cd096c5; LOGGING: writing to /cromwell_root/hail-20240117-2059-0.2.127-bb535cd096c5.log; 2024-01-17 21:01:32.019 Hail: INFO: Found 34523 samples in fam file.; 2024-01-17 21:01:32.020 Hail: INFO: Found 18377527 variants in bim file.; 2024-01-17 21:02:45.920 Hail: INFO: Found 34523 samples in fam file.; 2024-01-17 21:02:45.920 Hail: INFO: Found 18377527 variants in bim file.; Traceback (most recent call last):; File ""<stdin>"", line 38, in <module>; File ""<decorator-gen-1366>"", line 2, in write; File ""/usr/local/lib/python3.10/dist-packages/hail/typecheck/check.py"", line 584, in wr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:11028,load,load,11028,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['load'],['load']
Performance,hy==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting m,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:35499,cache,cached,35499,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"i>; <li>; <p>Replace <code>timeout</code> type from <code>Optional[int]</code> to <code>Optional[float]</code> <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/267"">#267</a></p>; </li>; </ul>; <h2>0.6.0 (2020-10-10)</h2>; <ul>; <li>; <p>Drop Python 3.5, the minimal supported version is Python 3.6</p>; </li>; <li>; <p>Support Python 3.9</p>; </li>; <li>; <p>Refomat with <code>black</code></p>; </li>; </ul>; <h2>0.5.0 (2020-04-23)</h2>; <ul>; <li>Remove explicit loop arguments and forbid creating queues outside event loops <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/246"">#246</a></li>; </ul>; <h2>0.4.0 (2018-07-28)</h2>; <ul>; <li>; <p>Add <code>py.typed</code> macro <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/89"">#89</a></p>; </li>; <li>; <p>Drop python 3.4 support and fix minimal version python3.5.3 <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/88"">#88</a></p>; </li>; <li>; <p>Add property with that indicates if queue is closed <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/86"">#86</a></p>; </li>; </ul>; <h2>0.3.2 (2018-07-06)</h2>; <ul>; <li>Fixed python 3.7 support <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/97"">#97</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/janus/commit/0783f9b7a9bb7e1c095e93ebb4aad4f1e219f512""><code>0783f9b</code></a> Fix coverage upload</li>; <li><a href=""https://github.com/aio-libs/janus/commit/41c49bafb1b192d2ee25b7394cead2386e452dc2""><code>41c49ba</code></a> Make deployment only if checks are green</li>; <li><a href=""https://github.com/aio-libs/janus/commit/ec94b35b2ae095dcb97827f1369c0cd31b7e8e5e""><code>ec94b35</code></a> Fix CI again</li>; <li><a href=""https://github.com/aio-libs/janus/commit/2303208c2f972e38445e7ecec54fda0f3203f566""><code>2303208</code></a> Fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11466:2300,queue,queue,2300,https://hail.is,https://github.com/hail-is/hail/pull/11466,1,['queue'],['queue']
Performance,iantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:5764,concurren,concurrent,5764,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['concurren'],['concurrent']
Performance,"ib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: ClassFormatError: Too many arguments in method signature in class file __C2866stream. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 8.0 failed 20 times, most recent failure: Lost task 3.19 in stage 8.0 (TID 54368) (leo-test-w-8.australia-southeast1-a.c.ourdna-browser.internal executor 14): java.lang.ClassFormatError: Too many arguments in method signature in class file __C2866stream; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:3797,load,load,3797,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['load'],['load']
Performance,ice.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:430); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68); E 	at reac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4291,concurren,concurrent,4291,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['concurren'],['concurrent']
Performance,"ich we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v localhost:32029; * Trying 127.0.0.1...; * TCP_NODELAY set; * Connected to localhost (127.0.0.1) port 32029 (#0); > GET / HTTP/1.1; > Host: localhost:32029; > User-Agent: curl/7.64.1; > Accept: */*; >; < HTTP/1.1 200 OK; < Content-Type: application/json; < Date: Wed, 05 Feb 2020 20:59:27 GMT; < Content-Length: 88; <; {; 	""service"": {; 		""namespace"": ""default"",; 		""name"": ""gateway""; 	},; 	""localEndpoints"": 1; }; ```; ```; }dking@gke-vdc-non-preemptible-pool-5-80798769-kp8n ~ $ curl -v localhost:32029; * Trying 127.0.0.1...; * TCP_NODELAY set; * Connected to ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8045:1546,Load,LoadBalancer,1546,https://hail.is,https://github.com/hail-is/hail/pull/8045,1,['Load'],['LoadBalancer']
Performance,"ield = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (to me), to have multiple versions of a function that differ so greatly in semantics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1469,cache,cachedVarid,1469,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437,9,"['cache', 'load']","['cachedRsid', 'cachedVarid', 'loadString']"
Performance,"ield on spills. The Z at the end of the next line indicates this field is a Boolean, not a byte. 31017 (PutFieldX PUTFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 31018 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;); 31019 25774 (InsnX I2B; 31020 25775 (InsnX IOR; 31021 25776 (InsnX IOR; 31022 25777 (InsnX IOR; 31023 25778 (InsnX IOR; 31024 25779 (LdcX 0 I); 31025 25780 (InsnX ISHL; 31026 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2346null Z; 31027 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31028 25782 (LdcX 0 I))); 31029 25783 (InsnX ISHL; 31030 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2348null Z; 31031 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SIns",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:4890,Load,LoadX,4890,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,"ient.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011161/48459558-9f645c80-e798-11e8-94db-0faa2e44e985.png). directly pr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2760,Load,LoadVCF,2760,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,"ients.20uploading.22.3F/near/353337596) has more context as well. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2150, in run; await self.jvm.execute(local_jar_location, self.scratch, self.log_file, self.jar_url, self.argv); File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:1334,concurren,concurrent,1334,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,ies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33597,cache,cached,33597,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"if the code snippet that I wrote above works, I think we should either cache the results of `classAsBytes` and reuse them in `result`, or honestly just throw an error if you try to call `result` multiple times on the same function builder. (what would be the use case? If we want to use the same function multiple times we generally call `val f = fb.result()` once and use `f` as many times as necessary to get instances of the desired function.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7384#issuecomment-546392520:71,cache,cache,71,https://hail.is,https://github.com/hail-is/hail/issues/7384#issuecomment-546392520,1,['cache'],['cache']
Performance,"if you cache the dataset (or set the global seed and generate it, both for the actual and the expected), do you get exactly the same result in your new test?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5250#issuecomment-461544717:7,cache,cache,7,https://hail.is,https://github.com/hail-is/hail/pull/5250#issuecomment-461544717,1,['cache'],['cache']
Performance,"if you cache x before the keyby, I think it would work",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3516#issuecomment-388387318:7,cache,cache,7,https://hail.is,https://github.com/hail-is/hail/issues/3516#issuecomment-388387318,1,['cache'],['cache']
Performance,ifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-com,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:34833,cache,cached,34833,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,il.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.101ms self 0.033ms children 0.068ms %children 67.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.Spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:209125,Optimiz,Optimize,209125,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,il.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.032ms children 0.070ms %children 68.44%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.019ms self 0.016ms children 0.003ms %children 16.42%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.Spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:217146,Optimiz,Optimize,217146,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,il.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.024ms self 0.021ms children 0.003ms %children 13.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.Spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:196521,Optimiz,Optimize,196521,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,il.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.108ms self 0.034ms children 0.074ms %children 68.32%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.067ms self 0.067ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.Spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:201173,Optimiz,Optimize,201173,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,il.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-f69b497; Error summary: SparkException: Failed to get broadcast_4_piece0 of broadcast_4; >>> ; ```; @danking,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:8386,concurren,concurrent,8386,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,2,['concurren'],['concurrent']
Performance,"ild/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(Ro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:2532,Load,LoadVCF,2532,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,ildren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.032ms children 0.070ms %children 68.44%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.Bac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:216707,Optimiz,OptimizePass,216707,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ildren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.108ms self 0.034ms children 0.074ms %children 68.32%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.067ms self 0.067ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.Bac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:200734,Optimiz,OptimizePass,200734,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ildren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.101ms self 0.033ms children 0.068ms %children 67.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.Bac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:208686,Optimiz,OptimizePass,208686,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ildren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.Bac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:196082,Optimiz,OptimizePass,196082,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"ileAndEvaluate._apply total 1.211s self 27.866ms children 1.183s %children 97.70%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR total 37.247ms self 0.358ms children 36.889ms %children 99.04%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Verify total 0.268ms self 0.268ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform total 36.599ms self 2.385ms children 34.214ms %children 93.48%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 0 total 4.078ms self 4.078ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 0 total 0.898ms self 0.898ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0 total 2.553ms self 0.011ms children 2.543ms %children 99.59%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 2.543ms self 2.543ms children 0.000ms %children 0.00%; timing is.hail.backen",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:2371,Optimiz,Optimize,2371,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,ileAndEvaluate.scala:47); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:635); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:635); at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:339); at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:483); at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:482); at jdk.internal.reflect.GeneratedMethodAccessor60.invoke(Unknown Source); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.base/java.lang.reflect.Method.invoke(Method.java:566); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.105-acd89e80c345; Error summary: ClassCastException: class org.apache.spark.sql.catalyst.expressions.GenericRow cannot be cast to class is.hail.variant.Locus (org.apache.spark.sql.catalyst.expressions.GenericRow is in unnamed module of loader 'app'; is.hail.variant.Locus is in unnamed module of loader org.apache.spark.util.MutableURLClassLoader @62435e70); ```. ### Version. 0.2.105. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:5729,load,loader,5729,https://hail.is,https://github.com/hail-is/hail/issues/13046,2,['load'],['loader']
Performance,"ill look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteristics (for instance k8 internal network routing latency). cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215:2778,perform,performance,2778,https://hail.is,https://github.com/hail-is/hail/pull/5215,2,"['latency', 'perform']","['latency', 'performance']"
Performance,"illow/issues/7706"">#7706</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use subprocess with CREATE_NO_WINDOW flag in ImageShow WindowsViewer <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7791"">#7791</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>When saving GIF frame that restores to background color, do not fill identical pixels <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7788"">#7788</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed reading PNG iCCP compression method <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7823"">#7823</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Allow writing IFDRational to UNDEFINED tag <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7840"">#7840</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix logged tag name when loading Exif data <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7842"">#7842</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use maximum frame size in IHDR chunk when saving APNG images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7821"">#7821</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Prevent opening P TGA images without a palette <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7797"">#7797</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use palette when loading ICO images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7798"">#7798</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use consistent arguments for load_read and load_seek <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7713"">#7713</a> [<a href=""https://github.com/radarhere""><c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:6222,load,loading,6222,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['load'],['loading']
Performance,"image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JSON objects that give more detailed information. You can clean it up a bit with `jq -c '(.Logs + .Statuses + .Vertexes) | add | {Timestamp, ID, Name}'`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10502:1692,cache,cached-as-possible,1692,https://hail.is,https://github.com/hail-is/hail/pull/10502,1,['cache'],['cached-as-possible']
Performance,"imension is zero in decompression bomb check <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7235"">#7235</a>; [radarhere]</p>; </li>; <li>; <p>Use --config-settings instead of deprecated --global-option <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7171"">#7171</a>; [radarhere]</p>; </li>; <li>; <p>Better C integer definitions <a href=""https://redirect.github.com/python-pillow/Pillow/issues/6645"">#6645</a>; [Yay295, hugovk]</p>; </li>; <li>; <p>Fixed finding dependencies on Cygwin <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7175"">#7175</a>; [radarhere]</p>; </li>; <li>; <p>Changed grabclipboard() to use PNG instead of JPG compression on macOS <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7219"">#7219</a>; [abey79, radarhere]</p>; </li>; <li>; <p>Added in_place argument to ImageOps.exif_transpose() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7092"">#7092</a>; [radarhere]</p>; </li>; <li>; <p>Fixed calling putpalette() on L and LA images before load() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7187"">#7187</a>; [radarhere]</p>; </li>; <li>; <p>Fixed saving TIFF multiframe images with LONG8 tag types <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7078"">#7078</a>; [radarhere]</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python-pillow/Pillow/commit/6e28ed1f36d0eb74053af54e1eddc9c29db698cd""><code>6e28ed1</code></a> 10.0.0 version bump</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/c827f3b30f50bf04fd65daeeba6bbfd56fc7b50e""><code>c827f3b</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7246"">#7246</a> from radarhere/deallocate</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/39a3b1d83edcf826c3864e26bedff5b4e4dd331b""><code>39a3b1d</code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:12054,load,load,12054,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['load'],['load']
Performance,impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:108); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:219216,concurren,concurrent,219216,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,import_bgen optional variants ir is not optimized,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5728:40,optimiz,optimized,40,https://hail.is,https://github.com/hail-is/hail/issues/5728,1,['optimiz'],['optimized']
Performance,import_table fails to load with exponential notation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4238:22,load,load,22,https://hail.is,https://github.com/hail-is/hail/issues/4238,1,['load'],['load']
Performance,import_vcf no longer scans variants. @tpoterba I imagine you want to fight me on this. I want to do binary search on chromosome boundaries when loading VCF files in which case this is quite expensive.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2183:144,load,loading,144,https://hail.is,https://github.com/hail-is/hail/pull/2183,1,['load'],['loading']
Performance,improve aggregator performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1223:19,perform,performance,19,https://hail.is,https://github.com/hail-is/hail/pull/1223,1,['perform'],['performance']
Performance,improve annotatevariants tsv performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/797:29,perform,performance,29,https://hail.is,https://github.com/hail-is/hail/issues/797,1,['perform'],['performance']
Performance,improved performance of VariantSubGen.gen,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2152:9,perform,performance,9,https://hail.is,https://github.com/hail-is/hail/pull/2152,1,['perform'],['performance']
Performance,improves optimization and opportunities for deforestation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4499:9,optimiz,optimization,9,https://hail.is,https://github.com/hail-is/hail/pull/4499,1,['optimiz'],['optimization']
Performance,"in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also clones the repo, so for my repo I wait at least 2 minutes before I even have a chance to get feedback; with this PR and #7534 I should wait like 45 seconds?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:1731,latency,latency,1731,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927,6,"['bottleneck', 'latency', 'throughput']","['bottleneck', 'latency', 'throughput']"
Performance,in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:50705,concurren,concurrent,50705,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nod,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:52032,concurren,concurrent,52032,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 11.0 in stage 0.0 (TID 11, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:54697,concurren,concurrent,54697,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:48051,concurren,concurrent,48051,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:49378,concurren,concurrent,49378,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 39.0 in stage 0.0 (TID 39, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:106346,concurren,concurrent,106346,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 29.0 in stage 0.0 (TID 29, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:100659,concurren,concurrent,100659,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 9 from BlockManagerMaster.; 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 9 requested; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_0,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:125335,concurren,concurrent,125335,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 17.0 in stage 0.0 (TID 17, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:118202,concurren,concurrent,118202,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 27.1 in stage 0.0 (TID 44, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:139113,concurren,concurrent,139113,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 25.1 in stage 0.0 (TID 41, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:147892,concurren,concurrent,147892,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 15.2 in stage 0.0 (TID 50, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:161022,concurren,concurrent,161022,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 7.2 in stage 0.0 (TID 53, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:171571,concurren,concurrent,171571,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 9.1 in stage 0.0 (TID 65, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:183070,concurren,concurrent,183070,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 35.3 in stage 0.0 (TID 62, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:192426,concurren,concurrent,192426,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011161/48459558",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2705,Load,LoadVCF,2705,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,"in order to work with our dynamically-generated Kubernetes test namespaces. Currently, we configure NGINX by creating server blocks that dynamically resolve and dispatch requests based on matching regular expressions on the host and path headers. This is in large part due that at gateway deploy time we do not statically know all of the namespaces and namespace-service combinations that will exist in the cluster in the future. This is true for `default`, but not test namespaces, and NGINX will refuse to start with statically-configured clusters that it cannot reach. Making the server blocks make the routing decisions dynamically circumvents this limitation. However, this prevents usage of NGINX [upstream](http://nginx.org/en/docs/http/ngx_http_upstream_module.html) blocks that provide connection pooling, at least in the community edition, and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate TLS with internal-gateway and free up cycles in the batch-driver python process. By using proper persistent connections, we can reduce the TLS overhead to single-digit percents of a core. This leads to the first goal of this transition: configure our load balancers to know the full cluster configuration at any point in time so they can properly maintain connection pools with upstream services. However, this is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:1707,perform,performance,1707,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['perform'],['performance']
Performance,"in the service, log everything to stdout; - [x] (trivial) d8104a1dc4 [query-service] do not catch CancelledError; - [x] (trivial) efcb345185 [query-service] slightly more useful error message when socket dies; - [ ] (@tpoterba) f79c4023cf [shuffler] if we have an ExecuteContext, use it; - [x] (@daniel-goldstein,fyi: @tpoterba) 259f70dd25 [query-service] JSON Logging; - [ ] (@catoverdrive) f5c3ffcbd1 [query-service] pervasively retry all idempotent operations; - [ ] (@tpoterba) 507db4b468 [hail] fix using; - [x] (@jigold) c32a253bb9 [query] when testing, ensure our thread has an event loop; - [ ] (@tpoterba) 110469c2da [query][lir] avoid dumping massive classes onto stderr; - [ ] (@tpoterba) e4aa1c15fe [query] do not print misleading log in RegionPool.finalizer; - [x] (trivial) 33eab9a80e [query-service] better logging information; - [ ] (@catoverdrive) e358e8feeb [query-service] remove race conditions in user management; - [ ] (@tpoterba) b60cb2bae5 [lir] make LIR genName thread-safe; - [ ] (@catoverdrive) 2d82e5faf5 [query-service] send a token for job identifiability; - [x] (@daniel-goldstein) fd78caedcb [query-service] reduce image size by ~2GB; - [ ] (@catoverdrive) 00d1840421 [query-service] retry CLOSE, CLOSED (i.e. connection dropped); - [ ] (@catoverdrive) c985d3e3de [query-service] remove old test code; - [ ] (@catoverdrive) 0a5dc8c651 [query-service] all operations are idempotent; - [ ] (@cseed) 6d02d173fa [make] fix config.mk; - [x] (@daniel-goldstein) d21df54e63 [devbin] teach devbin/functions.py about multiple containers; - [x] (@jigold) 38878f7874 [batch] remove batch_worker_image false dependency on service_base_image; - [x] (@daniel-goldstein) f03defab3d [java-services] avoid NPEs in isTransientError; - [x] (@jigold) e535bdc00d [dependencies] upgrade gcsfs to 0.7.2 to fix GoogleFS rmtree issue; - [x] (@cseed) 743b5ba62f [query-service] enable auto-scaling for PR and dev deploy; - [ ] (@cseed) 6a52d45f6f [query-service] retry EndOfStream errors from j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10100:3045,race condition,race conditions,3045,https://hail.is,https://github.com/hail-is/hail/pull/10100,1,['race condition'],['race conditions']
Performance,"in this release are 3.8-3.10, Python 3.7; has been dropped. Note that 32 bit wheels are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,; and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>; <p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>; <h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>; <p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that; users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both; deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with; the appropriate value for the <code>usemask</code> parameter.</p>; <p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: Prepare for the NumPy 1.22.0 release.</li>; <li><a href=""https://github.com/numpy/nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11939:2435,load,loads,2435,https://hail.is,https://github.com/hail-is/hail/pull/11939,2,['load'],['loads']
Performance,"information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. ah, cool. > Is there a reason you didn't make it a subdomain? I though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1555,optimiz,optimize,1555,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,2,['optimiz'],['optimize']
Performance,infrastructure for query optimizer,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1778:25,optimiz,optimizer,25,https://hail.is,https://github.com/hail-is/hail/pull/1778,1,['optimiz'],['optimizer']
Performance,"ing CPU time:; <img width=""1889"" alt=""Screen Shot 2022-03-21 at 5 10 13 PM"" src=""https://user-images.githubusercontent.com/24440116/159364769-6fd60840-5745-40ab-802e-68b8d4f32078.png"">; <img width=""1885"" alt=""Screen Shot 2022-03-21 at 5 12 33 PM"" src=""https://user-images.githubusercontent.com/24440116/159364787-ca7ec307-877d-479c-9c19-8746b5e82eab.png"">. Looking at Wall time, the before profile is nearly identical because at the current rate limit the driver uses 100% of its CPU shares under this benchmark. On this branch, CPU utilization drops to 40-60%, giving the following wall time profile:; <img width=""1879"" alt=""Screen Shot 2022-03-21 at 5 30 39 PM"" src=""https://user-images.githubusercontent.com/24440116/159367182-0830d6ff-3b6f-4fa7-8004-0fc43283ec4a.png"">. So we can be confident that driver CPU is no longer a bottleneck even in the increased rate limit. ## So what's the bottleneck now?; Since the higher rate limit still leaves the driver plenty of CPU room (I've seen it peak at 60% of a vCPU), why not crank it higher? Well, we're increasing concurrency so latent deadlocks start to be a bigger issue again. We start to see tens of deadlocks per second in the proposed rate limit and hundreds at higher rate limits. As a result, we're spending more cycles repeating queries instead of actually scheduling faster. Next steps should focus on eliminating deadlocks before we can continue to max out CPU use. ## Miscellaneous; We've lumped in a few other monitoring changes that were helpful in this process and tried to leave the commits tidy. The one potentially rude change is enforcing a minimum wait time of half a second for `run_if_changed` loops. This dramatically reduced the number of scheduling loop invocations we were executing, greatly reducing the number of `compute_fair_share` queries, while maintaining the same scheduling ability. This feels like a fine requirement, but I'm unfamiliar with other use cases and can introduce a less invasive change if desired. cc @",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11638:1442,bottleneck,bottleneck,1442,https://hail.is,https://github.com/hail-is/hail/pull/11638,2,"['bottleneck', 'concurren']","['bottleneck', 'concurrency']"
Performance,ing attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manyl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33426,cache,cached,33426,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,ing cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-non,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36277,cache,cached,36277,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,ing protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-an,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39018,cache,cached,39018,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,ing.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.525ms self 0.525ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.264ms self 0.264ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:109901,Optimiz,OptimizePass,109901,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ing.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.557ms self 0.557ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.257ms self 0.257ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:95463,Optimiz,OptimizePass,95463,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ing.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.736ms self 0.736ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.055ms self 0.055ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.391ms self 0.391ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:59886,Optimiz,OptimizePass,59886,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ing.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.743ms self 0.743ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.318ms self 0.318ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:74284,Optimiz,OptimizePass,74284,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ing.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.977ms self 0.977ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.476ms self 0.476ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:51358,Optimiz,OptimizePass,51358,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ing.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 4.091ms self 4.091ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.148ms self 0.148ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 1.317ms self 1.317ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:42830,Optimiz,OptimizePass,42830,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ing.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.938ms self 0.938ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.101ms self 0.049ms children 0.052ms %children 51.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.exp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:124553,Optimiz,OptimizePass,124553,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ing.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.490ms self 0.490ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.083ms self 0.043ms children 0.040ms %children 48.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.exp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:156058,Optimiz,OptimizePass,156058,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ing.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.058ms self 0.058ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.240ms self 0.240ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.087ms self 0.042ms children 0.045ms %children 51.66%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.exp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:140271,Optimiz,OptimizePass,140271,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ing.OptimizePass total 0.518ms self 0.004ms children 0.514ms %children 99.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.496ms self 0.040ms children 0.456ms %children 91.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.228ms self 0.228ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.064ms self 0.064ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:206807,Optimiz,OptimizePass,206807,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ing.OptimizePass total 0.525ms self 0.004ms children 0.521ms %children 99.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.503ms self 0.061ms children 0.443ms %children 87.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:214828,Optimiz,OptimizePass,214828,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ing.OptimizePass total 1.490ms self 0.008ms children 1.482ms %children 99.45%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.013ms self 0.013ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.460ms self 0.066ms children 1.394ms %children 95.49%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.654ms self 0.654ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:194203,Optimiz,OptimizePass,194203,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ing.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_matrix_ir$2(SparkBackend.scala:689); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:69); 	at is.hail.utils.package$.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2628,Load,LoadPlink,2628,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,ing==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:38433,cache,cached,38433,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"ingle field's missingness was flipped was a red herring -- all higher bits are flipped to 0 (defined)! Here's a look at the LIR looks like, though it was ultimately the JVM class file printout that tipped me off to the problem:. ```code. # I2B is stored as a class field on spills. The Z at the end of the next line indicates this field is a Boolean, not a byte. 31017 (PutFieldX PUTFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 31018 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;); 31019 25774 (InsnX I2B; 31020 25775 (InsnX IOR; 31021 25776 (InsnX IOR; 31022 25777 (InsnX IOR; 31023 25778 (InsnX IOR; 31024 25779 (LdcX 0 I); 31025 25780 (InsnX ISHL; 31026 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2346null Z; 31027 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31028 25782 (LdcX 0 I))); 31029 25783 (InsnX ISHL; 31030 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2348null Z; 31031 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:4624,Load,LoadX,4624,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,interpret calling compile calling optimize is a huge bug.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6050#issuecomment-490304711:34,optimiz,optimize,34,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-490304711,1,['optimiz'],['optimize']
Performance,invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(Writ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822:10819,concurren,concurrent,10819,https://hail.is,https://github.com/hail-is/hail/issues/1822,1,['concurren'],['concurrent']
Performance,"io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:5189,concurren,concurrent,5189,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['concurren'],['concurrent']
Performance,"io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:12285,concurren,concurrent,12285,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['concurren'],['concurrent']
Performance,"io.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:18583,load,load,18583,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['load'],['load']
Performance,"ion strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2339,load,load,2339,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,2,['load'],['load']
Performance,"ion.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Traversable",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:2274,load,loading,2274,https://hail.is,https://github.com/hail-is/hail/issues/1683,1,['load'],['loading']
Performance,ion: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:8037,concurren,concurrent,8037,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,2,['concurren'],['concurrent']
Performance,"ion: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:18522,Load,Loading,18522,https://hail.is,https://github.com/hail-is/hail/issues/9293,2,"['Load', 'load']","['Loading', 'load']"
Performance,ion: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:22.668 GoogleStorageFS$: INFO: close: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:33.788 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/1-day/o?name=parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g%3D/result.7028&uploadType=resumable&upload_id=ADPycdv7y3A6GjTh6Kv7vrUu2ap2Kv0peLVWsVTAghIs7RCZk9X3fI1BDkeHag1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPqmOq13; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdv7y3A6GjT,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:7031,concurren,concurrent,7031,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['concurren'],['concurrent']
Performance,ion: sun.reflect.generics.reflectiveObjects.NotImplementedException; Serialization trace:; m (is.hail.annotations.aggregators.KeyedRegionValueAggregator); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: sun.reflect.generics.reflectiveObjects.NotImplementedException; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 10 more; Driver stacktrace:; 	at org.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:1476,concurren,concurrent,1476,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['concurren'],['concurrent']
Performance,ion: sun.reflect.generics.reflectiveObjects.NotImplementedException; Serialization trace:; m (is.hail.annotations.aggregators.KeyedRegionValueAggregator); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)sun.reflect.generics.reflectiveObjects.NotImplementedException: null; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:6317,concurren,concurrent,6317,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['concurren'],['concurrent']
Performance,ionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3040:6537,concurren,concurrent,6537,https://hail.is,https://github.com/hail-is/hail/issues/3040,1,['concurren'],['concurrent']
Performance,"ions and Removals</h2>; <ul>; <li><code>[#468](https://github.com/pytest-dev/pytest-xdist/issues/468) &lt;https://github.com/pytest-dev/pytest-xdist/issues/468&gt;</code>_: The <code>--boxed</code> command line argument is deprecated. Install pytest-forked and use <code>--forked</code> instead. pytest-xdist 3.0.0 will remove the <code>--boxed</code> argument and pytest-forked dependency.</li>; </ul>; <h2>Features</h2>; <ul>; <li>; <p><code>[#722](https://github.com/pytest-dev/pytest-xdist/issues/722) &lt;https://github.com/pytest-dev/pytest-xdist/issues/722&gt;</code>_: Full compatibility with pytest 7 - no deprecation warnings or use of legacy features.</p>; </li>; <li>; <p><code>[#733](https://github.com/pytest-dev/pytest-xdist/issues/733) &lt;https://github.com/pytest-dev/pytest-xdist/issues/733&gt;</code>_: New <code>--dist=loadgroup</code> option, which ensures all tests marked with <code>@pytest.mark.xdist_group</code> run in the same session/worker. Other tests run distributed as in <code>--dist=load</code>.</p>; </li>; </ul>; <h2>Trivial Changes</h2>; <ul>; <li>; <p><code>[#708](https://github.com/pytest-dev/pytest-xdist/issues/708) &lt;https://github.com/pytest-dev/pytest-xdist/issues/708&gt;</code>_: Use <code>@pytest.hookspec</code> decorator to declare hook options in <code>newhooks.py</code> to avoid warnings in <code>pytest 7.0</code>.</p>; </li>; <li>; <p><code>[#719](https://github.com/pytest-dev/pytest-xdist/issues/719) &lt;https://github.com/pytest-dev/pytest-xdist/issues/719&gt;</code>_: Use up-to-date <code>setup.cfg</code>/<code>pyproject.toml</code> packaging setup.</p>; </li>; <li>; <p><code>[#720](https://github.com/pytest-dev/pytest-xdist/issues/720) &lt;https://github.com/pytest-dev/pytest-xdist/issues/720&gt;</code>_: Require pytest&gt;=6.2.0.</p>; </li>; <li>; <p><code>[#721](https://github.com/pytest-dev/pytest-xdist/issues/721) &lt;https://github.com/pytest-dev/pytest-xdist/issues/721&gt;</code>_: Started using type annotations and mypy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11491:1353,load,load,1353,https://hail.is,https://github.com/hail-is/hail/pull/11491,2,['load'],['load']
Performance,"ions: {regions}.'); 111 path = [dataset['url'][cloud][region]; 112 for dataset in datasets[name]['versions']; 113 if all([dataset['version'] == version,; 114 dataset['reference_genome'] == reference_genome])]; --> 115 assert len(path) == 1; 116 path = path[0]; 117 if path.startswith('s3://'):. AssertionError: ; ```. I'm a new Hail user and don't have the full context here, but it seems like there are at least three problems:. 1. An assert failed in production code, which indicates either the presence of a bug or an incorrect use of assert (e.g. using assert to check for value errors).; 2. The assert has no corresponding error message, so the user learns that something has gone wrong but can't easily tell what.; 3. The assert is bare. Bare asserts can get optimized out of code in ways that are difficult to foresee in advance, and are generally deprecated in favor of the `if error_condition: raise AssertionError(...)` pattern (see: https://discuss.python.org/t/stop-ignoring-asserts-when-running-in-optimized-mode/13132). **The Big Picture**. The bare assert pattern is used over 3k times in Hail. To be fair, many of these usages occur in test directories, where they're fine. But they also occur in application code, and often in the dangerous form `assert(expr1, expr2)` which will never fail (because a tuple with two falsy elements is truthy in python). These asserts are never actually getting checked. . Fixing all of them would be a heavy lift. One compromise solution might be to add a bare assert rule to the linter (e.g. https://pypi.org/project/flake8-assert-msg/). This would prevent the introduction of further bare asserts to the codebase, and encourage authors to clean up existing bare asserts on files they touch. The `assert` keyword is an unfortunate language wart that makes it very easy for developers to write error-checking code that is itself incorrect. I'd encourage considering the alternate pattern `if error_condition: raise AssertionError(...)` and gradually",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12952:1685,optimiz,optimized-mode,1685,https://hail.is,https://github.com/hail-is/hail/issues/12952,1,['optimiz'],['optimized-mode']
Performance,"ip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Coll",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1857,cache,cached,1857,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,ipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply total 77.695ms self 14.105ms children 63.590ms %children 81.85%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 47.766ms self 0.019ms children 47.747ms %children 99.96%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.210ms self 0.210ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:36640,Optimiz,OptimizePass,36640,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.loweri,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5646,Optimiz,Optimize,5646,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Optimiz'],['Optimize']
Performance,ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.O,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5896,Optimiz,Optimize,5896,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.ex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5545,Optimiz,Optimize,5545,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Optimiz'],['Optimize']
Performance,ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.635ms self 0.635ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.295ms self 0.295ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.033ms self 0.033ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:99099,Optimiz,Optimize,99099,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.641ms self 0.641ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.241ms self 0.241ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.039ms self 0.039ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:113537,Optimiz,Optimize,113537,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.756ms self 0.756ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.409ms self 0.409ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:63522,Optimiz,Optimize,63522,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.830ms self 0.830ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.348ms self 0.348ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.056ms self 0.056ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:77920,Optimiz,Optimize,77920,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.885ms self 0.885ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.450ms self 0.450ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.396ms self 0.396ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:54994,Optimiz,Optimize,54994,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.703ms self 4.703ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.607ms self 0.607ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.447ms self 0.447ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:46466,Optimiz,Optimize,46466,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.229ms self 0.229ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 3.719ms self 0.009ms children 3.709ms %children 99.75%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.030ms self 0.030ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.app,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:103717,Optimiz,OptimizePass,103717,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.260ms self 0.260ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 3.925ms self 0.009ms children 3.916ms %children 99.78%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.app,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:89279,Optimiz,OptimizePass,89279,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.333ms self 0.333ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 4.618ms self 0.011ms children 4.607ms %children 99.77%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.app,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:68100,Optimiz,OptimizePass,68100,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.046ms self 0.046ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.655ms self 0.655ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:151647,Optimiz,Optimize,151647,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.412ms self 0.412ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.609ms self 0.609ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:135860,Optimiz,Optimize,135860,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.598ms self 0.598ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.054ms self 0.054ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.566ms self 0.566ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:120142,Optimiz,Optimize,120142,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"irectory '/home/edmund/.local/src/hail/hail'; # # If hailtop.aiotools.copy gives you trouble:; # gcloud storage cp -r src/test/resources/\* gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/; # gcloud storage cp -r python/hail/docs/data/\* gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/; python3 -m hailtop.aiotools.copy -vvv 'null' '[\; {""from"":""src/test/resources"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/""},\; {""from"":""python/hail/docs/data"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/""}\; ]' --timeout 600; Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/runpy.py"", line 197, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.local/src/hail/hail/python/hailtop/aiotools/copy.py"", line 211, in <module>; asyncio.run(main()); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/asyncio/runners.py"", line 44, in run; return loop.run_until_complete(main); File ""uvloop/loop.pyx"", line 1517, in uvloop.loop.Loop.run_until_complete; File ""/home/edmund/.local/src/hail/hail/python/hailtop/aiotools/copy.py"", line 182, in main; files = json.loads(args.files); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/__init__.py"", line 346, in loads; return _default_decoder.decode(s); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/decoder.py"", line 355, in raw_decode; raise JSONDecodeError(""Expecting value"", s, err.value) from None; json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1); make: *** [Makefile:355: upload-remote-test-resources] Error 1; make: Leaving directory '/home/edmund/.local/src/hail/hail'; ```; What am I missing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1887744163:1463,load,loads,1463,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1887744163,2,['load'],['loads']
Performance,irements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33681,cache,cached,33681,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"irements.txt --output-file=/tmp/tmp.aWUFJ1BMnP; ../check_pip_requirements.sh: line 13: pip-compile: command not found; ```. While I do have pip-compile installed. ```sh ; pip-compile --help; Usage: pip-compile [OPTIONS] [SRC_FILES]... Compiles requirements.txt from requirements.in, pyproject.toml, setup.cfg,; or setup.py specs. Options:; ```. Note that `make clean` did not solve the issue. see logs attached. ### Version. 0.2.120. ### Relevant log output. ```shell; BUILD SUCCESSFUL in 2m 46s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; /usr/lib64/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'long_description_content_type'; warnings.warn(msg); installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.120.dist-info/WHEEL; creating 'dist/hail-0.2.120-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hail_logging.py'; adding 'hail/hail_pip_version'; adding 'hail/hail_revision'; adding 'hail/hail_version'; adding 'hail/matrixtable.py'; adding 'hail/table.py'; adding 'hail/backend/__init__.py'; adding 'hail/backend/backend.py'; adding 'hail/backend/hail-all-spark.jar'; adding 'hail/backend/local_backend.py'; adding 'hail/backend/py4j_backend.py'; adding 'hail/backend/service_backend.py'; adding 'hail/backend/spark_backend",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:1413,cache,cache,1413,https://hail.is,https://github.com/hail-is/hail/issues/13445,1,['cache'],['cache']
Performance,"is that the driver currently must wait for all the requests to workers in an iteration to complete before it starts the next iteration of the scheduler. This leaves the scheduler vulnerable to problematic workers or workers that happen to be preempted during the scheduling process. So, the driver sets a [2 second timeout](https://github.com/hail-is/hail/blob/b27737f67bf9e69f1abed2fec07fc7c921790ef8/batch/batch/driver/job.py#L585) on the call to `/api/v1alpha/batches/jobs/create`. Additionally, this general design means that in the event of a request timeout or transient error, Batch cannot guarantee that there is always at most one concurrent running attempt for a given job. This ends up being a fine (and intentional) concession in practice because the idempotent design of preemptible jobs tends to cover this scenario, but it is regardless wasted compute and cost to users. Nevertheless, we strive to minimize cases where we might halt the scheduling loop or double-schedule work, and one way to do that in the current design is to minimize the variance in latency of `/api/v1alpha/batches/jobs/create`. The largest source of this latency is the request to blob storage. While GCS and ABS are relatively fast and highly available, Batch in Azure Terra requires first obtaining SAS tokens from the Terra control plane, which can introduce much higher and more variable latency. There have also been occurrences in the past of corrupted or deleted specs, which introduce unexpected failure modes that should error the job but instead disrupt the scheduling loop. Many of these problems would be mitigated by moving the read from object storage outside of the `/api/v1alpha/batches/jobs/create` endpoint. The endpoint should push this read into the asynchronous task that ultimately runs the job and therefore return its acknowledgement to the driver faster. If the worker encounters errors later on while reading the spec, those should result in `error`ing the job instead of raising a 500 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14456:1927,latency,latency,1927,https://hail.is,https://github.com/hail-is/hail/issues/14456,1,['latency'],['latency']
Performance,"is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 1 total 0.025ms self 0.025ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.151ms self 0.151ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/PruneDeadFields, iteration: 1 total 0.714ms self 0.714ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Verify total 0.022ms self 0.022ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.TypeCheck.apply total 0.124ms self 0.124ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LowerMatrixToTable total 4.039ms self 0.016ms children 4.023ms %children 99.60%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LowerMatrixToTable/Verify total 0.012ms self 0.012ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LowerMatrixToTable/Transform total 3.992ms self 3.992ms children 0.000ms %children 0.00%; timing is.hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:8657,Optimiz,Optimize,8657,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.Eva,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:173209,Optimiz,OptimizePass,173209,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.154ms self 0.154ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.015ms self 0.015ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.056ms self 0.056ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.Eva,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:29892,Optimiz,OptimizePass,29892,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:19902,cache,cache,19902,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['cache'],['cache']
Performance,"is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:822); at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:447); at is.hail.backend.service.Main$.main(Main.scala:15); at is.hail.backend.service.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.write(get_aou_util_path('mt_sample_qc'), overwrite=args.overwrite); ```. Job log: https://batch.hail.is/batches/8058522/jobs/171029. <details>; <summary>The last TableIR logged</summary>. ```; 2023-10-13 02:14:44.213 : INFO: after optimize: darrayLowerer, after LowerAndExecuteShuffles: IR size 232: . !ht = TableRead [Table{global:Struct{},key:[locus,alleles],",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:6846,concurren,concurrent,6846,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['concurren'],['concurrent']
Performance,is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.030ms self 0.030ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.641ms self 0.052ms children 3.589ms %children 98.56%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.326ms self 0.326ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:105011,Optimiz,OptimizePass,105011,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.847ms self 0.051ms children 3.797ms %children 98.69%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.342ms self 0.342ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:90573,Optimiz,OptimizePass,90573,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.479ms self 0.057ms children 4.422ms %children 98.72%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.355ms self 0.355ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:69394,Optimiz,OptimizePass,69394,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.210ms self 0.210ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 47.463ms self 0.201ms children 47.262ms %children 99.58%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 6.134ms self 6.134ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowerin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:37936,Optimiz,OptimizePass,37936,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.340ms self 0.340ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.490ms self 0.490ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:155326,Optimiz,Optimize,155326,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.463ms self 0.463ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.058ms self 0.058ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.240ms self 0.240ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:139539,Optimiz,Optimize,139539,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.505ms self 0.505ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.938ms self 0.938ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:123821,Optimiz,Optimize,123821,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,is.hail.expr.ir.lowering.LoweringPipeline#apply total 4.179ms self 1.415ms children 2.764ms %children 66.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 1.490ms self 0.008ms children 1.482ms %children 99.45%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.013ms self 0.013ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.460ms self 0.066ms children 1.394ms %children 95.49%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.654ms self 0.654ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:193863,Optimiz,OptimizePass,193863,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:15295,concurren,concurrent,15295,https://hail.is,https://github.com/hail-is/hail/issues/5718,2,['concurren'],['concurrent']
Performance,"istributed"" which I think is your proposal). Blocks aren't heterogenous in our current setup (last block can be short) and I think that needs to be clarified in your proposal. Slicing has similar complications. I think the blocking should specify the list of sizes of each block, which makes it closed under slicing. I agree the user interface should do implicit broadcast but in the IR it should be explicit. > If we want to support sparse tensors. I vote we punt on sparse vectors for the time being. (In the sparse case, I think sparsity of the output should be inferred by analysis rather than specified. I think statically knowing the sparsity relationship is going to a rarity.). The key point here is that Patrick's operations are incredibly general so this proposal needs to be paired with an compilation strategy that guarantees his operations are never actually realized directly (e.g. deforestation, fusing operations (e.g. broadcast) into their consumers) while also generating BLAS-level performance. I propose we plan to build against BLAS while we investigate more general codegen strategies. To that end, I've suggested we read up on a few related projects in study group: taco (hat tip Patrick), TensorComprehensions, TVM (tensor virtual machine) and Halide. Might be other relevant ones (TF/XLA?) Few questions to ask:; - Can we steal good ideas for our IR?; - Can we target their IR?; - Can we integrate this with our stack?; - What's the performance like (compile time, runtime)?. We'll need some more prosaic operators:; - constructors: from array with shape, from shape and a function of indices; - get shape.; - index: get an element from a tuple of integers. This is only allowed for local arrays.; - slice: get another ndarray from a sequence of integers, ranges, or array of indices.; - reshape. Maybe local only? Or clarified in the distributed case. I think we'll need some additional basic operations supported by numpy (e.g. concatenate, tile), but this is a good start.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120:1621,perform,performance,1621,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120,1,['perform'],['performance']
Performance,it code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:53431,concurren,concurrent,53431,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"it code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 14.0 in stage 0.0 (TID 14, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:61902,concurren,concurrent,61902,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"it code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 26.0 in stage 0.0 (TID 26, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:69046,concurren,concurrent,69046,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"it's actually a bit slower (2x) than using f-strings. I could serialize a million-node IR tree in 100ms, though, so I don't think this is the limiting factor compared to the optimization in the backend.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5465#issuecomment-469782010:174,optimiz,optimization,174,https://hail.is,https://github.com/hail-is/hail/pull/5465#issuecomment-469782010,1,['optimiz'],['optimization']
Performance,iteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5505,Optimiz,Optimize,5505,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,"ithub-redirect.dependabot.com/googleapis/java-storage/issues/1806"">#1806</a>) (<a href=""https://github.com/googleapis/java-storage/commit/0f24a11c5289a4c07f27d8a3c29fab34520b036f"">0f24a11</a>)</li>; <li>Implement GrpcStorageImpl#deleteDefaultAcl (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1807"">#1807</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c78327717a7936492161ddcc64c86374db72c48c"">c783277</a>)</li>; <li>Implement GrpcStorageImpl#getDefaultAcl (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1802"">#1802</a>) (<a href=""https://github.com/googleapis/java-storage/commit/b9b7c49fcfcab285da156b34b186a007150e876f"">b9b7c49</a>)</li>; <li>Implement GrpcStorageImpl#listDefaultAcl (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1805"">#1805</a>) (<a href=""https://github.com/googleapis/java-storage/commit/03c2e6660721b4a8bfc09b241ef44f3e4e08865b"">03c2e66</a>)</li>; <li>Improve throughput of http based storage#reader between 100 MiB/s and 200 MiB/s (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1799"">#1799</a>) (<a href=""https://github.com/googleapis/java-storage/commit/94cd2887f22f6d1bb82f9929b388c27c63353d77"">94cd288</a>)</li>; <li>Update GrpcBlobReadChannel to allow seek/limit after read (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1834"">#1834</a>) (<a href=""https://github.com/googleapis/java-storage/commit/45dc983a4af8e7feb937263ce611bd34eda37e03"">45dc983</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>Add missing preconditions and update samples (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1753"">#1753</a>) (<a href=""https://github.com/googleapis/java-storage/commit/96beca2465158fb4633d58fe09a9776a4b171811"">96beca2</a>)</li>; <li><strong>grpc:</strong> Fix bucket logging conversion to allow clearing (<a href=""https://github-redirect.dependabot.com/googl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12598:2563,throughput,throughput,2563,https://hail.is,https://github.com/hail-is/hail/pull/12598,2,['throughput'],['throughput']
Performance,"ity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Open Redirect <br/>[SNYK-PYTHON-NOTEBOOK-1041707](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | Information Exposure <br/>[SNYK-PYTHON-NOTEBOOK-2441824](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Access Restriction Bypass <br/>[SNYK-PYTHON-NOTEBOOK-2928995](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | Race Condition <br/>[SNYK-PYTHON-PROMPTTOOLKIT-6141120](https://snyk.io/vuln/SNYK-PYTHON-PROMPTTOOLKIT-6141120) | `prompt-toolkit:` <br> `1.0.18 -> 3.0.13` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-1086606](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1086606) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | Denial of Service (DoS) <br/>[SNYK-PYTHON-PYGMENTS-1088505](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1088505) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-5750273](https://snyk.io/vuln/SNYK-PYTHO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14257:4329,Race Condition,Race Condition,4329,https://hail.is,https://github.com/hail-is/hail/pull/14257,2,['Race Condition'],['Race Condition']
Performance,iver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215939,concurren,concurrent,215939,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 1.780ms, total 487.281ms, tagged coverage 0.0; ...; ...; ...; 2019-11-06 18:44:11 root: INFO: Timer: aggregate:; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'Verify' (8): 8.109ms; 2019-11-06 18:44:11 root: INFO: Time taken fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2594,Optimiz,Optimize,2594,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOExcepti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:6714,concurren,concurrent,6714,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['concurren'],['concurrent']
Performance,"java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822:4399,concurren,concurrent,4399,https://hail.is,https://github.com/hail-is/hail/issues/1822,1,['concurren'],['concurrent']
Performance,java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: is.hail.asm4s.AsmFunction2; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:13982,load,loadOrDefineClass,13982,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['loadOrDefineClass']
Performance,java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218659,concurren,concurrent,218659,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$15(ServiceBackend.scala:225); 	at scala.util.Try$.apply(Try.scala:213); 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:995); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. The driver will have log output like this:; ```; 2023-09-22 19:11:13.051 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/8042383 response 200; 2023-09-22 19:11:13.052 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=: reading results; 2023-09-22 19:11:13.125 ServiceBackend$: INFO: all results read. 0.072746861 s. 0.0 result/s. 0.0 MiB/s.; 2023-09-22 19:11:13.125 : INFO: [collectDArray|table_native_writer]: executed 5 tasks in 1.822s; 2023-09-22 19:11:13.126 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.126 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:2197,concurren,concurrent,2197,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$15(ServiceBackend.scala:225); 	at scala.util.Try$.apply(Try.scala:213); 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:995); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. but the worker looks like this:; ```; 2023-09-22 19:11:12.125 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 2: /batch/fe537a243a3046d29d76861ffee94b92; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 3: /batch/fe537a243a3046d29d76861ffee94b92/log; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 4: gs://hail-query-ger0g/jars/b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 5: worker; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 6: gs://1-day/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:6195,concurren,concurrent,6195,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 : INFO: TaskReport: stage=0, partition=38854, attempt=0, peakBytes=65536, peakBytesReadable=64.00 KiB, chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:208) ~[scala-library-2.12.15.jar:?]; 	at is.hail.io.StreamBlockInputBuffer.readBlock(InputBuffers.scala:552) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:384) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e089",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:2761,concurren,concurrent,2761,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,2,['concurren'],['concurrent']
Performance,jects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:17: WARNING: py:data reference target not found: int64; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:19: WARNING: py:data reference target not found: float32; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.index_cols:17: WARNING: py:meth reference target not found: index_cols(exprs); /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.index_rows:17: WARNING: py:meth reference target not found: index_rows(exprs); /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.make_table:25: WARNING: py:func reference target not found: make_table; /Users/dking/projects/hail/hail/python/hail/table.py:docstring of hail.table.Table.cache:11: WARNING: py:func reference target not found: hail.Table.persist; /Users/dking/projects/hail/hail/python/hail/table.py:docstring of hail.table.Table.order_by:37: WARNING: py:class reference target not found: Ascending; /Users/dking/projects/hail/hail/python/hail/table.py:docstring of hail.table.Table.order_by:37: WARNING: py:class reference target not found: Descending; /Users/dking/projects/hail/hail/python/hail/table.py:docstring of hail.table.Table.parallelize:14: WARNING: py:class reference target not found: HailType:; /Users/dking/projects/hail/hail/python/hail/docs/linalg/hail.linalg.BlockMatrix.rst:49:<autosummary>:1: WARNING: py:obj reference target not found: hail.linalg.BlockMatrix.element_type; /Users/dking/projects/hail/hail/python/hail/linalg/blockmatrix.py:docstring of hail.linalg.BlockMatrix.export:133: WARNING: py:class reference target not found: str) -- Describes which entries to export. One of:; `; /Users/dking/projects/hail/hail/python/hail/linalg/blockmatrix.py:docstring of hail.linalg,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9403#issuecomment-685996666:16032,cache,cache,16032,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-685996666,2,['cache'],['cache']
Performance,"jects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . ~/projects/hail/python/hail/table.py in _process_joins(self, *exprs); 1463 def broadcast_f(left, data, jt):; 1464 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1465 return process_joins(self, exprs, broadcast_f); 1466 ; 1467 def cache(self):. ~/projects/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 365 all_uids.extend(list(t)); 366 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); --> 367 data_json = t._to_json(data); 368 left = broadcast_f(left, data_json, t._jtype); 369 . ~/projects/hail/python/hail/expr/types.py in _to_json(self, x); 176 def _to_json(self, x):; 177 converted = self._convert_to_json_na(x); --> 178 return json.dumps(converted); 179 ; 180 def _convert_to_json_na(self, x):. ~/anaconda2/envs/hail/lib/python3.6/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw); 229 cls is None and indent is None and separators is None and; 230 default is None and not sort_keys and not kw):; --> 231 return _default_encoder.encode(obj); 232 if cls is None:; 233 cls = JSONEncoder. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in encode(self, o); 1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3708:3229,cache,cache,3229,https://hail.is,https://github.com/hail-is/hail/issues/3708,1,['cache'],['cache']
Performance,jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:37252,cache,cached,37252,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,jre/include -I/etc/alternatives/jre/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux FS.cpp -MG -M -MF build/FS.d -MT build/FS.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Encoder.cpp -MG -M -MF build/Encoder.d -MT build/Encoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Decoder.cpp -MG -M -MF build/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_te; st.o; make[1]: Leaving directory `/mnt/tmp/hail/hail/src/main/c'; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/NativeBoot.d -MT build/NativeBoot.o -c NativeBoot.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alte,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:9488,cache,cache-tests,9488,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cache-tests']
Performance,"json (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2150"">#2150</a>) (<a href=""https://github.com/googleapis/java-storage/commit/330e795040592e5df22d44fb5216ad7cf2448e81"">330e795</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.14.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2151"">#2151</a>) (<a href=""https://github.com/googleapis/java-storage/commit/eba8b6a235919a27d1f6dadf770140c7d143aa1a"">eba8b6a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.24.0...v2.25.0"">2.25.0</a> (2023-07-24)</h2>; <h3>Features</h3>; <ul>; <li>BlobWriteChannelV2 - same throughput less GC (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2110"">#2110</a>) (<a href=""https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b"">1b52a10</a>)</li>; <li>Update Storage.createFrom(BlobInfo, Path) to have 150% higher throughput (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2059"">#2059</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4c2f44e28a1ff19ffb2a02e3cefc062a1dd98fdc"">4c2f44e</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>Update BlobWriteChannelV2 to properly carry forward offset after incremental flush (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2125"">#2125</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c099a2f4f8ea9afa6953270876653916b021fd9f"">c099a2f</a>)</li>; <li>Update GrpcStorageImpl.createFrom(BlobInfo, Path) to use RewindableContent (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2112"">#2112</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c80505129baa831e492a5514e937875407211595"">c805051</a>)</li>; </ul>; <h3>Documentation</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:8129,throughput,throughput,8129,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['throughput'],['throughput']
Performance,"k ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:14496,cache,cache,14496,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['cache'],['cache']
Performance,"k.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) t org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.sparkapache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.sparkitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDDputeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MaD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpointla:90) at org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:4cala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$ailureException: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/t sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.javannel(UnixFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:40rage.BlockManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.rator$$anon$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.sparkler.processFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.Ab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:7377,concurren,concurrent,7377,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,k.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:139); at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1063); at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1127); at is.hail.io.vcf.MatrixVCFReader.coercer$lzycompute(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.coercer(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.apply(LoadVCF.scala:1008); at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:647); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:57); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:32); at is.hail.variant.MatrixTable.write(MatrixTable.scala:1238); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Threa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:7282,Load,LoadVCF,7282,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['Load'],['LoadVCF']
Performance,k.serialization.ScalaValueWriter.write(ScalaValueWriter.scala:46); 	at org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:53); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:58); 	at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:168); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:67); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4250:5414,concurren,concurrent,5414,https://hail.is,https://github.com/hail-is/hail/issues/4250,1,['concurren'],['concurrent']
Performance,"k.serialization.ScalaValueWriter.write(ScalaValueWriter.scala:46); 	at org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:53); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:58); 	at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:168); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:67); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-408f188; Error summary: EsHadoopIllegalArgumentException: Spark SQL types are not handled through basic RDD saveToEs() calls; typically this is a mistake(as the SQL schema will be ignored). Use 'org.elasticsearch.spark.sql' package instead; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [ffc9fb0b99f64080b674ab7a07962df9] entered state [ERROR] while waiting for [DONE].; ```. Ideally it would get exported as nested objects: https://www.elastic.co/guide/en/elasticsearch/reference/current/nested.html#_using_literal_nested_literal_fields_for_arrays_of_objects. with elasticsearch mapping:; ```; u'vep': {'type': 'nested', 'properties': {u'category': {'type': 'keyword'}, u'major_consequence': {'type': 'keyword'}, u'gene_id': {'type': 'keyword'}, u'major_consequence_rank': {'type': 'integer'}, u'gene_symbol': {'type': 'keyword'}, u'transcript_id': {'type': 'keyword'}, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4250:10968,concurren,concurrent,10968,https://hail.is,https://github.com/hail-is/hail/issues/4250,1,['concurren'],['concurrent']
Performance,k.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19196,Load,LoadVCF,19196,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,kBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.600ms self 0.007ms children 0.594ms %children 98.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.575ms self 0.049ms children 0.527ms %children 91.53%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:180222,Optimiz,OptimizePass,180222,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,kBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.601ms self 0.006ms children 0.595ms %children 99.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.577ms self 0.041ms children 0.536ms %children 92.90%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:187247,Optimiz,OptimizePass,187247,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,kBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 1.157ms self 0.010ms children 1.147ms %children 99.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.096ms self 0.044ms children 1.052ms %children 96.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:13957,Optimiz,OptimizePass,13957,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,kSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7289,Optimiz,Optimize,7289,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['Optimize']
Performance,kage$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3040:6622,concurren,concurrent,6622,https://hail.is,https://github.com/hail-is/hail/issues/3040,1,['concurren'],['concurrent']
Performance,ke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:7885,Load,LoadVCF,7885,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,ke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). java.lang.ClassFormatError: Too many arguments in method signature in class file __C2866stream; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:11789,load,loadOrDefineClass,11789,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['load'],['loadOrDefineClass']
Performance,ke.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8338:11869,Optimiz,Optimize,11869,https://hail.is,https://github.com/hail-is/hail/issues/8338,1,['Optimiz'],['Optimize']
Performance,"kend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 20.0 in stage 0.0 (TID 20, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:76424,concurren,concurrent,76424,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"kend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 4; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 5 on scc-q08.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 32.0 in stage 0.0 (TID 32, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:91892,concurren,concurrent,91892,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"kend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 7; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 4 on scc-q07.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 23.0 in stage 0.0 (TID 23, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:84158,concurren,concurrent,84158,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,kend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:116875,concurren,concurrent,116875,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"kend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 35.0 in stage 0.0 (TID 35, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:126968,concurren,concurrent,126968,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,kend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.032ms children 0.070ms %children 68.44%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:216736,Optimiz,Optimize,216736,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,kend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.108ms self 0.034ms children 0.074ms %children 68.32%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.067ms self 0.067ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:200763,Optimiz,Optimize,200763,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,kend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.101ms self 0.033ms children 0.068ms %children 67.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:208715,Optimiz,Optimize,208715,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,kend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:196111,Optimiz,Optimize,196111,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"kendUtils.scala:52); app//is.hail.backend.BackendUtils$Lambda$783/0x000000080080c040.apply(Unknown Source); app//is.hail.utils.package$.using(package.scala:635); app//is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); app//is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51); app//is.hail.backend.BackendUtils$Lambda$757/0x00000008007bcc40.apply(Unknown Source); app//is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:751); app//org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); app//org.apache.spark.rdd.RDD.iterator(RDD.scala:329); app//org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); app//org.apache.spark.scheduler.Task.run(Task.scala:136); app//org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); app//org.apache.spark.executor.Executor$TaskRunner$Lambda$608/0x0000000800652c40.apply(Unknown Source); app//org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); app//org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); java.base@11.0.17/java.lang.Thread.run(Thread.java:829); ```. A few things:; 1. Verify that this case statement is evaluated intelligently. In particular, we really want to evaluate each predicate once, and only if necessary.; 2. We *should not allocate* just to evaluate these reference genome predicates, but that is [exactly what we do](https://github.com/hail-is/hail/blob/main/hail/src/main/scala/is/hail/expr/ir/functions/LocusFunctions.scala#L67-L72). It seems like the right fix is for the ReferenceGenome's intervals to be shipped as literals so that we can perform `inXPar` or `isAutosomal` checks without allocating contig strings or locus objects. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13862:3582,concurren,concurrent,3582,https://hail.is,https://github.com/hail-is/hail/issues/13862,3,"['concurren', 'perform']","['concurrent', 'perform']"
Performance,"ker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95""],""grace"":""48h"",""recursive"":true,""tag_filter_all"":""cache-pr-.*""}; ```. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:2063,cache,cache-pr,2063,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['cache'],['cache-pr']
Performance,kingOutputStream.scala:23); 	at is.hail.io.index.IndexWriterUtils.close(IndexWriter.scala:225); 	at __C1756collect_distributed_array_table_native_writer.apply_region99_120(Unknown Source); 	at __C1756collect_distributed_array_table_native_writer.apply_region5_223(Unknown Source); 	at __C1756collect_distributed_array_table_native_writer.apply(Unknown Source); 	at __C1756collect_distributed_array_table_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$16(BackendUtils.scala:91); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:172); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.125-6e6f46797aed; Error summary: NullPointerException: null; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:9616,concurren,concurrent,9616,https://hail.is,https://github.com/hail-is/hail/issues/13937,6,['concurren'],['concurrent']
Performance,"ks / 384.0K chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": """,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:10088,concurren,concurrent,10088,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"ks requested=51, cache hits=0; 2023-09-24 01:58:51.513 : INFO: RegionPool: FREE: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	| ; 	at is.hail.relocated.com.google.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:4560,concurren,concurrent,4560,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['concurren'],['concurrent']
Performance,l (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:37868,cache,cached,37868,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"l 19 15:54 /usr/bin/spark-shell; /usr/bin/spark-shell; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-shell; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.21; Branch ; Compiled by user release on 2023-07-19T15:12:33Z; Revision ; Url ; Type --help for more information.; ```. ```sh; -rwxr-xr-x 1 root root 141 Jul 19 15:54 /usr/bin/spark-submit; /usr/bin/spark-submit; -rwxr-xr-x 1 root root 141 Jul 19 15:54 /usr/bin/spark-submit; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.21; Branch ; Compiled by user release on 2023-07-19T15:12:33Z; Revision ; Url ; Type --help for more information.; ```. ```sh; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-class; /usr/bin/spark-class; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-class; SPARK_SCALA_VERSION=; ```; <details><summary>>>>>>>>>>> before load-spark-env.sh <<<<<<<<<</summary>; <p>; ```sh; XDG_SESSION_ID=38; HOSTNAME=ip-192-168-96-172; TERM=xterm-256color; SHELL=/bin/bash; HISTSIZE=1000; SSH_CLIENT=103.37.196.84 57805 22; QTDIR=/usr/lib64/qt-3.3; QTINC=/usr/lib64/qt-3.3/include; SSH_TTY=/dev/pts/0; USER=hadoop; LS_COLORS=rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045:1286,load,load-spark-env,1286,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045,1,['load'],['load-spark-env']
Performance,"l upgrades. We also encourage upgrading to MarkupSafe 2.1.1, the latest version at this time.</p>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-0"">https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-0</a></li>; <li>Milestone: <a href=""https://github.com/pallets/jinja/milestone/8?closed=1"">https://github.com/pallets/jinja/milestone/8?closed=1</a></li>; <li>MarkupSafe changes: <a href=""https://markupsafe.palletsprojects.com/en/2.1.x/changes/#version-2-1-1"">https://markupsafe.palletsprojects.com/en/2.1.x/changes/#version-2-1-1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/main/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 3.1.2</h2>; <p>Released 2022-04-28</p>; <ul>; <li>Add parameters to <code>Environment.overlay</code> to match <code>__init__</code>.; :issue:<code>1645</code></li>; <li>Handle race condition in <code>FileSystemBytecodeCache</code>. :issue:<code>1654</code></li>; </ul>; <h2>Version 3.1.1</h2>; <p>Released 2022-03-25</p>; <ul>; <li>The template filename on Windows uses the primary path separator.; :issue:<code>1637</code></li>; </ul>; <h2>Version 3.1.0</h2>; <p>Released 2022-03-24</p>; <ul>; <li>; <p>Drop support for Python 3.6. :pr:<code>1534</code></p>; </li>; <li>; <p>Remove previously deprecated code. :pr:<code>1544</code></p>; <ul>; <li><code>WithExtension</code> and <code>AutoEscapeExtension</code> are built-in now.</li>; <li><code>contextfilter</code> and <code>contextfunction</code> are replaced by; <code>pass_context</code>. <code>evalcontextfilter</code> and; <code>evalcontextfunction</code> are replaced by <code>pass_eval_context</code>.; <code>environmentfilter</code> and <code>environmentfunction</code> are replaced; by <code>pass_environment</code>.</li>; <li><code>Markup</code> and <code>escape</code> should be imported from MarkupSafe.</li>; <li>Comp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12173:2438,race condition,race condition,2438,https://hail.is,https://github.com/hail-is/hail/pull/12173,1,['race condition'],['race condition']
Performance,l$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.gua,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216577,concurren,concurrent,216577,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,l$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217999,concurren,concurrent,217999,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"l.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213625,concurren,concurrent,213625,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"l.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(Yarn",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213955,concurren,concurrent,213955,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"l.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6271,Load,LoadVCF,6271,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,l.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6199,Optimiz,Optimize,6199,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,l.expr.ir.Optimize.apply total 29.693ms self 9.419ms children 20.274ms %children 68.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 4.611ms self 3.590ms children 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:3691,Optimiz,OptimizePass,3691,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,l.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.137ms self 0.137ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.862ms self 0.753ms children 1.109ms %children 59.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.743ms self 0.743ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:72883,Optimiz,Optimize,72883,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,l.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.010ms self 0.828ms children 1.182ms %children 58.80%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.736ms self 0.736ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:58485,Optimiz,Optimize,58485,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,l.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.144ms self 0.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.561ms self 0.732ms children 0.829ms %children 53.12%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.525ms self 0.525ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:108500,Optimiz,Optimize,108500,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,l.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.222ms self 0.222ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.451ms self 0.949ms children 1.502ms %children 61.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.977ms self 0.977ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:49957,Optimiz,Optimize,49957,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,l.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.292ms self 0.292ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.500ms self 0.649ms children 0.852ms %children 56.76%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.557ms self 0.557ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:94062,Optimiz,Optimize,94062,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,l.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6124,Load,LoadPlink,6124,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"l; sp.check_output(args, stderr=sp.STDOUT, **kwargs); File ""/opt/conda/default/lib/python3.11/subprocess.py"", line 466, in check_output; return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/opt/conda/default/lib/python3.11/subprocess.py"", line 571, in run; raise CalledProcessError(retcode, process.args,; subprocess.CalledProcessError: Command '('pip', 'install', 'setuptools', 'mkl<2020', 'lxml<5', 'https://github.com/hail-is/jgscm/archive/v0.1.13+hail.zip', 'ipykernel==6.22.0', 'ipywidgets==8.0.6', 'jupyter-console==6.6.3', 'nbconvert==7.3.1', 'notebook==6.5.6', 'qtconsole==5.4.2', 'aiodns==2.0.0', 'aiohttp==3.9.5', 'aiosignal==1.3.1', 'async-timeout==4.0.3', 'attrs==23.2.0', 'avro==1.11.3', 'azure-common==1.1.28', 'azure-core==1.30.2', 'azure-identity==1.17.1', 'azure-mgmt-core==1.4.0', 'azure-mgmt-storage==20.1.0', 'azure-storage-blob==12.20.0', 'bokeh==3.3.4', 'boto3==1.34.138', 'botocore==1.34.138', 'cachetools==5.3.3', 'certifi==2024.6.2', 'cffi==1.16.0', 'charset-normalizer==3.3.2', 'click==8.1.7', 'commonmark==0.9.1', 'contourpy==1.2.1', 'cryptography==42.0.8', 'decorator==4.4.2', 'deprecated==1.2.14', 'dill==0.3.8', 'frozenlist==1.4.1', 'google-auth==2.31.0', 'google-auth-oauthlib==0.8.0', 'humanize==1.1.0', 'idna==3.7', 'isodate==0.6.1', 'janus==1.0.0', 'jinja2==3.1.4', 'jmespath==1.0.1', 'jproperties==2.1.1', 'markupsafe==2.1.5', 'msal==1.29.0', 'msal-extensions==1.2.0', 'msrest==0.7.1', 'multidict==6.0.5', 'nest-asyncio==1.6.0', 'numpy==1.26.4', 'oauthlib==3.2.2', 'orjson==3.10.6', 'packaging==24.1', 'pandas==2.2.2', 'parsimonious==0.10.0', 'pillow==10.4.0', 'plotly==5.22.0', 'portalocker==2.10.0', 'protobuf==3.20.2', 'py4j==0.10.9.7', 'pyasn1==0.6.0', 'pyasn1-modules==0.4.0', 'pycares==4.4.0', 'pycparser==2.22', 'pygments==2.18.0', 'pyjwt==2.8.0', 'python-dateutil==2.9.0.post0', 'python-json-logger==2.0.7', 'pytz==2024.1', 'pyyaml==6.0.1', 'regex==2024.5.15', 'requests==2.32.3'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14652:5944,cache,cachetools,5944,https://hail.is,https://github.com/hail-is/hail/issues/14652,1,['cache'],['cachetools']
Performance,"lHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatche",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213528,concurren,concurrent,213528,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,lLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.635ms self 0.635ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.295ms self 0.295ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.033ms self 0.033ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:99070,Optimiz,OptimizePass,99070,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.641ms self 0.641ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.241ms self 0.241ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.039ms self 0.039ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:113508,Optimiz,OptimizePass,113508,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.756ms self 0.756ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.409ms self 0.409ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:63493,Optimiz,OptimizePass,63493,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.830ms self 0.830ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.348ms self 0.348ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.056ms self 0.056ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:77891,Optimiz,OptimizePass,77891,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.885ms self 0.885ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.450ms self 0.450ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.396ms self 0.396ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:54965,Optimiz,OptimizePass,54965,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.703ms self 4.703ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.607ms self 0.607ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.447ms self 0.447ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:46437,Optimiz,OptimizePass,46437,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 47.463ms self 0.201ms children 47.262ms %children 99.58%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 6.134ms self 6.134ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.135ms self 0.135ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:38630,Optimiz,Optimize,38630,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,la:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_matrix_ir$2(SparkBackend.scala:689); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:69); 	at is.hail.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6465,Load,LoadPlink,6465,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,la:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3276:2518,concurren,concurrent,2518,https://hail.is,https://github.com/hail-is/hail/issues/3276,2,['concurren'],['concurrent']
Performance,"la:630); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:848); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open databa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:15311,cache,cache,15311,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['cache'],['cache']
Performance,"la:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.close(ServiceCacheableFS.scala:46); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 	at is.hail.utils.package$.using(package.scala:658); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$5(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2194/482268176.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$4(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2191/716305671.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). ""pool-2-thread-1"" #26 prio=5 os_prio=0 tid=0x00007f502866e000 nid=0x88c waiting on condition [0x00007f50275fd000]; java.lang.Thread.State: TIM",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:3049,concurren,concurrent,3049,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,la:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:3302,concurren,concurrent,3302,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['concurren'],['concurrent']
Performance,la:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:12480,load,loadClass,12480,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['load'],['loadClass']
Performance,"lang.Thread.run(Thread.java:745); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 35 in stage 7.0 failed 20 times, most recent failure: Lost task 35.19 in stage 7.0 (TID 6963, gnomad-prod-sw-m8lk.c.broad-mpg-gnomad.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$han",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:4784,concurren,concurrent,4784,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['concurren'],['concurrent']
Performance,latest test yields:; ```; write aggregate-throughput: 0.263 GiB/s; read aggregate-throughput: 0.083 GiB/s; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7593#issuecomment-558340651:42,throughput,throughput,42,https://hail.is,https://github.com/hail-is/hail/pull/7593#issuecomment-558340651,2,['throughput'],['throughput']
Performance,"lation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting deco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1783,cache,cached,1783,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,lationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.137ms self 0.137ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.862ms self 0.753ms children 1.109ms %children 59.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.743ms self 0.743ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:72854,Optimiz,OptimizePass,72854,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.010ms self 0.828ms children 1.182ms %children 58.80%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.736ms self 0.736ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:58456,Optimiz,OptimizePass,58456,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.144ms self 0.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.561ms self 0.732ms children 0.829ms %children 53.12%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.525ms self 0.525ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:108471,Optimiz,OptimizePass,108471,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.222ms self 0.222ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.451ms self 0.949ms children 1.502ms %children 61.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.977ms self 0.977ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:49928,Optimiz,OptimizePass,49928,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.292ms self 0.292ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.500ms self 0.649ms children 0.852ms %children 56.76%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.557ms self 0.557ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:94033,Optimiz,OptimizePass,94033,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"lazy, meaning it's an RDD. The computation to produce row loadings won't happen unless you perform an action on it, like count or something. Passing a reference through Python is fine.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2454#issuecomment-348587603:58,load,loadings,58,https://hail.is,https://github.com/hail-is/hail/pull/2454#issuecomment-348587603,2,"['load', 'perform']","['loadings', 'perform']"
Performance,"lctl/describe.py:104 in describe │; │ │; │ 101 │ ''' │; │ 102 │ Describe the MatrixTable or Table at path FILE. │; │ 103 │ ''' │; │ ❱ 104 │ asyncio.get_event_loop().run_until_complete(async_describe(file, requester_pays_proj │; │ 105 │; │ 106 │; │ 107 async def async_describe( │; │ │; │ /opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.1 │; │ 0/asyncio/base_events.py:649 in run_until_complete │; │ │; │ 646 │ │ if not future.done(): │; │ 647 │ │ │ raise RuntimeError('Event loop stopped before Future completed.') │; │ 648 │ │ │; │ ❱ 649 │ │ return future.result() │; │ 650 │ │; │ 651 │ def stop(self): │; │ 652 │ │ """"""Stop running the event loop. │; │ │; │ /Users/cvittal/src/hail/hail/python/hailtop/hailctl/describe.py:119 in async_describe │; │ │; │ 116 │ │ gcs_kwargs['project'] = requester_pays_project_id │; │ 117 │ │; │ 118 │ async with aio_contextlib.closing(RouterAsyncFS(gcs_kwargs=gcs_kwargs)) as fs: │; │ ❱ 119 │ │ j = orjson.loads(decompress(await fs.read(path.join(file, 'metadata.json.gz')), │; │ 120 │ │ │; │ 121 │ │ # Get the file schema │; │ 122 │ │ file_schema = parse_schema(j[next(k for k in j.keys() if k.endswith('type'))]) │; │ │; │ /Users/cvittal/src/hail/hail/python/hailtop/aiotools/fs/fs.py:281 in read │; │ │; │ 278 │ │ │ pass │; │ 279 │ │; │ 280 │ async def read(self, url: str) -> bytes: │; │ ❱ 281 │ │ async with await self.open(url) as f: │; │ 282 │ │ │ return await f.read() │; │ 283 │ │; │ 284 │ async def read_from(self, url: str, start: int) -> bytes: │; │ │; │ /Users/cvittal/src/hail/hail/python/hailtop/aiotools/router_fs.py:75 in open │; │ │; │ 72 │ │ return self._load_fs(uri) │; │ 73 │ │; │ 74 │ async def open(self, url: str) -> ReadableStream: │; │ ❱ 75 │ │ fs = self._get_fs(url) │; │ 76 │ │ return await fs.open(url) │; │ 77 │ │; │ 78 │ async def _open_from(self, url: str, start: int, *, length: Optional[int] = None) -> │; │ │; │ /Users/cvittal/src/hail/hail/python/hailtop/aiotools/router_fs.py:72 in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13793:1542,load,loads,1542,https://hail.is,https://github.com/hail-is/hail/issues/13793,1,['load'],['loads']
Performance,lczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.readInt(ServiceBackend.scala:510); 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:561); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:462); 	at is.hail.backend.service.ServiceBackendSocketAP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:4569,concurren,concurrent,4569,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,"le if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremental structures all line up at the end of the tests. The only issue I see is I loop through all running batches for a user in the scheduler. If a user submits many many small batches, this could be an issue. I plan to address this in a later PR. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7933:1865,load,load,1865,https://hail.is,https://github.com/hail-is/hail/pull/7933,1,['load'],['load']
Performance,"le with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2585,Load,LoadVCF,2585,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Load'],['LoadVCF']
Performance,leAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.600ms self 0.007ms children 0.594ms %children 98.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.575ms self 0.049ms children 0.527ms %children 91.53%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPip,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:180532,Optimiz,OptimizePass,180532,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,leAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.601ms self 0.006ms children 0.595ms %children 99.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.577ms self 0.041ms children 0.536ms %children 92.90%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPip,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:187557,Optimiz,OptimizePass,187557,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,leAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 1.157ms self 0.010ms children 1.147ms %children 99.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.096ms self 0.044ms children 1.052ms %children 96.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPip,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:14267,Optimiz,OptimizePass,14267,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"leElement 0 (Ref __iruid_490))))))\n (Let __iruid_491\n (MakeTuple (0)\n (ResultOp 0 (Collect (CollectStateSig +PInt32))))\n (MakeStruct\n (idx (GetTupleElement 0 (Ref __iruid_491)))))))))\n2022-11-15 20:30:18.180 root: INFO: optimize optimize: compileLowerer, after InlineApplyIR: before: IR size 56: \n(MakeTuple (0)\n (Let __iruid_484\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (CollectStateSig +PInt32)) ()))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32))))\n (Let __iruid_485\n (CollectDistributedArray\n table_aggregate_singlestage __iruid_486\n __iruid_487\n (ToStream False\n (Literal Array[Struct{start:Int32,end:Int32}]\n <literal value>))\n (MakeStruct (__iruid_458 (Ref __iruid_484)))\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0\n (GetField __iruid_458 (Ref __iruid_487)))))\n (StreamFor __iruid_488\n (StreamMap __iruid_489\n (StreamRange -1 True\n (GetField start (Ref __iruid_486))\n (GetField end (Ref __iruid_486))\n (I32 1))\n (MakeStruct (idx (Ref __iruid_489))))\n (Begin\n (SeqOp 0 (Collect (CollectStateSig +PInt32))\n ((GetField idx (Ref __iruid_488)))))))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32))))\n (NA String))\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0 (Ref __iruid_484))))\n (StreamFor __iruid_490\n (ToStream True (Ref __iruid_485))\n (Begin\n (CombOpValue 0 (Collect (CollectStateSig +PInt32))\n (GetTupleElement 0 (Ref __iruid_490))))))\n (Let __iruid_491\n (MakeTuple (0)\n (ResultOp 0 (Collect (CollectStateSig +PInt32))))\n (MakeStruct\n (idx (GetTupleElement 0 (Ref __iruid_491)))))))))\n2022-11-15 20:30:18.191 root: INFO: optimize optimize: compileLowerer, after InlineApplyIR: after: IR size 56:\n(MakeTuple (0)\n (Let __iruid_508\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (CollectStateSig",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:6316,optimiz,optimize,6316,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,4,['optimiz'],['optimize']
Performance,lePass/is.hail.expr.ir.lowering.CompilableIR total 0.037ms self 0.037ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.839ms self 0.009ms children 0.830ms %children 98.97%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:168366,Optimiz,OptimizePass,168366,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"leQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-63d60cc; Error summary: HailException: invalid allele ""<DEL>"". ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:9512,concurren,concurrent,9512,https://hail.is,https://github.com/hail-is/hail/issues/3413,2,['concurren'],['concurrent']
Performance,"leases"">orjson's releases</a>.</em></p>; <blockquote>; <h2>3.10.0</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:1185,optimiz,optimization,1185,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['optimiz'],['optimization']
Performance,"lect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz: caught java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(Ro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:9628,Load,LoadVCF,9628,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"lected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cach",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1542,cache,cached,1542,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,lection.AbstractTraversable.map(Traversable.scala:108); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:113); at is.hail.expr.ir.Pretty.header(Pretty.scala:405); at is.hail.expr.ir.Pretty.pretty$1(Pretty.scala:463); at is.hail.expr.ir.Pretty.$anonfun$sexprStyle$4(Pretty.scala:453); at scala.collection.Iterator$$anon$10.next(Iterator.scala:459); at scala.collection.Iterator$ConcatIterator.next(Iterator.scala:230); at is.hail.utils.richUtils.RichIterator$$anon$3.next(RichIterator.scala:67); at is.hail.utils.prettyPrint.Doc$.advance$1(PrettyPrintWriter.scala:68); at is.hail.utils.prettyPrint.Doc$.render(PrettyPrintWriter.scala:139); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:163); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:167); at is.hail.expr.ir.Pretty.sexprStyle(Pretty.scala:466); at is.hail.expr.ir.Pretty.apply(Pretty.scala:429); at is.hail.expr.ir.Pretty$.apply(Pretty.scala:22); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:45); at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(Wrapped,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:2769,Optimiz,Optimize,2769,https://hail.is,https://github.com/hail-is/hail/issues/13046,1,['Optimiz'],['Optimize']
Performance,ler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.019ms self 0.016ms children 0.003ms %children 16.42%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:218314,Optimiz,OptimizePass,218314,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:210293,Optimiz,OptimizePass,210293,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:202341,Optimiz,OptimizePass,202341,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.024ms self 0.021ms children 0.003ms %children 13.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:197689,Optimiz,OptimizePass,197689,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"les in the batch-driver python process. By using proper persistent connections, we can reduce the TLS overhead to single-digit percents of a core. This leads to the first goal of this transition: configure our load balancers to know the full cluster configuration at any point in time so they can properly maintain connection pools with upstream services. However, this is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are create",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:2999,load,load-balancer,2999,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['load'],['load-balancer']
Performance,less to optimize.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3333:8,optimiz,optimize,8,https://hail.is,https://github.com/hail-is/hail/issues/3333,1,['optimiz'],['optimize']
Performance,lete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactiv,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218898,concurren,concurrent,218898,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,lf 0.009ms children 0.830ms %children 98.97%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.809ms self 0.073ms children 0.736ms %children 91.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.280ms self 0.280ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:169391,Optimiz,Optimize,169391,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,lf 0.010ms children 1.356ms %children 99.24%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.029ms self 0.029ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.303ms self 0.051ms children 1.252ms %children 96.10%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.383ms self 0.383ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:26074,Optimiz,Optimize,26074,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,lf 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.809ms self 0.073ms children 0.736ms %children 91.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.280ms self 0.280ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:169905,Optimiz,Optimize,169905,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,lf 0.029ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.303ms self 0.051ms children 1.252ms %children 96.10%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.383ms self 0.383ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:26588,Optimiz,Optimize,26588,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,lhost): org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'; 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:247); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 6.0.0; 	at org.elasticsearch.hadoop.util.EsMajorVersion.parse(EsMajorVersion.java:79); 	at org.elasticsearch.hadoop.rest.RestClient.remoteEsVersion(RestClient.java:613); 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:240); 	... 10 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(Array,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4138:2579,concurren,concurrent,2579,https://hail.is,https://github.com/hail-is/hail/issues/4138,1,['concurren'],['concurrent']
Performance,line#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.lowering.MatrixLoweredToTable total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.lowering.CompilableIR total 0.007ms self 0.007ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.601ms self 0.006ms children 0.595ms %children 99.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.577ms self 0.041ms children 0.536ms %children 92.90%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.256ms self 0.256ms childr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:186659,Optimiz,OptimizePass,186659,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"linux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5874,cache,cached,5874,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"listing all batches readble by that user. This change fixes that; regression by making use index hints and `STRAIGHT_JOIN`s. The index hint tells MySQL to never consider the index `batches_deleted` as it; has very low cardinality. In some forms of this query, the planner tries to use; it to its peril. A problem in query 0 with #14629 (see below) was that fewer filters on batches; made the optimiser consider joins in a suboptimal order - it did a table scan ; on `job_groups` first then sorted the results by to `batches.id DESC` instead; of doing an index scan on `batches` in reverse. Using `STRAIGHT_JOIN`s instead of `INNER JOIN` mades the optimiser start from; `batches` and read its index in reverse before considering other tables in ; subsequent joins. From the [documentation](https://dev.mysql.com/doc/refman/8.4/en/join.html):. > STRAIGHT_JOIN is similar to JOIN, except that the left table is always read; before the right table. This can be used for those (few) cases for which the; join optimizer processes the tables in a suboptimal order. This is advantageous for a couple of reasons:; - We want to list newer batches first; - For this query, the `batches` table has more applicables indexes; - We want the variable to order by to be in the primary key of the first; table so we can read the index in reverse. Before and after timings, collected by running the query 5 times, then using; profiles gathered by MySQL.; ```; +-------+---------------------------------------------------*; | query | description | ; +-------+---------------------------------------------------+; | 0 | All batches accessible to user `ci` |; | 1 | All batches accessible to user `ci` owned by `ci` |; +-------+---------------------------------------------------*. +-------+--------+--------------------------------------------------------+------------+------------+; | query | branch | timings | mean | stdev | ; +-------+--------+--------------------------------------------------------+------------+----",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14649:1097,optimiz,optimizer,1097,https://hail.is,https://github.com/hail-is/hail/pull/14649,1,['optimiz'],['optimizer']
Performance,"llecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6449,cache,cached,6449,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"llecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:7293,cache,cached,7293,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,llecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39608,cache,cached,39608,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,llecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39975,cache,cached,39975,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"lled from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapRows.execute(MatrixIR.scala:1352); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:18",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733:1299,load,loadLibrary,1299,https://hail.is,https://github.com/hail-is/hail/issues/4733,1,['load'],['loadLibrary']
Performance,load .gtf and .reg files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9:0,load,load,0,https://hail.is,https://github.com/hail-is/hail/issues/9,1,['load'],['load']
Performance,load BCF files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/35:0,load,load,0,https://hail.is,https://github.com/hail-is/hail/issues/35,1,['load'],['load']
Performance,load FASTA files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6:0,load,load,0,https://hail.is,https://github.com/hail-is/hail/issues/6,1,['load'],['load']
Performance,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7829:0,load,loadField,0,https://hail.is,https://github.com/hail-is/hail/issues/7829,12,['load'],"['load', 'loadElement', 'loadField', 'loadInt', 'loadLength']"
Performance,"loadIRIntermediate load binary and array, so it must be passed the element, but the element offset. It probably shouldn't do that but this was a quicker fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3321:0,load,loadIRIntermediate,0,https://hail.is,https://github.com/hail-is/hail/pull/3321,2,['load'],"['load', 'loadIRIntermediate']"
Performance,"loading a file that has 12 partitions. when setting `hl.init(min_block_size=0)` and then `hl.import_table(..., min_partitions=100)`, now getting only 3 partitions.... ☹️",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5603:0,load,loading,0,https://hail.is,https://github.com/hail-is/hail/issues/5603,1,['load'],['loading']
Performance,loading a second HailContext() does not produce an error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1137:0,load,loading,0,https://hail.is,https://github.com/hail-is/hail/issues/1137,1,['load'],['loading']
Performance,loading pedigree with samples not in dataset fails,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/94:0,load,loading,0,https://hail.is,https://github.com/hail-is/hail/issues/94,1,['load'],['loading']
Performance,lob.specialized.BlobClientBase.getProperties(BlobClientBase.java:1348); 		at is.hail.io.fs.AzureStorageFS.$anonfun$openNoCompression$1(AzureStorageFS.scala:223); 		at is.hail.io.fs.AzureStorageFS.$anonfun$handlePublicAccessError$1(AzureStorageFS.scala:175); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.io.fs.AzureStorageFS.handlePublicAccessError(AzureStorageFS.scala:174); 		at is.hail.io.fs.AzureStorageFS.openNoCompression(AzureStorageFS.scala:220); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:20); 		at is.hail.io.fs.FS.openNoCompression(FS.scala:322); 		at is.hail.io.fs.FS.openNoCompression$(FS.scala:322); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:3); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:459); 		at is.hail.backend.service.Main$.main(Main.scala:15); 		at is.hail.backend.service.Main.main(Main.scala); 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 		at java.lang.reflect.Method.invoke(Method.java:498); 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at java.util.concurrent.FutureTask.run(FutureTask.java:266); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at java.util.concurrent.FutureTask.run(FutureTask.java:266); 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 		... 1 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430:2147,concurren,concurrent,2147,https://hail.is,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430,6,['concurren'],['concurrent']
Performance,"local notebook works fine for me as well, looks to be just dataproc that's not working as expected. submitting that test command as a script finished in 36.2s. notebook is currently still hanging with this output (it's been 11 minutes):; ```; BokehJS 3.2.2 successfully loaded.; Initializing Hail with default parameters...; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; SPARKMONITOR_LISTENER: Started SparkListener for Jupyter Notebook; SPARKMONITOR_LISTENER: Port obtained from environment: 55989; SPARKMONITOR_LISTENER: Application Started: application_1695402030462_0001 ...Start Time: 1695402594764; Running on Apache Spark version 3.3.0; SparkUI available at http://notebook-slowdown-repro-m.c.broad-ctsa.internal:43055/; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-ee7fef6fc40d; LOGGING: writing to /home/hail/hail-20230922-1709-0.2.124-ee7fef6fc40d.log; [Stage 0:> (0 + 2) / 2]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1731791216:270,load,loaded,270,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1731791216,1,['load'],['loaded']
Performance,located.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.close(GoogleStorageFS.scala:306); 		at java.io.FilterOutputStream.close(FilterOutputStrea,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:5835,concurren,concurrent,5835,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['concurren'],['concurrent']
Performance,"logging; import urllib.parse; -from typing import ClassVar, List; +from typing import Any, Dict, List, Optional, TypedDict; ; import aiohttp.web; import google.auth.transport.requests; import google.oauth2.id_token; import google_auth_oauthlib.flow; +import jwt; import msal; ; -from gear.cloud_config import get_global_config; +from hailtop import httpx; +from hailtop.utils import retry_transient_errors; +; +log = logging.getLogger('auth'); ; ; class FlowResult:; - def __init__(self, login_id: str, email: str, token: dict):; + def __init__(self, login_id: str, email: str, refresh_token: str):; self.login_id = login_id; self.email = email; - self.token = token; + self.refresh_token = refresh_token; ; ; class Flow(abc.ABC):; @@ -35,9 +44,24 @@ class Flow(abc.ABC):; """"""Concludes the OAuth2 flow by returning the user's identity and credentials.""""""; raise NotImplementedError; ; + @staticmethod; + @abc.abstractmethod; + def perform_installed_app_login_flow(oauth2_client: Dict[str, Any]) -> Dict[str, Any]:; + """"""Performs an OAuth2 flow for credentials installed on the user's machine.""""""; + raise NotImplementedError; +; + @staticmethod; + @abc.abstractmethod; + async def get_identity_uid_from_access_token(session: httpx.ClientSession, access_token: str, *, oauth2_client: dict) -> Optional[str]:; + """"""; + Validate a user-provided access token. If the token is valid, return the identity; + to which it belongs. If it is not valid, return None.; + """"""; + raise NotImplementedError; +; ; class GoogleFlow(Flow):; - scopes: ClassVar[List[str]] = [; + scopes = [; 'https://www.googleapis.com/auth/userinfo.profile',; 'https://www.googleapis.com/auth/userinfo.email',; 'openid',; @@ -48,7 +72,7 @@ class GoogleFlow(Flow):; ; def initiate_flow(self, redirect_uri: str) -> dict:; flow = google_auth_oauthlib.flow.Flow.from_client_secrets_file(; - self._credentials_file, scopes=self.scopes, state=None; + self._credentials_file, scopes=GoogleFlow.scopes, state=None; ); flow.redirect_uri = redire",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329:1534,Perform,Performs,1534,https://hail.is,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329,1,['Perform'],['Performs']
Performance,"looks like this was on loading an int, which is obviously a little foolish. i don't think it's a bug then, but would be nice to be able to parse this if possible",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4238#issuecomment-417320400:23,load,loading,23,https://hail.is,https://github.com/hail-is/hail/issues/4238#issuecomment-417320400,1,['load'],['loading']
Performance,lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.257ms self 0.257ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.074ms self 0.035ms children 0.040ms %children 53.33%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEv,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:96922,Optimiz,OptimizePass,96922,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.264ms self 0.264ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.073ms self 0.037ms children 0.036ms %children 49.79%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEv,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:111360,Optimiz,OptimizePass,111360,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.318ms self 0.318ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.089ms self 0.043ms children 0.047ms %children 52.21%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEv,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:75743,Optimiz,OptimizePass,75743,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.391ms self 0.391ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.098ms self 0.049ms children 0.049ms %children 50.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEv,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:61345,Optimiz,OptimizePass,61345,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.476ms self 0.476ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.138ms self 0.089ms children 0.049ms %children 35.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEv,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:52817,Optimiz,OptimizePass,52817,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 1.317ms self 1.317ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.144ms self 0.075ms children 0.069ms %children 48.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.069ms self 0.069ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEv,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:44289,Optimiz,OptimizePass,44289,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"lp. In [1]: import hail as hl ; hl.import_; In [2]: t = hl.import_table('/tmp/bar') ; ...: t.describe() ; ...: t = t.key_by('sample_id') ; Initializing Spark and Hail with default parameters...; using hail jar at /usr/local/lib/python3.7/site-packages/hail/hail-all-spark.jar; 19/06/13 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.1; SparkUI available at http://wm06b-953.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.14-5cb00c115421; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; '﻿sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-2-6b119cf7ec41> in <module>; 1 t = hl.import_table('/tmp/bar'); 2 t.describe(); ----> 3 t = t.key_by('sample_id'). </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-958> in key_by(self, *keys, **named_keys). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6342:1865,Load,Loading,1865,https://hail.is,https://github.com/hail-is/hail/issues/6342,1,['Load'],['Loading']
Performance,ls.package$.using(package.scala:640); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:430); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4227,concurren,concurrent,4227,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['concurren'],['concurrent']
Performance,"ls.py:398>>]; path = '/tmp/JnQ2m'. async def rm_dir(pool: OnlineBoundedGather2,; contents_tasks: List[asyncio.Task],; path: str):; assert listener is not None; listener(1); if contents_tasks:; await pool.wait(contents_tasks); try:; > await self.rmdir(path). /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:378: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:352: in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); /usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:162: in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; /usr/lib/python3.9/asyncio/futures.py:284: in __await__; yield self # This tells Task to wait for completion.; /usr/lib/python3.9/asyncio/tasks.py:328: in __wakeup; future.result(); /usr/lib/python3.9/asyncio/futures.py:201: in result; raise self._exception; /usr/lib/python3.9/concurrent/futures/thread.py:58: in run; result = self.fn(*self.args, **self.kwargs); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . > thread_pool, lambda: fun(*args, **kwargs)); E OSError: [Errno 39] Directory not empty: '/tmp/JnQ2m'. /usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:163: OSError. During handling of the above exception, another exception occurred:. self = <hailtop.aiotools.local_fs.LocalAsyncFS object at 0x7f264046f700>; sema = <asyncio.locks.Semaphore object at 0x7f263d7a61c0 [unlocked, value:50]>; url = '/tmp/JnQ2m'; listener = <function LocalAsyncFS.rmtree.<locals>.<lambda> at 0x7f263e041820>. async def rmtree(self,; sema: Optional[asyncio.Semaphore],; url: str,; listener: Optional[Callable[[int], None]] = None) -> None:; path = self._get_path(url); if listener is None:; listener = lambda _: None; if sema is None:; sema = asyncio.Semaphore(50); ; async def rm_file(path: str):; assert listener is not None; listener(1); await self.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:6146,concurren,concurrent,6146,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,ls==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:34747,cache,cached,34747,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"lus/kubernetes_asyncio/pull/218"">#218</a>, <a href=""https://github.com/tomplus""><code>@​tomplus</code></a>)</li>; </ul>; <h1>v24.2.1</h1>; <ul>; <li>fixed watch.stream bug of not working with apis with follow kwarg (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/pull/216"">#216</a>, <a href=""https://github.com/mcreng""><code>@​mcreng</code></a>)</li>; </ul>; <h1>v24.2.0</h1>; <p>Kubernetes API Version: v1.24.2</p>; <h3>API Change</h3>; <ul>; <li>Add 2 new options for kube-proxy running in winkernel mode. <code>--forward-healthcheck-vip</code>, if specified as true, health check traffic whose destination is service VIP will be forwarded to kube-proxy's healthcheck service. <code>--root-hnsendpoint-name</code> specifies the name of the hns endpoint for the root network namespace. This option enables the pass-through load balancers like Google's GCLB to correctly health check the backend services. Without this change, the health check packets is dropped, and Windows node will be considered to be unhealthy by those load balancers. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/99287"">kubernetes/kubernetes#99287</a>, <a href=""https://github.com/anfernee""><code>@​anfernee</code></a>)</li>; <li>Added CEL runtime cost calculation into CustomerResource validation. CustomerResource validation will fail if runtime cost exceeds the budget. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108482"">kubernetes/kubernetes#108482</a>, <a href=""https://github.com/cici37""><code>@​cici37</code></a>)</li>; <li>Added a new metric <code>webhook_fail_open_count</code> to monitor webhooks that fail to open. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/107171"">kubernetes/kubernetes#107171</a>, <a href=""https://github.com/ltagliamonte-dd""><code>@​ltagliamonte-dd</code></a>)</li>; <li>Adds a new Status subresource in Network Policy objects (<a href=""https://github-redirect.dependabot.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:1473,load,load,1473,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['load'],['load']
Performance,ly execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.566ms self 0.566ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.445ms self 0.445ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.252ms self 0.765ms children 1.487ms %children 66.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:121579,Optimiz,OptimizePass,121579,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ly execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.609ms self 0.609ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.195ms self 0.195ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.409ms self 0.648ms children 0.762ms %children 54.04%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:137297,Optimiz,OptimizePass,137297,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ly execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.655ms self 0.655ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.184ms self 0.184ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.228ms self 1.353ms children 0.875ms %children 39.29%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:153084,Optimiz,OptimizePass,153084,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ly total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.123ms self 0.054ms children 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:189666,Optimiz,OptimizePass,189666,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ly total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.114ms self 0.048ms children 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:182641,Optimiz,OptimizePass,182641,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ly total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.315ms self 0.116ms children 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.153ms self 0.153ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.038ms self 0.029ms children 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:16376,Optimiz,OptimizePass,16376,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ly total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.328ms self 0.115ms children 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.164ms self 0.164ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.044ms self 0.035ms children 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:9392,Optimiz,OptimizePass,9392,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ly total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.328ms self 0.115ms children 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.Opti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:8011,Optimiz,Optimize,8011,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ly total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 4.611ms self 3.590ms children 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.442ms self 0.442ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.526ms self 0.526ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:5101,Optimiz,OptimizePass,5101,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ly$20.apply(ContextRDD.scala:280); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-76c42fe; Error summary: ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:9333,concurren,concurrent,9333,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['concurren'],['concurrent']
Performance,ly(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:285); at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-241800222:1781,Cache,CacheManager,1781,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241800222,6,"['Cache', 'concurren']","['CacheManager', 'concurrent']"
Performance,ly(ContextRDD.scala:237); 	at is.hail.sparkextras.ContextRDD$$anonfun$6$$anonfun$apply$11.apply(ContextRDD.scala:235); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:235); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:234); 	at scala.Function1$$anonfun$andThen$1.apply(Function1.scala:52); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```. _All_ of my workers had this error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:4,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:6342,concurren,concurrent,6342,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['concurren'],['concurrent']
Performance,ly/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:7628,Optimiz,OptimizePass,7628,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ly/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.575ms self 0.049ms children 0.527ms %children 91.53%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:180877,Optimiz,OptimizePass,180877,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ly/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.577ms self 0.041ms children 0.536ms %children 92.90%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:187902,Optimiz,OptimizePass,187902,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ly/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.096ms self 0.044ms children 1.052ms %children 96.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:14612,Optimiz,OptimizePass,14612,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"m Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826:1125,load,load,1125,https://hail.is,https://github.com/hail-is/hail/issues/7826,1,['load'],['load']
Performance,"ma, url, listener); /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:409: in rmtree; await rm_dir(pool, contents_tasks_by_dir.get(path, []), path); /usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:463: in __aexit__; raise self._exception; /usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:402: in run_and_cleanup; retval = await f(*args, **kwargs); /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:367: in rm_file; await self.remove(path); /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:348: in remove; return await blocking_to_async(self._thread_pool, os.remove, path); /usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:162: in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; /usr/lib/python3.9/asyncio/base_events.py:819: in run_in_executor; executor.submit(func, *args), loop=self); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <concurrent.futures.thread.ThreadPoolExecutor object at 0x7f263d862100>; fn = <function blocking_to_async.<locals>.<lambda> at 0x7f263d781040>, args = (); kwargs = {}. def submit(self, fn, /, *args, **kwargs):; > with self._shutdown_lock, _global_shutdown_lock:; E Failed: Timeout >600.0s. /usr/lib/python3.9/concurrent/futures/thread.py:162: Failed; ---------------------------- Captured log teardown -----------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 352, in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 162, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/asyncio/futu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:11791,concurren,concurrent,11791,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,make array elements required for INFO and FORMAT signatures in LoadVCF,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2417:63,Load,LoadVCF,63,https://hail.is,https://github.com/hail-is/hail/pull/2417,1,['Load'],['LoadVCF']
Performance,malizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.123ms self 0.054ms children 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:189315,Optimiz,Optimize,189315,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,malizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.114ms self 0.048ms children 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:182290,Optimiz,Optimize,182290,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,malizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.315ms self 0.116ms children 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.153ms self 0.153ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:16025,Optimiz,Optimize,16025,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,malizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.328ms self 0.115ms children 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.164ms self 0.164ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:9041,Optimiz,Optimize,9041,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,malizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 4.611ms self 3.590ms children 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.442ms self 0.442ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.526ms self 0.526ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:4750,Optimiz,Optimize,4750,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"mance</h3>; <ul>; <li>Speed-up the new backtracking parser about 4X in general (enabled when <code>--target-version</code> is set to 3.10 and higher). (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2728"">#2728</a>)</li>; <li>Black is now compiled with mypyc for an overall 2x speed-up. 64-bit Windows, MacOS, and Linux (not including musl) are supported. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/1009"">#1009</a>, <a href=""https://github-redirect.dependabot.com/psf/black/issues/2431"">#2431</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not accept bare carriage return line endings in pyproject.toml (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2408"">#2408</a>)</li>; <li>Add configuration option (<code>python-cell-magics</code>) to format cells with custom magics in Jupyter Notebooks (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2744"">#2744</a>)</li>; <li>Allow setting custom cache directory on all platforms with environment variable <code>BLACK_CACHE_DIR</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2739"">#2739</a>).</li>; <li>Enable Python 3.10+ by default, without any extra need to specify -<code>-target-version=py310</code>. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2758"">#2758</a>)</li>; <li>Make passing <code>SRC</code> or <code>--code</code> mandatory and mutually exclusive (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2804"">#2804</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>Improve error message for invalid regular expression (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2678"">#2678</a>)</li>; <li>Improve error message when parsing fails during AST safety check by embedding the underlying SyntaxError (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2693"">#2693</a>)</li>; <li>No longer color diff headers white as it's unreadable in light themed terminals (<a href=""https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11468:4956,cache,cache,4956,https://hail.is,https://github.com/hail-is/hail/pull/11468,1,['cache'],['cache']
Performance,mand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.71-f3a54b530979; Error summary: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:15426,concurren,concurrent,15426,https://hail.is,https://github.com/hail-is/hail/issues/10682,2,['concurren'],['concurrent']
Performance,"master:; ```; 2019-08-31 07:54:03,500: INFO: [1/1] Running import_bgen_info_score...; 2019-08-31 07:55:45,687: INFO: burn in: 102.19s; 2019-08-31 07:57:25,618: INFO: run 1: 99.93s; 2019-08-31 07:59:05,480: INFO: run 2: 99.86s; 2019-08-31 08:00:45,759: INFO: run 3: 100.28s; ```. PR:; ```; 2019-08-31 07:14:26,080: INFO: [1/1] Running import_bgen_info_score...; 2019-08-31 07:16:14,620: INFO: burn in: 108.54s; 2019-08-31 07:17:58,944: INFO: run 1: 104.32s; 2019-08-31 07:19:42,457: INFO: run 2: 103.51s; 2019-08-31 07:21:25,404: INFO: run 3: 102.95s; ```. Since just importing and force-counting the BGEN takes about 80s, thisi PR is ~10-15% slower than master. Note that this is an `annotate_rows` aggregation which uses **old** aggs, so if this doesn't get faster when we switch to new aggs, we should be concerned about the performance of the primitive aggregators (sum, etc).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6960#issuecomment-526828725:827,perform,performance,827,https://hail.is,https://github.com/hail-is/hail/pull/6960#issuecomment-526828725,1,['perform'],['performance']
Performance,maybe we should make an issue to support this optimization for non-ascending orders?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6757#issuecomment-516101033:46,optimiz,optimization,46,https://hail.is,https://github.com/hail-is/hail/pull/6757#issuecomment-516101033,1,['optimiz'],['optimization']
Performance,"md64 <code>musllinux_1_1</code> wheels to PyPI.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix build requiring <code>python</code> on <code>PATH</code>.</li>; </ul>; <h2>3.6.6</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing <code>datetime.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.6.7 - 2022-02-14</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of deserializing almost-empty documents.</li>; <li>Publish arm7l <code>manylinux_2_17</code> wheels to PyPI.</li>; <li>Publish amd4 <code>musllinux_1_1</code> wheels to PyPI.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix build requiring <code>python</code> on <code>PATH</code>.</li>; </ul>; <h2>3.6.6 - 2022-01-21</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing <code>datetime.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5 - 2021-12-05</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://git",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11572:1424,perform,performance,1424,https://hail.is,https://github.com/hail-is/hail/pull/11572,1,['perform'],['performance']
Performance,"me.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5 - 2021-12-05</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/aee8a9fed45f84d227cf2cb7102656aa65a4890a""><code>aee8a9f</code></a> 3.6.7</li>; <li><a href=""https://github.com/ijl/orjson/commit/622cd7b1167262ffe458f6a2c15ec239f015d174""><code>622cd7b</code></a> Add special casing for deserializing empty objects, lists and strings</li>; <li><a href=""https://github.com/ijl/orjson/commit/5da14a00fed93dc55a5e01e4eba0e3d77b0a89fc""><code>5da14a0</code></a> Add benchmark for loading empty objects</li>; <li><a href=""https://github.com/ijl/orjson/commit/12b867c7bfbd9c6404b2f2e859c134822af05e73""><code>12b867c</code></a> cargo update, nightly-2022-02-13</li>; <li><a href=""https://github.com/ijl/orjson/commit/ab633b6d0fa064b0c4b248bee8dc1062f0fe9d32""><code>ab633b6</code></a> Build x86_64 musllinux wheels (<a href=""https://github-redirect.dependabot.com/ijl/orjson/issues/242"">#242</a>)</li>; <li><a href=""https://github.com/ijl/orjson/commit/8bf078b27e7479f2cfbea1bac7155d4449ce7e30""><code>8bf078b</code></a> Cross compile wheels for armv7l on GitHub Actions (<a href=""https://github-redirect.dependabot.com/ijl/orjson/issues/241"">#241</a>)</li>; <li><a href=""https://github.com/ijl/orjson/commit/c196f0e55bd51d3693d381ccc06f2fd4b5443d86""><code>c196f0e</code></a> 3.6.6</li>; <li><a href=""https://github.com/ijl/orjson/commit/81890b097f7a479d1c1e697d21467952e0be24a9""><code>81890b0</code></a> Fix 53-bit error on value between isize and usize</li>;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11572:2855,load,loading,2855,https://hail.is,https://github.com/hail-is/hail/pull/11572,1,['load'],['loading']
Performance,"ment 0 (Ref __iruid_514))))))\n (Let __iruid_515\n (MakeTuple (0)\n (ResultOp 0 (Collect (CollectStateSig +PInt32))))\n (MakeStruct\n (idx (GetTupleElement 0 (Ref __iruid_515)))))))))\n2022-11-15 20:30:18.195 root: INFO: optimize optimize: compileLowerer, after LowerArrayAggsToRunAggs: before: IR size 56: \n(MakeTuple (0)\n (Let __iruid_508\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (CollectStateSig +PInt32)) ()))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32))))\n (Let __iruid_509\n (CollectDistributedArray\n table_aggregate_singlestage __iruid_510\n __iruid_511\n (ToStream False\n (Literal Array[Struct{start:Int32,end:Int32}]\n <literal value>))\n (MakeStruct (__iruid_458 (Ref __iruid_508)))\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0\n (GetField __iruid_458 (Ref __iruid_511)))))\n (StreamFor __iruid_512\n (StreamMap __iruid_513\n (StreamRange -1 True\n (GetField start (Ref __iruid_510))\n (GetField end (Ref __iruid_510))\n (I32 1))\n (MakeStruct (idx (Ref __iruid_513))))\n (Begin\n (SeqOp 0 (Collect (CollectStateSig +PInt32))\n ((GetField idx (Ref __iruid_512)))))))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32))))\n (NA String))\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0 (Ref __iruid_508))))\n (StreamFor __iruid_514\n (ToStream True (Ref __iruid_509))\n (Begin\n (CombOpValue 0 (Collect (CollectStateSig +PInt32))\n (GetTupleElement 0 (Ref __iruid_514))))))\n (Let __iruid_515\n (MakeTuple (0)\n (ResultOp 0 (Collect (CollectStateSig +PInt32))))\n (MakeStruct\n (idx (GetTupleElement 0 (Ref __iruid_515)))))))))\n2022-11-15 20:30:18.206 root: INFO: optimize optimize: compileLowerer, after LowerArrayAggsToRunAggs: after: IR size 56:\n(MakeTuple (0)\n (Let __iruid_532\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:9474,optimiz,optimize,9474,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,4,['optimiz'],['optimize']
Performance,"ments, like <code>case Foo(bar=baz as quux)</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2749"">#2749</a>)</li>; <li>Tuple unpacking on <code>return</code> and <code>yield</code> constructs now implies 3.8+ (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2700"">#2700</a>)</li>; <li>Unparenthesized tuples on annotated assignments (e.g <code>values: Tuple[int, ...] = 1, 2, 3</code>) now implies 3.8+ (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2708"">#2708</a>)</li>; <li>Fix handling of standalone <code>match()</code> or <code>case()</code> when there is a trailing newline or a comment inside of the parentheses. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2760"">#2760</a>)</li>; <li><code>from __future__ import annotations</code> statement now implies Python 3.7+ (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2690"">#2690</a>)</li>; </ul>; <h3>Performance</h3>; <ul>; <li>Speed-up the new backtracking parser about 4X in general (enabled when <code>--target-version</code> is set to 3.10 and higher). (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2728"">#2728</a>)</li>; <li>Black is now compiled with mypyc for an overall 2x speed-up. 64-bit Windows, MacOS, and Linux (not including musl) are supported. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/1009"">#1009</a>, <a href=""https://github-redirect.dependabot.com/psf/black/issues/2431"">#2431</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not accept bare carriage return line endings in pyproject.toml (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2408"">#2408</a>)</li>; <li>Add configuration option (<code>python-cell-magics</code>) to format cells with custom magics in Jupyter Notebooks (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2744"">#2744</a>)</li>; <li>Allow setting custom cache directory on all platforms with environment variable <cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11468:3976,Perform,Performance,3976,https://hail.is,https://github.com/hail-is/hail/pull/11468,1,['Perform'],['Performance']
Performance,"mer$.time(ExecutionTimer.scala:52); at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$3(ServiceBackend.scala:650); at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:822); at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:447); at is.hail.backend.service.Main$.main(Main.scala:15); at is.hail.backend.service.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.write(get_aou_util_path('mt_sample_qc'), overwrite=args.overwrite); ```. Job log: https://batch.hail.is/batches/8058522/jobs/171029. <detail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:6625,concurren,concurrent,6625,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['concurren'],['concurrent']
Performance,mespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Us,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:37333,cache,cached,37333,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,minor decode optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3415:13,optimiz,optimization,13,https://hail.is,https://github.com/hail-is/hail/pull/3415,1,['optimiz'],['optimization']
Performance,minor import_bgen optimizations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3529:18,optimiz,optimizations,18,https://hail.is,https://github.com/hail-is/hail/pull/3529,1,['optimiz'],['optimizations']
Performance,mise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Ba,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217591,concurren,concurrent,217591,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,mise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$D,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216876,concurren,concurrent,216876,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"missed during testing: there is a brief interval where z-height of placeholder navbar is smaller than the sphinx side nav element, causing a flash from blue to white after load.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8546:172,load,load,172,https://hail.is,https://github.com/hail-is/hail/pull/8546,1,['load'],['load']
Performance,"mitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[28] at mapPartitions at ContextRDD.scala:137) (first 15 tasks are for partitions Vector(0)); 2018-10-09 15:04:38 TaskSchedulerImpl: INFO: Adding task set 5.0 with 1 tasks; 2018-10-09 15:04:38 TaskSetManager: INFO: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4777 bytes); 2018-10-09 15:04:38 Executor: INFO: Running task 0.0 in stage 5.0 (TID 5); 2018-10-09 15:04:38 BlockManager: INFO: Found block rdd_9_0 locally; 2018-10-09 15:04:38 CodeGenerator: INFO: Code generated in 14.135243 ms; 2018-10-09 15:04:38 CodeGenerator: INFO: Code generated in 8.306294 ms; 2018-10-09 15:04:38 Executor: INFO: Finished task 0.0 in stage 5.0 (TID 5). 1119 bytes result sent to driver; ```; </details>. <details>; <summary>Broken hail.log</summary>. ```; 2018-10-09 14:46:38 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 14:46:38 Hail: INFO: Running Hail version devel-e7552fd55a9d; 2018-10-09 14:46:38 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 14:46:38 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 14:46:38 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@28f0ac7{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@49a30f89{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4495af6e{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6baf9f3b{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:31112,load,loading,31112,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['load'],['loading']
Performance,"mize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:1280,Optimiz,Optimize,1280,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"mize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:1715,Optimiz,Optimize,1715,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"mize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2014,Optimiz,Optimize,2014,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,mizePass total 0.839ms self 0.009ms children 0.830ms %children 98.97%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.809ms self 0.073ms children 0.736ms %children 91.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.280ms self 0.280ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:169362,Optimiz,OptimizePass,169362,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,mizePass total 1.367ms self 0.010ms children 1.356ms %children 99.24%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.029ms self 0.029ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.303ms self 0.051ms children 1.252ms %children 96.10%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.383ms self 0.383ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:26045,Optimiz,OptimizePass,26045,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39351,cache,cached,39351,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,move load/store methods onto Region object,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6644:5,load,load,5,https://hail.is,https://github.com/hail-is/hail/pull/6644,1,['load'],['load']
Performance,"moved bootstrap dependencies. Did we ever actually use these?; - Removed ""clipboard.js"" dependency. Also not clear from where this came.; - Removed use of the `subtitle` tag, which isn't actually an HTML tag?. Future work:. - Simplify our CSS. It's not possible to logically reason about our CSS. And it; interacts in bad ways with the latent RTD themes. I want a unified Hail visual; theme.; - Clean up the search-related JavaScript in nav-bottom.html and; search.html. These both seem too complicated to just make search work. ---. The thrust of this PR is to restructure Hail's website and documentation to; entirely rely on Jinja2 templates. Previously, we used a mix of Jinja2, XSLT,; and in-browser JavaScript DOM manipulation to piece together a web page. Now, all of Hail's non-service website derives from; `site/templates/base.html`. It is a Jinja2 template with four blocks: title,; meta_description, head, and content. It ensures that:; - Hail's CSS is loaded,; - the Hail icon is set,; - the fonts are loaded,; - the source code highlighter is loaded (prism.js, only used outside the docs); - the nav bar is present and configured.; The nav bar is somewhat complicated. There are two pieces. `nav-top.html`; contains the nav bar HTML elements. `nav-bottom.html` contains the JavaScript; code that hooks the search bar up to Algolia and sets the active page in the; navigation. I believe JavaScript which modifies the HTML DOM is traditionally; placed at the bottom of the `body` tag so that it is executed *after* the HTML; DOM is mostly rendered. That's why the navigation/search bar is split across two; files. I also load the `prism.js` source code highlighter at the end of the; body. All of the non-docs pages are defined by html files in `pages`. Each one of; these is a Jinja2 template which derives from the base template. `make render`; converts every template in `pages` into a real HTML file in `www`. Check out; 404.html for a simple example. Once I had the site in working or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9597:1783,load,loaded,1783,https://hail.is,https://github.com/hail-is/hail/pull/9597,3,['load'],['loaded']
Performance,mp-4.9 --with-gxx-include-dir=/opt/local/include/gcc49/c++/ --with-gmp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --disable-isl-version-check --with-cloog=/opt/local --disable-cloog-version-check --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc49 4.9.3_0' --with-build-config=bootstrap-debug; Thread model: posix; gcc version 4.9.3 (MacPorts gcc49 4.9.3_0) . **sysctl -a | grep machdep.cpu**; machdep.cpu.tsc_ccc.denominator: 0; machdep.cpu.tsc_ccc.numerator: 0; machdep.cpu.thread_count: 8; machdep.cpu.core_count: 4; machdep.cpu.address_bits.virtual: 48; machdep.cpu.address_bits.physical: 36; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1: 1 0 0 0; machdep.cpu.xsave.extended_state: 7 832 832 0; machdep.cpu.thermal.energy_policy: 0; machdep.cpu.thermal.hardware_feedback: 0; machdep.cpu.thermal.package_thermal_intr: 1; machdep.cpu.thermal.fine_grain_clock_mod: 1; machdep.cpu.thermal.core_power_limits: 1; machdep.cpu.thermal.ACNT_MCNT: 1; machdep.cpu.thermal.thresholds: 2; machdep.cpu.thermal.invariant_APIC_timer: 1; machdep.cpu.thermal.dynamic_acceleration: 1; machdep.cpu.thermal.sensor: 1; machdep.cpu.mwait.sub_Cstates: 135456; machdep.cpu.mwait.extensions: 3; machdep.cpu.mwait.linesize_max: 64; machdep.cpu.mwait.linesize_min: 64; machdep.cpu.processor_flag:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:1628,cache,cache,1628,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543,1,['cache'],['cache']
Performance,mp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --disable-isl-version-check --with-cloog=/opt/local --disable-cloog-version-check --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc49 4.9.3_0' --with-build-config=bootstrap-debug; Thread model: posix; gcc version 4.9.3 (MacPorts gcc49 4.9.3_0) . **sysctl -a | grep machdep.cpu**; machdep.cpu.tsc_ccc.denominator: 0; machdep.cpu.tsc_ccc.numerator: 0; machdep.cpu.thread_count: 8; machdep.cpu.core_count: 4; machdep.cpu.address_bits.virtual: 48; machdep.cpu.address_bits.physical: 36; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1: 1 0 0 0; machdep.cpu.xsave.extended_state: 7 832 832 0; machdep.cpu.thermal.energy_policy: 0; machdep.cpu.thermal.hardware_feedback: 0; machdep.cpu.thermal.package_thermal_intr: 1; machdep.cpu.thermal.fine_grain_clock_mod: 1; machdep.cpu.thermal.core_power_limits: 1; machdep.cpu.thermal.ACNT_MCNT: 1; machdep.cpu.thermal.thresholds: 2; machdep.cpu.thermal.invariant_APIC_timer: 1; machdep.cpu.thermal.dynamic_acceleration: 1; machdep.cpu.thermal.sensor: 1; machdep.cpu.mwait.sub_Cstates: 135456; machdep.cpu.mwait.extensions: 3; machdep.cpu.mwait.linesize_max: 64; machdep.cpu.mwait.linesize_min: 64; machdep.cpu.processor_flag: 4; machdep.cpu.microcode_version: 21; machdep.cpu.cores_per_package,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:1696,cache,cache,1696,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543,1,['cache'],['cache']
Performance,"mpatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h2>3.9.11 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:1753,load,loads,1753,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['load'],['loads']
Performance,"mplates. Previously, we used a mix of Jinja2, XSLT,; and in-browser JavaScript DOM manipulation to piece together a web page. Now, all of Hail's non-service website derives from; `site/templates/base.html`. It is a Jinja2 template with four blocks: title,; meta_description, head, and content. It ensures that:; - Hail's CSS is loaded,; - the Hail icon is set,; - the fonts are loaded,; - the source code highlighter is loaded (prism.js, only used outside the docs); - the nav bar is present and configured.; The nav bar is somewhat complicated. There are two pieces. `nav-top.html`; contains the nav bar HTML elements. `nav-bottom.html` contains the JavaScript; code that hooks the search bar up to Algolia and sets the active page in the; navigation. I believe JavaScript which modifies the HTML DOM is traditionally; placed at the bottom of the `body` tag so that it is executed *after* the HTML; DOM is mostly rendered. That's why the navigation/search bar is split across two; files. I also load the `prism.js` source code highlighter at the end of the; body. All of the non-docs pages are defined by html files in `pages`. Each one of; these is a Jinja2 template which derives from the base template. `make render`; converts every template in `pages` into a real HTML file in `www`. Check out; 404.html for a simple example. Once I had the site in working order, I turned my eyes to the docs. I converted; `docs/_templates/layout.html`, the base template for our docs, into a template; which derives from `site/templates/base.html`. That ensures everyone is using; the same CSS, the same navigation/search bar, same icon set, etc. Convincing; Sphinx to work like this was actually really easy because Sphinx already uses; Jinja2 templates! I just added site's templates folder to the Sphinx; `templates_path`. I eliminated a few conditionals that are only relevant if your docs are also; rendered on RTD's server, which ours are not. Finally, in order to experiment quickly with this, I changed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9597:2451,load,load,2451,https://hail.is,https://github.com/hail-is/hail/pull/9597,1,['load'],['load']
Performance,"mpty, return an empty list.</li>; </ul>; <h1>v4.11.0</h1>; <ul>; <li>bpo-46246: Added <code>__slots__</code> to <code>EntryPoints</code>.</li>; </ul>; <h1>v4.10.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/365"">#365</a> and bpo-46546: Avoid leaking <code>method_name</code> in; <code>DeprecatedList</code>.</li>; </ul>; <h1>v4.10.1</h1>; <h1>v2.1.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/361"">#361</a>: Avoid potential REDoS in <code>EntryPoint.pattern</code>.</li>; </ul>; <h1>v4.10.0</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/354"">#354</a>: Removed <code>Distribution._local</code> factory. This; functionality was created as a demonstration of the; possible implementation. Now, the; <code>pep517 &lt;https://pypi.org/project/pep517&gt;</code>_ package; provides this functionality directly through; <code>pep517.meta.load &lt;https://github.com/pypa/pep517/blob/a942316305395f8f757f210e2b16f738af73f8b8/pep517/meta.py#L63-L73&gt;</code>_.</li>; </ul>; <h1>v4.9.0</h1>; <ul>; <li>Require Python 3.7 or later.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python/importlib_metadata/commit/99a2ec4489da45407d8224be2804ff323a164ac0""><code>99a2ec4</code></a> Update changelog.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/dbe114cbdc49ff42026974e48ca7178a091e7530""><code>dbe114c</code></a> Add docstring with tests for EntryPoint.matches. Ref <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/373"">#373</a>.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/ee566d048c0061b4f846f100ebfd93eefbcbf608""><code>ee566d0</code></a> Remove cast of path items to strings. Ref <a href=""https://github-redirect.dependabot.com/python/importlib_met",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11596:1952,load,load,1952,https://hail.is,https://github.com/hail-is/hail/pull/11596,1,['load'],['load']
Performance,"mpty, return an empty list.</li>; </ul>; <h1>v4.11.0</h1>; <ul>; <li>bpo-46246: Added <code>__slots__</code> to <code>EntryPoints</code>.</li>; </ul>; <h1>v4.10.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/365"">#365</a> and bpo-46546: Avoid leaking <code>method_name</code> in; <code>DeprecatedList</code>.</li>; </ul>; <h1>v4.10.1</h1>; <h1>v2.1.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/361"">#361</a>: Avoid potential REDoS in <code>EntryPoint.pattern</code>.</li>; </ul>; <h1>v4.10.0</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/354"">#354</a>: Removed <code>Distribution._local</code> factory. This; functionality was created as a demonstration of the; possible implementation. Now, the; <code>pep517 &lt;https://pypi.org/project/pep517&gt;</code>_ package; provides this functionality directly through; <code>pep517.meta.load &lt;https://github.com/pypa/pep517/blob/a942316305395f8f757f210e2b16f738af73f8b8/pep517/meta.py#L63-L73&gt;</code>_.</li>; </ul>; <h1>v4.9.0</h1>; <ul>; <li>Require Python 3.7 or later.</li>; </ul>; <h1>v4.8.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/357"">#357</a>: Fixed requirement generation from egg-info when a</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python/importlib_metadata/commit/14cce75299645467adcd17352cb07caada32c444""><code>14cce75</code></a> Prefer re.findall, which returns materialized results. Fixes <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/369"">#369</a>.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/b4661fd8988b4101d4042e4cc4a8ed74423ec410""><code>b4661fd</code></a> Add test capturing missed expectation on extras. Ref <a href=""https://gi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11525:1763,load,load,1763,https://hail.is,https://github.com/hail-is/hail/pull/11525,1,['load'],['load']
Performance,mt.filter_entries(...).entries() and mt.entries().filter(...) should have equivalent performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6905:85,perform,performance,85,https://hail.is,https://github.com/hail-is/hail/issues/6905,1,['perform'],['performance']
Performance,"multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130:2185,load,loading,2185,https://hail.is,https://github.com/hail-is/hail/pull/5130,1,['load'],['loading']
Performance,mysterious latency in service after change to networking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8047:11,latency,latency,11,https://hail.is,https://github.com/hail-is/hail/issues/8047,1,['latency'],['latency']
Performance,n 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.140ms self 0.140ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.154ms self 0.069ms children 0.085ms %children 55.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:171562,Optimiz,Optimize,171562,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,n 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.187ms self 0.187ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.031ms self 0.031ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.346ms self 0.121ms children 0.225ms %children 65.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:28245,Optimiz,Optimize,28245,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,n 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass total 0.035ms self 0.018ms children 0.018ms %children 50.25%; is.hail.backend.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:184512,Optimiz,Optimize,184512,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,n 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.023ms self 0.023ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply total 20.227ms self 0.484ms children 19.744ms %children 97.61%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:191537,Optimiz,Optimize,191537,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,n 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.193ms self 0.193ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.051ms self 0.051ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerMatrixToTablePass total 4.041ms self 3.971ms children 0.070ms %children 1.74%; is.hail.backend.BackendHttpH,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:11263,Optimiz,Optimize,11263,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,n 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.189ms self 0.189ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LiftRelationalValuesToRelationalLets total 1.058ms self 0.966ms children 0.092ms %children 8.70%; is.hail.backen,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:18247,Optimiz,Optimize,18247,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,n 91.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.280ms self 0.280ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.140ms self 0.140ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:170454,Optimiz,Optimize,170454,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,n 96.10%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.383ms self 0.383ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.187ms self 0.187ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:27137,Optimiz,Optimize,27137,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"n by the CI-in-the-PR not overwrite the cache. Every image build for a PR (which is tested by default namespace CI) will still overwrite the cache tag. AFAICT, this; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/foo; ```; Will use as a cache source the `latest` tag in the `gcr.io/hail-vdc/foo` repository. It is *not* sufficient for an image to be present in the repository and untagged or with a different tag from `latest`. In particular, every push to the `cache` tag prevents us from using other images even though they are in the registry! For example, I pushed two images to `cache`:. ```; (base) # gcloud container images list-tags gcr.io/hail-vdc/dktest; DIGEST TAGS TIMESTAMP; fb551d9bdb94 2022-06-10T14:16:39; afb4c5ad2d7b cache,latest 2022-06-10T14:15:55; ```. If I rebuild [1] the most recently pushed image with; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/dktest:cache; ```; it succeeds in getting the cache. If I rebuild the other image with the same import-cache, it does not see that the (untagged) image is already there! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; 1. The last ten SHAs on the branch should speed up repeated builds when you're fixing little",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800:1372,cache,cache,1372,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800,2,['cache'],['cache']
Performance,"n failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_unscheduled` must be the culprit. But how?; `mark_unscheduled` is called when a pod is evicted or by the k8s update loop if; there exists no pod. In those cases a message a special log is printed. That log; appears later (because the pod is missing) but it does not appear during the; initial sequence of events. Let's set that aside and focus on the other path by; which `mark_unscheduled` is called: `mark_complete` when the pod log cannot be; retrieved. Proposed sequence of events:. - pod is created; - k8s sends an event that the pod is terminated (but without timing information); - we load a Job object from the db. the job is Pending, the batch is cancelled; - we enter `update_job_with_pod` then `mark_complete`; - we fail to retrieve the logs; - we try to mark unscheduled, but the batch is cancelled so we do nothing.; ...; - we realize a Pending pod was never created, we try to mark_unscheduled, but; the batch is cancelled, so we do nothing; ...; - we realize a Pending pod was never created, we try ... ad infinitum",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6617:19529,load,load,19529,https://hail.is,https://github.com/hail-is/hail/issues/6617,1,['load'],['load']
Performance,"n$apply$2$$anonfun$apply$3.apply(RowStore.scala:767); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:766); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:766); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:763); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:763); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-abac611; Error summary: NumberFormatException: For input string: ""-66.2667,0,-25.4754""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:16011,concurren,concurrent,16011,https://hail.is,https://github.com/hail-is/hail/issues/3361,2,['concurren'],['concurrent']
Performance,"n's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/respo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2451,perform,performance,2451,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['perform'],['performance']
Performance,"n(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:7142,concurren,concurrent,7142,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['concurren'],['concurrent']
Performance,"n, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all error messages here, including the full java stack trace):; No error messages, but my pipeline craps out at the following lines:; ```; pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(); ```. `.count()` works instantaneously up until that `pcloadings.annotate()` line. After that line, it gets stuck at 0 of 1 tasks for ~20 minutes before I give up and cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953:3941,load,loadings,3941,https://hail.is,https://github.com/hail-is/hail/issues/3953,1,['load'],['loadings']
Performance,"n-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3297,cache,cached,3297,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"n.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:15941,concurren,concurrent,15941,https://hail.is,https://github.com/hail-is/hail/issues/3379,2,['concurren'],['concurrent']
Performance,"n.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 		at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; 	Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": ""forbidden""; }; ]; }; }. 		at com.google.api.client.http.HttpResponseException$Builder.build(HttpResponseExcep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:25529,concurren,concurrent,25529,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,n; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:144); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$2(LocalBack,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:1241,Cache,Cache,1241,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,2,['Cache'],['Cache']
Performance,"nPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:12.481 GoogleStorageFS$: INFO: createNoCompression: gs://neale-bge/foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index; 2023-09-22 19:11:12.486 : INFO: RegionPool: REPORT_THRESHOLD: 257.0K allocated (129.0K blocks / 128.0K chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.486 : INFO: RegionPool: REPORT_THRESHOLD: 577.0K allocated (193.0K blocks / 384.0K chunks), regions.size = 4, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.486 GoogleStorageFS$: INFO: createNoCompression: gs://neale-bge/foo.ht/rows/parts/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79; 2023-09-22 19:11:12.625 GoogleStorageFS$: INFO: close: gs://neale-bge/foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index; 2023-09-22 19:11:12.656 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=656384, peakBytesReadable=641.00 KiB, chunks requested=4, cache hits=2; 2023-09-22 19:11:12.656 : INFO: RegionPool: FREE: 641.0K allocated (257.0K blocks / 384.0K chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at ja",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:9000,cache,cache,9000,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['cache'],['cache']
Performance,na-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cach,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:37045,cache,cached,37045,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,nalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.737ms self 0.071ms children 3.665ms %children 98.09%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.412ms self 0.412ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:135132,Optimiz,Optimize,135132,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,nalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.531ms self 0.089ms children 4.442ms %children 98.03%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.046ms self 0.046ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:150919,Optimiz,Optimize,150919,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,nalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 8.695ms self 0.088ms children 8.607ms %children 98.99%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.598ms self 0.598ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.054ms self 0.054ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:119414,Optimiz,Optimize,119414,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,nalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.326ms self 0.326ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.561ms self 0.561ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:106372,Optimiz,OptimizePass,106372,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,nalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.342ms self 0.342ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.614ms self 0.614ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:91934,Optimiz,OptimizePass,91934,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,nalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.355ms self 0.355ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.759ms self 0.759ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:70755,Optimiz,OptimizePass,70755,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,nalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.396ms self 0.396ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.785ms self 0.785ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:56357,Optimiz,OptimizePass,56357,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,nalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.447ms self 0.447ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.051ms self 0.051ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.948ms self 0.948ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:47829,Optimiz,OptimizePass,47829,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,nalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 6.134ms self 6.134ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.135ms self 0.135ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 3.651ms self 3.651ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:39299,Optimiz,OptimizePass,39299,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,nally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9677,Load,LoadVCF,9677,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,native_writer.apply(Unknown Source); 	at __C1310collect_distributed_array_table_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSet,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:7947,concurren,concurrent,7947,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['concurren'],['concurrent']
Performance,"native_writer.apply(Unknown Source); 	at __C1310collect_distributed_array_table_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:19537,concurren,concurrent,19537,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['concurren'],['concurrent']
Performance,"nc, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/methods/impex.py in read_matrix_table(path, _intervals, _filter_intervals, _drop_cols, _drop_rows, _n_partitions); 2009 :class:`.MatrixTable`; 2010 """"""; -> 2011 for rg_config in Env.backend().load_references_from_dataset(path):; 2012 hl.ReferenceGenome._from_config(rg_config); 2013 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in load_references_from_dataset(self, path); 321 ; 322 def load_references_from_dataset(self, path):; --> 323 return json.loads(Env.hail().variant.ReferenceGenome.fromHailDataset(self.fs._jfs, path)); 324 ; 325 def from_fasta_file(self, name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par):. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args); 1302 ; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10530:2362,load,loads,2362,https://hail.is,https://github.com/hail-is/hail/issues/10530,1,['load'],['loads']
Performance,ncurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:2556,Load,LoadVCF,2556,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,ncurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217432,concurren,concurrent,217432,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,nd$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:146565,concurren,concurrent,146565,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"nd': ['/bin/bash',; '-c',; 'set -e; mkdir -p /io/pipeline/pipeline-f0c3c92aa1c4/__TASK__0/; true'],; 'image': 'gcr.io/hail-vdc/benchmark_tpoterba:latest',; 'job_id': 1,; 'mount_docker_socket': False,; 'resources': {'cpu': '1', 'memory': '7G'},; 'pvc_size': '100G',; 'secrets': [{'namespace': 'batch-pods',; 'name': 'dking-gsa-key',; 'mount_path': '/gsa-key',; 'mount_in_copy': True}],; 'env': []},; 'attributes': {'task_uid': '__TASK__0', 'name': 'replicate_0'},; 'status': {'worker': 'batch-worker-default-5t5e9',; 'batch_id': 767,; 'job_id': 1,; 'attempt_id': 'be692b',; 'user': 'dking',; 'state': 'succeeded',; 'container_statuses': {'main': {'name': 'main',; 'state': 'succeeded',; 'timing': {'pulling': {'start_time': 1576710190946,; 'finish_time': 1576710248882,; 'duration': 57936},; 'creating': {'start_time': 1576710248882,; 'finish_time': 1576710248963,; 'duration': 81},; 'runtime': {'start_time': 1576710248963,; 'finish_time': 1576710250461,; 'duration': 1498},; 'starting': {'start_time': 1576710248963,; 'finish_time': 1576710249898,; 'duration': 935},; 'running': {'start_time': 1576710249898,; 'finish_time': 1576710250461,; 'duration': 563},; 'uploading_log': {'start_time': 1576710250464,; 'finish_time': 1576710250742,; 'duration': 278},; 'deleting': {'start_time': 1576710250743,; 'finish_time': 1576710250776,; 'duration': 33}},; 'container_status': {'state': 'exited',; 'started_at': '2019-12-18T23:04:09.890460985Z',; 'finished_at': '2019-12-18T23:04:10.111873413Z',; 'out_of_memory': False,; 'exit_code': 0}}},; 'start_time': 1576710248963,; 'end_time': 1576710250461},; 'msec_mcpu': 2796766,; 'cost': '$0.0000'}; ```. I'd like to be able to load this list as a struct with one command. One would think this arcane magic would do it:; ```; In [16]: t = hl.Table.parallelize([hl.struct(**x) for x in jobs]) ; ```; but of course, nested fields. Probably some partial specification of the type will be necessary, but I would like to avoid specifying the whole thing if possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7778:1780,load,load,1780,https://hail.is,https://github.com/hail-is/hail/issues/7778,1,['load'],['load']
Performance,"nd.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/PruneDeadFields, iteration: 0 total 9.943ms self 9.943ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 1 total 0.414ms self 0.414ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 1 total 0.021ms self 0.021ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1 total 0.384ms self 0.005ms children 0.379ms %children 98.72%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 1 total 0.103ms self 0.103ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1 total 0.579ms self 0.273ms children 0.306ms %children 52.84%; timing is.hail.backend.BackendHttpHa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:6077,Optimiz,Optimize,6077,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,nd.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.032ms children 0.070ms %children 68.44%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:216362,Optimiz,Optimize,216362,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,nd.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.064ms self 0.064ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.101ms self 0.033ms children 0.068ms %children 67.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:208341,Optimiz,Optimize,208341,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,nd.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.070ms self 0.070ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.108ms self 0.034ms children 0.074ms %children 68.32%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.067ms self 0.067ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:200389,Optimiz,Optimize,200389,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,nd.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:195737,Optimiz,Optimize,195737,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,nd.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.070ms self 0.070ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:199259,Optimiz,Optimize,199259,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"nd.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadable=129.00 KiB, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.560 : INF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:21733,concurren,concurrent,21733,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['concurren'],['concurrent']
Performance,nd.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:464); 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:237); 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109); 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1400); 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1368); 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73); 	at sun.security.ssl.SSLSocketImp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:14019,concurren,concurrent,14019,https://hail.is,https://github.com/hail-is/hail/issues/12982,3,['concurren'],['concurrent']
Performance,"nd.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/FoldConstants, iteration: 0 total 0.307ms self 0.307ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ExtractIntervalFilters, iteration: 0 total 0.016ms self 0.016ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0 total 0.363ms self 0.004ms children 0.358ms %children 98.82%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.358ms self 0.358ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/Simplify, iteration: 0 total 0.077ms self 0.077ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0 total 0.659ms self 0.292ms children 0.367ms %children 55.66%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.367ms",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:11924,Optimiz,Optimize,11924,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,"nd_hail_fs.py::test_hadoop_methods_3[local] PASSED; +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_1 (139802083059456) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_0 (139802091452160) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (139802248206080) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = wo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:1490,concurren,concurrent,1490,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,"ndencies &amp; tests</li>; <li>fix site deployment</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.3 stable</h2>; <ul>; <li>fix minor typo (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>minor example fix (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>misc tidying &amp; refactoring</li>; <li>misc build/dev framework updates; <ul>; <li>update dependencies</li>; <li>update linters</li>; <li>update docs deployment branches</li>; </ul>; </li>; <li>misc test/ci updates; <ul>; <li>test forks</li>; <li>tidy OS &amp; Python version tests</li>; <li>bump primary python version 3.7 =&gt; 3.8</li>; <li>beta py3.10 testing</li>; <li>fix py2.7 tests</li>; <li>better timeout handling</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.2 stable</h2>; <ul>; <li>fix notebook memory leak (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1216"">#1216</a>)</li>; <li>fix <code>contrib.concurrent</code> with generators (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1233"">#1233</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1231"">#1231</a>)</li>; </ul>; <h2>tqdm v4.62.1 stable</h2>; <ul>; <li><code>contrib.logging</code>: inherit existing handler output stream (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1191"">#1191</a>)</li>; <li>fix <code>PermissionError</code> by using <code>weakref</code> in <code>DisableOnWriteError</code> (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1207"">#1207</a>)</li>; <li>fix <code>contrib.telegram</code> creation rate limit handling (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1223"">#1223</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1221"">#1221</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1220"">#1220</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1076"">#1076</a>)</li>; <li>tests: fix py27 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11587:2097,concurren,concurrent,2097,https://hail.is,https://github.com/hail-is/hail/pull/11587,1,['concurren'],['concurrent']
Performance,ndle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:219109,Optimiz,OptimizePass,219109,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ndle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:203136,Optimiz,OptimizePass,203136,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ndle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:211088,Optimiz,OptimizePass,211088,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ndle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Compile,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:198484,Optimiz,OptimizePass,198484,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"ndlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1561.1 in stage 8.0 (TID 21701, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:3670,concurren,concurrent,3670,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,"ne: 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20) 	at is.hail.utils.package$.fatal(package.scala:26) 	at is.hail.utils.Context.wrapException(Context.scala:19) 	at is.hail.utils.WithContext.foreach(Context.scala:51) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2.apply(TextTableReader.scala:126) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2.apply(TextTableReader.scala:126) 	at scala.collection.Iterator$class.foreach(Iterator.scala:893) 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) 	at is.hail.utils.TextTableReader$$anonfun$5.apply(TextTableReader.scala:126) 	at is.hail.utils.TextTableReader$$anonfun$5.apply(TextTableReader.scala:122) 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:108) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) Caused by: is.hail.utils.HailException: expected 13 fields, but found 1 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9) 	at is.hail.utils.package$.fatal(package.scala:26) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2$$anonfun$apply$3.apply(TextTableReader.scala:129) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2$$anonfun$apply$3.apply(TextTableReader.scala:126) 	at is.hail.utils.WithContext.foreach(Context.scala:49) 	... 17 more; --. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4100:1783,concurren,concurrent,1783,https://hail.is,https://github.com/hail-is/hail/issues/4100,2,['concurren'],['concurrent']
Performance,"nerated/C2.apply instruction count: 12; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C2.method1 instruction count: 112; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C2.method2 instruction count: 82; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C2.method3 instruction count: 252; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.<init> instruction count: 3; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.apply instruction count: 28; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.apply instruction count: 12; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method1 instruction count: 100; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method2 instruction count: 106; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method3 instruction count: 252; 2019-01-22 13:11:48 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:50 root: INFO: Index reader cache queries: 1168; 2019-01-22 13:11:50 root: INFO: Index reader cache hit rate: 0.747431506849315; 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,varid:String},entry:Struct{GT:Call,GP:+Array[+Float64],dosage:+Float64}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:27331,cache,cache,27331,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,4,['cache'],['cache']
Performance,nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-no,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:38048,cache,cached,38048,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,nfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2803:3055,concurren,concurrent,3055,https://hail.is,https://github.com/hail-is/hail/issues/2803,2,['concurren'],['concurrent']
Performance,"ng Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; ```; ----------------------------; ```; >>> rdd = sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; ```; ----------------------------------; ```; >>> vds = hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no files; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071:2588,Load,LoadVCF,2588,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071,2,['Load'],['LoadVCF']
Performance,"ng is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Verify total 0.011ms self 0.011ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform total 2.548ms self 0.302ms children 2.246ms %children 88.15%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/FoldConstants, iteration: 0 total 0.307ms self 0.307ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ExtractIntervalFilters, iteration: 0 total 0.016ms self 0.016ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0 total 0.363ms self 0.004ms children 0.358ms %children 98.82%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.358ms self 0.358ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/Simplify, iteration: 0 total 0.077ms self 0.077ms children 0.000m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:11331,Optimiz,Optimize,11331,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,"ng newest version of hail:; ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:1013,Load,LoadVCF,1013,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,"ng server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've deve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:3567,load,loaded,3567,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['load'],['loaded']
Performance,ng.AnyIR total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.809ms self 0.073ms children 0.736ms %children 91.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.280ms self 0.280ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:169876,Optimiz,OptimizePass,169876,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ng.AnyIR total 0.029ms self 0.029ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.303ms self 0.051ms children 1.252ms %children 96.10%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.383ms self 0.383ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:26559,Optimiz,OptimizePass,26559,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ng.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.046ms self 0.046ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.655ms self 0.655ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.184ms self 0.184ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:152384,Optimiz,Optimize,152384,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.609ms self 0.609ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.195ms self 0.195ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:136597,Optimiz,Optimize,136597,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.054ms self 0.054ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.566ms self 0.566ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.445ms self 0.445ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:120879,Optimiz,Optimize,120879,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.083ms self 0.043ms children 0.040ms %children 48.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.621ms self 0.621ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvalu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:157582,Optimiz,Optimize,157582,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.087ms self 0.042ms children 0.045ms %children 51.66%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.650ms self 0.650ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvalu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:141795,Optimiz,Optimize,141795,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.101ms self 0.049ms children 0.052ms %children 51.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.283ms self 4.283ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvalu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:126077,Optimiz,Optimize,126077,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.575ms self 0.049ms children 0.527ms %children 91.53%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:180906,Optimiz,Optimize,180906,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.577ms self 0.041ms children 0.536ms %children 92.90%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:187931,Optimiz,Optimize,187931,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.096ms self 0.044ms children 1.052ms %children 96.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:14641,Optimiz,Optimize,14641,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"ng/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:3091,Load,LoadMatrix,3091,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,ngDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:7312,Optimiz,Optimize,7312,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"ngMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:2629,load,loading,2629,https://hail.is,https://github.com/hail-is/hail/issues/1683,1,['load'],['loading']
Performance,ngPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.525ms self 0.004ms children 0.521ms %children 99.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.503ms self 0.061ms children 0.443ms %children 87.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:214488,Optimiz,OptimizePass,214488,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ngPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.518ms self 0.004ms children 0.514ms %children 99.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.496ms self 0.040ms children 0.456ms %children 91.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.228ms self 0.228ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:206467,Optimiz,OptimizePass,206467,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ngPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.621ms self 0.621ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.245ms self 0.245ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:158324,Optimiz,OptimizePass,158324,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ngPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.650ms self 0.650ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.254ms self 0.254ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:142537,Optimiz,OptimizePass,142537,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ngPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.283ms self 4.283ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.308ms self 0.308ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:126819,Optimiz,OptimizePass,126819,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"nginx can use more than one core and at max load + PRs we're maxing out internal-gateway cycles, which add latency, timeouts, retries, and generally degrade the experience in one namespace based on activity in others. Allowing nginx to use more cores (in this case this is up to half our node size) got our system back into its intended state with graceful throttling. I'll admit, other than being half a node size, 4 is a bit arbitrary here. I think our k8s nodes are annoyingly underutilized enough that we shouldn't see issues in practice with letting internal-gateway use cores that are very likely idle.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11876:44,load,load,44,https://hail.is,https://github.com/hail-is/hail/pull/11876,2,"['latency', 'load']","['latency', 'load']"
Performance,"nished task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1); 2018-10-09 15:04:37 TaskSchedulerImpl: INFO: Removed TaskSet 4.0, whose tasks have all completed, from pool ; 2018-10-09 15:04:37 DAGScheduler: INFO: ResultStage 4 (collect at utils.scala:197) finished in 0.008 s; 2018-10-09 15:04:37 DAGScheduler: INFO: Job 2 finished: collect at utils.scala:197, took 0.051042 s; 2018-10-09 15:04:37 CodeGenerator: INFO: Code generated in 5.011153 ms; 2018-10-09 15:04:37 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:37 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table8508c46074` AS `zzz1`; WHERE (0 = 1); 2018-10-09 15:04:37 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:37 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table8508c46074`; 2018-10-09 15:04:38 root: INFO: optimize: before:; (TableCount; (TableKeyBy () False; (TableLiteral))); 2018-10-09 15:04:38 root: INFO: optimize: after:; (TableCount; (TableLiteral)); 2018-10-09 15:04:38 SparkContext: INFO: Starting job: fold at RVD.scala:361; 2018-10-09 15:04:38 DAGScheduler: INFO: Got job 3 (fold at RVD.scala:361) with 1 output partitions; 2018-10-09 15:04:38 DAGScheduler: INFO: Final stage: ResultStage 5 (fold at RVD.scala:361); 2018-10-09 15:04:38 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 15:04:38 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 15:04:38 DAGScheduler: INFO: Submitting ResultStage 5 (MapPartitionsRDD[28] at mapPartitions at ContextRDD.scala:137), which has no missing parents; 2018-10-09 15:04:38 MemoryStore: INFO: Block broadcast_5 stored as values in memory (estimated size 19.8 KB, free 366.2 MB); 2018-10-09 15:04:38 MemoryStore: INFO: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.1 KB, free 366.2 MB); 2018-10-09 15:04:38 BlockManagerInfo: INFO: Added broadcast_5_piece0 in m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:28828,optimiz,optimize,28828,https://hail.is,https://github.com/hail-is/hail/issues/4513,2,['optimiz'],['optimize']
Performance,"nitialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 : INFO: TaskReport: stage=0, partition=38854, attempt=0, peakBytes=65536, peakBytesReadable=64.00 KiB, chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:208) ~[scala-library-2.12.15.jar:?]; 	at is.hail.io.StreamBlockInputBuffer.readBlock(InputBuffers.scala:552) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:384) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.readInt(InputBuffers.scala:409) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c04",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:2930,concurren,concurrent,2930,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,2,['concurren'],['concurrent']
Performance,"nly used functions, improvements to F2PY, and better documentation.</p>; <p>The Python versions supported in this release are 3.8-3.10, Python 3.7; has been dropped. Note that 32 bit wheels are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,; and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>; <p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>; <h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>; <p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that; users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both; deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with; the appropriate value for the <code>usemask</code> parameter.</p>; <p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd6",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11939:2340,load,loads,2340,https://hail.is,https://github.com/hail-is/hail/pull/11939,2,['load'],['loads']
Performance,nnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); I0824 16:44:07.061986 9121 sched.cpp:1771] Asked to stop the driver; I0824 16:44:07.062144 8743 sched.cpp:1040] Stopping framework '0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932'`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:11564,concurren,concurrent,11564,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,2,['concurren'],['concurrent']
Performance,"no, its just a question of where we put the checking logic. When we add a reference genome in Emit, we'll probably just have to set some boolean flag that tells us if something has already been set, and only call the `loader` code if it hasn't been set.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3357#issuecomment-380970181:218,load,loader,218,https://hail.is,https://github.com/hail-is/hail/pull/3357#issuecomment-380970181,1,['load'],['loader']
Performance,non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:50766,concurren,concurrent,50766,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContaine,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:52093,concurren,concurrent,52093,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 11.0 in stage 0.0 (TID 11, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:54758,concurren,concurrent,54758,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:48112,concurren,concurrent,48112,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:49439,concurren,concurrent,49439,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 39.0 in stage 0.0 (TID 39, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:106407,concurren,concurrent,106407,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 29.0 in stage 0.0 (TID 29, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:100720,concurren,concurrent,100720,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 9 from BlockManagerMaster.; 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 9 requested; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:125396,concurren,concurrent,125396,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 17.0 in stage 0.0 (TID 17, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:118263,concurren,concurrent,118263,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 27.1 in stage 0.0 (TID 44, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:139174,concurren,concurrent,139174,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 25.1 in stage 0.0 (TID 41, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:147953,concurren,concurrent,147953,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:12:02 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 15.2 in stage 0.0 (TID 50, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:161083,concurren,concurrent,161083,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:12:03 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 7.2 in stage 0.0 (TID 53, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:171632,concurren,concurrent,171632,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:12:05 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 9.1 in stage 0.0 (TID 65, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:183131,concurren,concurrent,183131,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 35.3 in stage 0.0 (TID 62, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:192487,concurren,concurrent,192487,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33220,cache,cached,33220,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/resources/vep-GRCh37.sh python/hailtop/hailctl/dataproc/resources/vep-GRCh38.sh build/deploy/dist/hail-0.2.129-py3-none-any.whl gs://hail-common/hailctl/dataproc/0.2.129; gcloud storage objects update -r gs://hail-common/hailctl/dataproc/0.2.129 --add-acl-grant=entity=AllUsers,role=READER; gcloud storage objects update ""gs://hail-common/hailctl/dataproc/0.2.129/*"" --temporary-hold; ```. Note the following:; - mill is not invoked; - deploy.yaml is re-made with the correct uris; - the wheel is built",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1837,cache,cache,1837,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,1,['cache'],['cache']
Performance,nonfun$apply$11.apply(TableIR.scala:627); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:626); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1264); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1258); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1138); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5174:2681,concurren,concurrent,2681,https://hail.is,https://github.com/hail-is/hail/issues/5174,2,['concurren'],['concurrent']
Performance,nonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3559,concurren,concurrent,3559,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['concurren'],['concurrent']
Performance,nonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1014); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.RVD$$anonfun$4$$anon$1.hasNext(RVD.scala:226); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1014); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f2b0dca9f506; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4114:8971,concurren,concurrent,8971,https://hail.is,https://github.com/hail-is/hail/issues/4114,2,['concurren'],['concurrent']
Performance,nonfun$main$4(ServiceBackend.scala:460); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:21517,concurren,concurrent,21517,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['concurren'],['concurrent']
Performance,nonfun$main$4(ServiceBackend.scala:460); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:464); 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:237); 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109); 	at sun.security.ssl.SSLSocketImpl.dec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:13803,concurren,concurrent,13803,https://hail.is,https://github.com/hail-is/hail/issues/12982,3,['concurren'],['concurrent']
Performance,nonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.N,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218376,concurren,concurrent,218376,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"nsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1): E",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:3405,concurren,concurrent,3405,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,"nscript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1178,cache,cached,1178,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"nsequence_terms); worst_csq = hl.literal(CSQ_ORDER).find(lambda x: all_csq_terms.contains(x)); return hl.struct(worst_csq=worst_csq, protein_coding=protein_coding, lof=lof, no_lof_flags=no_lof_flags). protein_coding = ht.vep.transcript_consequences.filter(lambda x: x.biotype == 'protein_coding'); return ht.annotate(**hl.case(missing_false=True); .when(hl.len(protein_coding) > 0, get_worst_csq(protein_coding, True)); .when(hl.len(ht.vep.transcript_consequences) > 0, get_worst_csq(ht.vep.transcript_consequences, False)); .when(hl.len(ht.vep.regulatory_feature_consequences) > 0, get_worst_csq(ht.vep.regulatory_feature_consequences, False)); .when(hl.len(ht.vep.motif_feature_consequences) > 0, get_worst_csq(ht.vep.motif_feature_consequences, False)); .default(get_worst_csq(ht.vep.intergenic_consequences, False))); ```; When the `csq_list = hl.cond(hl.is_defined(lof), csq_list.filter(lambda x: x.lof == lof), csq_list)` line triggers, this seems to fail to `find` the consequences entirely:; ```; all_csq_terms = csq_list.flatmap(lambda x: x.consequence_terms); all_csq_terms.show(); 2019-03-09 17:48:20 Hail: INFO: interval filter loaded 1 of 9997 partitions; +---------------+------------+-------------------------------+; | locus | alleles | <expr> |; +---------------+------------+-------------------------------+; | locus<GRCh37> | array<str> | array<str> |; +---------------+------------+-------------------------------+; | 1:55509603 | [""C"",""T""] | [""stop_gained"",""stop_gained""] |; +---------------+------------+-------------------------------+; worst_csq = hl.literal(CSQ_ORDER).find(lambda x: all_csq_terms.contains(x)); worst_csq.show(); 2019-03-09 17:48:32 Hail: INFO: interval filter loaded 1 of 9997 partitions; +---------------+------------+--------+; | locus | alleles | <expr> |; +---------------+------------+--------+; | locus<GRCh37> | array<str> | str |; +---------------+------------+--------+; | 1:55509603 | [""C"",""T""] | NA |; +---------------+------------+--------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5575:1751,load,loaded,1751,https://hail.is,https://github.com/hail-is/hail/issues/5575,2,['load'],['loaded']
Performance,nstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:415); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/120:4152,concurren,concurrent,4152,https://hail.is,https://github.com/hail-is/hail/issues/120,1,['concurren'],['concurrent']
Performance,"nt (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the original data. Alex Kotlar: Would we want to associate an instance of a PType with a single region?. Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hol",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826:1507,load,load,1507,https://hail.is,https://github.com/hail-is/hail/issues/7826,1,['load'],['load']
Performance,nt-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.readInt(ServiceBackend.scala:510); 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:561); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:4507,concurren,concurrent,4507,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,"ntained in `body` (including in nested `Group`s) by their `ifFlat` alternative (almost always either "" "" or """"), or, if that would cause the line to exceed `width`; * print `body` normally, as described in the previous paragraph, allowing nested `Group`s to print either flat or normally. This pretty-printer DSL has become fairly standard, with some common enhancements that I don't think we need. It was first described in [A prettier printer](https://homepages.inf.ed.ac.uk/wadler/papers/prettier/prettier.pdf) by Wadler (though my implementation is completely different). This achieves stack safety by `Concat` taking an `Iterable`, so each contained `Doc` can be produced on demand. `render` pulls from these iterators, keeping in memory only things that might print to the current line, but where the format hasn't been decided yet. As soon as the formatting of a group is decided, as much of its body as possible is written to the `java.io.Writer`. A very quick and dirty performance comparison had the new pretty printer about 20% slower. That's paying for both the stack safety and the added smarts. And I think there's still room for optimization if it becomes necessary. Here is a snippet of the IR generated by `test_ld_score_regression`, first on master, then this PR:; ```; (InsertFields; (SelectFields (SNP A1 A2 N Z); (Ref row)); None; (chi_squared; (Apply pow () Float64; (GetField Z; (Ref row)); (ApplyIR toFloat64 () Float64; (I32 2)))); (n; (GetField N; (Ref row))); (ld_score; (GetField L2; (GetField __uid_3; (Ref row)))); (locus; (Apply Locus () Locus(GRCh37); (GetField CHR; (GetField __uid_4; (Ref row))); (GetField BP; (GetField __uid_5; (Ref row))))); (alleles; (MakeArray Array[String]; (GetField A2; (Ref row)); (GetField A1; (Ref row)))); (phenotype; (Str ""50_irnt""))))); (InsertFields; (SelectFields (locus alleles chi_squared n ld_score phenotype); (SelectFields (SNP A1 A2 N Z chi_squared n ld_score locus alleles phenotype); (Ref row))); None)); ```; ```; (InsertFiel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9652:3556,perform,performance,3556,https://hail.is,https://github.com/hail-is/hail/pull/9652,1,['perform'],['performance']
Performance,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:5903,load,load,5903,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['load'],['load']
Performance,ntext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.schedu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5783,Load,LoadMatrix,5783,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,ntextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.Batc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217284,concurren,concurrent,217284,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ntig_row_list = pca_rows.collect(); print('finished collecting'); contig_reformed = [(x['contig'], x['file_row_idx']) for x in contig_row_list]; print('reformed'); from collections import defaultdict; contig_row_dict = defaultdict(list); for k, v in contig_reformed:; contig_row_dict[k].append(v); print('dictionary created'). with hl.hadoop_open(contig_row_dict_location, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all error messages here, including the full java stack t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953:3408,load,load,3408,https://hail.is,https://github.com/hail-is/hail/issues/3953,1,['load'],['load']
Performance,ntime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28); E 	at java.util.concurren,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4378,concurren,concurrent,4378,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['concurren'],['concurrent']
Performance,null cachedAltAlleles when updating region,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2485:5,cache,cachedAltAlleles,5,https://hail.is,https://github.com/hail-is/hail/pull/2485,1,['cache'],['cachedAltAlleles']
Performance,nvokeChannelRead(AbstractChannelHandlerCetty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at lerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac0) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at ocessSelectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at ava:459) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunct,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:11038,concurren,concurrent,11038,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,nvokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHaetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerCactChannelHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at nnelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(A1359) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.Abstracead(DefaultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at ed(NioEventLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.Nio at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thrpark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at eChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerCetty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at lerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac0) at io.netty.channel.DefaultChannelPipel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:9541,concurren,concurrent,9541,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,nvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at org.apache.hadoop.fs.Path.initialize(Path.java:205); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSyst,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:5208,concurren,concurrentGlobInternal,5208,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrentGlobInternal']
Performance,"ny.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, go",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6618,cache,cached,6618,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"o report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-ccaf3640241f. ### What you did:. ```; ht = hl.import_table('gs://gnomad/annotations/hail-0.2/ht/genomes/score_rankings/gnomad.sites.RF.newStats24.txt.bgz', types={'chrom': hl.tstr}, impute=True, min_partitions=100).cache(); ht.export('gs://gnomad-tmp/genomes_rf.txt.bgz', parallel=True); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-17-671d2e9c22c8> in <module>(); 1 #ht = hl.import_table('gs://gnomad/annotations/hail-0.2/ht/genomes/score_rankings/gnomad.sites.RF.newStats24.txt.bgz', types={'chrom': hl.tstr}, impute=True, min_partitions=100).cache(); ----> 2 ht.export('gs://gnomad-tmp/genomes_rf.txt.bgz', parallel=True). /home/hail/hail.zip/hail/table.py in export(self, output, types_file, header, parallel); 994 """"""; 995 ; --> 996 self._jt.export(output, types_file, header, Env.hail().utils.ExportType.getExportType(parallel)); 997 ; 998 def group_by(self, *exprs, **named_exprs) -> 'GroupedTable':. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 186 import pyspark; 187 try:; --> 188 return f(*args, **kwargs); 189 except py4j.protocol.Py4JJavaError as e:; 190 s = e.java_exception.toString(). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 321 raise Py4JE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4033:991,cache,cache,991,https://hail.is,https://github.com/hail-is/hail/issues/4033,1,['cache'],['cache']
Performance,"o short for Google Cloud Storage. I am not sure why but we; timeout substantially more frequently. I have observed this myself on my laptop. Just this; morning I saw it happen to Daniel. 2. When using an `aiohttp.AsyncIterablePayload`, it is *critical* to always check if the coroutine; which actually writes to GCS (which is stashed in the variable `request_task`) is still; alive. In the current `main`, we do not do this which causes hangs (in particular the timeout; exceptions are never thrown ergo we never retry). To understand the second problem, you must first recall how writing works in aiogoogle. There are; two Tasks and an `asyncio.Queue`. The terms ""writer"" and ""reader"" are somewhat confusing, so let's; use left and right. The left Task has the owning reference to both the source ""file"" and the; destination ""file"". In particular, it is the *left* Task which closes both ""files"". Moreover, the; left Task reads chunks from the source file and places those chunks on the `asyncio.Queue`. The; right Task takes chunks off the queue and writes those chunks to the destination file. This situation can go awry in two ways. First, if the right Task encounters any kind of failure, it will stop taking chunks off of the; queue. When the queue (which has a size limit of one) is full, the left Task will hang. The system; is stuck. The left Task will wait forever for the right Task to empty the queue. The second scenario is exactly the same except that the left Task is trying to add the ""stop""; message to the queue rather than a chunk. In either case, it is critical that the left Task waits simultaneously on the queue operation *and*; on the right Task completing. If the right Task has died, no further writes can occur and the left; Task must raise an exception. In the first scenario, we do not observe the right Task's exception; because that will be done when we close the `InsertObjectStream` (which represents the destination; ""file""). ---. I also added several types, assertio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11830:1160,Queue,Queue,1160,https://hail.is,https://github.com/hail-is/hail/pull/11830,1,['Queue'],['Queue']
Performance,"o single-digit percents of a core. This leads to the first goal of this transition: configure our load balancers to know the full cluster configuration at any point in time so they can properly maintain connection pools with upstream services. However, this is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:3102,load,load-balancer,3102,https://hail.is,https://github.com/hail-is/hail/pull/12095,2,['load'],"['load-balance', 'load-balancer']"
Performance,o==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33518,cache,cached,33518,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"oadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11328 25772 (MethodStmtX INVOKEINTERFACE is/hail/io/OutputBuffer.writeByte (B)Vinterface; 11329 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2354null Lis/hail/io/OutputBuffer;; 11330 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11331 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 11332 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;))); ```. # Pervasiveness. There is absolutely nothing about this bug that is whole-stage-codegen-specific, but I suspect the much larger single IRs compiled in whole stage code generation made it exponentially more likely for this corner case to occur. I imagine it would be possible to construct a failing pipeline with whole stage code generation turned off. # Testing. This is super hard to reproduce using small/public examples, and any unit tests to capture this *crazy edge case* are pretty much meaningless. John suggested we programmatically check the TypeInfo inference against some JVM reference, and I agree that's the best bet, but don't want to block this critical fix on that project. I fixed BALOAD for the same reason, but it doesn't appear that has caused trouble yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:6538,Load,LoadX,6538,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,"oadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.ut",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:1926,Load,LoadPlink,1926,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2526,load,loadClass,2526,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460,3,['load'],['loadClass']
Performance,ocketAPI2$.$anonfun$main$5$adapted(ServiceBackend.scala:430); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:430); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.pu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4148,concurren,concurrent,4148,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['concurren'],['concurrent']
Performance,"ode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add ter",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1417,Load,LoadSelfFirstURLClassLoader,1417,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['Load'],['LoadSelfFirstURLClassLoader']
Performance,"ode>~cryptography.x509.CertificateBuilder</code>, other X.509 builders, and; PKCS7 has been removed.</li>; <li><strong>BACKWARDS INCOMPATIBLE:</strong> Dropped support for macOS 10.10 and 10.11, macOS; users must upgrade to 10.12 or newer.</li>; <li><strong>ANNOUNCEMENT:</strong> The next version of <code>cryptography</code> (40.0) will change; the way we link OpenSSL. This will only impact users who build; <code>cryptography</code> from source (i.e., not from a <code>wheel</code>), and specify their; own version of OpenSSL. For those users, the <code>CFLAGS</code>, <code>LDFLAGS</code>,; <code>INCLUDE</code>, <code>LIB</code>, and <code>CRYPTOGRAPHY_SUPPRESS_LINK_FLAGS</code> environment; variables will no longer be respected. Instead, users will need to; configure their builds <code>as documented here</code>_.</li>; <li>Added support for; :ref:<code>disabling the legacy provider in OpenSSL 3.0.x&lt;legacy-provider&gt;</code>.</li>; <li>Added support for disabling RSA key validation checks when loading RSA; keys via; :func:<code>~cryptography.hazmat.primitives.serialization.load_pem_private_key</code>,; :func:<code>~cryptography.hazmat.primitives.serialization.load_der_private_key</code>,; and; :meth:<code>~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateNumbers.private_key</code>.; This speeds up key loading but is :term:<code>unsafe</code> if you are loading potentially; attacker supplied keys.</li>; <li>Significantly improved performance for; :class:<code>~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/d6951dca25de45abd52da51b608055371fbcde4e""><code>d6951dc</code></a> changelog + security fix backport (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8231"">#8231</a>)</li>; <li><a href=""https://github.com/pyca/cryptography",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12668:2666,load,loading,2666,https://hail.is,https://github.com/hail-is/hail/pull/12668,4,['load'],['loading']
Performance,"of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h2>3.9.11 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/a348f59f0b55d92a1364523560f52f5b3cf9c12a""><code>a348f59</code></a> 3.9.15</li>; <li><a href=""https://github.com/ijl/orjson/commit/b0e4d2c06ce06c6e63981bf0276e4b7c74e5845e""><code>b0e4d2c</code></a> yyjson 0eca326, recursion limit</li>; <li><a href=""https://github.com/ijl/orjson/commit/5067eadc84cf516e4eb33bcb09ad756bb59dc42e""><code>5067ead</code></a> impl_escape_unchecked() byte exact read</li>; <li><a href=""https://github.com/ijl/orjson/commit/e04ea735b087742b6cee738aa295d8b835c3a195""><code>e04ea73</code></a> cargo update, build misc</li>; <li><a href=""https://github.com/ijl/orjson/commit/ba8c701292e4720b4e10210b266be5666d098fb6""><code>ba8c701</code></a> 3.9.14</li>; <li><",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:2745,perform,performance,2745,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['perform'],['performance']
Performance,"og output. ```shell; File ~/Library/Python/3.9/lib/python/site-packages/hail/table.py:2814, in Table.collect(self, _localize, _timed); 2812 e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); 2813 if _localize:; → 2814 return Env.backend().execute(e._ir, timed=_timed); 2815 else:; 2816 return e. File ~/Library/Python/3.9/lib/python/site-packages/hail/backend/backend.py:188, in Backend.execute(self, ir, timed); 186 payload = ExecutePayload(self._render_ir(ir), ‘{“name”:“StreamBufferSpec”}’, timed); 187 try:; → 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; 190 raise e.maybe_user_error(ir) from None. File ~/Library/Python/3.9/lib/python/site-packages/hail/backend/py4j_backend.py:218, in Py4JBackend._rpc(self, action, payload); 216 path = action_routes[action]; 217 port = self._backend_server_port; → 218 resp = self._requests_session.post(f’http://localhost:{port}{path}', data=data); 219 if resp.status_code >= 400:; 220 error_json = orjson.loads(resp.content). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:637, in Session.post(self, url, data, json, **kwargs); 626 def post(self, url, data=None, json=None, **kwargs):; 627 r""""“Sends a POST request. Returns :class:Response object.; 628; 629 :param url: URL for the new :class:Request object.; (…); 634 :rtype: requests.Response; 635 “””; → 637 return self.request(“POST”, url, data=data, json=json, **kwargs). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:589, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json); 584 send_kwargs = {; 585 “timeout”: timeout,; 586 “allow_redirects”: allow_redirects,; 587 }; 588 send_kwargs.update(settings); → 589 resp = self.send(prep, **send_kwargs); 591 return resp. File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:703, in Session.send(self, request, **kwargs); 700 start = preferr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14557:1527,load,loads,1527,https://hail.is,https://github.com/hail-is/hail/issues/14557,1,['load'],['loads']
Performance,ogle.cloud.storage.ApiaryUnbufferedReadableByteChannel.read(ApiaryUnbufferedReadableByteChannel.java:104); 	at is.hail.relocated.com.google.cloud.storage.UnbufferedReadableByteChannelSession$UnbufferedReadableByteChannel.read(UnbufferedReadableByteChannelSession.java:36); 	at is.hail.relocated.com.google.cloud.storage.DefaultBufferedReadableByteChannel.read(DefaultBufferedReadableByteChannel.java:106); 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$SynchronizedBufferedReadableByteChannel.read(StorageByteChannels.java:84); 	at is.hail.relocated.com.google.cloud.storage.BaseStorageReadChannel.read(BaseStorageReadChannel.java:91); 	at is.hail.io.fs.GoogleStorageFS$$anon$1.readHandlingRequesterPays(GoogleStorageFS.scala:205); 	at is.hail.io.fs.GoogleStorageFS$$anon$1.fill(GoogleStorageFS.scala:242); 	at is.hail.io.fs.FSSeekableInputStream.read(FS.scala:164); 	at java.io.DataInputStream.read(DataInputStream.java:100); 	at is.hail.expr.ir.GenericLines$$anon$2.loadBuffer(GenericLines.scala:84); 	at is.hail.expr.ir.GenericLines$$anon$2.readLine(GenericLines.scala:194); 	at is.hail.expr.ir.GenericLines$$anon$2.hasNext(GenericLines.scala:214); 	at __C18collect_distributed_array_shuffle_initial_write.apply_region1_42(Unknown Source); 	at __C18collect_distributed_array_shuffle_initial_write.apply(Unknown Source); 	at __C18collect_distributed_array_shuffle_initial_write.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$2(BackendUtils.scala:38); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$1(BackendUtils.scala:37); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:36); 	at __C5Compiled.__m7split_Let(Emit.scala); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.ja,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:5982,load,loadBuffer,5982,https://hail.is,https://github.com/hail-is/hail/issues/12983,4,['load'],['loadBuffer']
Performance,"ography/issues/10442"">#10442</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/7a4d012991061974da5d9cb7614de65eac94f49b""><code>7a4d012</code></a> Fixes <a href=""https://redirect.github.com/pyca/cryptography/issues/10422"">#10422</a> -- don't crash when a PKCS#12 key and cert don't match (<a href=""https://redirect.github.com/pyca/cryptography/issues/10423"">#10423</a>) ...</li>; <li><a href=""https://github.com/pyca/cryptography/commit/df314bb182bdfd661333969a94325e4680d785f6""><code>df314bb</code></a> backport actions m1 switch to 42.0.x (<a href=""https://redirect.github.com/pyca/cryptography/issues/10415"">#10415</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/c49a7a5271178c6e8ef36fa1c499f62c63ec19b9""><code>c49a7a5</code></a> changelog and version bump for 42.0.3 (<a href=""https://redirect.github.com/pyca/cryptography/issues/10396"">#10396</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/396bcf64c5be826ec00e7d7f45838c858c049cbc""><code>396bcf6</code></a> fix provider loading take two (<a href=""https://redirect.github.com/pyca/cryptography/issues/10390"">#10390</a>) (<a href=""https://redirect.github.com/pyca/cryptography/issues/10395"">#10395</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/0e0e46f5f73f477b8ee9682738c42129d5d60177""><code>0e0e46f</code></a> backport: initialize openssl's legacy provider in rust (<a href=""https://redirect.github.com/pyca/cryptography/issues/10323"">#10323</a>) (<a href=""https://redirect.github.com/pyca/cryptography/issues/10333"">#10333</a>)</li>; <li>See full diff in <a href=""https://github.com/pyca/cryptography/compare/42.0.2...42.0.4"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=cryptography&package-manager=pip&previous-version=42.0.2&new-version=42.0.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14332:2537,load,loading,2537,https://hail.is,https://github.com/hail-is/hail/pull/14332,3,['load'],['loading']
Performance,"oh, man, this is super exciting. 3x on the combiner? yes please!. We can probably make incremental performance improvements to the LIR method splitting code to bring the compile and execute back down, and that one I consider a little less critical anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8963#issuecomment-650837044:99,perform,performance,99,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650837044,1,['perform'],['performance']
Performance,"oh, whoa. splitting the code up and performing an action in between (`filter` followed by `collect` followed by `show`) fixes the problem",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7393#issuecomment-547003344:36,perform,performing,36,https://hail.is,https://github.com/hail-is/hail/issues/7393#issuecomment-547003344,1,['perform'],['performing']
Performance,oke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at java.util.concurrent.AbstractExecutorService.doInvokeAny(AbstractExecutorService.java:193); 	at java.util.concurrent.AbstractExecutorService.invokeAny(AbstractExecutorService.java:215); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1282); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCom,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:3458,concurren,concurrentGlobInternal,3458,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrentGlobInternal']
Performance,oke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Hail version: devel-9a5678f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3516:7609,concurren,concurrent,7609,https://hail.is,https://github.com/hail-is/hail/issues/3516,2,['concurren'],['concurrent']
Performance,oke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at java.util.concurrent.AbstractExecutorService.doInvokeAny(AbstractExecutorService.java:193); 	at java.util.concurrent.AbstractExecutorService.invokeAny(AbstractExecutorService.java:215); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1282); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(Reflectio,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:3316,concurren,concurrent,3316,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,oldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collecti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7560,Optimiz,Optimize,7560,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['Optimize']
Performance,oldConstants.scala:47); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7162,Optimiz,Optimize,7162,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['Optimize']
Performance,ollection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:3871,Cache,CacheManager,3871,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,1,['Cache'],['CacheManager']
Performance,ollection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:9054,concurren,concurrent,9054,https://hail.is,https://github.com/hail-is/hail/issues/8545,1,['concurren'],['concurrent']
Performance,ollection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:81881,concurren,concurrent,81881,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['concurren'],['concurrent']
Performance,"ollowing error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(Thread.java:745)`. `Hail version: 0.1-4238176`; `Error summary: HailException: arguments refer to no files`. It's probably something quick, but I can't seem to figure it out?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2070:1177,Load,LoadVCF,1177,https://hail.is,https://github.com/hail-is/hail/issues/2070,2,['Load'],['LoadVCF']
Performance,"om Step 1 in groups of 100 attempts; 4. Randomize the offsets and have a burn in period of 5000 to avoid the birthday problem where we populate the `aggregated_*_resources_by_date` tables.; 5. In 10-way parallelism (maxes out a 4 core database), randomly populate the tables for each chunk.; 6. From the last offset (original first running batch id), we sequentially process attempts in groups of 100. We take note of where we are at with tracking any updates to the attempts table (`attempts_time_msecs_diff`), populate the `aggregated_*_resources_by_date` tables, and then do a final catchup step where we apply any updates from `attempts_time_msecs_diff` for any attempts that we have already processed.; 7. Once we have reached the ""end"" of the attempts table, we lock all tables of interest especially the `attempts` table, and do one last final processing step before we add the new triggers that will auto-populate the `aggregated_*_resources_by_date` tables.; 8. Then we perform an audit and make sure things look correct. (I might need to change or eliminate the billing_project audit query because there are 5 batches with ~20 jobs that aren't perfectly tracked when we did the original switch over to the new billing tables).; 9. If there are any failures, we revert the triggers back to the original state. Also to note, is the new table for billing_projects is keyed by (billing_project, user) which will make queries much faster so they don't have to scan the batches aggregated resources table. I ran the migration successfully on a full test database and the audit was clean for jobs and batches except for the 5 batches that were running right when we started populating the original aggregated billing tables. . I'd like to gather all feedback and then will run the migration one more time to do a final test. Note, that most of the queries in the migration are not tested. The key thing to double check is the triggers will continue to insert data into the old tables and we get th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11996:1833,perform,perform,1833,https://hail.is,https://github.com/hail-is/hail/pull/11996,1,['perform'],['perform']
Performance,oml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-an,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33857,cache,cached,33857,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,ommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5822,Load,LoadPlink,5822,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,ommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:6629,concurren,concurrent,6629,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['concurren'],['concurrent']
Performance,"ommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; <decorator-gen-398>:2: in export_vcf; ???; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. func = <function export_vcf at 0x7fa13c4d9938>, args = (<hail.dataset.VariantDataset object at 0x7fa13c3c9390>, 'file:///scratch/test_vcf_export.vcf.bgz', None, False, False), kwargs = {}; e = Py4JJavaError(u'An error occurred while calling o160.exportVCF.\n', JavaObject id",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:1693,load,load,1693,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['load'],['load']
Performance,"on$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:11148,concurren,concurrent,11148,https://hail.is,https://github.com/hail-is/hail/issues/3063,1,['concurren'],['concurrent']
Performance,on$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:4884,concurren,concurrent,4884,https://hail.is,https://github.com/hail-is/hail/issues/3063,2,['concurren'],['concurrent']
Performance,"on.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAcc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:2767,load,loading,2767,https://hail.is,https://github.com/hail-is/hail/issues/1683,1,['load'],['loading']
Performance,on.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8338:11925,Optimiz,Optimize,11925,https://hail.is,https://github.com/hail-is/hail/issues/8338,1,['Optimiz'],['Optimize']
Performance,"on.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n'; error_id = -1. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); > raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E ; E Java stack trace:; E java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E 	at is.hail.annotations.Memory.checkAddress(Memory.java:226); E 	at is.hail.annotations.Memory.loadInt(Memory.java:140); E 	at is.hail.annotations.Region$.loadInt(Region.scala:20); E 	at __C92844etypeDecode.__m92863ord_compareNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92862ord_compareNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92861ord_ltNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92860ord_lt(Unknown Source); E 	at __C92844etypeDecode.__m92857arraySorter_merge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(U",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:3871,load,loadInt,3871,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['load'],['loadInt']
Performance,onalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.737ms self 0.071ms children 3.665ms %children 98.09%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.412ms self 0.412ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:134439,Optimiz,Optimize,134439,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,onalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.531ms self 0.089ms children 4.442ms %children 98.03%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:150226,Optimiz,Optimize,150226,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,onalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.039ms self 0.039ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 8.695ms self 0.088ms children 8.607ms %children 98.99%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.598ms self 0.598ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:118721,Optimiz,Optimize,118721,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,onalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.500ms self 0.649ms children 0.852ms %children 56.76%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.557ms self 0.557ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:94730,Optimiz,OptimizePass,94730,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,onalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.561ms self 0.732ms children 0.829ms %children 53.12%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.525ms self 0.525ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:109168,Optimiz,OptimizePass,109168,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,onalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.862ms self 0.753ms children 1.109ms %children 59.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.743ms self 0.743ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:73551,Optimiz,OptimizePass,73551,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,onalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.010ms self 0.828ms children 1.182ms %children 58.80%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.736ms self 0.736ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.055ms self 0.055ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:59153,Optimiz,OptimizePass,59153,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,onalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.451ms self 0.949ms children 1.502ms %children 61.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.977ms self 0.977ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:50625,Optimiz,OptimizePass,50625,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,onalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 9.513ms self 3.957ms children 5.556ms %children 58.40%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 4.091ms self 4.091ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.148ms self 0.148ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:42097,Optimiz,OptimizePass,42097,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"once this is in, am I right that the plan is:; - get transposed array<struct> etype in similarly; - make transposed array<struct> ptype; - optimize these three in tandem to beat current performance / file size",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7821#issuecomment-595928559:139,optimiz,optimize,139,https://hail.is,https://github.com/hail-is/hail/pull/7821#issuecomment-595928559,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"oncurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. The driver will have log output like this:; ```; 2023-09-22 19:11:13.051 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/8042383 response 200; 2023-09-22 19:11:13.052 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=: reading results; 2023-09-22 19:11:13.125 ServiceBackend$: INFO: all results read. 0.072746861 s. 0.0 result/s. 0.0 MiB/s.; 2023-09-22 19:11:13.125 : INFO: [collectDArray|table_native_writer]: executed 5 tasks in 1.822s; 2023-09-22 19:11:13.126 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.126 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.126 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 128.0K allocated (128.0K blocks / 0 chunks), regions.size = 2, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.138 : ERROR: GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:3194,cache,cache,3194,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['cache'],['cache']
Performance,oncurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19361,Load,LoadVCF,19361,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,one-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36117,cache,cached,36117,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"one-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3477,cache,cached,3477,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"one-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3569,cache,cached,3569,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"onious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done.; ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08] failed: Initialization action failed. Failed action 'gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py', see output in: gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output.; Traceback (most recent call last):; File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6634:2044,perform,performance,2044,https://hail.is,https://github.com/hail-is/hail/issues/6634,2,['perform'],['performance']
Performance,oogle_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36745,cache,cached,36745,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,oolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-1908254; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822:12608,concurren,concurrent,12608,https://hail.is,https://github.com/hail-is/hail/issues/1822,2,['concurren'],['concurrent']
Performance,"ools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1371,cache,cached,1371,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"oops, still need to fix the `null` loadFrom stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9646#issuecomment-728896170:35,load,loadFrom,35,https://hail.is,https://github.com/hail-is/hail/pull/9646#issuecomment-728896170,1,['load'],['loadFrom']
Performance,op (JI)V; RETURN; L1; LOCALVARIABLE get_tup_elem_o J L0 L1 6; LOCALVARIABLE get_tup_elem_o J L0 L1 8; LOCALVARIABLE invoke I L0 L1 10; LOCALVARIABLE local11 I L0 L1 11; LOCALVARIABLE invoke I L0 L1 12; MAXSTACK = 5; MAXLOCALS = 13; ```. ```; /// PREVIOUS VERSION; // access flags 0x1; public __m85wrapped(Lis/hail/annotations/Region;JJ)V; L0; GOTO L1; L1; FRAME SAME; LLOAD 4; LSTORE 6; GOTO L2; L2; FRAME APPEND [J]; LDC 0; ISTORE 8; GOTO L3; L3; FRAME APPEND [I]; ALOAD 0; ILOAD 8; PUTFIELD __C46CompiledWithAggs.__f78vm : Z; GOTO L4; L4; FRAME SAME; LLOAD 4; LSTORE 9; GOTO L5; L5; FRAME APPEND [J]; LDC 0; ISTORE 11; GOTO L6; L6; FRAME APPEND [I]; ALOAD 0; ILOAD 11; PUTFIELD __C46CompiledWithAggs.__f77km : Z; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; IFNE L7; GOTO L8; L8; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 12; ILOAD 12; ISTORE 13; ALOAD 0; ALOAD 1; ILOAD 13; INVOKEVIRTUAL __C46CompiledWithAggs.__m86str (Lis/hail/annotations/Region;I)J; LSTORE 14; GOTO L9; L9; FRAME APPEND [T T J]; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f77km : Z; IFNE L10; GOTO L11; L11; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 9; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 16; LDC 9999; ILOAD 16; ISUB; ISTORE 17; GOTO L12; L12; FRAME APPEND [T I]; ALOAD 0; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; LLOAD 14; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f77km : Z; ILOAD 17; INVOKEVIRTUAL __C46CompiledWithAggs.__m69take_by_seqop (ZJZI)V; RETURN; L10; FRAME CHOP 2; LDC 0; ISTORE 17; GOTO L12; L7; FRAME CHOP 3; LDC 0; LSTORE 14; GOTO L9; L13; LOCALVARIABLE get_tup_elem_o J L0 L13 6; LOCALVARIABLE bool Z L0 L13 8; LOCALVARIABLE get_tup_elem_o J L0 L13 9; LOCALVARIABLE bool Z L0 L13 11; LOCALVARIABLE invoke I L0 L13 12; LOCALVARIABLE local13 I L0 L13 13; LOCALVAR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408:1771,load,loadInt,1771,https://hail.is,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408,1,['load'],['loadInt']
Performance,"operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2; /_/; ; Using Scala version 2.12.13, OpenJDK 64-Bit Server VM, 11.0.21; Branch HEAD; Compiled by user liangchi on 2023-02-11T02:24:04Z; Revision 5103e00c4ce5fcc4264ca9c4df12295d42557af6; Url https://github.com/apache/spark; Type --help for more information.; ```. ```sh; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-class; /usr/bin/spark-class; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-class; SPARK_SCALA_VERSION=; ```. <details><summary>>>>>>>>>>> before load-spark-env.sh <<<<<<<<<</summary>; <p>; ```sh; XDG_SESSION_ID=10; HOSTNAME=ip-192-168-124-160; TERM=xterm-256color; SHELL=/bin/bash; HISTSIZE=1000; SSH_CLIENT=103.37.196.84 60539 22; QTDIR=/usr/lib64/qt-3.3; QTINC=/usr/lib64/qt-3.3/include; SSH_TTY=/dev/pts/0; USER=hadoop; LS_COLORS=rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045:6185,load,load-spark-env,6185,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045,1,['load'],['load-spark-env']
Performance,optimize splitmulti by adding flag to importvcf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/821:0,optimiz,optimize,0,https://hail.is,https://github.com/hail-is/hail/issues/821,1,['optimiz'],['optimize']
Performance,optimizer / compiler bug. Shouldn't be too hard to fix!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5212#issuecomment-457887049:0,optimiz,optimizer,0,https://hail.is,https://github.com/hail-is/hail/issues/5212#issuecomment-457887049,1,['optimiz'],['optimizer']
Performance,option to add variant loadings to variant annotations in PCA,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/166:22,load,loadings,22,https://hail.is,https://github.com/hail-is/hail/issues/166,1,['load'],['loadings']
Performance,option to load just a bit of a file for testing pipelines,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1719:10,load,load,10,https://hail.is,https://github.com/hail-is/hail/issues/1719,1,['load'],['load']
Performance,options for further improving filter intervals performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6084:47,perform,performance,47,https://hail.is,https://github.com/hail-is/hail/issues/6084,1,['perform'],['performance']
Performance,or 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 10 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 10 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:107861,concurren,concurrent,107861,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 19.0 in stage 0.0 (TID 19, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:102174,concurren,concurrent,102174,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 9.0 in stage 0.0 (TID 9, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:103628,concurren,concurrent,103628,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,or 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:105080,concurren,concurrent,105080,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,or 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 11 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 11 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:144989,concurren,concurrent,144989,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 17.1 in stage 0.0 (TID 47, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:140628,concurren,concurrent,140628,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 37.1 in stage 0.0 (TID 45, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:143535,concurren,concurrent,143535,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 7.1 in stage 0.0 (TID 46, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:142082,concurren,concurrent,142082,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,or 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:198396,concurren,concurrent,198396,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:193941,concurren,concurrent,193941,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 15.3 in stage 0.0 (TID 63, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:196942,concurren,concurrent,196942,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:195488,concurren,concurrent,195488,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 BlockManagerMaster: INFO: Removal of executor 13 requested; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Trying to remove executor 13 from BlockManagerMaster.; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 13; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q12.scc.bu.edu:45213 with 21.2 GB RAM, BlockManagerId(21, scc-q12.scc.bu.edu, 45213, None); 2019-01-22 13:12:03 BlockManagerInfo: INFO: Added broadcast_1_piece0 in memory on scc-q12.scc.bu.edu:45213 (size: 24.5 KB, free: 21.2 GB); 2019-01-22 13:12:04 BlockManagerInfo: I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:177447,concurren,concurrent,177447,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 17.2 in stage 0.0 (TID 54, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:175993,concurren,concurrent,175993,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 27.2 in stage 0.0 (TID 55, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:173085,concurren,concurrent,173085,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 37.2 in stage 0.0 (TID 52, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:174539,concurren,concurrent,174539,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 BlockManagerMaster: INFO: Removal of executor 15 requested; 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Trying to remove executor 15 from BlockManagerMaster.; 2019-01-22 13:12:02 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 15; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.187:49620) with ID 12; 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12, partition 25, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 TaskSetMa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:166898,concurren,concurrent,166898,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 25.2 in stage 0.0 (TID 51, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:165444,concurren,concurrent,165444,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 35.2 in stage 0.0 (TID 49, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:162537,concurren,concurrent,162537,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:163991,concurren,concurrent,163991,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 18 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 18 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 18; 2019-01-22 13:12:00 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.194:35002) with ID 15; 2019-01-22 13:12:00 TaskSetManager: INFO: Starting task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15, partition 5, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:00 TaskSetMana",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:153768,concurren,concurrent,153768,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:149407,concurren,concurrent,149407,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 35.1 in stage 0.0 (TID 43, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:150861,concurren,concurrent,150861,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 5.1 in stage 0.0 (TID 42, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:152315,concurren,concurrent,152315,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,or 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 BlockManagerMaster: INFO: Removal of executor 21 requested; 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Trying to remove executor 21 from BlockManagerMaster.; 2019-01-22 13:12:05 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 21; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 14.; 2019-01-22 13:12:06 DAGScheduler: INFO: Executor lost: 14 (epoch 16); 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 14 from BlockManagerMaster.; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Removing block manager BlockMa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:188946,concurren,concurrent,188946,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:187492,concurren,concurrent,187492,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:186038,concurren,concurrent,186038,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:184584,concurren,concurrent,184584,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/plinkTest.assoc.linear ... done. real 0m13.167s; user 0m13.071s; sys 0m0.080s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/50#issuecomment-152273684:1360,load,loaded,1360,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684,3,['load'],['loaded']
Performance,"or.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. 	at is.hail.utils.ErrorHandling.fatal(E",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:4990,Cache,CacheDir,4990,https://hail.is,https://github.com/hail-is/hail/issues/14513,4,['Cache'],['CacheDir']
Performance,"or.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.Spa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:2372,Cache,CacheDir,2372,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Cache'],['CacheDir']
Performance,or.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:7252,concurren,concurrent,7252,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['concurren'],['concurrent']
Performance,or.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseL,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:1573,concurren,concurrent,1573,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['concurren'],['concurrent']
Performance,"orImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:6184,Load,LoadVCF,6184,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,"orImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz: caught java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:9582,Load,LoadVCF,9582,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,ore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_6,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:34656,cache,cached,34656,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	... 34 more; Caused by: org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: java.lang.ArrayIndexOutOfBoundsException; ```. And the actual informative trace nested in the `hail.log`:; ```; Caused by: java.lang.ArrayIndexOutOfBoundsException: 1; at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.BinaryFun.apply(Fun.scala:108); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:143); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$36.apply(FunctionRegistry.scala:228); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOpti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:7608,concurren,concurrent,7608,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['concurren'],['concurrent']
Performance,"ormed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Exec",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2503,Load,LoadVCF,2503,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Load'],['LoadVCF']
Performance,"ort any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.1</h2>; <p>This is a patch release in the 1.5.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.1/whatsnew/v1.5.1.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.0/whatsnew/v1.5.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.5.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <p><code>conda install -c conda-forge pandas</code></p>; <p>Or via PyPI:</p>; <p><code>python3 -m pip install --upgrade pandas</code></p>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.5.0rc0</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pandas-dev/pandas/commit/8dab54d6573f7186ff0c3b6364d5e4dd635ff3e7""><code>8dab54d</code></a> RLS: 1.5.2</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12564:1813,perform,performance,1813,https://hail.is,https://github.com/hail-is/hail/pull/12564,1,['perform'],['performance']
Performance,"ost task 5.1 in stage 0.0 (TID 42, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 18 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 18 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 18; 2019-01-22 13:12:00 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.194:35002) with ID 15; 2019-01-22 13:12:00 TaskSetManager: INFO: Starting task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15, partition 5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:153707,concurren,concurrent,153707,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ost task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 25.2 in stage 0.0 (TID 51, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:165383,concurren,concurrent,165383,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:195427,concurren,concurrent,195427,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ost task 7.1 in stage 0.0 (TID 46, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 37.1 in stage 0.0 (TID 45, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:143474,concurren,concurrent,143474,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ost task 7.2 in stage 0.0 (TID 53, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 27.2 in stage 0.0 (TID 55, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:173024,concurren,concurrent,173024,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ost task 9.1 in stage 0.0 (TID 65, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:184523,concurren,concurrent,184523,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,otal 0.526ms self 0.526ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:6207,Optimiz,OptimizePass,6207,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"otocol); * In IntelliJ, go to File->Open, and choose the hail root directory; * When the project is open, go to File->Project Structure; * in the Project pane, set an sdk (8 or 11), and set the language level to 8; * in the Modules pane, delete the existing root module, click the plus sign -> Import Module, choose the `hail/` subdirectory, and choose ""Import module from external model"" and `BSP`; * you should see a progress bar at the bottom as it imports the project; * when it's done, quit and reopen IntelliJ. There should now be a bsp icon (two bars with two arrows between them) on the right, where the gradle elephant used to be. Just like before, sometimes you'll need to click the ""reload"" icon in there if things get wonky.; * if it says ""scalafmt configuration detected"", go ahead and enable the formatter. ## Metals setup. * delete any `.metals` directories; * open the hail repo in VSCode (even if you won't use VSCode, this seems to be the best way to get metals set up initially); * it should ask you to import a Mill build; * when that finishes, at the bottom it should say it's connected to a Bloop build server. In general, I think using Mill as the BSP directly will work best, but I don't have much experience to say for sure. To switch, run `Metals: switch build server` from the command palette. ## Debug and release builds. As before, debug mode adds some (fairly expensive) checking to our native memory system. But now there are a few other differences:; * treat warnings as errors only in release mode, so you can still compile, run tests, etc. during development without fixing all warnings; * enable optimization in scalac only in release mode. The intention is that we use debug mode during development, and release mode ony for published artifacts, or performance profiling. Mill will use debug mode by default. To enable release mode, define `HAIL_RELEASE_MODE` in your environment. Note changing this will invalidate all mill intermediates and rebuild from scratch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147:5452,optimiz,optimization,5452,https://hail.is,https://github.com/hail-is/hail/pull/14147,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"ound. In the world of low-level container runtimes there exists the term ""bundle"", which basically means the pair of a root filesystem and a `config.json` file containing all of the other necessary information to run the container. If you invoke `crun` or `runc` with `--bundle /path/to/bundle`, the runtime assumes the following:. - The configuration file for the container is located at `/path/to/bundle/config.json`; - That `config.json` contains a field [`root.path`](https://github.com/opencontainers/runtime-spec/blob/main/config.md#root) that specifies the location of the root filesystem, most commonly as a path relative to `/path/to/bundle`. `crun` offers a way to explicitly reference the location of `config.json` through its `--config` flag. This seems fairly innocuous, but specifying a custom `--config` path can have some unfortunate unintended consequences because it invalidates the assumption in the specification that the configuration resides at `/path/to/bundle/config.json`. Specifically, it breaks [Hooks](https://github.com/opencontainers/runtime-spec/blob/main/config.md#posix-platform-hooks). When a hook is run, the runtime (crun) feeds it the [container state](https://github.com/opencontainers/runtime-spec/blob/main/runtime.md#state), a JSON of information about the container including the `bundle` path. Any hook that attempts to load the `config.json`, like for example, the `nvidia-container-runtime-hook`, will crash. ### Change. This change stops using the `--config` flag for crun and instead does the following to create a well-formed bundle:. - Instead of the bundle being the merged directory of the container overlay, it is the container's scratch directory; - `root.path` is adjusted inside of `config.json` to now point to the merged directory of the container overlay. I've opted to use an absolute path here because why use a relative path.; - Move `config.json` into the container scratch directory so that it is inside the root of the bundle directory.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13438:1373,load,load,1373,https://hail.is,https://github.com/hail-is/hail/pull/13438,1,['load'],['load']
Performance,ources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../reso,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5659:1974,cache,cache-tests,1974,https://hail.is,https://github.com/hail-is/hail/issues/5659,1,['cache'],['cache-tests']
Performance,"out); File ""/usr/local/lib/python3.7/asyncio/tasks.py"", line 442, in wait_for; return fut.result(); File ""/usr/local/lib/python3.7/asyncio/futures.py"", line 181, in result; raise self._exception; File ""/usr/local/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/usr/local/lib/python3.7/site-packages/aiodocker/images.py"", line 46, in get; return await self.inspect(name); File ""/usr/local/lib/python3.7/site-packages/aiodocker/images.py"", line 36, in inspect; response = await self.docker._query_json(""images/{name}/json"".format(name=name)); File ""/usr/local/lib/python3.7/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.7/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.7/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'no such image: gcr.io/hail-vdc/query:tfkm2kev7zcf: No such image: gcr.io/hail-vdc/query:tfkm2kev7zcf'). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.7/site-packages/batch/worker/worker.py"", line 372, in run; await self.ensure_image_is_pulled(); File ""/usr/local/lib/python3.7/site-packages/batch/worker/worker.py"", line 363, in ensure_image_is_pulled; docker.images.pull, self.image); File ""/usr/local/lib/python3.7/site-packages/batch/worker/worker.py"", line 111, in wrapper; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/local/lib/python3.7/asyncio/tasks.py"", line 442, in wait_for; return fut.result(); File ""/usr/local/lib/python3.7/asyncio/futures.py"", line 181, in result; raise self._exception; File ""/usr/local/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/usr/l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9902:2646,load,loads,2646,https://hail.is,https://github.com/hail-is/hail/pull/9902,1,['load'],['loads']
Performance,"ow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-df17cef; Error summary: IndexOutOfBoundsException: 3; ```; (NB: a custom VEP/LOFTEE, but that shouldn't matter - ran same thing on `devel-cd48e11` and it worked fine)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:6869,concurren,concurrent,6869,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,2,['concurren'],['concurrent']
Performance,owerAndExecute total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply total 1m41.8s self 1m40.6s children 1.127s %children 1.11%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply total 1.106s self 33.060ms children 1.072s %children 97.01%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 31.509ms self 1.445ms children 30.064ms %children 95.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.344ms self 0.344ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 29.693ms self 9.419ms children 20.274ms %children 68.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.appl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:2371,Optimiz,OptimizePass,2371,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,p-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-n,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32461,cache,cached,32461,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"p.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$4(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2191/716305671.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). ""pool-2-thread-1"" #26 prio=5 os_prio=0 tid=0x00007f502866e000 nid=0x88c waiting on condition [0x00007f50275fd000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.close(ServiceCacheableFS.scala:46); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 	at java.io.ObjectOutputStream$BlockD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:3644,concurren,concurrent,3644,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,"p/ensembl-vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 241. Traceback (most recent call last):; File ""/hail-vep/vep.py"", line 218, in <module>; main(action, consequence, tolerate_parse_error, block_size, input_file, output_file, part_id, vep_cmd); File ""/hail-vep/vep.py"", line 199, in main; results = run_vep(vep_cmd, input_file, block_size, consequence, tolerate_parse_error, part_id, os.environ); File ""/hail-vep/vep.py"", line 127, in run_vep; raise ValueError(f'VEP command {vep_cmd} failed with non-zero exit status {proc.returncode}\n'; ValueError: VEP command ['/vep/vep', '--input_file', '/io/input', '--format', 'vcf', '--json', '--everything', '--allele_number', '--no_stats', '--cache', '--offline', '--minimal', '--assembly', 'GRCh38', '--fasta', '/vep_data//homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz', '--plugin', 'LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:/vep_data//gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/vep_data//human_ancestor.fa.gz,conservation_file:/vep_data//loftee.sql', '--dir_plugins', '/vep/ensembl-vep/Plugins/', '--dir_cache', '/vep_data/', '-o', 'STDOUT'] failed with non-zero exit status -9; VEP error output:; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /vep/ensembl-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224:1731,cache,cache,1731,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224,1,['cache'],['cache']
Performance,"p/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89; 2023-09-24 01:58:51.513 : INFO: TaskReport: stage=0, partition=9571, attempt=0, peakBytes=4507648, peakBytesReadable=4.30 MiB, chunks requested=51, cache hits=0; 2023-09-24 01:58:51.513 : INFO: RegionPool: FREE: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:4314,concurren,concurrent,4314,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['concurren'],['concurrent']
Performance,p://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:144); 	a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:1164,Cache,Cache,1164,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,2,['Cache'],['Cache']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.032ms children 0.070ms %children 68.44%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:215616,Optimiz,Optimize,215616,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.228ms self 0.228ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.064ms self 0.064ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.101ms self 0.033ms children 0.068ms %children 67.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:207595,Optimiz,Optimize,207595,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.070ms self 0.070ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.108ms self 0.034ms children 0.074ms %children 68.32%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:199643,Optimiz,Optimize,199643,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.654ms self 0.654ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:194991,Optimiz,Optimize,194991,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.101ms self 0.033ms children 0.068ms %children 67.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.13%; is.hail.backend.BackendHttpHandler#handle x$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:209096,Optimiz,OptimizePass,209096,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.032ms children 0.070ms %children 68.44%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.019ms self 0.016ms children 0.003ms %children 16.42%; is.hail.backend.BackendHttpHandler#handle x$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:217117,Optimiz,OptimizePass,217117,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.024ms self 0.021ms children 0.003ms %children 13.55%; is.hail.backend.BackendHttpHandler#handle x$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:196492,Optimiz,OptimizePass,196492,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.108ms self 0.034ms children 0.074ms %children 68.32%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.067ms self 0.067ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.30%; is.hail.backend.BackendHttpHandler#handle x$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:201144,Optimiz,OptimizePass,201144,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3235:5427,concurren,concurrent,5427,https://hail.is,https://github.com/hail-is/hail/issues/3235,1,['concurren'],['concurrent']
Performance,"pache.commons.math3.distribution.{ChiSquaredDistribution, NormalDistribution}; import org.apache.commons.math3.random.JDKRandomGenerator; import org.apache.commons.math3.util.CombinatoricsUtils.factorialLog; import org.apache.hadoop; import org.apache.log4j.{ConsoleAppender, PatternLayout}; import org.apache.spark.SparkContext; import org.apache.spark.SparkException; import org.apache.spark.mllib.linalg.Vectors; import org.apache.spark.mllib.linalg.distributed.{DistributedMatrix, IndexedRow, IndexedRowMatrix}; import org.apache.spark.rdd.RDD; import org.apache.spark.sql.Row; import org.apache.spark.storage.StorageLevel; import org.apache.{hadoop => hd}; import org.json4s.JValue; import org.json4s.JsonAST._; import org.json4s._; import org.json4s.jackson.JsonMethods; import org.json4s.jackson.JsonMethods._; import org.json4s.jackson.Serialization; import org.json4s.jackson.{JsonMethods, Serialization}; import org.json4s.{DefaultFormats, Formats}; import org.sparkproject.guava.util.concurrent.MoreExecutors; ```. We explicitly depend on; - `htsjdk`; - `breeze`; - `json4s`. That leaves:. ```; import org.apache.avro.SchemaBuilder; import org.apache.avro.file.DataFileWriter; import org.apache.avro.generic.{GenericDatumWriter, GenericRecord, GenericRecordBuilder}; import org.apache.commons.io.IOUtils; import org.apache.commons.math3.distribution.ChiSquaredDistribution; import org.apache.commons.math3.distribution.{ChiSquaredDistribution, NormalDistribution}; import org.apache.commons.math3.random.JDKRandomGenerator; import org.apache.commons.math3.util.CombinatoricsUtils.factorialLog; import org.apache.hadoop; import org.apache.log4j.{ConsoleAppender, PatternLayout}; import org.apache.spark.SparkContext; import org.apache.spark.SparkException; import org.apache.spark.mllib.linalg.Vectors; import org.apache.spark.mllib.linalg.distributed.{DistributedMatrix, IndexedRow, IndexedRowMatrix}; import org.apache.spark.rdd.RDD; import org.apache.spark.sql.Row; import org.apache.spar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741:3953,concurren,concurrent,3953,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741,2,['concurren'],['concurrent']
Performance,pache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:81,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:2426,Load,LoadVCF,2426,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed. /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call -------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 37,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:23067,concurren,concurrent,23067,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed.; ```. ### Version. 0.2.115-f6017673dbb6. ### Relevant log output. ```shell; ________________________________ test_spectra_4 ________________________________; [gw2] linux -- Python 3.8.10 /usr/bin/python3. def test_spectra_4():; > spectra_helper(spec4). test/hail/methods/test_pca.py:229: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/hail/methods/test_pca.py:172: in spectra_helper; hail_V = (np.array(scores.scores.collect()) / singulars).T; <decorator-gen-538>:2: in collect; ???; /usr/local/lib/python3.8/dist-packages/hail/typecheck/check.py:584: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.8/dist-packages/hail/expr/expressions/base_expression.py:1132: in collect; return hl.eval(e); <decorato",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:9449,concurren,concurrent,9449,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,park$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.gua,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215862,concurren,concurrent,215862,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"park-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2802,cache,cached,2802,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"park-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.Spark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825:1066,load,loadClass,1066,https://hail.is,https://github.com/hail-is/hail/issues/825,1,['load'],['loadClass']
Performance,"park.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/Simplify, iteration: 0 total 0.077ms self 0.077ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0 total 0.659ms self 0.292ms children 0.367ms %children 55.66%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.367ms self 0.367ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.127ms self 0.127ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardRelationalLets, iteration: 0 total 0.030ms self 0.030ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/PruneDeadFields, iteration: 0 total 0.572ms self 0.572ms children 0.000ms %ch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:13155,Optimiz,Optimize,13155,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,parkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.019ms self 0.016ms children 0.003ms %children 16.42%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:218761,Optimiz,Optimize,218761,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,parkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:210740,Optimiz,Optimize,210740,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,parkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:202788,Optimiz,Optimize,202788,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,parkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.024ms self 0.021ms children 0.003ms %children 13.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:198136,Optimiz,Optimize,198136,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,part of query optimizer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/821#issuecomment-301784690:14,optimiz,optimizer,14,https://hail.is,https://github.com/hail-is/hail/issues/821#issuecomment-301784690,1,['optimiz'],['optimizer']
Performance,"passes on local now: test_rectangles_to_numpy. I think the easiest thing to do is to just switch on spark vs non-spark. For spark its important we're not using HDFS for tofile, but for Local and Service, ideally, we just use a normal `hl.TemporaryFilename` and load it via `hl.current_backend().fs.open`. I'm not sure how finicky numpy.fromfile / tofile are about their file objects though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11637#issuecomment-1078079261:261,load,load,261,https://hail.is,https://github.com/hail-is/hail/pull/11637#issuecomment-1078079261,1,['load'],['load']
Performance,"path.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1729,load,loading,1729,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['load'],['loading']
Performance,"pc_rel['kin'] >= 0.0883); print(""finding Max ind set""); related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,keep=False); print(""writing related samples to remove""); related_samples_to_remove.export(""file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/related_samples_""+pop+"".tsv""); result = mt.filter_cols(; hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False); print(""final unrelated count""); print(result.count); eigenvalues, scores, loadings = hl.hwe_normalized_pca(result.GT, k=10); scores.export('file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/unrelated_pcs_'+pop+'.tsv'). ```. ```; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to logs/adgc_pc_relate.autosome_all.log; 2020-08-20 10:14:00 Hail: INFO: Reading table to impute column types; [Stage 0:===========================================================(1 + 0) / 1]2020-08-20 10:14:07 Hail: INFO: Loading 76 fields. Counts by type:; 66 fields: user-specified int32; 6 fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Sta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:4491,Load,Loading,4491,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Load'],['Loading']
Performance,"pe hints for sync and async queues <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/374"">#374</a></li>; </ul>; <h2>0.6.2 (2021-10-24)</h2>; <ul>; <li>Fix Python 3.10 compatibility <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/358"">#358</a></li>; </ul>; <h2>0.6.1 (2020-10-26)</h2>; <ul>; <li>; <p>Raise RuntimeError on queue.join() after queue closing. <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/295"">#295</a></p>; </li>; <li>; <p>Replace <code>timeout</code> type from <code>Optional[int]</code> to <code>Optional[float]</code> <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/267"">#267</a></p>; </li>; </ul>; <h2>0.6.0 (2020-10-10)</h2>; <ul>; <li>; <p>Drop Python 3.5, the minimal supported version is Python 3.6</p>; </li>; <li>; <p>Support Python 3.9</p>; </li>; <li>; <p>Refomat with <code>black</code></p>; </li>; </ul>; <h2>0.5.0 (2020-04-23)</h2>; <ul>; <li>Remove explicit loop arguments and forbid creating queues outside event loops <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/246"">#246</a></li>; </ul>; <h2>0.4.0 (2018-07-28)</h2>; <ul>; <li>; <p>Add <code>py.typed</code> macro <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/89"">#89</a></p>; </li>; <li>; <p>Drop python 3.4 support and fix minimal version python3.5.3 <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/88"">#88</a></p>; </li>; <li>; <p>Add property with that indicates if queue is closed <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/86"">#86</a></p>; </li>; </ul>; <h2>0.3.2 (2018-07-06)</h2>; <ul>; <li>Fixed python 3.7 support <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/97"">#97</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/janus/commit/0783f9b7a9bb7e1c095e93ebb4aad4f1e2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11466:1798,queue,queues,1798,https://hail.is,https://github.com/hail-is/hail/pull/11466,1,['queue'],['queues']
Performance,"peline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1561.1 in stage 8.0 (T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:3573,concurren,concurrent,3573,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,performance tests should catch stuff like this,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4178#issuecomment-414487778:0,perform,performance,0,https://hail.is,https://github.com/hail-is/hail/pull/4178#issuecomment-414487778,1,['perform'],['performance']
Performance,"pha/batches/8042383 response 200; 2023-09-22 19:11:13.052 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=: reading results; 2023-09-22 19:11:13.125 ServiceBackend$: INFO: all results read. 0.072746861 s. 0.0 result/s. 0.0 MiB/s.; 2023-09-22 19:11:13.125 : INFO: [collectDArray|table_native_writer]: executed 5 tasks in 1.822s; 2023-09-22 19:11:13.126 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.126 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.126 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 128.0K allocated (128.0K blocks / 0 chunks), regions.size = 2, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.138 : ERROR: GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; From is.hail.relocated.com.google.cloud.storage.StorageException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:3587,cache,cache,3587,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['cache'],['cache']
Performance,pl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-ebabd77; Error summary: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. ​; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:6638,concurren,concurrent,6638,https://hail.is,https://github.com/hail-is/hail/issues/1806,2,['concurren'],['concurrent']
Performance,"pleElement 0 (Ref __iruid_490))))))\n (Let __iruid_491\n (MakeTuple (0)\n (ResultOp 0 (Collect (CollectStateSig +PInt32))))\n (MakeStruct\n (idx (GetTupleElement 0 (Ref __iruid_491)))))))))\n2022-11-15 20:30:18.191 root: INFO: optimize optimize: compileLowerer, after InlineApplyIR: after: IR size 56:\n(MakeTuple (0)\n (Let __iruid_508\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (CollectStateSig +PInt32)) ()))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32))))\n (Let __iruid_509\n (CollectDistributedArray\n table_aggregate_singlestage __iruid_510\n __iruid_511\n (ToStream False\n (Literal Array[Struct{start:Int32,end:Int32}]\n <literal value>))\n (MakeStruct (__iruid_458 (Ref __iruid_508)))\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0\n (GetField __iruid_458 (Ref __iruid_511)))))\n (StreamFor __iruid_512\n (StreamMap __iruid_513\n (StreamRange -1 True\n (GetField start (Ref __iruid_510))\n (GetField end (Ref __iruid_510))\n (I32 1))\n (MakeStruct (idx (Ref __iruid_513))))\n (Begin\n (SeqOp 0 (Collect (CollectStateSig +PInt32))\n ((GetField idx (Ref __iruid_512)))))))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32))))\n (NA String))\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0 (Ref __iruid_508))))\n (StreamFor __iruid_514\n (ToStream True (Ref __iruid_509))\n (Begin\n (CombOpValue 0 (Collect (CollectStateSig +PInt32))\n (GetTupleElement 0 (Ref __iruid_514))))))\n (Let __iruid_515\n (MakeTuple (0)\n (ResultOp 0 (Collect (CollectStateSig +PInt32))))\n (MakeStruct\n (idx (GetTupleElement 0 (Ref __iruid_515)))))))))\n2022-11-15 20:30:18.195 root: INFO: optimize optimize: compileLowerer, after LowerArrayAggsToRunAggs: before: IR size 56: \n(MakeTuple (0)\n (Let __iruid_508\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (Coll",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:7896,optimiz,optimize,7896,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,4,['optimiz'],['optimize']
Performance,"plink removed 160 of 7516. Hail removed 424. Also, Hail spent 2s removing 419, and 9 minutes removing the other 5. ```; 2018-10-06 09:42:36 Hail: INFO: ld_prune: running local pruning stage with max queue size of 2166480 variants; 2018-10-06 09:42:37 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:42:38 Hail: INFO: wrote 7097 items in 100 partitions; 2018-10-06 09:42:40 Hail: INFO: wrote 7097 items in 100 partitions to file:/tmp/hail.jM6D3jREhNqh/Jx7rAbqyTP; 2018-10-06 09:42:40 Hail: INFO: ld_prune: local pruning stage retained 7097 variants; 2018-10-06 09:42:41 Hail: INFO: Wrote all 2 blocks of 7097 x 284 matrix with block size 4096.; 2018-10-06 09:42:59 Hail: INFO: Ordering unsorted dataset with network shuffle; 2018-10-06 09:46:52 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:46:52 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:48:58 Hail: INFO: Ordering unsorted dataset with network shuffle; 2018-10-06 09:48:58 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:46 Hail: INFO: Ordering unsorted dataset with network shuffle; 2018-10-06 09:51:46 Hail: INFO: wrote 5 items in 3 partitions; 2018-10-06 09:51:46 Hail: INFO: ld_prune: correlation graph of locally-pruned variants has 5 edges,; finding maximal independent set...; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: wrote 7092 items in 100 partitions; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4506#issuecomment-427575965:199,queue,queue,199,https://hail.is,https://github.com/hail-is/hail/issues/4506#issuecomment-427575965,1,['queue'],['queue']
Performance,"plit. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) truncate on store and load a single bit. Our value of `3` was stored as a class Boolean, and came out `1`. The fact that a single field's missingness was flipped was a red herring -- all higher bits are flipped to 0 (defined)! Here's a look at the LIR looks like, though it was ultimately the JVM class file printout that tipped me off to the problem:. ```code. # I2B is stored as a class field on spills. The Z at the end of the next line indicates this field is a Boolean, not a byte. 31017 (PutFieldX PUTFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 31018 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;); 31019 25774 (InsnX I2B; 31020 25775 (InsnX IOR; 31021 25776 (InsnX IOR; 31022 25777 (InsnX IOR; 31023 25778 (InsnX IOR; 31024 25779 (LdcX 0 I); 31025 25780 (InsnX ISHL; 31026 (GetFieldX GETFIELD __C2316__m1984ENCODE_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:3633,load,load,3633,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['load'],['load']
Performance,"plitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Python 3.10 ssl module</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d725a9b56bb8baf87c9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10544:1638,perform,performance,1638,https://hail.is,https://github.com/hail-is/hail/pull/10544,1,['perform'],['performance']
Performance,plus some router config fixes. Mainly I'm PR'ing this so I can use the cached base image when testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6936:71,cache,cached,71,https://hail.is,https://github.com/hail-is/hail/pull/6936,1,['cache'],['cached']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.046ms self 0.046ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.655ms self 0.655ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._appl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:151618,Optimiz,OptimizePass,151618,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.412ms self 0.412ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.609ms self 0.609ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._appl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:135831,Optimiz,OptimizePass,135831,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.598ms self 0.598ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.054ms self 0.054ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.566ms self 0.566ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._appl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:120113,Optimiz,OptimizePass,120113,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.525ms self 0.525ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.264ms self 0.264ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:109930,Optimiz,Optimize,109930,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.557ms self 0.557ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.257ms self 0.257ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:95492,Optimiz,Optimize,95492,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.736ms self 0.736ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.055ms self 0.055ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.391ms self 0.391ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:59915,Optimiz,Optimize,59915,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.743ms self 0.743ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.318ms self 0.318ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:74313,Optimiz,Optimize,74313,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.977ms self 0.977ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.476ms self 0.476ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:51387,Optimiz,Optimize,51387,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 4.091ms self 4.091ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.148ms self 0.148ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 1.317ms self 1.317ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:42859,Optimiz,Optimize,42859,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-15f58831fe57; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4263:12081,concurren,concurrent,12081,https://hail.is,https://github.com/hail-is/hail/issues/4263,2,['concurren'],['concurrent']
Performance,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f80a6f10bc84; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:12087,concurren,concurrent,12087,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719,2,['concurren'],['concurrent']
Performance,ply$4.apply(LinearRegression.scala:130); at org.broadinstitute.hail.methods.LinearRegression$$anonfun$apply$4.apply(LinearRegression.scala:129); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$20.next(Iterator.scala:635); at scala.collection.Iterator$$anon$20.next(Iterator.scala:633); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/336:3005,concurren,concurrent,3005,https://hail.is,https://github.com/hail-is/hail/issues/336,2,['concurren'],['concurrent']
Performance,ply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217684,concurren,concurrent,217684,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,ply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.Cont,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13105,Load,LoadMatrix,13105,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,ply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.023ms self 0.023ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply total 20.227ms self 0.484ms children 19.744ms %children 97.61%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.NormalizeNames.apply total 0.154ms self 0.154ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:191849,Optimiz,OptimizePass,191849,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass total 0.035ms self 0.018ms children 0.018ms %children 50.25%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.lowering.MatrixLoweredToTable total 0.010ms self 0.010ms children 0.000ms %ch,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:184824,Optimiz,OptimizePass,184824,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.189ms self 0.189ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LiftRelationalValuesToRelationalLets total 1.058ms self 0.966ms children 0.092ms %children 8.70%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LiftRelationalValuesToRelationalLets/is.hail.expr.ir.lowering.MatrixLoweredToTable total 0.033ms self 0.033ms children 0.000m,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:18559,Optimiz,OptimizePass,18559,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.193ms self 0.193ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.051ms self 0.051ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerMatrixToTablePass total 4.041ms self 3.971ms children 0.070ms %children 1.74%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerMatrixToTablePass/is.hail.expr.ir.lowering.AnyIR total 0.027ms self 0.027ms children 0.000ms %children 0.00%; is.hail.backend.BackendH,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:11575,Optimiz,OptimizePass,11575,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPip,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:7283,Optimiz,OptimizePass,7283,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,poch 11); 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 11 on scc-q17.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:137847,concurren,concurrent,137847,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,poch 13); 2019-01-22 13:12:02 YarnScheduler: ERROR: Lost executor 15 on scc-q10.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:159756,concurren,concurrent,159756,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,poch 15); 2019-01-22 13:12:05 YarnScheduler: ERROR: Lost executor 21 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:181804,concurren,concurrent,181804,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,poch 16); 2019-01-22 13:12:06 YarnScheduler: ERROR: Lost executor 12 on scc-q03.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:191160,concurren,concurrent,191160,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"port get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3330,load,loads,3330,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['load'],['loads']
Performance,pply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.409ms self 0.648ms children 0.762ms %children 54.04%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.463ms self 0.463ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.058ms self 0.058ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.Spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:138747,Optimiz,OptimizePass,138747,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.228ms self 1.353ms children 0.875ms %children 39.29%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.340ms self 0.340ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.Spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:154534,Optimiz,OptimizePass,154534,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.252ms self 0.765ms children 1.487ms %children 66.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.505ms self 0.505ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.Spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:123029,Optimiz,OptimizePass,123029,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply(ContextRDD.scala:373); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$30.apply(ContextRDD.scala:373); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:153); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:153); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5371:8285,concurren,concurrent,8285,https://hail.is,https://github.com/hail-is/hail/issues/5371,2,['concurren'],['concurrent']
Performance,pply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.bac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8338:11774,Optimiz,Optimize,11774,https://hail.is,https://github.com/hail-is/hail/issues/8338,1,['Optimiz'],['Optimize']
Performance,pply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189); 	at scala.Option.foreach(Option.scala:407); 	at org.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:8042,concurren,concurrent,8042,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['concurren'],['concurrent']
Performance,"pply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:19632,concurren,concurrent,19632,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['concurren'],['concurrent']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.759ms self 0.759ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.137ms self 0.137ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:71462,Optimiz,OptimizePass,71462,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.561ms self 0.561ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.144ms self 0.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:107079,Optimiz,OptimizePass,107079,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.785ms self 0.785ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:57064,Optimiz,OptimizePass,57064,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.614ms self 0.614ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.292ms self 0.292ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:92641,Optimiz,OptimizePass,92641,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.051ms self 0.051ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.948ms self 0.948ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.222ms self 0.222ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:48536,Optimiz,OptimizePass,48536,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.135ms self 0.135ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 3.651ms self 3.651ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 12.144ms self 12.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:40006,Optimiz,OptimizePass,40006,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.073ms self 0.037ms children 0.036ms %children 49.79%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.641ms self 0.641ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:112067,Optimiz,OptimizePass,112067,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.074ms self 0.035ms children 0.040ms %children 53.33%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.635ms self 0.635ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:97629,Optimiz,OptimizePass,97629,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.089ms self 0.043ms children 0.047ms %children 52.21%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.830ms self 0.830ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:76450,Optimiz,OptimizePass,76450,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.098ms self 0.049ms children 0.049ms %children 50.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.756ms self 0.756ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:62052,Optimiz,OptimizePass,62052,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.138ms self 0.089ms children 0.049ms %children 35.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.885ms self 0.885ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:53524,Optimiz,OptimizePass,53524,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.144ms self 0.075ms children 0.069ms %children 48.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.069ms self 0.069ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.703ms self 4.703ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:44996,Optimiz,OptimizePass,44996,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.240ms self 0.240ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.087ms self 0.042ms children 0.045ms %children 51.66%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:141058,Optimiz,Optimize,141058,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.490ms self 0.490ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.083ms self 0.043ms children 0.040ms %children 48.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:156845,Optimiz,Optimize,156845,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.938ms self 0.938ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.101ms self 0.049ms children 0.052ms %children 51.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:125340,Optimiz,Optimize,125340,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"pport for jetty via <a href=""http://kohlschutter.github.io/junixsocket/junixsocket-jetty/"">junixsocket-jetty</a></li>; <li>Fix Selector logic (more bug fixes)</li>; <li>Documentation updates</li>; </ul>; <h2>junixsocket 2.5.0</h2>; <ul>; <li>New supported platforms: AIX 7 Power64, IBM i Power64, Windows ARM64, Windows Server 2019 &amp; 2022</li>; <li>Generic rework to support more than just Unix Domain sockets</li>; <li>Add support for AF_TIPC (on Linux)</li>; <li>Add support for using sockets passed as standard input</li>; <li>Add support for address-specific, non-standard URIs (for example; unix:// and tipc://), as well as socat addresses</li>; <li>Add support for using FileDescriptor for ProcessBuilder Redirects (Java 9+)</li>; <li>Add support for peer credentials (PID) on Windows</li>; <li>Fix Selector logic</li>; <li>Fix cross-compilation on Apple Silicon</li>; <li>Fix a file descriptor leak (regression in 2.4.0)</li>; <li>Improve behavior on partially unsupported platforms and allow loading of Windows 10 native; library on other Windows versions (e.g., Windows Server 2022, Windows 8.1).</li>; <li>Javadoc improvements, Code cleanup</li>; <li>Deprecate AFUNIXSocketCapability in favor of AFSocketCapability</li>; <li>Drop support for Java 7</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/8bfc43a332d5573397b72b778fed2b8c13d1dfc1""><code>8bfc43a</code></a> Fix PMD warning</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/53e668df0d279b368d81db8b67576342927ad892""><code>53e668d</code></a> native: Disable DEBUG by default</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/f8423eaee2871623113bd0200f510a292aa165d1""><code>f8423ea</code></a> docs: Update GraalVM instructions</li>; <li><a href=""https://github.com/kohlschutter/junixsocket/commit/90a31b6309e653d2714dd6a35d43b45bc8e94002""><",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12483:3040,load,loading,3040,https://hail.is,https://github.com/hail-is/hail/pull/12483,1,['load'],['loading']
Performance,"pported --output minimaljre --strip-debug --no-man-pages --no-header-files --compress=2; ```; comes in at under 30MB gzipped, which would increase the PyPI package by about 20% in size, while allowing users to install and run Hail in _any_ supported python environment without having to consider Java versions at all. Alternatively, have you ever considered distributing Hail through conda-forge or bioconda, where you could specify a JRE that should be installed with it and automatically linked?. Is there a better channel than Github Issues for feature requests? I realize this is not a bug report, and if you want to just close it and say ""nope"" that's fine, but I've seen a good number of first-time hail users get a bad impression because of this. . ### Ramble about other nitpicks below. I don't want to spam this repo with issues, but I also noticed while poking around at hail:; 1. It seems to use the default Java GC, which is now G1 in Java 9+. Performance on newer Javas would likely improve with `-XX:+UseParallelGC` in java opts; 2. The Hail jar includes module-info.class from azure storage, this broke my first attempt to use `jdeps` to see what modules it needs. Specifically, `hail-all-spark.jar` says it exports:; ```; backend % jar --describe-module --file=hail-all-spark.jar; releases: 9. com.azure.storage.blob@12.22.0 jar:file:///Users/alex/src/hail/hail/build/deploy/hail/backend/hail-all-spark.jar!/module-info.class; exports com.azure.storage.blob; exports com.azure.storage.blob.models; exports com.azure.storage.blob.options; exports com.azure.storage.blob.sas; exports com.azure.storage.blob.specialized; requires com.azure.storage.common transitive; requires com.azure.storage.internal.avro; requires com.fasterxml.jackson.dataformat.xml; requires java.base mandated; qualified exports com.azure.storage.blob.implementation to com.azure.storage.blob.batch com.azure.storage.blob.cryptography com.azure.storage.file.datalake; qualified exports com.azure.storage.blob.imple",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14433:3539,Perform,Performance,3539,https://hail.is,https://github.com/hail-is/hail/issues/14433,1,['Perform'],['Performance']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.737ms self 0.071ms children 3.665ms %children 98.09%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.412ms self 0.412ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:134410,Optimiz,OptimizePass,134410,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.531ms self 0.089ms children 4.442ms %children 98.03%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:150197,Optimiz,OptimizePass,150197,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.039ms self 0.039ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 8.695ms self 0.088ms children 8.607ms %children 98.99%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.598ms self 0.598ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:118692,Optimiz,OptimizePass,118692,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.500ms self 0.649ms children 0.852ms %children 56.76%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.557ms self 0.557ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:94759,Optimiz,Optimize,94759,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.561ms self 0.732ms children 0.829ms %children 53.12%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.525ms self 0.525ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:109197,Optimiz,Optimize,109197,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.862ms self 0.753ms children 1.109ms %children 59.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.743ms self 0.743ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:73580,Optimiz,Optimize,73580,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.010ms self 0.828ms children 1.182ms %children 58.80%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.736ms self 0.736ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.055ms self 0.055ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:59182,Optimiz,Optimize,59182,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.451ms self 0.949ms children 1.502ms %children 61.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.977ms self 0.977ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:50654,Optimiz,Optimize,50654,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 9.513ms self 3.957ms children 5.556ms %children 58.40%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 4.091ms self 4.091ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.148ms self 0.148ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:42126,Optimiz,Optimize,42126,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pr.ir.lowering.InlineApplyIR.transform total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.InlineApplyIR/is.hail.expr.ir.lowering.CompilableIRNoApply total 0.015ms self 0.015ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.518ms self 0.004ms children 0.514ms %children 99.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.496ms self 0.040ms children 0.456ms %children 91.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:205819,Optimiz,OptimizePass,205819,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,precated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:35745,cache,cached,35745,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"present. These should have written the byte. 1<<0 | 1<<1 | 0<<2 | 0<<3; ==> b00000011; ==> 3. But instead wrote the byte `b00000001 or 1`, incorrectly leading readers to try to read field B when it was missing (and not written). This is due to the load-bearing and incorrect type of an I2B instruction generated [here](https://github.com/hail-is/hail/blob/8bd9b7b2224b77372a72f02f2b13806267892a35/hail/src/main/scala/is/hail/types/encoded/EBaseStruct.scala#L107). I2B is an instruction that truncates an integer to a byte, and it is used in various places in code generation but primarily encoding missing bits in arrays and structs. . I2B loads a byte to the stack, not a boolean. TypeInfos are mostly non-structural since they rarely influence the bytecode generated. Here is an exception, and that's where the method splitter comes in. Method splitting exists in the Hail compiler because not only does the JVM have limits on how large methods can be, but also the JIT compiler handles small methods much more effectively than large methods (and so splitting a large method into two small ones can make an order of magnitude or more in performance difference). We have three forms of method splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:1561,perform,performance,1561,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['perform'],['performance']
Performance,print line of file on error in LoadMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3849:31,Load,LoadMatrix,31,https://hail.is,https://github.com/hail-is/hail/pull/3849,1,['Load'],['LoadMatrix']
Performance,project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.r,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218567,concurren,concurrent,218567,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ps://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no files; ```; How can I solve it ?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076:2282,Load,LoadVCF,2282,https://hail.is,https://github.com/hail-is/hail/issues/2076,2,['Load'],['LoadVCF']
Performance,"ps://redirect.github.com/googleapis/java-storage/issues/2150"">#2150</a>) (<a href=""https://github.com/googleapis/java-storage/commit/330e795040592e5df22d44fb5216ad7cf2448e81"">330e795</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.14.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2151"">#2151</a>) (<a href=""https://github.com/googleapis/java-storage/commit/eba8b6a235919a27d1f6dadf770140c7d143aa1a"">eba8b6a</a>)</li>; </ul>; <h2>v2.25.0</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.24.0...v2.25.0"">2.25.0</a> (2023-07-24)</h2>; <h3>Features</h3>; <ul>; <li>BlobWriteChannelV2 - same throughput less GC (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2110"">#2110</a>) (<a href=""https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b"">1b52a10</a>)</li>; <li>Update Storage.createFrom(BlobInfo, Path) to have 150% higher throughput (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2059"">#2059</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4c2f44e28a1ff19ffb2a02e3cefc062a1dd98fdc"">4c2f44e</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>Update BlobWriteChannelV2 to properly carry forward offset after incremental flush (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2125"">#2125</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c099a2f4f8ea9afa6953270876653916b021fd9f"">c099a2f</a>)</li>; <li>Update GrpcStorageImpl.createFrom(BlobInfo, Path) to use RewindableContent (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2112"">#2112</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c80505129baa831e492a5514e937875407211595"">c805051</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://gi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:3685,throughput,throughput,3685,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['throughput'],['throughput']
Performance,ptions=-Xss4M|||spark:spark.executor.extraJavaOptions=-Xss4M|||spark:spark.speculation=true|||hdfs:dfs.replication=1|||dataproc:dataproc.logging.stackdriver.enable=false|||dataproc:dataproc.monitoring.stackdriver.enable=false|||spark:spark.driver.memory=36g|||yarn:yarn.nodemanager.resource.memory-mb=29184|||yarn:yarn.scheduler.maximum-allocation-mb=14592|||spark:spark.executor.cores=4|||spark:spark.executor.memory=5837m|||spark:spark.executor.memoryOverhead=8755m|||spark:spark.memory.storageFraction=0.2|||spark:spark.executorEnv.HAIL_WORKER_OFF_HEAP_MEMORY_PER_CORE_MB=3648' \; 9c9; < --metadata=^|||^WHEEL=gs://hail-30-day/hailctl/dataproc/dking-dev/0.2.126-a51eabd65859/hail-0.2.126-py3-none-any.whl|||PKGS=aiodns==2.0.0|aiohttp==3.9.1|aiosignal==1.3.1|async-timeout==4.0.3|attrs==23.1.0|avro==1.11.3|azure-common==1.1.28|azure-core==1.29.5|azure-identity==1.15.0|azure-mgmt-core==1.4.0|azure-mgmt-storage==20.1.0|azure-storage-blob==12.19.0|bokeh==3.3.1|boto3==1.33.1|botocore==1.33.1|cachetools==5.3.2|certifi==2023.11.17|cffi==1.16.0|charset-normalizer==3.3.2|click==8.1.7|commonmark==0.9.1|contourpy==1.2.0|cryptography==41.0.7|decorator==4.4.2|deprecated==1.2.14|dill==0.3.7|frozenlist==1.4.0|google-auth==2.23.4|google-auth-oauthlib==0.8.0|humanize==1.1.0|idna==3.6|isodate==0.6.1|janus==1.0.0|jinja2==3.1.2|jmespath==1.0.1|jproperties==2.1.1|markupsafe==2.1.3|msal==1.25.0|msal-extensions==1.0.0|msrest==0.7.1|multidict==6.0.4|nest-asyncio==1.5.8|numpy==1.26.2|oauthlib==3.2.2|orjson==3.9.10|packaging==23.2|pandas==2.1.3|parsimonious==0.10.0|pillow==10.1.0|plotly==5.18.0|portalocker==2.8.2|protobuf==3.20.2|py4j==0.10.9.5|pyasn1==0.5.1|pyasn1-modules==0.3.0|pycares==4.4.0|pycparser==2.21|pygments==2.17.2|pyjwt[crypto]==2.8.0|python-dateutil==2.8.2|python-json-logger==2.0.7|pytz==2023.3.post1|pyyaml==6.0.1|regex==2023.10.3|requests==2.31.0|requests-oauthlib==1.3.1|rich==12.6.0|rsa==4.9|s3transfer==0.8.0|scipy==1.11.4|six==1.16.0|sortedcontainers==2.4.0|tabulate==0.9.0|tenacity=,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14127:2095,cache,cachetools,2095,https://hail.is,https://github.com/hail-is/hail/pull/14127,1,['cache'],['cachetools']
Performance,"ptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1613,cache,cached,1613,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"py"", line 551, in pull; await docker_call_retry(; File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 840, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 460, in timed_out_f; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/lib/python3.9/asyncio/tasks.py"", line 479, in wait_for; return fut.result(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 484, in _pull_with_auth_refresh; return await docker.images.pull(image_ref_str, auth=credentials); File ""/usr/local/lib/python3.9/dist-packages/aiodocker/images.py"", line 133, in _handle_list; async with cm as response:; File ""/usr/local/lib/python3.9/dist-packages/aiodocker/utils.py"", line 309, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.9/dist-packages/aiodocker/docker.py"", line 275, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, 'Head ""https://us-docker.pkg.dev/v2/1/does-not-exist/manifests/latest"": denied: Permission ""artifactregistry.repositories.downloadArtifacts"" denied on resource ""projects/1/locations/us/repositories/does-not-exist"" (or it may not exist)'). The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 915, in run; await self.create(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 840, in create; await self._run_until_done_or_deleted(self.image.pull); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1012, in _run_until_done_or_deleted; return await run_until_done_or_deleted(self.deleted_event, f, *args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 682, in run_until_done_or_deleted; return step.result(); F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13907:1335,load,loads,1335,https://hail.is,https://github.com/hail-is/hail/issues/13907,1,['load'],['loads']
Performance,"py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 PyYAML-5.4.1 aiohttp-3.7.4 aiohttp-session-2.7.0 asyn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:7388,cache,cached,7388,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2722,cache,cached,2722,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:7063,Load,LoadVCF,7063,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Load'],['LoadVCF']
Performance,"python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37. ```. ### Traces No. 1: . ```java ; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 16.0 failed 4 tim",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:4215,load,loads,4215,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['load'],['loads']
Performance,"python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:29200,load,loads,29200,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['load'],['loads']
Performance,"q12.scc.bu.edu, executor 21, partition 29, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 YarnScheduler: ERROR: Lost executor 13 on scc-q16.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:170244,concurren,concurrent,170244,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"qq uses key_by, but yeah, it's an optimization. I mean, there's just so few cases where you actually want to use a spark shuffle. I don't think that many people are creating qq plots with 1 billion points.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7144#issuecomment-535750439:34,optimiz,optimization,34,https://hail.is,https://github.com/hail-is/hail/issues/7144#issuecomment-535750439,1,['optimiz'],['optimization']
Performance,"quote>; <h1>v24.2.2</h1>; <ul>; <li>fix: config reader handles bool types (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/pull/218"">#218</a>, <a href=""https://github.com/tomplus""><code>@​tomplus</code></a>)</li>; </ul>; <h1>v24.2.1</h1>; <ul>; <li>fixed watch.stream bug of not working with apis with follow kwarg (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/pull/216"">#216</a>, <a href=""https://github.com/mcreng""><code>@​mcreng</code></a>)</li>; </ul>; <h1>v24.2.0</h1>; <p>Kubernetes API Version: v1.24.2</p>; <h3>API Change</h3>; <ul>; <li>Add 2 new options for kube-proxy running in winkernel mode. <code>--forward-healthcheck-vip</code>, if specified as true, health check traffic whose destination is service VIP will be forwarded to kube-proxy's healthcheck service. <code>--root-hnsendpoint-name</code> specifies the name of the hns endpoint for the root network namespace. This option enables the pass-through load balancers like Google's GCLB to correctly health check the backend services. Without this change, the health check packets is dropped, and Windows node will be considered to be unhealthy by those load balancers. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/99287"">kubernetes/kubernetes#99287</a>, <a href=""https://github.com/anfernee""><code>@​anfernee</code></a>)</li>; <li>Added CEL runtime cost calculation into CustomerResource validation. CustomerResource validation will fail if runtime cost exceeds the budget. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108482"">kubernetes/kubernetes#108482</a>, <a href=""https://github.com/cici37""><code>@​cici37</code></a>)</li>; <li>Added a new metric <code>webhook_fail_open_count</code> to monitor webhooks that fail to open. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/107171"">kubernetes/kubernetes#107171</a>, <a href=""https://github.com/ltagliamonte-dd""><code>@​ltagliamonte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:1272,load,load,1272,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['load'],['load']
Performance,"quote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>urlize</code> filter by reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix whitespace being removed before tags in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents early; evaluation which could change the value of an expression.; :issue:<code>1186</code></li>; </ul>; <h2>Version 2.1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10209:3794,load,loaders,3794,https://hail.is,https://github.com/hail-is/hail/pull/10209,1,['load'],['loaders']
Performance,"r an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly different from keys) PType. ## Core Methods. ```scala; def elementType: PStruct; ```. - The PStruct representation of the key/value pair. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:5203,load,loads,5203,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loads']
Performance,"r both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't ne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:1259,cache,cache,1259,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659,2,['cache'],['cache']
Performance,"r later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <det",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:2208,load,loads,2208,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['load'],['loads']
Performance,"r than just being marked ""distributed"" which I think is your proposal). Blocks aren't heterogenous in our current setup (last block can be short) and I think that needs to be clarified in your proposal. Slicing has similar complications. I think the blocking should specify the list of sizes of each block, which makes it closed under slicing. I agree the user interface should do implicit broadcast but in the IR it should be explicit. > If we want to support sparse tensors. I vote we punt on sparse vectors for the time being. (In the sparse case, I think sparsity of the output should be inferred by analysis rather than specified. I think statically knowing the sparsity relationship is going to a rarity.). The key point here is that Patrick's operations are incredibly general so this proposal needs to be paired with an compilation strategy that guarantees his operations are never actually realized directly (e.g. deforestation, fusing operations (e.g. broadcast) into their consumers) while also generating BLAS-level performance. I propose we plan to build against BLAS while we investigate more general codegen strategies. To that end, I've suggested we read up on a few related projects in study group: taco (hat tip Patrick), TensorComprehensions, TVM (tensor virtual machine) and Halide. Might be other relevant ones (TF/XLA?) Few questions to ask:; - Can we steal good ideas for our IR?; - Can we target their IR?; - Can we integrate this with our stack?; - What's the performance like (compile time, runtime)?. We'll need some more prosaic operators:; - constructors: from array with shape, from shape and a function of indices; - get shape.; - index: get an element from a tuple of integers. This is only allowed for local arrays.; - slice: get another ndarray from a sequence of integers, ranges, or array of indices.; - reshape. Maybe local only? Or clarified in the distributed case. I think we'll need some additional basic operations supported by numpy (e.g. concatenate, tile),",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120:1164,perform,performance,1164,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120,1,['perform'],['performance']
Performance,"r#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform total 2.548ms self 0.302ms children 2.246ms %children 88.15%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/FoldConstants, iteration: 0 total 0.307ms self 0.307ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ExtractIntervalFilters, iteration: 0 total 0.016ms self 0.016ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0 total 0.363ms self 0.004ms children 0.358ms %children 98.82%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.358ms self 0.358ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/Simplify, iteration: 0 total 0.077ms self 0.077ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0 total 0.659ms self 0.292ms children 0.367ms %chil",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:11631,Optimiz,Optimize,11631,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,r#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.092ms self 0.092ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.exp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:175549,Optimiz,Optimize,175549,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.201ms self 0.201ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.exp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:32232,Optimiz,Optimize,32232,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9736,Load,LoadVCF,9736,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"r, f). /home/hail/hail.zip/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . /home/hail/hail.zip/hail/table.py in _process_joins(self, *exprs); 1459 def broadcast_f(left, data, jt):; 1460 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1461 return process_joins(self, exprs, broadcast_f); 1462 ; 1463 def cache(self):. /home/hail/hail.zip/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 354 for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; 355 if j not in used_joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3728:3554,cache,cache,3554,https://hail.is,https://github.com/hail-is/hail/issues/3728,1,['cache'],['cache']
Performance,r-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:35086,cache,cached,35086,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,r-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706. Java stack trace:; is.hail.relocated.com.google.cloud.storage.StorageException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/wes-bipolar-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706; 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:163); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:297); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:730); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$24(StorageImpl.java:574); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:60); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1476); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:574); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:563); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:288); 	at is.hail.services.package$.retryTransientErrors(package.scala:163); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:286); 	at is.hail.io.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:5391,load,load,5391,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['load'],['load']
Performance,"r.home=/home/users/schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@753: Client environment:user.dir=/mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmult",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2086,queue,queue,2086,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['queue'],['queue']
Performance,"r.ir.CompileAndEvaluate._apply/LowerMatrixToTable/Transform total 3.992ms self 3.992ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LowerMatrixToTable/Verify total 0.019ms self 0.019ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.TypeCheck.apply total 0.132ms self 0.132ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable total 2.591ms self 0.010ms children 2.581ms %children 99.63%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Verify total 0.011ms self 0.011ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform total 2.548ms self 0.302ms children 2.246ms %children 88.15%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/FoldConstants, iteration: 0 total 0.307ms self 0.307ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ExtractIntervalFilters, iteration: 0 total 0.016ms self 0.016ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:10516,Optimiz,Optimize,10516,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,r.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.lowering.CompilableIR total 0.007ms self 0.007ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.601ms self 0.006ms children 0.595ms %children 99.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.577ms self 0.041ms children 0.536ms %children 92.90%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:186938,Optimiz,OptimizePass,186938,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.737ms self 0.071ms children 3.665ms %children 98.09%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.412ms self 0.412ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:135103,Optimiz,OptimizePass,135103,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.531ms self 0.089ms children 4.442ms %children 98.03%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.046ms self 0.046ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:150890,Optimiz,OptimizePass,150890,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 8.695ms self 0.088ms children 8.607ms %children 98.99%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.598ms self 0.598ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.054ms self 0.054ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:119385,Optimiz,OptimizePass,119385,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.326ms self 0.326ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.561ms self 0.561ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:106401,Optimiz,Optimize,106401,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.342ms self 0.342ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.614ms self 0.614ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:91963,Optimiz,Optimize,91963,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.355ms self 0.355ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.759ms self 0.759ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:70784,Optimiz,Optimize,70784,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.396ms self 0.396ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.785ms self 0.785ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:56386,Optimiz,Optimize,56386,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.447ms self 0.447ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.051ms self 0.051ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.948ms self 0.948ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:47858,Optimiz,Optimize,47858,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 6.134ms self 6.134ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.135ms self 0.135ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 3.651ms self 3.651ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:39328,Optimiz,Optimize,39328,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.lowering.LowerAndExecuteShufflesPass#after total 0.035ms self 0.035ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 1.367ms self 0.010ms children 1.356ms %children 99.24%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.029ms self 0.029ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:25049,Optimiz,OptimizePass,25049,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,r.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.566ms self 0.566ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.445ms self 0.445ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.252ms self 0.765ms children 1.487ms %children 66.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:121608,Optimiz,Optimize,121608,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.609ms self 0.609ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.195ms self 0.195ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.409ms self 0.648ms children 0.762ms %children 54.04%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:137326,Optimiz,Optimize,137326,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.655ms self 0.655ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.184ms self 0.184ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.228ms self 1.353ms children 0.875ms %children 39.29%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:153113,Optimiz,Optimize,153113,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"r==2.8.2|protobuf==3.20.2|py4j==0.10.9.5|pyasn1==0.5.1|pyasn1-modules==0.3.0|pycares==4.4.0|pycparser==2.21|pygments==2.17.2|pyjwt[crypto]==2.8.0|python-dateutil==2.8.2|python-json-logger==2.0.7|pytz==2023.3.post1|pyyaml==6.0.1|regex==2023.10.3|requests==2.31.0|requests-oauthlib==1.3.1|rich==12.6.0|rsa==4.9|s3transfer==0.8.0|scipy==1.11.4|six==1.16.0|sortedcontainers==2.4.0|tabulate==0.9.0|tenacity==8.2.3|tornado==6.3.3|typer==0.9.0|typing-extensions==4.8.0|tzdata==2023.3|urllib3==1.26.18|uvloop==0.19.0;sys_platform!=""win32""|wrapt==1.16.0|xyzservices==2023.10.1|yarl==1.9.3 \; ---; > '--metadata=^|||^WHEEL=gs://hail-30-day/hailctl/dataproc/dking-dev/0.2.126-a51eabd65859/hail-0.2.126-py3-none-any.whl|||PKGS=aiodns==2.0.0|aiohttp==3.9.1|aiosignal==1.3.1|async-timeout==4.0.3|attrs==23.1.0|avro==1.11.3|azure-common==1.1.28|azure-core==1.29.5|azure-identity==1.15.0|azure-mgmt-core==1.4.0|azure-mgmt-storage==20.1.0|azure-storage-blob==12.19.0|bokeh==3.3.1|boto3==1.33.1|botocore==1.33.1|cachetools==5.3.2|certifi==2023.11.17|cffi==1.16.0|charset-normalizer==3.3.2|click==8.1.7|commonmark==0.9.1|contourpy==1.2.0|cryptography==41.0.7|decorator==4.4.2|deprecated==1.2.14|dill==0.3.7|frozenlist==1.4.0|google-auth==2.23.4|google-auth-oauthlib==0.8.0|humanize==1.1.0|idna==3.6|isodate==0.6.1|janus==1.0.0|jinja2==3.1.2|jmespath==1.0.1|jproperties==2.1.1|markupsafe==2.1.3|msal==1.25.0|msal-extensions==1.0.0|msrest==0.7.1|multidict==6.0.4|nest-asyncio==1.5.8|numpy==1.26.2|oauthlib==3.2.2|orjson==3.9.10|packaging==23.2|pandas==2.1.3|parsimonious==0.10.0|pillow==10.1.0|plotly==5.18.0|portalocker==2.8.2|protobuf==3.20.2|py4j==0.10.9.5|pyasn1==0.5.1|pyasn1-modules==0.3.0|pycares==4.4.0|pycparser==2.21|pygments==2.17.2|pyjwt[crypto]==2.8.0|python-dateutil==2.8.2|python-json-logger==2.0.7|pytz==2023.3.post1|pyyaml==6.0.1|regex==2023.10.3|requests==2.31.0|requests-oauthlib==1.3.1|rich==12.6.0|rsa==4.9|s3transfer==0.8.0|scipy==1.11.4|six==1.16.0|sortedcontainers==2.4.0|tabulate==0.9.0|tenacity=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14127:3693,cache,cachetools,3693,https://hail.is,https://github.com/hail-is/hail/pull/14127,1,['cache'],['cachetools']
Performance,rVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-08a1543; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:10061,concurren,concurrent,10061,https://hail.is,https://github.com/hail-is/hail/issues/2743,2,['concurren'],['concurrent']
Performance,r_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:27); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:33); at is.hail.annotations.Region$.apply(Region.scala:15); at is.hail.rvd.RVDContext$.default(RVDContext.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:6); at is.hail.utils.package$.point(package.scala:593); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:1724,load,load,1724,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186,1,['load'],['load']
Performance,r_struct_of_r_int32ANDr_int64END_TO_r_dict_of_r_int32ANDr_int64(Unknown Source); E 	at __C3100collect_distributed_array.__m3141DECODE_r_struct_of_r_int32ANDr_array_of_r_int32ANDr_float64ANDr_array_of_r_float64ANDr_int64ANDr_array_of_r_struct_of_r_int32ANDr_int64ENDEND_TO_SBaseStructPointer(Unknown Source); E 	at __C3100collect_distributed_array.__m3128split_StreamFor_region24_75(Unknown Source); E 	at __C3100collect_distributed_array.__m3128split_StreamFor(Unknown Source); E 	at __C3100collect_distributed_array.__m3125begin_group_0(Unknown Source); E 	at __C3100collect_distributed_array.__m3110split_Let(Unknown Source); E 	at __C3100collect_distributed_array.apply(Unknown Source); E 	at __C3100collect_distributed_array.apply(Unknown Source); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$2(BackendUtils.scala:31); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$1(BackendUtils.scala:30); E 	at is.hail.backend.service.Worker$.main(Worker.scala:133); E 	at is.hail.backend.service.Main$.main(Main.scala:32); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11681#issuecomment-1079785317:2972,concurren,concurrent,2972,https://hail.is,https://github.com/hail-is/hail/pull/11681#issuecomment-1079785317,6,['concurren'],['concurrent']
Performance,rator.scala:1334); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:655); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:653); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:7296,concurren,concurrent,7296,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307,2,['concurren'],['concurrent']
Performance,rator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:12544,concurren,concurrent,12544,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['concurren'],['concurrent']
Performance,rator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5211,concurren,concurrent,5211,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['concurren'],['concurrent']
Performance,raversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5409,Optimiz,Optimize,5409,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,"rbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1783,load,loader,1783,https://hail.is,https://github.com/hail-is/hail/pull/10279,2,['load'],"['load', 'loader']"
Performance,readLocal: perform read before file is closed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3512:11,perform,perform,11,https://hail.is,https://github.com/hail-is/hail/pull/3512,1,['perform'],['perform']
Performance,"readPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13308,Load,LoadVCF,13308,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,recreating because the CI page is getting too slow to load.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7523#issuecomment-557588150:54,load,load,54,https://hail.is,https://github.com/hail-is/hail/pull/7523#issuecomment-557588150,1,['load'],['load']
Performance,"reemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underlying pod IPs so Envoy can handle load-balancing instead of kube-proxy. This allows Envoy to make smart load-balancing decisions and correctly enforce rate-limiting when using connection pools. The namespace tracking in CI in Point ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:4214,load,load,4214,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['load'],['load']
Performance,relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.close(GoogleStorageFS.scala:326); 		at java.io.FilterOutputStream.close(FilterOutputStrea,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:6026,concurren,concurrent,6026,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['concurren'],['concurrent']
Performance,remove loadconda,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5905:7,load,loadconda,7,https://hail.is,https://github.com/hail-is/hail/pull/5905,1,['load'],['loadconda']
Performance,remove race condition where we might observe a complete job before the callback is sent.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5861:7,race condition,race condition,7,https://hail.is,https://github.com/hail-is/hail/pull/5861,1,['race condition'],['race condition']
Performance,remove unused LoadVCF.lineRef,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3568:14,Load,LoadVCF,14,https://hail.is,https://github.com/hail-is/hail/pull/3568,1,['Load'],['LoadVCF']
Performance,removed required alleles from LoadPlink,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2962:30,Load,LoadPlink,30,https://hail.is,https://github.com/hail-is/hail/pull/2962,1,['Load'],['LoadPlink']
Performance,ren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.154ms self 0.069ms children 0.085ms %children 55.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:172654,Optimiz,Optimize,172654,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.346ms self 0.121ms children 0.225ms %children 65.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.154ms self 0.154ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.015ms self 0.015ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:29337,Optimiz,Optimize,29337,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ren 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:183019,Optimiz,OptimizePass,183019,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ren 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:190044,Optimiz,OptimizePass,190044,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ren 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.153ms self 0.153ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.038ms self 0.029ms children 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:16754,Optimiz,OptimizePass,16754,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ren 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.164ms self 0.164ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.044ms self 0.035ms children 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:9770,Optimiz,OptimizePass,9770,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ren 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.442ms self 0.442ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.526ms self 0.526ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:5479,Optimiz,OptimizePass,5479,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"rence_data.drop('END'); + if 'END' in reference_data.entry:; + reference_data = reference_data.drop('END'); + else: # if END is missing, add it, _add_end is a no-op if END is already present; + reference_data = VariantDataset._add_end(reference_data); +; vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; ```. There was nothing in the IR that stood out when I examined it, but I will admit that I'm not the best at digging into it. ### Version. https://github.com/chrisvittal/hail/tree/vds/repro-example. ### Relevant log output. ```shell; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; E; E Java stack trace:; E java.lang.RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; E 	at is.hail.annotations.Memory.checkAddress(Memory.java:226); E 	at is.hail.annotations.Memory.loadByte(Memory.java:130); E 	at is.hail.annotations.Region$.loadByte(Region.scala:28); E 	at is.hail.annotations.Region$.loadBit(Region.scala:86); E 	at __C23148collect_distributed_array_matrix_native_writer.__m23333split_ToArray(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region478_486(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region16_503(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region14_529(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:2582,load,loadByte,2582,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['load'],['loadByte']
Performance,rent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216721,concurren,concurrent,216721,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,rent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.im,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218143,concurren,concurrent,218143,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,resolves some memory problems related to task results queueing up.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6823:54,queue,queueing,54,https://hail.is,https://github.com/hail-is/hail/issues/6823,1,['queue'],['queueing']
Performance,result.7028; 2023-09-27 16:44:22.668 GoogleStorageFS$: INFO: close: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:33.788 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/1-day/o?name=parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g%3D/result.7028&uploadType=resumable&upload_id=ADPycdv7y3A6GjTh6Kv7vrUu2ap2Kv0peLVWsVTAghIs7RCZk9X3fI1BDkeHag1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPqmOq13; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdv7y3A6GjTh6Kv7vrUu2ap2Kv0peLVWsVTAghIs7RCZk9X3fI1BDkeHag1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPq,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:7123,concurren,concurrent,7123,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['concurren'],['concurrent']
Performance,rfc-0000: Cache CollectDistributedArray Executions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13194:10,Cache,Cache,10,https://hail.is,https://github.com/hail-is/hail/pull/13194,1,['Cache'],['Cache']
Performance,rg.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Prom,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215547,concurren,concurrent,215547,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,rg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.sche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9425,Load,LoadVCF,9425,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"rg.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:922); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:915); at is.hail.utils.package$.using(package.scala:577); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:915); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:12129,Load,LoadVCF,12129,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['Load'],['LoadVCF']
Performance,richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:7626,concurren,concurrent,7626,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['concurren'],['concurrent']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.759ms self 0.759ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.137ms self 0.137ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:71491,Optimiz,Optimize,71491,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.561ms self 0.561ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.144ms self 0.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:107108,Optimiz,Optimize,107108,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.785ms self 0.785ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:57093,Optimiz,Optimize,57093,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.614ms self 0.614ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.292ms self 0.292ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:92670,Optimiz,Optimize,92670,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.051ms self 0.051ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.948ms self 0.948ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.222ms self 0.222ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:48565,Optimiz,Optimize,48565,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.135ms self 0.135ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 3.651ms self 3.651ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 12.144ms self 12.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:40035,Optimiz,Optimize,40035,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.073ms self 0.037ms children 0.036ms %children 49.79%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.641ms self 0.641ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:112096,Optimiz,Optimize,112096,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.074ms self 0.035ms children 0.040ms %children 53.33%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.635ms self 0.635ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:97658,Optimiz,Optimize,97658,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.089ms self 0.043ms children 0.047ms %children 52.21%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.830ms self 0.830ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:76479,Optimiz,Optimize,76479,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.098ms self 0.049ms children 0.049ms %children 50.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.756ms self 0.756ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:62081,Optimiz,Optimize,62081,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.138ms self 0.089ms children 0.049ms %children 35.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.885ms self 0.885ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:53553,Optimiz,Optimize,53553,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.144ms self 0.075ms children 0.069ms %children 48.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.069ms self 0.069ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.703ms self 4.703ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:45025,Optimiz,Optimize,45025,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 47.766ms self 0.019ms children 47.747ms %children 99.96%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.210ms self 0.210ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 47.463ms self 0.201ms children 47.262ms %children 99.58%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.exp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:37274,Optimiz,OptimizePass,37274,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,riteRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2087,concurren,concurrent,2087,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['concurren'],['concurrent']
Performance,"riter = ir.MatrixNativeWriter(output, overwrite, stage_locally, _codec_spec, _partitions, _partitions_type); -> 2734 Env.backend().execute(ir.MatrixWrite(self._mir, writer)). File /opt/conda/lib/python3.10/site-packages/hail/backend/backend.py:180, in Backend.execute(self, ir, timed); 178 result, timings = self._rpc(ActionTag.EXECUTE, payload); 179 except FatalError as e:; --> 180 raise e.maybe_user_error(ir) from None; 181 if ir.typ == tvoid:; 182 value = None. File /opt/conda/lib/python3.10/site-packages/hail/backend/backend.py:178, in Backend.execute(self, ir, timed); 176 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 177 try:; --> 178 result, timings = self._rpc(ActionTag.EXECUTE, payload); 179 except FatalError as e:; 180 raise e.maybe_user_error(ir) from None. File /opt/conda/lib/python3.10/site-packages/hail/backend/py4j_backend.py:213, in Py4JBackend._rpc(self, action, payload); 211 if resp.status_code >= 400:; 212 error_json = orjson.loads(resp.content); --> 213 raise fatal_error_from_java_error_triplet(error_json['short'], error_json['expanded'], error_json['error_id']); 214 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: HailException: cannot set missing field for required type +PFloat64. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 6.0 failed 4 times, most recent failure: Lost task 5.3 in stage 6.0 (TID 67) (saturn-machinenumber.c.terra-code.internal executor 4): is.hail.utils.HailException: gs://path/to/bucket/chrY.0002.hard_filtered_with_genotypes.vcf.gz:offset 23933331019603: error while parsing line; chrY	113	.	GG	G,*,AG,CG	596	PASS	AC=2,4,6,1;AF=1.23e-03,5.550e-05,4.44e-05,2.00e-04;AN=265;AS_AltDP=10,0,3,10;AS_BaseQRankSum=0.000,.,0.100,0.500;AS_FS=7.777,.,2.144,8.001;AS_MQ=55.75,.,38.98,40.20;AS_MQRankSum=0.200,.,-1.050,-0.500;AS_QD=0.50,0.00,0.25,0.52;AS_ReadPosRankSum=-0.200,.,0.500,-0.220;AS_SOR=2.300,.,1.600,3.000;BaseQRankSum=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:5654,load,loads,5654,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['load'],['loads']
Performance,rk.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5846,Load,LoadMatrix,5846,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,rk_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217142,concurren,concurrent,217142,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"rking to wait for <0x00000000e8ddaea0> (a scala.concurrent.impl.Promise$CompletionLatch); 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); 	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242); 	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258); 	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263); 	at scala.concurrent.Await$.$anonfun$result$1(package.scala:220); 	at scala.concurrent.Await$$$Lambda$2201/1092639564.apply(Unknown Source); 	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:57); 	at scala.concurrent.Await$.result(package.scala:146); 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:145); ...; ```. This is the line that waits to upload the compiled code for the workers to Google Cloud Storage. The other threads appear to be waiting on the memory service:; ```; ""pool-2-thread-2"" #27 prio=5 os_prio=0 tid=0x00007f5028ad9000 nid=0x88d waiting on condition [0x00007f50274fc000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.close(ServiceCacheableFS.scala:46); 	at java.io.FilterOutputStream.close(FilterOutputStr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:1327,concurren,concurrent,1327,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,"rks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've develop a large application in AngularJS, and have spent a bit of time with Angular2+. There is no comparison: Angular takes months to know well, React days at worst. Also, by not buying into a full framework, we achieve modularity: If we end up finding React too slow, even with [planned 2019 improvements](https://reactjs.org/blog/2018/11/27/react-16-roadmap.html), there are plenty of others view layers we can migrate to, without gutting our entire app. Next makes this somewhat trickier, but s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:3985,perform,performant,3985,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['perform'],['performant']
Performance,rnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.Ca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215800,concurren,concurrent,215800,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ror_id = tpl._1(), tpl._2(), tpl._3(); ---> 35 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 36 except pyspark.sql.utils.CapturedException as e:; 37 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 38 'Hail version: %s\n'; 39 'Error summary: %s' % (e.desc, e.stackTrace, hail.__version__, e.desc)) from None. FatalError: ClassCastException: class is.hail.types.physical.stypes.concrete.SIndexablePointer cannot be cast to class is.hail.types.physical.stypes.concrete.SJavaArrayString (is.hail.types.physical.stypes.concrete.SIndexablePointer and is.hail.types.physical.stypes.concrete.SJavaArrayString are in unnamed module of loader 'app'). Java stack trace:; java.lang.ClassCastException: class is.hail.types.physical.stypes.concrete.SIndexablePointer cannot be cast to class is.hail.types.physical.stypes.concrete.SJavaArrayString (is.hail.types.physical.stypes.concrete.SIndexablePointer and is.hail.types.physical.stypes.concrete.SJavaArrayString are in unnamed module of loader 'app'); 	at is.hail.expr.ir.functions.RegistryFunctions.unwrapReturn(Functions.scala:364); 	at is.hail.expr.ir.Emit.$anonfun$emitI$85(Emit.scala:1173); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:352); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:1153); 	at is.hail.expr.ir.streams.EmitStream$.is$hail$expr$ir$streams$EmitStream$$emit$1(EmitStream.scala:148); 	at is.hail.expr.ir.streams.EmitStream$.produce(EmitStream.scala:321); 	at is.hail.expr.ir.Emit.emitStream$2(Emit.scala:821); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:1177); 	at is.hail.expr.ir.Emit.$anonfun$emitSplitMethod$1(Emit.scala:607); 	at is.hail.expr.ir.Emit.$anonfun$emitSplitMethod$1$adapted(Emit.scala:605); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:19); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:29); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1086); 	at is.hail.expr.ir.Emit.emitSplitMethod(Emit.scala:605); 	at is.hail.e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:4470,load,loader,4470,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['load'],['loader']
Performance,"rt). I leave these as future tasks, because enough changes are present in this PR. Summary of changes:; * Refined homepage styles, ensured navbar matches to pixel between docs and hail.is root (surprisingly difficult); * Improved mobile styles, especially mobile nav menu (much smoother animation, larger, easier to click on links); * Optimized icon sizes (50KB -> 3.3KB); * Removed all use of bootstrap on hail.is/*.html pages (bootstrap remains on docs, future pr).; * Removed index.md contents. The markdown format is pretty limited. To have a richly-marked up site with consistent styling, the syntax it provides is not enough. The solution is either to add html to index.md, or just write html in a the index.xslt file. I chose the latter, because it's simpler. Future reorganization should simplify this and docs further, though I think I still recommend NextJS and the build system that provides.; * Added threeR115.min.js. This is regrettably large, but doesn't impact page rendering performance in any meaningful way, because it is loaded after all html content (and is cached after the first visit). Future work can go to webgl directly, potentially. There is also ongoing work by the ThreeJS maintainers to allow tree-shaking and smaller builds.; * Added heavily modified fork of VantaJS. Because we are not using something like NextJS, there is no package manager to rely on, so I just checked the file in manually (I have this in a separate repo, we can use that if preferred). License is in line with a note about modifications. Vanta performs very, very poorly (order of 40% CPU usage, old/slow ways of observing whether animated element is in view, unnecessary object generation, etc), this does not, removes a bunch of totally unnecessary OO abstractions, and provides some additional effects (hover highlighting of vertices), hence the fork. License is MIT, no issue for us.; * Made sure this all works, looks nice with docs. Future works will bring doc style in line with homepage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8634:1327,perform,performance,1327,https://hail.is,https://github.com/hail-is/hail/pull/8634,4,"['cache', 'load', 'perform']","['cached', 'loaded', 'performance', 'performs']"
Performance,"rted due to stage failure: Task 35 in stage 7.0 failed 20 times, most recent failure: Lost task 35.19 in stage 7.0 (TID 6963, gnomad-prod-sw-m8lk.c.broad-mpg-gnomad.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:4869,concurren,concurrent,4869,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['concurren'],['concurrent']
Performance,ruction count: 16: __C1124collect_distributed_array_table_aggregate_singlestage.__m1168ENCODE_SBinaryPointer_TO_r_binary\n2022-11-15 20:30:18.227 root: INFO: instruction count: 9: __C1124collect_distributed_array_table_aggregate_singlestage.setPartitionIndex\n2022-11-15 20:30:18.227 root: INFO: instruction count: 4: __C1124collect_distributed_array_table_aggregate_singlestage.addPartitionRegion\n2022-11-15 20:30:18.227 root: INFO: instruction count: 4: __C1124collect_distributed_array_table_aggregate_singlestage.setPool\n2022-11-15 20:30:18.227 root: INFO: instruction count: 3: __C1124collect_distributed_array_table_aggregate_singlestage.addHailClassLoader\n2022-11-15 20:30:18.227 root: INFO: instruction count: 3: __C1124collect_distributed_array_table_aggregate_singlestage.addFS\n2022-11-15 20:30:18.228 root: INFO: instruction count: 3: __C1148Tuple3.<init>\n2022-11-15 20:30:18.228 root: INFO: instruction count: 12: __C1148Tuple3.<init>\n2022-11-15 20:30:18.237 root: INFO: encoder cache hit\n2022-11-15 20:30:18.237 root: INFO: instruction count: 3: __C1093HailClassLoaderContainer.<init>\n2022-11-15 20:30:18.238 root: INFO: instruction count: 3: __C1093HailClassLoaderContainer.<clinit>\n2022-11-15 20:30:18.238 root: INFO: instruction count: 3: __C1094FSContainer.<init>\n2022-11-15 20:30:18.238 root: INFO: instruction count: 3: __C1094FSContainer.<clinit>\n2022-11-15 20:30:18.245 root: INFO: instruction count: 3: __C1095Compiled.<init>\n2022-11-15 20:30:18.245 root: INFO: instruction count: 222: __C1095Compiled.apply\n2022-11-15 20:30:18.245 root: INFO: instruction count: 73: __C1095Compiled.__m1103begin_group_0\n2022-11-15 20:30:18.245 root: INFO: instruction count: 11: __C1095Compiled.__m1106setup_null\n2022-11-15 20:30:18.245 root: INFO: instruction count: 109: __C1095Compiled.__m1108blockLinkedListSerialize\n2022-11-15 20:30:18.245 root: INFO: instruction count: 4: __C1095Compiled.__m1109ENCODE_SInt32$_TO_r_int32\n2022-11-15 20:30:18.246 root: INFO: instruction co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:17784,cache,cache,17784,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['cache'],['cache']
Performance,"ructural since they rarely influence the bytecode generated. Here is an exception, and that's where the method splitter comes in. Method splitting exists in the Hail compiler because not only does the JVM have limits on how large methods can be, but also the JIT compiler handles small methods much more effectively than large methods (and so splitting a large method into two small ones can make an order of magnitude or more in performance difference). We have three forms of method splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:2164,load,loaded,2164,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['load'],['loaded']
Performance,"run(Thread.java:748)org.apache.spark.SparkException: Job aborted due to stage failure: Task 754 in stage 1.0 failed 1 times, most recent failure: Lost task 754.0 in stage 1.0 (TID 1625, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(Ordere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:5045,concurren,concurrent,5045,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['concurren'],['concurrent']
Performance,"rver will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter_server_extension; Mar 01 19:59:04 dk-m python[5149]: from .handlers import SparkHandler; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 8, in <module>; Mar 01 19:59:04 dk-m python[5149]: class SparkHandler(IPythonHandler):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 13, in SparkHandler; Mar 01 19:59:04 dk-m python[5149]: @tornado.web.asynchronous; Mar 01 19:59:04 dk-m python[5149]: AttributeError: module 'tornado.web' has no attribute 'asynchronous'; ```. It appears that Jupyter starts even though on",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5505:1788,load,loading,1788,https://hail.is,https://github.com/hail-is/hail/issues/5505,1,['load'],['loading']
Performance,"ry-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 		at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; 	Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:25257,concurren,concurrent,25257,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"ry:; driver_output = await self._async_fs.open(output_uri); except FileNotFoundError as exc:; raise FatalError('Hail internal error. Please contact the Hail team and provide the following information.\n\n' + yamlx.dump({; 'service_backend_debug_info': self.debug_info(),; 'batch_debug_info': await self._batch.debug_info(); })) from exc; ; async with driver_output as outfile:; success = await read_bool(outfile); if success:; return await read_bytes(outfile); ; short_message = await read_str(outfile); expanded_message = await read_str(outfile); error_id = await read_int(outfile); ; reconstructed_error = fatal_error_from_java_error_triplet(short_message, expanded_message, error_id); if ir is None:; raise reconstructed_error; > raise reconstructed_error.maybe_user_error(ir); E hail.utils.java.FatalError: RuntimeException: Stream is already closed.; E ; E Java stack trace:; E java.util.concurrent.ExecutionException: java.lang.RuntimeException: Stream is already closed.; E 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); E 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); E 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:150); E 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:44); E 	at __C256669Compiled.__m256730split_CollectDistributedArray(Emit.scala); E 	at __C256669Compiled.__m256689split_Let(Emit.scala); E 	at __C256669Compiled.apply(Emit.scala); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); E 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:13807,concurren,concurrent,13807,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"ry>; <ul>; <li><a href=""https://github.com/PyCQA/pylint/commit/95cbd2bd14576cb5d9eade4798e73e8601c884de""><code>95cbd2b</code></a> Bump pylint to 2.13.5, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/2e9b33b13264a3cc229e879e7c03b36acd523554""><code>2e9b33b</code></a> Bump black from 22.1.0 to 22.3.0 (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6176"">#6176</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/f251131eaf88c0a6b30983b9ccd8d2924e28fe38""><code>f251131</code></a> Add <code>subclassed-final-class</code> message to the <code>check_messages</code> decorator (#...</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/a03b6e77bef920a7c72be9f3e2c2babddecd2fd2""><code>a03b6e7</code></a> Prevent <code>used-before-assignment</code> for assignment via nonlocal after type annot...</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/074131312977fbd423fe4faff004d4fa8dbba4e5""><code>0741313</code></a> Only emit <code>lru-cache-decorating-method</code> when <code>maxsize</code> is <code>None</code> (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6181"">#6181</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/702474327d7c756b61b82a1805efdd32c2d78ca8""><code>7024743</code></a> Fix false positive for <code>unused-import</code> when disabling both ``used-before-as...</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/4213b3c9a1d4ea7213636b67954dfbd95e290e91""><code>4213b3c</code></a> Fix handling of &quot;for x in x&quot; homonyms (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6154"">#6154</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/22b5dc1ae716e870873ac0b7d8b3369ca9896c38""><code>22b5dc1</code></a> Account for more node types in handling of except block homonyms with compreh...</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/05990167978b3acbb1fbf37b079602a057ee4774""><code>0599016</code></a> <code>redefined-slots-in-subclas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11739:3481,cache,cache-decorating-method,3481,https://hail.is,https://github.com/hail-is/hail/pull/11739,1,['cache'],['cache-decorating-method']
Performance,"s < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/backend/backend.py"", line 108, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/BROAD.MIT.EDU/cvittal/.local/opt/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/utils/java.py"", line 221, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus). Java stack trace:; scala.MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus); 	at is.hail.expr.ir.ExtractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:2068,load,loads,2068,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['load'],['loads']
Performance,s children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.154ms self 0.069ms children 0.085ms %children 55.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowerin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:172625,Optimiz,OptimizePass,172625,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.346ms self 0.121ms children 0.225ms %children 65.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.154ms self 0.154ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.015ms self 0.015ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowerin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:29308,Optimiz,OptimizePass,29308,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"s is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:3486,load,load-balance,3486,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['load'],['load-balance']
Performance,s is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using ca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32936,cache,cached,32936,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"s""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (Let g; (ArrayRef; (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-08 18:19:48 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (ArrayRef; (GetField `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (Ref row)); (Ref i))))))); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5100:1335,optimiz,optimize,1335,https://hail.is,https://github.com/hail-is/hail/issues/5100,1,['optimiz'],['optimize']
Performance,s$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5837,Optimiz,Optimize,5837,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Optimiz'],['Optimize']
Performance,s$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(D,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9653,Load,LoadVCF,9653,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"s%20only%20(no%2Daddress)%20is%20set%20by%20default%20when%20creating%20a%20Dataproc%202.2%20image%20version%20cluster.%20You%20can%20use%20the%20gcloud%20dataproc%20clusters%20create%20%2D%2Dpublic%2Dip%2Daddress%20flag%20to%20enable%20public%20IP%20addresses.), clusters are created without public internet access by default. A workaround is to pass the `--public-ip-address` flag to the command. Error message:. ```python; pip packages are ['setuptools', 'mkl<2020', 'lxml<5', 'https://github.com/hail-is/jgscm/archive/v0.1.13+hail.zip', 'ipykernel==6.22.0', 'ipywidgets==8.0.6', 'jupyter-console==6.6.3', 'nbconvert==7.3.1', 'notebook==6.5.6', 'qtconsole==5.4.2', 'aiodns==2.0.0', 'aiohttp==3.9.5', 'aiosignal==1.3.1', 'async-timeout==4.0.3', 'attrs==23.2.0', 'avro==1.11.3', 'azure-common==1.1.28', 'azure-core==1.30.2', 'azure-identity==1.17.1', 'azure-mgmt-core==1.4.0', 'azure-mgmt-storage==20.1.0', 'azure-storage-blob==12.20.0', 'bokeh==3.3.4', 'boto3==1.34.138', 'botocore==1.34.138', 'cachetools==5.3.3', 'certifi==2024.6.2', 'cffi==1.16.0', 'charset-normalizer==3.3.2', 'click==8.1.7', 'commonmark==0.9.1', 'contourpy==1.2.1', 'cryptography==42.0.8', 'decorator==4.4.2', 'deprecated==1.2.14', 'dill==0.3.8', 'frozenlist==1.4.1', 'google-auth==2.31.0', 'google-auth-oauthlib==0.8.0', 'humanize==1.1.0', 'idna==3.7', 'isodate==0.6.1', 'janus==1.0.0', 'jinja2==3.1.4', 'jmespath==1.0.1', 'jproperties==2.1.1', 'markupsafe==2.1.5', 'msal==1.29.0', 'msal-extensions==1.2.0', 'msrest==0.7.1', 'multidict==6.0.5', 'nest-asyncio==1.6.0', 'numpy==1.26.4', 'oauthlib==3.2.2', 'orjson==3.10.6', 'packaging==24.1', 'pandas==2.2.2', 'parsimonious==0.10.0', 'pillow==10.4.0', 'plotly==5.22.0', 'portalocker==2.10.0', 'protobuf==3.20.2', 'py4j==0.10.9.7', 'pyasn1==0.6.0', 'pyasn1-modules==0.4.0', 'pycares==4.4.0', 'pycparser==2.22', 'pygments==2.18.0', 'pyjwt==2.8.0', 'python-dateutil==2.9.0.post0', 'python-json-logger==2.0.7', 'pytz==2024.1', 'pyyaml==6.0.1', 'regex==2024.5.15', 'requests==2.32.3'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14652:1219,cache,cachetools,1219,https://hail.is,https://github.com/hail-is/hail/issues/14652,1,['cache'],['cachetools']
Performance,"s. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:4756,load,load,4756,https://hail.is,https://github.com/hail-is/hail/pull/8513,2,['load'],['load']
Performance,"s.HadoopFS.exists(HadoopFS.scala:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonClientException; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:419); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352); 	... 27 more. During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:218); 	at is.hail.backend.spark.Sp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:2386,load,loadClass,2386,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['load'],['loadClass']
Performance,s.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.245ms self 0.245ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.034ms self 0.034ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.200ms self 0.200ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:159778,Optimiz,OptimizePass,159778,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.254ms self 0.254ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:143991,Optimiz,OptimizePass,143991,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.308ms self 0.308ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.305ms self 0.305ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:128273,Optimiz,OptimizePass,128273,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.032ms children 0.070ms %children 68.44%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluat,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:215587,Optimiz,OptimizePass,215587,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.228ms self 0.228ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.064ms self 0.064ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.101ms self 0.033ms children 0.068ms %children 67.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluat,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:207566,Optimiz,OptimizePass,207566,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.070ms self 0.070ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.108ms self 0.034ms children 0.074ms %children 68.32%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluat,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:199614,Optimiz,OptimizePass,199614,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.654ms self 0.654ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluat,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:194962,Optimiz,OptimizePass,194962,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.backend.BackendUtils.$anonfun$collectDArray$16(BackendUtils.scala:91); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:172); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.lang.NullPointerException: null; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionPutTask.call(JsonResumableSessionPutTask.java:201); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:81); 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:5313,concurren,concurrent,5313,https://hail.is,https://github.com/hail-is/hail/issues/13937,1,['concurren'],['concurrent']
Performance,s.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.exp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5811,Optimiz,Optimize,5811,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,s.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.641ms self 0.052ms children 3.589ms %children 98.56%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.326ms self 0.326ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:105674,Optimiz,OptimizePass,105674,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.847ms self 0.051ms children 3.797ms %children 98.69%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.342ms self 0.342ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:91236,Optimiz,OptimizePass,91236,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.479ms self 0.057ms children 4.422ms %children 98.72%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.355ms self 0.355ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:70057,Optimiz,OptimizePass,70057,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_matrix_ir$2(SparkBackend.scala:689); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.annotations.RegionPool$.scoped(Reg,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6557,Load,LoadPlink,6557,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,s.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 0; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 		at is.hail.io.fs.FSPositionedOutputStream.write(FS.scala:227); 		at java.io.DataOutputStream.write(DataOutputStream.java:107); 		a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:8069,concurren,concurrent,8069,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['concurren'],['concurrent']
Performance,s.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c37301a; Error summary: IllegalArgumentException: requirement failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465:18282,concurren,concurrent,18282,https://hail.is,https://github.com/hail-is/hail/issues/3465,2,['concurren'],['concurrent']
Performance,s.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$app,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7331,Optimiz,Optimize,7331,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['Optimize']
Performance,s.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:220); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:2803,concurren,concurrent,2803,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['concurren'],['concurrent']
Performance,"s.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:220); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-e6de08e; Error summary: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [3c5f402fed564ccd85257c0919d4bffb] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""pyhail.py"", line 128, in <module>; main(args, pass_through_args); File ""pyhail.py"", line 109, in main; subprocess.check_output(job); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 573, in check_output; raise CalledProcessError(retcode, cmd, output=output); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', '/Users/gtiao/gnomad_qc/hail/sample_qc/assign_subpops.py', '--cluster', 'gt1', '--files=gs://hail-common/builds/devel/jars/hail-devel-38dbf156b630-Spark-2.2.0.jar', '--py-files=",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:7322,concurren,concurrent,7322,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['concurren'],['concurrent']
Performance,"s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11:23 Client: INFO: Setting up container launch context for our AM; 2019-01-22 13:11:23 Client: INFO: Setting up the launch environment for our AM container; 2019-01-22 13:11:24 Client: INFO: Preparing resources for our AM container; 2019-01-22 13:11:24 HadoopFSCredentialProvider: INFO: getting token for: hdfs://scc/user/farrell; 2019-01-22 13:11:24 DFSClient: INFO: Created HDFS_DELEGATION_TOKEN token 11364 for farrell on ha-hdfs:scc; 2019-01-22 13:11:26 Client: WARN: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to up",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:14253,load,loaded,14253,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['load'],['loaded']
Performance,"s.openCostInBytes=1099511627776 --conf spark.sql.files.maxPartitionBytes=1099511627776 --conf spark.hadoop.parquet.block.size=1099511627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; /usr/local/lib/python3.5/site-packages/IPython/core/history.py:228: UserWarning: IPython History requires SQLite, your history will not be saved; warn(""IPython History requires SQLite, your history will not be saved""); Python 3.5.2 (default, Jul 12 2017, 14:00:23) ; Type ""copyright"", ""credits"" or ""license"" for more information. IPython 5.1.0 -- An enhanced Interactive Python.; ? -> Introduction and overview of IPython's features.; %quickref -> Quick reference.; help -> Python's own help system.; object? -> Details about 'object', use 'object??' for extra details.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/09 12:51:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/09 12:51:55 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 12 2017 14:00:23); SparkSessi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609:1149,load,load,1149,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609,1,['load'],['load']
Performance,"s.openCostInBytes=1099511627776 --conf spark.sql.files.maxPartitionBytes=1099511627776 --conf spark.hadoop.parquet.block.size=1099511627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; /usr/local/lib/python3.5/site-packages/IPython/core/history.py:228: UserWarning: IPython History requires SQLite, your history will not be saved; warn(""IPython History requires SQLite, your history will not be saved""); Python 3.5.2 (default, Jul 12 2017, 14:00:23) ; Type ""copyright"", ""credits"" or ""license"" for more information. IPython 5.1.0 -- An enhanced Interactive Python.; ? -> Introduction and overview of IPython's features.; %quickref -> Quick reference.; help -> Python's own help system.; object? -> Details about 'object', use 'object??' for extra details.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/10 08:41:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/10 08:41:32 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/10 08:41:32 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/10 08:41:32 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 12 2017 14:00:23); SparkSession available as 'spark'. In [1]: ; ```; -----------------------------; Step2 : read the file with sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160:1133,load,load,1133,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160,1,['load'],['load']
Performance,s.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:456); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.se,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:7761,concurren,concurrent,7761,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['concurren'],['concurrent']
Performance,s.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.net.SocketTimeoutException: connect timed out; E 	at java.net.PlainSocketImpl.socketConnect(Native Method); E 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); E 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); E 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); E 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); E 	at java.net.Socket.connect(Socket.java:607); E 	at is.hail.relocated.org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75); E 	at is.hail.relocated.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConn,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:7475,concurren,concurrent,7475,https://hail.is,https://github.com/hail-is/hail/issues/13074,1,['concurren'],['concurrent']
Performance,"s/batch/worker/worker.py"", line 2272, in run; await self.jvm.execute(; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 2872, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: java.lang.IllegalArgumentException: bound must be positive; 	at java.util.Random.nextInt(Random.java:388); 	at scala.util.Random.nextInt(Random.scala:70); 	at is.hail.services.package$.delayMsForTry(package.scala:47); 	at is.hail.services.package$.retryTransientErrors(package.scala:186); 	at is.hail.io.fs.GoogleStorageFS$$anon$1.retryingRead(GoogleStorageFS.scala:220); 	at is.hail.io.fs.GoogleStora",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888:1086,concurren,concurrent,1086,https://hail.is,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888,1,['concurren'],['concurrent']
Performance,"s: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; appris: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; tsl: int32,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'xpos': int64; 'xstart': int64; 'xstop': int64; ----------------------------------------; Entry fields:; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'PL': array<int32>; 'BX': array<str>; 'PS': int32; 'PQ': int32; 'JQ': int32; 'MIN_DP': int32; 'PGT': call; 'PID': str; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 4:===================================================> (480 + 20) / 500]2020-04-05 14:09:48 Hail: INFO: Coerced almost-sorted dataset; [Stage 5:======================================================>(498 + 2) / 500]2020-04-05 14:09:50 Hail: INFO: Coerced almost-sorted dataset; [Stage 7:> (0 + 108) / 500]ERROR: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) failed SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:46930,load,loading-cluster-m,46930,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['load'],['loading-cluster-m']
Performance,"s://redirect.github.com/sphinx-doc/sphinx/issues/11418"">#11418</a>)</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/db546189ce1d2a345f4399367ced6ecdd538be5d""><code>db54618</code></a> Support Docutils 0.20 (<a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11411"">#11411</a>)</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/8942a1dddf2355928f088d6b631db8658034eaae""><code>8942a1d</code></a> Test with Docutils 0.20</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/c9d0933e5d8e34aa9d2c1d88c5a80b46b575730e""><code>c9d0933</code></a> linkcheck: Use context managers for HTTP requests (<a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11318"">#11318</a>)</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/2b1c106bbff5265e8a6076318db5d083c329d575""><code>2b1c106</code></a> Update documentation workflow</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/ba7408209e84ee413f240afc20f3c6b484a81f8f""><code>ba74082</code></a> Change concurrency groups for GitHub workflows</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/b546879539200ec4128bcc6d0ed911ebf28bb3cb""><code>b546879</code></a> Bump version</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/d568b2f4f7cca743fcbf70814d15602d8129b790""><code>d568b2f</code></a> Bump to 7.0.0 final</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/ff79edf353f5cc6e02036f58e0295dc704c5e681""><code>ff79edf</code></a> Remove <code>jsdump</code> references post removal</li>; <li>Additional commits viewable in <a href=""https://github.com/sphinx-doc/sphinx/compare/v6.2.1...v7.0.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx&package-manager=pip&previous-version=6.2.1&new-version=7.0.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13054:6025,concurren,concurrency,6025,https://hail.is,https://github.com/hail-is/hail/pull/13054,1,['concurren'],['concurrency']
Performance,"s=""analysis_type=SelectVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unfiltered.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] select_expressions=[] excludeNonVariants=false excludeFiltered=false regenotype=false restrictAllelesTo=ALL kee",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:11975,perform,performanceLog,11975,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['perform'],['performanceLog']
Performance,s=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32078,cache,cached,32078,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:12948,Load,LoadVCF,12948,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:11232,concurren,concurrent,11232,https://hail.is,https://github.com/hail-is/hail/issues/3063,1,['concurren'],['concurrent']
Performance,scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:4968,concurren,concurrent,4968,https://hail.is,https://github.com/hail-is/hail/issues/3063,2,['concurren'],['concurrent']
Performance,self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:6595,Optimiz,OptimizePass,6595,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:299); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:2537,load,loadField,2537,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['load'],['loadField']
Performance,ser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Col,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39535,cache,cached,39535,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:14844,Perform,Performance,14844,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['Perform'],['Performance']
Performance,"set; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2139,load,loadInt,2139,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['load'],['loadInt']
Performance,"sets of intervals. The visual conception of the partitioning of this matrix table (with its globals, column margin data, row margin data, and entry data) might look like:. ```; +--+ +-----+--+--++---+------+------+; | | | | | || | | |; +--+ +-----+--+--++---+------+------+. ck1 ck2 ...; +--+ rk1 +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; +--+ rk2 +-----+--+--++---+------+------+; | | | | | || | | |; +--+ ... +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; | | | | | || | | |; +--+ +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; +--+ +-----+--+--++---+------+------+; +--+ +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; | | | | | || | | |; +--+ +-----+--+--++---+------+------+; ```. The first row-key interval is `[rk1, rk2)`. The first col-key interval is `[ck1, ck2)`. These intervals are define a ""rectangle"" corresponding to the first partition. . All the partitions in the fourth partition column are empty (perhaps these column keys are absent in this dataset). Likewise, all the partitions in the fifth partition row are empty. ---. Global values are still global and must be stored in memory for each partition. The columns table becomes a distributed table like the rows table. Its partitioning must match the column partitioning of the blocks. `annotate_cols` and `aggregate_cols` become distributed operations. The rows table is mostly unchanged. `annotate_rows` and `annotate_cols`, when used with aggregation, become symmetric. They both aggregate across partitions as column aggregation did before. `annotate_entries`, `annotate_rows` with aggregation, and `annotate_cols` with aggregation now read the corresponding row and column partition. Each row partition is read once per column block. Similarly for column partitions. In particular, we need not load the entire column table into RAM for each partition. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13800:2737,load,load,2737,https://hail.is,https://github.com/hail-is/hail/issues/13800,1,['load'],['load']
Performance,"simpler:; ```; In [16]: import hail as hl; ...: ; ...: t1kg = hl.utils.range_matrix_table(1,1); ...: t1kg = t1kg.key_rows_by(locus=hl.locus(hl.str(t1kg.row_idx+1), t1kg.row_idx+1), alleles=['A','T']); ...: t1kg.write('/tmp/foo.mt', overwrite=True); 2018-10-11 13:42:55 Hail: INFO: Coerced sorted dataset; 2018-10-11 13:42:55 Hail: INFO: wrote 1 items in 1 partitions to /tmp/foo.mt; ^[[A; In [17]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt'); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```; error:; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; after: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; ```; describe:; ```; In [18]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt').describe(); ...: ; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'col_idx': int32 ; ----------------------------------------; Row fields:; 'row_idx': int32 ; 'locus': locus<GRCh37> ; 'alleles': array<str> ; ----------------------------------------; Entry fields:; None; ----------------------------------------; Column key: ['col_idx']; Row key: ['locus', 'alleles']; ----------------------------------------; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397:583,optimiz,optimization,583,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397,2,['optimiz'],['optimization']
Performance,since the lowering is not actually a C++ step. (also pulled out the table lowering step into an explicit step so that I could add an optimization pass afterwards),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6053:133,optimiz,optimization,133,https://hail.is,https://github.com/hail-is/hail/pull/6053,1,['optimiz'],['optimization']
Performance,"site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5213,cache,cached,5213,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,sk.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13260,Load,LoadMatrix,13260,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"sn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:7190,cache,cached,7190,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"so I can remove the computeEigenvalues option and just always return them, but I'm less clear on the loadings. @tpoterba are you suggesting that I can remove the computeLoadings option because the computation is lazy? Does passing the KeyTable object through to python count as ""using"", though?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2454#issuecomment-348580540:101,load,loadings,101,https://hail.is,https://github.com/hail-is/hail/pull/2454#issuecomment-348580540,2,['load'],['loadings']
Performance,"so like to report a similar `orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0)` bug first reported by https://discuss.hail.is/t/hail-fails-after-installing-it-on-a-single-computer/3653. Hail installed from https://anaconda.org/sfe1ed40/hail; EDIT: the same error occurs after `pip install hail` into a fresh conda env, which produced hail `version 0.2.130-bea04d9c79b5`. Terminal output: ; ```; Python 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; hl.init(); >>> hl.init(); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.4.1; SparkUI available at http://xxxx:xxxx; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-d18228b9bc5b; LOGGING: writing to xxxx.log; >>> hl.utils.range_table(10).collect(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1234>"", line 2, in collect; File ""/xxxx/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/xxxx/lib/python3.10/site-packages/hail/table.py"", line 2213, in collect; return Env.backend().execute(e._ir, timed=_timed); File ""/xxxx/lib/python3.10/site-packages/hail/backend/backend.py"", line 188, in execute; result, timings = self._rpc(ActionTag.EXECUTE, payload); File ""/xxxx/lib/python3.10/site-packages/hail/backend/py4j_backend.py"", line 219, in _rpc; error_json = orjson.loads(resp.content); orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0); ```. Log file:; ```; 2024-04-25 16:07:16.773 Hail: INFO: SparkUI: http://xxxx:xxxx; 2024-04-25 16:07:21.589 Hail: INFO: Running Hail version 0.2.127-d18228b9bc5b; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076:1793,load,loads,1793,https://hail.is,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076,1,['load'],['loads']
Performance,software.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:5109,load,loadClass,5109,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['load'],['loadClass']
Performance,"sorry my internet was bad and wasn't reloading the bottom of the page for a while. Can respond now:. > As an aside, ApplySpecial seems to allow operations on non-identical types, which would require algebraic types, rather than a singular: any time the types of itsSeq[IR]'s are different. > I have similar issues with other array operations, which want nodes that we currently don't support, including aggregator nodes. Note that I am only calling inferSetPType in Compile (immediately after optimization pass). The ptype inference for apply methods is handled by some stuff I wrote recently. IRFunction now has a `returnPType` method that takes arg ptypes. > PVoid. Void isn't a catch-all type like Nothing in Scala - it's a specific I-don't-return-anything type used by IRs like TableWrite. The exception in your above message is coming from the Apply node being inferred as a `PVoid` by your `case _ => PVoid` code. Writing the rule for the apply node should fix that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-513005812:493,optimiz,optimization,493,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513005812,1,['optimiz'],['optimization']
Performance,spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:219138,Optimiz,Optimize,219138,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.InlineApplyIR total 0.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:203165,Optimiz,Optimize,203165,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerArrayAggsToRunAgg,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:211117,Optimiz,Optimize,211117,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:198513,Optimiz,Optimize,198513,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.Mappark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iteratoadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartiti288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.Abst",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:20197,concurren,concurrent,20197,https://hail.is,https://github.com/hail-is/hail/issues/8106,2,['concurren'],['concurrent']
Performance,"spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSet",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6059,Load,LoadVCF,6059,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) truncate on store and load a single bit. Our value of `3` was stored as a class Boolean, and came out `1`. The fact that a single field's missingness was flipped was a red herring -- all higher bits are flipped to 0 (defined)! Here's a look at the LIR looks like, though it was ultimately the JVM class file printout that tipped me off to the problem:. ```code. # I2B is stored as a class field on spills. The Z at the end of the next line indicates this field is a Boolean, not a byte. 31017 (PutFieldX PUTFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 31018 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;); 31019 25774 (InsnX I2B; 31020 25775 (InsnX IOR; 31021 25776 (InsnX IOR; 31022 25777 (InsnX IOR; 31023 25778 (InsnX IOR; 31024 25779 (LdcX 0 I); 31025 25780 (InsnX ISHL; 31026 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2346null Z; 31027 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31028 25782 (LdcX 0 I))); 31029 25783 (InsnX ISHL; 31030 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2348null Z; 31031 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills._",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:4241,Load,LoadX,4241,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,"ss org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825:1124,load,loadClass,1124,https://hail.is,https://github.com/hail-is/hail/issues/825,1,['load'],['loadClass']
Performance,"ss-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053:3567,concurren,concurrent,3567,https://hail.is,https://github.com/hail-is/hail/issues/3053,1,['concurren'],['concurrent']
Performance,ssLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:139); at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1063); at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1127); at is.hail.io.vcf.MatrixVCFReader.coercer$lzycompute(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.coercer(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.apply(LoadVCF.scala:1008); at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:647); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:57); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:32); at is.hail.variant.MatrixTable.write(MatrixTable.scala:1238); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.Ca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:7160,Load,LoadVCF,7160,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['Load'],['LoadVCF']
Performance,"st <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-xdist/issues/738"">#738</a> from pytest-dev/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/a25c14bef59ad728e39cabc64f71190aaad73b0a""><code>a25c14b</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/110c114025202d11570737be823de158d1bb8d99""><code>110c114</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-xdist/issues/734"">#734</a> from nicoddemus/revamp-readme</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/83bdbf4b95c914a889d1faa8fba8d506bcc2f8c7""><code>83bdbf4</code></a> Revamp README</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/630c1eb6f2c31dcb4c38c75bb62f868237cdde94""><code>630c1eb</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-xdist/issues/733"">#733</a> from baekdohyeop/feature-loadgroup</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/62e50d00977b41e175b5f119381f9db760459ddc""><code>62e50d0</code></a> Address review</li>; <li>Additional commits viewable in <a href=""https://github.com/pytest-dev/pytest-xdist/compare/v2.2.1...v2.5.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pytest-xdist&package-manager=pip&previous-version=2.2.1&new-version=2.5.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11491:5408,load,loadgroup,5408,https://hail.is,https://github.com/hail-is/hail/pull/11491,2,['load'],['loadgroup']
Performance,"st succeed to install hail on AWS but still have some environment issue:. * I am trying to install Hail v0.2.124; * on AWS EMR v6.9.1 (latest version with Spark 3.3.0 suggested on hail doc); * I upgrade to python 3.9.18; ```sh; $ python --version; Python 3.9.18; ```; I activate java 11.0.20.1; ```sh; $ java -version; openjdk version ""11.0.20.1"" 2023-08-22 LTS; OpenJDK Runtime Environment Corretto-11.0.20.9.1 (build 11.0.20.1+9-LTS); OpenJDK 64-Bit Server VM Corretto-11.0.20.9.1 (build 11.0.20.1+9-LTS, mixed mode); ```; * I clone hail; ```sh; $ cd /tmp; $ git clone --branch 0.2.124 --depth 1 https://github.com/broadinstitute/hail.git; ```; * I build hail; ```sh; $ cd hail/hail/; $ make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.0; [...]; Successfully installed hail-0.2.124; hailctl config set query/backend spark; ```; * At this point Hail seems correcly installed; ```sh; $ pip show hail; Name: hail; Version: 0.2.124; Summary: Scalable library for exploring and analyzing genomic data.; Home-page: https://hail.is; Author: Hail Team; Author-email: hail@broadinstitute.org; License: UNKNOWN; Location: /home/hadoop/.local/lib/python3.9/site-packages; ```; * For sake of configuration I create a symlink of the hail backend; ```sh; sudo ln -sf /home/hadoop/.local/lib/python3.9/site-packages/hail/backend /opt/hail/backend; ```; * Confident of the. installation I try to run spark shell; ```sh; $ spark-shell; [...]; Exception in thread ""main"" java.lang.NoSuchMethodError: 'scala.reflect.internal.settings.MutableSettings ; ```. I am out of idea on how to solve the current situation. ; Thanks. ### Version. 0.2.124. ### Relevant log output. ```shell; $ spark-shell; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignorin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837:1026,Scalab,Scalable,1026,https://hail.is,https://github.com/hail-is/hail/issues/13837,1,['Scalab'],['Scalable']
Performance,"st task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 35.1 in stage 0.0 (TID 43, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:150800,concurren,concurrent,150800,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 15.2 in stage 0.0 (TID 50, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 35.2 in stage 0.0 (TID 49, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:162476,concurren,concurrent,162476,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 15.3 in stage 0.0 (TID 63, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:198335,concurren,concurrent,198335,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 17.1 in stage 0.0 (TID 47, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 7.1 in stage 0.0 (TID 46, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:142021,concurren,concurrent,142021,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 17.2 in stage 0.0 (TID 54, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 BlockManagerMaster: INFO: Removal of executor 13 requested; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Trying to remove executor 13 from BlockManagerMaster.; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 13; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q12.scc.bu.edu:45213 with 21.2 GB RAM, BlockManagerId(21, scc-q12.scc.bu.edu, 45213, None); 2019-01-22 13:12:03 BlockManagerInfo: INFO: Added broadcast_1_piece0 in memory on scc-q12.scc.bu.edu:45213 (size: 24.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:177386,concurren,concurrent,177386,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 19.0 in stage 0.0 (TID 19, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 9.0 in stage 0.0 (TID 9, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:103567,concurren,concurrent,103567,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 BlockManagerMaster: INFO: Removal of executor 21 requested; 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Trying to remove executor 21 from BlockManagerMaster.; 2019-01-22 13:12:05 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 21; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 14.; 2019-01-22 13:12:06 DAGScheduler: INFO: Executor lost: 14 (epoch 16); 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 14 from BlockManagerMaster.; 2019-01-22 13:12:06 Blo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:188885,concurren,concurrent,188885,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 25.1 in stage 0.0 (TID 41, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:149346,concurren,concurrent,149346,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 25.2 in stage 0.0 (TID 51, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 BlockManagerMaster: INFO: Removal of executor 15 requested; 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Trying to remove executor 15 from BlockManagerMaster.; 2019-01-22 13:12:02 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 15; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.187:49620) with ID 12; 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12, partition ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:166837,concurren,concurrent,166837,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 15.3 in stage 0.0 (TID 63, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:196881,concurren,concurrent,196881,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 27.1 in stage 0.0 (TID 44, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 17.1 in stage 0.0 (TID 47, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:140567,concurren,concurrent,140567,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 27.2 in stage 0.0 (TID 55, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 37.2 in stage 0.0 (TID 52, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:174478,concurren,concurrent,174478,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 29.0 in stage 0.0 (TID 29, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 19.0 in stage 0.0 (TID 19, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:102113,concurren,concurrent,102113,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:187431,concurren,concurrent,187431,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 35.1 in stage 0.0 (TID 43, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 5.1 in stage 0.0 (TID 42, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:152254,concurren,concurrent,152254,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 35.2 in stage 0.0 (TID 49, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:163930,concurren,concurrent,163930,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 35.3 in stage 0.0 (TID 62, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hado",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:193880,concurren,concurrent,193880,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 37.1 in stage 0.0 (TID 45, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 11 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 11 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_01",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:144928,concurren,concurrent,144928,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 37.2 in stage 0.0 (TID 52, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 17.2 in stage 0.0 (TID 54, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:175932,concurren,concurrent,175932,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 39.0 in stage 0.0 (TID 39, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 10 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 10 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_017",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:107800,concurren,concurrent,107800,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:185977,concurren,concurrent,185977,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st, I took the JAR URL out of the ""command"" of the job spec. This ""command"" is just an array of strings. The fact that certain parts of that array *must* be the JAR URL and the SHA-1 is confusing. Instead, there are now two keys in a JVM process specification:; 1. `jar_spec`, which may be either `{""type"": ""jar_url"", ""value"": ""gs://..../abc123....jar""}` or `{""type"":""git_revision"", ""value"": ""abc123...""}`.; 2. `argv`, an opaque list of strings which are passed, by the JVMEntryway, along with a few more args, to `is.hail.backend.service.Main`. The `Main` class dispatches to either `ServiceBackendSocketAPI2` or the `Worker` based on the first element of `argv`. Each class expects different contents in `argv` that suits its needs. Second, I completely eliminated the HAIL_SHA/revision from the Worker and Hail Query Java code. This was only ever used as unique name for the JAR. Instead, I just use the full JAR URL as a unique name for the JAR. If you need to defeat the cache, just create a new git commit before running `make -C query ipython`. If defeating the cache becomes a common problem, we can add a ""reload_jar"" parameter or similar to the job spec. Third, I renamed `push-jar` in `query/Makefile` to `upload-query-jar` to mirror the build.yaml step. Fourth, I embraced the use of `NAMEPSACE` in `query/Makefile` instead of relying on the minor hack that our laptop usernames match our namespace names. This does mean you need to always specify NAMESPACE when uploading a jar. Finally, a pleasant outcome of this change is the elimination of a bunch of conditional build.yaml logic in the service backend tests!. I think this will simplify the use of Hail Query by Australia et al. because I've isolated the use of hail-specific data to `query/Makefile`. If there's a way to access the relevant global-config variables from `query/Makefile`, I can also fix the `query/Makefile` to be deployment-independent. cc: @lgruen @illusional @tpoterba . [1] For our default namespace deployment,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645:2272,cache,cache,2272,https://hail.is,https://github.com/hail-is/hail/pull/11645,1,['cache'],['cache']
Performance,"stacked on #7446. This is the first step toward broad integration of the timer throughout the compiler. This gives us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:259,Optimiz,Optimize,259,https://hail.is,https://github.com/hail-is/hail/pull/7476,6,['Optimiz'],['Optimize']
Performance,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:563,cache,cachefrom,563,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708,1,['cache'],['cachefrom']
Performance,"stination.; 1998 """"""; 2000 hl.current_backend().validate_file(output); -> 2002 Env.backend().execute(; 2003 ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec)); 2004 ). File ~/projects/hail/hail/python/hail/backend/backend.py:190, in Backend.execute(self, ir, timed); 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; --> 190 raise e.maybe_user_error(ir) from None; 191 if ir.typ == tvoid:; 192 value = None. File ~/projects/hail/hail/python/hail/backend/backend.py:188, in Backend.execute(self, ir, timed); 186 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 187 try:; --> 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; 190 raise e.maybe_user_error(ir) from None. File ~/projects/hail/hail/python/hail/backend/py4j_backend.py:223, in Py4JBackend._rpc(self, action, payload); 221 if resp.status_code >= 400:; 222 error_json = orjson.loads(resp.content); --> 223 raise fatal_error_from_java_error_triplet(; 224 error_json['short'], error_json['expanded'], error_json['error_id']; 225 ); 226 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: NoSuchElementException: Ref with name __iruid_1834 could not be resolved in env BindingEnv((__iruid_1832 -> struct{},__iruid_2157 -> struct{}),None,None,()). Java stack trace:; is.hail.utils.HailException: error after applying LowerToDistributedArray; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:32); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:2903,load,loads,2903,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['load'],['loads']
Performance,"stomers and partners can automate and simplify their SAP system administration tasks such as backup/restore of SAP HANA.</li>; <li>api-change:<code>stepfunctions</code>: [<code>botocore</code>] Update stepfunctions client to latest version</li>; <li>api-change:<code>transfer</code>: [<code>botocore</code>] Adds a NONE encryption algorithm type to AS2 connectors, providing support for skipping encryption of the AS2 message body when a HTTPS URL is also specified.</li>; </ul>; <h1>1.26.12</h1>; <ul>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Adds a new value (WEB_COMPUTE) to the Platform enum that allows customers to create Amplify Apps with Server-Side Rendering support.</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow simplifies the preparation and cataloging of SaaS data into the AWS Glue Data Catalog where your data can be discovered and accessed by AWS analytics and ML services. AppFlow now also supports data field partitioning and file size optimization to improve query performance and reduce cost.</li>; <li>api-change:<code>appsync</code>: [<code>botocore</code>] This release introduces the APPSYNC_JS runtime, and adds support for JavaScript in AppSync functions and AppSync pipeline resolvers.</li>; <li>api-change:<code>dms</code>: [<code>botocore</code>] Adds support for Internet Protocol Version 6 (IPv6) on DMS Replication Instances</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/f38ce50a317baf6715870b2706100d43b80b0c73""><code>f38ce50</code></a> Merge branch 'release-1.26.16'</li>; <li><a href=""https://github.com/boto/boto3/commit/33d7d6f020510890b93edf49de3f81c0ba208cb3""><code>33d7d6f</code></a> Bumping version to 1.26.16</li>; <li><a href=""https://github.com/boto/boto3/commit/fb642196bd5dda0f48636e3eeae5f983835fcef5""><code>fb64219</code></a> Add changelog entries from botocore</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12502:6171,optimiz,optimization,6171,https://hail.is,https://github.com/hail-is/hail/pull/12502,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"stomers and partners can automate and simplify their SAP system administration tasks such as backup/restore of SAP HANA.</li>; <li>api-change:<code>stepfunctions</code>: [<code>botocore</code>] Update stepfunctions client to latest version</li>; <li>api-change:<code>transfer</code>: [<code>botocore</code>] Adds a NONE encryption algorithm type to AS2 connectors, providing support for skipping encryption of the AS2 message body when a HTTPS URL is also specified.</li>; </ul>; <h1>1.26.12</h1>; <ul>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Adds a new value (WEB_COMPUTE) to the Platform enum that allows customers to create Amplify Apps with Server-Side Rendering support.</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow simplifies the preparation and cataloging of SaaS data into the AWS Glue Data Catalog where your data can be discovered and accessed by AWS analytics and ML services. AppFlow now also supports data field partitioning and file size optimization to improve query performance and reduce cost.</li>; <li>api-change:<code>appsync</code>: [<code>botocore</code>] This release introduces the APPSYNC_JS runtime, and adds support for JavaScript in AppSync functions and AppSync pipeline resolvers.</li>; <li>api-change:<code>dms</code>: [<code>botocore</code>] Adds support for Internet Protocol Version 6 (IPv6) on DMS Replication Instances</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds a new optional parameter &quot;privateIpAddress&quot; for the CreateNatGateway API. PrivateIPAddress will allow customers to select a custom Private IPv4 address instead of having it be auto-assigned.</li>; <li>api-change:<code>elbv2</code>: [<code>botocore</code>] Update elbv2 client to latest version</li>; <li>api-change:<code>emr-serverless</code>: [<code>botocore</code>] Adds support for AWS Graviton2 based applications. You can now select CPU architecture when creating new applications or updating exist",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:5700,optimiz,optimization,5700,https://hail.is,https://github.com/hail-is/hail/pull/12498,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,stractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5741,Load,LoadPlink,5741,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,support TableHead(TableOrderBy) optimization with descending sort,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6758:32,optimiz,optimization,32,https://hail.is,https://github.com/hail-is/hail/issues/6758,1,['optimiz'],['optimization']
Performance,t is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.Lowering,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5975,Optimiz,Optimize,5975,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,t is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.fs.FS.open(FS.scala:569); E 	at is.hail.io.fs.FS.open$(FS.scala:569); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.index.IndexReader$.readTypes(IndexReader.scala:65); E 	at is.hail.io.bgen.LoadBgen$.$anonfun$getBgenFileMetadata$2(LoadBgen.scala:208); E 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); E 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); E 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); E 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198); E 	at is.hail.io.bgen.LoadBgen$.getBgenFileMetadata(LoadBgen.scala:207); E 	at is.hail.io.bgen.MatrixBGENReader$.apply(LoadBgen.scala:387); E 	at is.hail.io.bgen.MatrixBGENReader$.fromJValue(LoadBgen.scala:365); E 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:116); E 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1815); E 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1738); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:2164); E 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:2136); E 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:2164); E 	at is.hail.backend.Backend.$anonfun$matrixTableType$2(Backend.scala:186); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); E 	at is.hail.utils.package$.using(package.scala:664); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); E 	at is.hail.utils,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001:3283,Load,LoadBgen,3283,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001,1,['Load'],['LoadBgen']
Performance,t java.io.ObjectInputStream.readObject(ObjectInputStream.java:461) ~[?:1.8.0_392]; 	at is.hail.backend.service.Worker$.$anonfun$main$4(Worker.scala:136) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.utils.package$.using(package.scala:657) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$3(Worker.scala:135) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$2(Worker.scala:134) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]; 	... 3 more; Caused by: javax.crypto.AEADBadTagException: Tag mismatch!; 	at com.sun.crypto.provider.GaloisCounterMode.decryptFinal(GaloisCounterMode.java:620) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.finalNoPadding(CipherCore.java:1116) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.fillOutputBuffer(CipherCo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:6928,concurren,concurrent,6928,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['concurren'],['concurrent']
Performance,"t org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13142,Load,LoadVCF,13142,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,t org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apach,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9491,Load,LoadVCF,9491,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"t sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/backend/hail-all-spark.jar\"" \; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:18615,load,load,18615,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['load'],['load']
Performance,t(Kryo.java:651); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:270); at org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:321); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:323); at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:140); at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95); at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34); at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75); at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539); at is.hail.backend.spark.SparkBackend.broadcast(SparkBackend.scala:411); at is.hail.io.plink.MatrixPLINKReader.executeGeneric(LoadPlink.scala:390); at is.hail.io.plink.MatrixPLINKReader.lower(LoadPlink.scala:561); at is.hail.expr.ir.TableReader.lower(TableIR.scala:663); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1062); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:728); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1021); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:4767,Load,LoadPlink,4767,https://hail.is,https://github.com/hail-is/hail/issues/14168,3,['Load'],['LoadPlink']
Performance,t.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Itera,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:10508,Load,LoadMatrix,10508,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,t.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217361,concurren,concurrent,217361,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"t.scala:65); at is.hail.backend.service.ServiceBackend.$anonfun$withExecuteContext$1(ServiceBackend.scala:426); at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); at is.hail.backend.service.ServiceBackend.withExecuteContext(ServiceBackend.scala:415); at is.hail.backend.service.ServiceBackendAPI.$anonfun$doAction$1(ServiceBackend.scala:608); at is.hail.services.package$.retryTransientErrors(package.scala:186); at is.hail.backend.service.ServiceBackendAPI.doAction(ServiceBackend.scala:585); at is.hail.backend.service.ServiceBackendAPI.executeOneCommand(ServiceBackend.scala:662); at is.hail.backend.service.ServiceBackendAPI$.main(ServiceBackend.scala:497); at is.hail.backend.service.Main$.main(Main.scala:10); at is.hail.backend.service.Main.main(Main.scala); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.base/java.lang.reflect.Method.invoke(Method.java:566); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.128-17247d8990c6; Error summary: RuntimeException: IR is.hail.expr.ir.StreamFlatMap of type stream<struct{oldContext: str, nRows: int64, nCols: int64}> is not realizable; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14537:11478,concurren,concurrent,11478,https://hail.is,https://github.com/hail-is/hail/issues/14537,6,['concurren'],['concurrent']
Performance,"tFrameDecoder.java:182); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedExcepti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213356,concurren,concurrent,213356,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"t_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2620,cache,cached,2620,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"ta/SupplementaryDatabase. #Data is copied for use with Nirvana 1.6.2 as of June 19 2017; gsutil -m cp -r gs://hail-common/nirvana/Data/Cache/24/GRCh37 /nirvana/Data/Cache; gsutil -m cp gs://hail-common/nirvana/Data/References/5/Homo_sapiens.GRCh37.Nirvana.dat /nirvana/Data/References; gsutil -m cp -r gs://hail-common/nirvana/Data/SupplementaryDatabase/39/GRCh37 /nirvana/Data/SupplementaryDatabase; gsutil -m cp -r gs://hail-common/nirvana/netcoreapp1.1 /nirvana; gsutil -m cp gs://hail-common/nirvana/nirvana-cloud-GRCh37.properties /nirvana. chmod -R 777 /nirvana. apt-get -y install curl libunwind8 gettext; curl -sSL -o dotnet.tar.gz https://go.microsoft.com/fwlink/?linkid=843453; mkdir -p /opt/dotnet && sudo tar zxf dotnet.tar.gz -C /opt/dotnet; ln -s /opt/dotnet/dotnet /usr/local/bin; ```. The properties file `nirvana-cloud-GRCh37.properties` points Nirvana to these local resources:; ```; hail.nirvana.location = /nirvana/netcoreapp1.1/Nirvana.dll; hail.nirvana.cache = /nirvana/Data/Cache/GRCh37/Ensembl84; hail.nirvana.reference = /nirvana/Data/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.supplementaryAnnotationDirectory = /nirvana/Data/SupplementaryDatabase/GRCh37; ```. I started a cluster with the init script and ran Nirvana on all of `profile225.vcf`, and later exported results for just a region bounding the gene CABIN1:; ```; from hail import *; hc = (HailContext()). (hc; .import_vcf(path='gs://jbloom/profile225.vcf.bgz'); .filter_multi(); .nirvana(block_size=10000, config='/nirvana/nirvana-cloud-GRCh37.properties'); .variants_table(); .filter(expr='v.start > 24430000 && v.start < 24580000'); .export(output='gs://jbloom/nirvana_cabin1.tsv')); ```. The top-level categories show reasonable-looking variation, except for ""clinvar"" and ""genes"" which are all `null` valued. Comparing a few variants in [gnomad](http://gnomad.broadinstitute.org/gene/ENSG00000099991), the annotations line up nicely. Here's an example of a common missense variant:; ```; 22:244683",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701:1544,cache,cache,1544,https://hail.is,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701,2,"['Cache', 'cache']","['Cache', 'cache']"
Performance,"table_location). ### determine the file locations of the pca variants; if (generate_contig_row_dict):; mt = hl.methods.import_bgen(bgen_files,; [],; contig_recoding=contigs,; _row_fields=['file_row_idx']); pca_rows = mt.filter_rows(hl.is_defined(pcloadings[mt.row_key])).rows(); print('about to collect'); # remove all unnecessary data, dropping keys and other irrelevant fields; pca_rows = pca_rows.key_by(); pca_rows = pca_rows.select(pca_rows.locus.contig, pca_rows.file_row_idx); contig_row_list = pca_rows.collect(); print('finished collecting'); contig_reformed = [(x['contig'], x['file_row_idx']) for x in contig_row_list]; print('reformed'); from collections import defaultdict; contig_row_dict = defaultdict(list); for k, v in contig_reformed:; contig_row_dict[k].append(v); print('dictionary created'). with hl.hadoop_open(contig_row_dict_location, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953:2936,load,load,2936,https://hail.is,https://github.com/hail-is/hail/issues/3953,1,['load'],['load']
Performance,"tage 5:=====================================================> (344 + 8) / 352]; 	[PASS] with 352 partitions: (50000, 1000); 	2020-06-10 10:30:13 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 7:=================================> (222 + 80) / 353]; 	[PASS] with 353 partitions: (50000, 973); 	2020-06-10 10:30:15 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 9:> (0 + 18) / 18]; 	[FAIL] with 354 partitions; 	Traceback (most recent call last):; 	 File ""test_11_cluster_sampleqc.py"", line 20, in <module>; 		print(""\n[PASS] with"", N, ""partitions:"", Y.count()); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/matrixtable.py"", line 2426, in count; 		return Env.backend().execute(count_ir); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 296, in execute; 		result = json.loads(self._jhc.backend().executeJSON(jir)); 	 File ""/bmrn/apps/spark/2.4.5/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 41, in deco; 		'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 	hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutput",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:3354,load,loads,3354,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['load'],['loads']
Performance,tageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:5494,concurren,concurrent,5494,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['concurren'],['concurrent']
Performance,tal 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:183748,Optimiz,OptimizePass,183748,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,tal 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.023ms self 0.023ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:190773,Optimiz,OptimizePass,190773,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,tal 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.038ms self 0.029ms children 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.189ms self 0.189ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:17483,Optimiz,OptimizePass,17483,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,tal 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.044ms self 0.035ms children 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.193ms self 0.193ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:10499,Optimiz,OptimizePass,10499,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"tandard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:4633,load,load,4633,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['load'],['load']
Performance,"tarting a dataproc cluster with VEP, e.g.; ```{bash}; hailctl dataproc start hail-test --region australia-southeast1 --project my-project --vep GRCh38 --packages gnomad --num-workers 2; ```; the dataproc cluster command would be provided the following environment variable through the `--metadata` flag: `VEP_REPLICATE=aus-sydney`. This variable is used within the script `gs://hail-common/hailctl/dataproc/0.2.115/vep-GRCh38.sh` to determine which bucket to pull the VEP cache data from. In more recent versions (tested with 0.2.130), this `VEP_REPLICATE` variable has been changed to `VEP_REPLICATE=australia-southeast1`, however the Australian bucket containing the VEP cache data is still `aus-sydney`, meaning that the VEP data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:1103,cache,cache,1103,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['cache'],['cache']
Performance,"tation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly different from keys) PType. ## Core Methods. ```scala; def elementType: PStruct; ```. - The PStruct representation of the key/value pair. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalDict. A PCanonicalArray-backed implementation of PDict. # <a name=""parray""></a> PNDArray. An abstract class for multidimensional arrays (tensors) that have a row-major or column-major layout. ## Core Methods. ```scala; val shape: StaticallyKnownField[PTuple, Long]; val strides: StaticallyKnownField[PTuple, Long]; ```. - Defines the tensor shape. ```scala; def loadElementToIRIntermediate(indices: Array[Code[Long]], ndAddress: Code[Long], mb: MethodBuilder): Code[_]; ```. - Load the element's primitive representation, as indexed by `indices`, which specifies the element index at every dimension in the PNDArray's shape. ```scala; def linearizeIndicesRowMajor(indices: Array[Code[Long]], shapeArray: Array[Code[Long]], mb: MethodBuilder): Code[Long]; ```. - Get the off-heap index of the element (since NDArray elements are stored as a 1D series of bytes off-heap). ```scala; def unlinearizeIndexRowMajor(index: Code[Long], shapeArray: Array[Code[Long]], mb: MethodBuilder): (Code[Unit], Array[Code[Long]]); ```. - Generate the index path that represents the virtual, shape-dependent index into an arbitrary tensor. ```scala; def copyRowMajorToColumnMajor(rowMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]. def copyColumnMajorToRowMajor(colMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]; ```. - Interconvert between column and row major. ```scala; def construct(flags: Code[Int], offset: Code[In",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:6727,Load,Load,6727,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['Load'],['Load']
Performance,"tation</strong></p>; <ul>; <li>Various typo fixes and doc improvements.</li>; </ul>; <p><strong>Packaging</strong></p>; <ul>; <li>Requests has started adopting some modern packaging practices.; The source files for the projects (formerly <code>requests</code>) is now located; in <code>src/requests</code> in the Requests sdist. (<a href=""https://redirect.github.com/psf/requests/issues/6506"">#6506</a>)</li>; <li>Starting in Requests 2.33.0, Requests will migrate to a PEP 517 build system; using <code>hatchling</code>. This should not impact the average user, but extremely old; versions of packaging utilities may have issues with the new packaging format.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/psf/requests/commit/d6ebc4a2f1f68b7e355fb7e4dd5ffc0845547f9f""><code>d6ebc4a</code></a> v2.32.0</li>; <li><a href=""https://github.com/psf/requests/commit/9a40d1277807f0a4f26c9a37eea8ec90faa8aadc""><code>9a40d12</code></a> Avoid reloading root certificates to improve concurrent performance (<a href=""https://redirect.github.com/psf/requests/issues/6667"">#6667</a>)</li>; <li><a href=""https://github.com/psf/requests/commit/0c030f78d24f29a459dbf39b28b4cc765e2153d7""><code>0c030f7</code></a> Merge pull request <a href=""https://redirect.github.com/psf/requests/issues/6702"">#6702</a> from nateprewitt/no_char_detection</li>; <li><a href=""https://github.com/psf/requests/commit/555b870eb19d497ddb67042645420083ec8efb02""><code>555b870</code></a> Allow character detection dependencies to be optional in post-packaging steps</li>; <li><a href=""https://github.com/psf/requests/commit/d6dded3f00afcf56a7e866cb0732799045301eb0""><code>d6dded3</code></a> Merge pull request <a href=""https://redirect.github.com/psf/requests/issues/6700"">#6700</a> from franekmagiera/update-redirect-to-invalid-uri-test</li>; <li><a href=""https://github.com/psf/requests/commit/bf24b7d8d17da34be720c19e5978b2d3bf94a53b""><code>bf24b7d</code></a> Use an ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14555:7816,concurren,concurrent,7816,https://hail.is,https://github.com/hail-is/hail/pull/14555,2,"['concurren', 'perform']","['concurrent', 'performance']"
Performance,"te `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - Remove the auto-scaling and remove the k8s grace period. Neither of these really works right. We; will live without the auto-scaling for now. I have to fix the grace period thing before we let; users run real pipelines.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10390:3590,concurren,concurrent,3590,https://hail.is,https://github.com/hail-is/hail/pull/10390,1,['concurren'],['concurrent']
Performance,"te sphinx requirement (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/175"">#175</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/f4e11b7223e546515e99c984f9948b6caa06622a""><code>f4e11b7</code></a> chore: update version</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/bf4a9a8eb24149cd68efbc6ae61a6445121f4b70""><code>bf4a9a8</code></a> chore: update setup.py</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/f028cc793e3a2c519be6a52a49fb77ff0b014c9b""><code>f028cc7</code></a> [feat] regenerate client for v1.19.15 (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/172"">#172</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/c876bce774cebdb1eec90c8c957a7b45ec3c1404""><code>c876bce</code></a> chore: update changlog</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/e625e8d296aa8f68d5b0a285e0414c43877f63f5""><code>e625e8d</code></a> [feat] Load kubeconfig from dict (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/169"">#169</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/1722b0b9e19d8f25eb653a2f27bdc97c28a1c713""><code>1722b0b</code></a> chore(deps): bump actions/setup-python from 2.3.0 to 2.3.1 (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/168"">#168</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/tomplus/kubernetes_asyncio/compare/v9.1.0...v19.15.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=kubernetes-asyncio&package-manager=pip&previous-version=9.1.0&new-version=19.15.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:16110,Load,Load,16110,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['Load'],['Load']
Performance,"te(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 840, in create; await self._run_until_done_or_deleted(self.image.pull); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1014, in _run_until_done_or_deleted; raise ContainerDeletedError from e; ContainerDeletedError. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1887, in run_container; await container.run(on_completion); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 920, in run; await on_completion(*args, **kwargs); File ""/usr/lib/python3.9/contextlib.py"", line 137, in __exit__; self.gen.throw(typ, value, traceback); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 1154, in step; yield; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1873, in on_completion; await self.worker.fs.read(container.log_path),; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/fs/fs.py"", line 281, in read; async with await self.open(url) as f:; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/router_fs.py"", line 76, in open; return await fs.open(url); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 252, in open; f = await blocking_to_async(self._thread_pool, open, self._get_path(url), 'rb'); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 181, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 182, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); FileNotFoundError: [Errno 2] No such file or directory: '/batch/35055eff18f547de9c77b9e80744e362/main/container.log'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13906:2489,concurren,concurrent,2489,https://hail.is,https://github.com/hail-is/hail/issues/13906,1,['concurren'],['concurrent']
Performance,te(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:788); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:64); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:37); at is.hail.utils.package$.using(package.scala:587); at is.hail.rvd.RVDPartitionInfo$.apply(RVDPartitionInfo.scala:37); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:9170,Load,LoadVCF,9170,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['Load'],['LoadVCF']
Performance,"te/__init__.py'; adding 'hailtop/utils/validate/validate.py'; adding 'hail-0.2.124.dist-info/METADATA'; adding 'hail-0.2.124.dist-info/WHEEL'; adding 'hail-0.2.124.dist-info/entry_points.txt'; adding 'hail-0.2.124.dist-info/top_level.txt'; adding 'hail-0.2.124.dist-info/RECORD'; emoving build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Defaulting to user installation because normal site-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:29792,cache,cached,29792,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,te/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.046ms self 0.046ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.655ms self 0.655ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.184ms self 0.184ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:152355,Optimiz,OptimizePass,152355,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,te/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.609ms self 0.609ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.195ms self 0.195ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:136568,Optimiz,OptimizePass,136568,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,te/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.054ms self 0.054ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.566ms self 0.566ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.445ms self 0.445ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:120850,Optimiz,OptimizePass,120850,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,te/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.083ms self 0.043ms children 0.040ms %children 48.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.621ms self 0.621ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:157553,Optimiz,OptimizePass,157553,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,te/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.087ms self 0.042ms children 0.045ms %children 51.66%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.650ms self 0.650ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:141766,Optimiz,OptimizePass,141766,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,te/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.101ms self 0.049ms children 0.052ms %children 51.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.283ms self 4.283ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:126048,Optimiz,OptimizePass,126048,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,teBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(Low,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5449,Optimiz,Optimize,5449,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,teBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5440,Optimiz,Optimize,5440,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Optimiz'],['Optimize']
Performance,teOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3235:5512,concurren,concurrent,5512,https://hail.is,https://github.com/hail-is/hail/issues/3235,1,['concurren'],['concurrent']
Performance,"tected.; 2017-08-28 21:47:35 Hail: INFO: Ordering unsorted dataset with network shuffle; [Stage 2234:============================================> (4 + 1) / 5]2017-08-28 21:47:37 Hail: WARN: Found 2 samples with missing sex information (not 1 or 2).; Missing sex identifiers: [ 0 ]; 2017-08-28 21:47:37 Hail: WARN: 2 samples discarded from .fam: sex of child is missing.; 2017-08-28 21:47:38 Hail: INFO: Found 250 samples in fam file.; 2017-08-28 21:47:38 Hail: INFO: Found 2000 variants in bim file.; 2017-08-28 21:47:38 Hail: INFO: Coerced sorted dataset; 2017-08-28 21:47:38 Hail: INFO: Modified the genotype schema with annotateGenotypesExpr.; Original: Struct{GT:Call}; New: Genotype; 2017-08-28 21:47:38 Hail: INFO: Reading table to impute column types; [Stage 2258:============================> (1 + 1) / 2]2017-08-28 21:47:40 Hail: INFO: Finished type imputation; Loading column `f0' as type String (imputed); Loading column `f1' as type String (imputed); Loading column `f2' as type Float64 (imputed); 2017-08-28 21:47:41 Hail: INFO: Reading table to impute column types; 2017-08-28 21:47:41 Hail: INFO: Finished type imputation; Loading column `f0' as type String (imputed); Loading column `f1' as type String (imputed); Loading column `f2' as type Float64 (imputed); 2017-08-28 21:47:41 Hail: INFO: rrm: Computing Realized Relationship Matrix...; [Stage 2263:============================> (1 + 1) / 2]2017-08-28 21:47:44 Hail: INFO: rrm: RRM computed using 1000 variants.; 2017-08-28 21:47:45 Hail: INFO: lmmreg: running lmmreg on 250 samples with 2 sample covariates including intercept...; 2017-08-28 21:47:45 Hail: INFO: lmmreg: Computing eigendecomposition of kinship matrix...; 2017-08-28 21:47:45 Hail: INFO: lmmreg: Estimating delta using REML... ; 2017-08-28 21:47:45 Hail: INFO: lmmreg: Evals 1 to 20: 14.94768, 2.08278, 2.02984, 1.99490, 1.97532, 1.96462, 1.95253, 1.92972, 1.91744, 1.90489, 1.87748, 1.86775, 1.84180, 1.82938, 1.81619, 1.79946, 1.78303, 1.77441, 1.75651, 1.7511",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:1676,Load,Loading,1676,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,6,['Load'],['Loading']
Performance,ted yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:53370,concurren,concurrent,53370,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ted yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 14.0 in stage 0.0 (TID 14, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:61841,concurren,concurrent,61841,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ted yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 26.0 in stage 0.0 (TID 26, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:68985,concurren,concurrent,68985,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"tegories, new fields to the API response, &quot;aliases&quot; and &quot;categories&quot;</li>; <li>api-change:<code>securityhub</code>: [<code>botocore</code>] Documentation updates for Security Hub</li>; <li>api-change:<code>ssm-incidents</code>: [<code>botocore</code>] RelatedItems now have an ID field which can be used for referencing them else where. Introducing event references in TimelineEvent API and increasing maximum length of &quot;eventData&quot; to 12K characters.</li>; </ul>; <h1>1.26.7</h1>; <ul>; <li>api-change:<code>autoscaling</code>: [<code>botocore</code>] This release adds a new price capacity optimized allocation strategy for Spot Instances to help customers optimize provisioning of Spot Instances via EC2 Auto Scaling, EC2 Fleet, and Spot Fleet. It allocates Spot Instances based on both spare capacity availability and Spot Instance price.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds a new price capacity optimized allocation strategy for Spot Instances to help customers optimize provisioning of Spot Instances via EC2 Auto Scaling, EC2 Fleet, and Spot Fleet. It allocates Spot Instances based on both spare capacity availability and Spot Instance price.</li>; <li>api-change:<code>ecs</code>: [<code>botocore</code>] This release adds support for task scale-in protection with updateTaskProtection and getTaskProtection APIs. UpdateTaskProtection API can be used to protect a service managed task from being terminated by scale-in events and getTaskProtection API to get the scale-in protection status of a task.</li>; <li>api-change:<code>es</code>: [<code>botocore</code>] Amazon OpenSearch Service now offers managed VPC endpoints to connect to your Amazon OpenSearch Service VPC-enabled domain in a Virtual Private Cloud (VPC). This feature allows you to privately access OpenSearch Service domain without using public IPs or requiring traffic to traverse the Internet.</li>; <li>api-change:<code>resource-explorer-2</code>: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12458:2342,optimiz,optimized,2342,https://hail.is,https://github.com/hail-is/hail/pull/12458,4,['optimiz'],"['optimize', 'optimized']"
Performance,"ter to take a detour from adding job groups and get rid of how we currently do the batch update in MJC to allow for job groups in the future before putting in job groups tables so that I could slot in the appropriate state and time_completed updates to both batches and job_groups tables in the same place rather than relying on a trigger for the updates. I can think about which set of changes should go first (I'm not wedded to either PR coming first -- just thought this way was conceptually easier to understand when there was just a batches table). I haven't 100% convinced myself this change to tokenize the `batches_n_jobs_in_complete_states` table is absolutely necessary for nested job groups, but it seemed like we would want the performance improvements regardless. > 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. This is for performance reasons to avoid serialization and race conditions. If the update occurs in the same transaction, then each transaction will be serialized checking if `n_jobs == n_complete`. If we don't have a `SELECT ... FOR UPDATE`, I couldn't convince myself that there wouldn't be a race condition where a batch completion event isn't accidentally missed. . I think the healing loop would only happen if the batch driver got restarted in between:; 1. CALL mark_job_complete() in the database; 2. mark_batch_complete(). If 1. errors, the operation is aborted entirely. On second thought, it might be possible to use an optimistic locking strategy, but that's more complicated and I'd have to think about it some more. > It seems to me like it would be preferable to instead first update application code to mark the batch complete if it is not complete, then remove the now redundant marking complete of the batch from the trigger. Then there is no delay after the migration where batches are not complete for some time. Ah, g",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:2866,perform,performance,2866,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,2,"['perform', 'race condition']","['performance', 'race conditions']"
Performance,terOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed. /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call -------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:7,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:22980,concurren,concurrent,22980,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"terOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed.; ```. ### Version. 0.2.115-f6017673dbb6. ### Relevant log output. ```shell; ________________________________ test_spectra_4 ________________________________; [gw2] linux -- Python 3.8.10 /usr/bin/python3. def test_spectra_4():; > spectra_helper(spec4). test/hail/methods/test_pca.py:229: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/hail/methods/test_pca.py:172: in spectra_helper; hail_V = (np.array(scores.scores.collect()) / singulars).T; <decorator-gen-538>:2: in collect; ???; /usr/local/lib/python3.8/dist-packages/hail/typecheck/check.py:584: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.8/dist-packages/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:9362,concurren,concurrent,9362,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"teratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Under ""Failed Stages"", these were the details for what I was running:; ```; org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:285); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); is.hail.table.Table.showString(Table.scala:1031); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.refle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508:6043,concurren,concurrent,6043,https://hail.is,https://github.com/hail-is/hail/issues/3508,1,['concurren'],['concurrent']
Performance,teratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3516:3233,concurren,concurrent,3233,https://hail.is,https://github.com/hail-is/hail/issues/3516,1,['concurren'],['concurrent']
Performance,"tes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-tes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:8146,perform,performed,8146,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['perform'],['performed']
Performance,"test-dev/pytest-xdist/blob/master/CHANGELOG.rst"">pytest-xdist's changelog</a>.</em></p>; <blockquote>; <h1>pytest-xdist 2.5.0 (2021-12-10)</h1>; <h2>Deprecations and Removals</h2>; <ul>; <li><code>[#468](https://github.com/pytest-dev/pytest-xdist/issues/468) &lt;https://github.com/pytest-dev/pytest-xdist/issues/468&gt;</code>_: The <code>--boxed</code> command line argument is deprecated. Install pytest-forked and use <code>--forked</code> instead. pytest-xdist 3.0.0 will remove the <code>--boxed</code> argument and pytest-forked dependency.</li>; </ul>; <h2>Features</h2>; <ul>; <li>; <p><code>[#722](https://github.com/pytest-dev/pytest-xdist/issues/722) &lt;https://github.com/pytest-dev/pytest-xdist/issues/722&gt;</code>_: Full compatibility with pytest 7 - no deprecation warnings or use of legacy features.</p>; </li>; <li>; <p><code>[#733](https://github.com/pytest-dev/pytest-xdist/issues/733) &lt;https://github.com/pytest-dev/pytest-xdist/issues/733&gt;</code>_: New <code>--dist=loadgroup</code> option, which ensures all tests marked with <code>@pytest.mark.xdist_group</code> run in the same session/worker. Other tests run distributed as in <code>--dist=load</code>.</p>; </li>; </ul>; <h2>Trivial Changes</h2>; <ul>; <li>; <p><code>[#708](https://github.com/pytest-dev/pytest-xdist/issues/708) &lt;https://github.com/pytest-dev/pytest-xdist/issues/708&gt;</code>_: Use <code>@pytest.hookspec</code> decorator to declare hook options in <code>newhooks.py</code> to avoid warnings in <code>pytest 7.0</code>.</p>; </li>; <li>; <p><code>[#719](https://github.com/pytest-dev/pytest-xdist/issues/719) &lt;https://github.com/pytest-dev/pytest-xdist/issues/719&gt;</code>_: Use up-to-date <code>setup.cfg</code>/<code>pyproject.toml</code> packaging setup.</p>; </li>; <li>; <p><code>[#720](https://github.com/pytest-dev/pytest-xdist/issues/720) &lt;https://github.com/pytest-dev/pytest-xdist/issues/720&gt;</code>_: Require pytest&gt;=6.2.0.</p>; </li>; <li>; <p><code>[#721](https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11491:1175,load,loadgroup,1175,https://hail.is,https://github.com/hail-is/hail/pull/11491,2,['load'],['loadgroup']
Performance,"textRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:3248,concurren,concurrent,3248,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186,1,['concurren'],['concurrent']
Performance,"that the local filesystem can, infrequently, stall when executing `rmtree`. Note that the error about the directory being non-empty is because we have a bug in `rm_dir`: we try to remove the directory even if the children tasks failed. It oddly seems to have happened on both a deploy batch and a PR batch:; - PR: https://ci.hail.is/batches/7706444/jobs/170; - deploy: https://ci.hail.is/batches/7707793/jobs/172. ```; [2023-08-02 05:33:14] test/hail/utils/test_hl_hadoop_and_hail_fs.py::test_hadoop_methods_3[local] PASSED; +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_1 (139802083059456) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_0 (139802091452160) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = wo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:1017,concurren,concurrent,1017,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,"the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1587,load,load,1587,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['load'],['load']
Performance,"the `loader` code is currently just a chunk of code---it gets run whenever it's invoked. When I put it into emit, I was going to protect it from getting run every time the function was evaluated with some checking, but I can put it directly into this function instead--that might be better.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3357#issuecomment-380965877:5,load,loader,5,https://hail.is,https://github.com/hail-is/hail/pull/3357#issuecomment-380965877,1,['load'],['loader']
Performance,"the current structure is easier to follow:. 1. Fit the null model.; 2. If wald, assume the beta for the genotypes is zero and use the rest of the parameters from the null model fit to compute the score (i.e. the gradient of the likelihood). Recall calculus: gradient near zero => value near the maximum. Return: this is the test.; 3. Otherwise, fit the full model starting at the null fit parameters.; 4. Test the ""goodness"" of this new & full fit. ---. Poisson regression is similar but with a different likelihood function and gradient thereof. Notice that I `key_cols_by()` to indicate to Hail that the order of the cols is irrelevant (the result is a locus-keyed table after all). This is necessary at least until #12753 merges. I think it's generally a good idea though: it indicates to Hail that the ordering of the columns is irrelevant, which is potentially useful information for the optimizer!. ---. Both logistic and Poisson regression can benefit from BLAS3 by running at least the score test for multiple variants at once. ---. I'll attach an image in the comments, but I spend ~6 seconds compiling this trivial model and ~140ms testing it. ```python3; import hail as hl; mt = hl.utils.range_matrix_table(1, 3); mt = mt.annotate_entries(x=hl.literal([1, 3, 10, 5])); ht = hl.poisson_regression_rows(; 'wald', y=hl.literal([0, 1, 1, 0])[mt.col_idx], x=mt.x[mt.col_idx], covariates=[1], max_iterations=2); ht.collect(); ```. I grabbed some [sample code from; scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html) for Poisson regression (doing a score test rather than a wald test) and timed it. It takes ~8ms. So we're 3 orders of magnitude including the compiler, and ~1.2 orders of magnitude off without the compiler. Digging in a bit:; - ~65ms for class loading.; - ~15ms for region allocation.; - ~20ms various little spots. Leaving about 40ms strictly executing generated code That's about 5x which is starting to feel reasonable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12793:2832,load,loading,2832,https://hail.is,https://github.com/hail-is/hail/pull/12793,1,['load'],['loading']
Performance,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6860:944,load,loadInt,944,https://hail.is,https://github.com/hail-is/hail/issues/6860,2,['load'],['loadInt']
Performance,"the worker. The worker starts one jvm per core on; startup. The mainclass is a new class called `JVMEntryway`. This entryway starts a UNIX socket and; speaks a very simple binary protocol. It accepts only one type of message:. ```; int32 the number of strings to expect; (; int32 the number of bytes in the next string; byte* UTF-8 string; )*; ```. The array of strings is interpreted as:. ```; comma-spearated-classpath; main-class-name; arg0; arg1; ...; ```. The entryway constructs a URLClassLoader with the given classpath, reflectively allocates an; instance of the mainclass and invokes the `main` method with the remaining arguments. This is; obviously a security risk. The system bans JARs from locations not controlled (and locked down) by; Hail Team. You should require me to hardcode the mainclass as; `is.hail.backend.service.ServiceBackendSocketAPI2` before we merge; however, this flexibility was; useful during development. The JVMEntryway will eventually be useful because we will keep a ClassLoader full of a bunch of; JIT-optimized Hail classes. I did not include that in this PR because we need to finish eliminating; global state used by Hail. Currently, two executions would try to re-use compiled class names for; different code, leading to very weird errors. # Changes to File Systems. Hail has three four file system interfaces:. | File System Interface | Public | Language | Async |; | ----------------------- | ------ | -------- | ----- |; | hail.utils.hadoop_utils | Yes | Python | no |; | hail.fs | Yes | Python | no |; | hailtop.aiotools.fs | No | Python | yes |; | is.hail.io.fs | No | Scala | no |. `hail.fs` is technically in the public API (via `hl.current_backend().fs`), but I doubt anyone uses; it. `hail.utils.hadoop_utils` is a shim over `hail.fs`, there are no direct concrete implementations of; it. This PR adds `hail.fs.RouterFS` to `hail.fs`, a synchronous wrapper around; `hailtop.aiotools.fs.AsyncRouterFS`. A ""router"" file system is one which operates on ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194:4001,optimiz,optimized,4001,https://hail.is,https://github.com/hail-is/hail/pull/11194,1,['optimiz'],['optimized']
Performance,"there will definitely be a regime where that's faster, yeah. But that's just an optimization to order_by, yes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7144#issuecomment-535749300:80,optimiz,optimization,80,https://hail.is,https://github.com/hail-is/hail/issues/7144#issuecomment-535749300,1,['optimiz'],['optimization']
Performance,thread-2; 2023-09-22 19:11:13.138 : ERROR: GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; From is.hail.relocated.com.google.cloud.storage.StorageException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:165); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:298); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:729); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$20(StorageImpl.java:610); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:4901,load,load,4901,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['load'],['load']
Performance,"thub-redirect.dependabot.com/bokeh/bokeh/issues/11422"">#11422</a> [component: bokehjs] [BUG] <code>DeserializationError</code> when trying to change a <code>DataTable</code>'s columns with <code>CustomJS</code></li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11800"">#11800</a> [BUG] DeserializationError when plotting graphs</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11801"">#11801</a> [component: bokehjs] [BUG] Log axis figures don't render if they're not visible at start</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11807"">#11807</a> [component: bokehjs] Work around issues with initialization-time change discovery</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11808"">#11808</a> Don't unnecessarily update node/edge renderers in graphs</li>; </ul>; </li>; <li>; <p>tasks:</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11613"">#11613</a> [component: docs] Cache-bust custom.css for docs</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11791"">#11791</a> [component: docs] Update issue template to use new GH forms</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11761"">#11761</a> [component: docs] Clarify use of color in first steps guide</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11762"">#11762</a> [component: docs] Replace slash with backslash for PS commands</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11767"">#11767</a> [component: bokehjs] Upgrade jquery-ui to resolve security concerns</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11781"">#11781</a> [component: examples] fix transform jitter example</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/11786"">#11786</a> bokeh 2.4.2 backports</li>; <li><a href=""https://github-redirect.dependabot.c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11540:1382,Cache,Cache-bust,1382,https://hail.is,https://github.com/hail-is/hail/pull/11540,1,['Cache'],['Cache-bust']
Performance,"tialized for thread 8: pool-1-thread-1\n2022-11-15 20:30:18.114 ServiceBackend$: INFO: executing: cEPZ5IV9gUtSnCiAiHXOPs None\n2022-11-15 20:30:18.127 root: INFO: optimize optimize: darrayLowerer, initial IR: before: IR size 17: \n(Let __rng_state\n (RNGStateLiteral (0 0 0 0))\n (MakeTuple (0)\n (TableAggregate\n (TableMapRows\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (InsertFields\n (SelectFields () (SelectFields (idx) (Ref row)))\n None\n (idx (GetField idx (Ref row)))))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row)))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, initial IR: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, after LowerMatrixToTable: before: IR size 8: \n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.148 root: INFO: optimize optimize: darrayLowerer, after LowerMatrixToTable: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.160 root: INFO: Aggregate: useTreeAggregate=false\n2022-11-15 20:30:18.160 root: INFO: Aggregate: commutative=false\n2022-11-15 20:30:18.163 root: INFO: optimize optimize: compileLowerer, initial IR: before: IR size 70: \n(MakeTuple (0)\n (Let __iruid_455\n (MakeStruct)\n (Let __iruid_458\n (Let global\n (Ref __iruid_455)\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (CollectStateSig +PInt32)) ()))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32)))))\n (Let __iruid_460\n (CollectDistributedArray\n table_aggregate_singlestage __iruid_456\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:2124,optimiz,optimize,2124,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,4,['optimiz'],['optimize']
Performance,til.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)sun.reflect.generics.reflectiveObjects.NotImplementedException: null; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Hail version: devel-438801a84105; Error summary: NotImplementedException: null; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:7946,concurren,concurrent,7946,https://hail.is,https://github.com/hail-is/hail/issues/4215,2,['concurren'],['concurrent']
Performance,tils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_matrix_ir$2(SparkBackend.scala:689); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:69); 	at is.hail.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2608,Load,LoadPlink,2608,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"tils/utils.py:402: in run_and_cleanup; retval = await f(*args, **kwargs); /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:367: in rm_file; await self.remove(path); /usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py:348: in remove; return await blocking_to_async(self._thread_pool, os.remove, path); /usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:162: in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; /usr/lib/python3.9/asyncio/base_events.py:819: in run_in_executor; executor.submit(func, *args), loop=self); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <concurrent.futures.thread.ThreadPoolExecutor object at 0x7f263d862100>; fn = <function blocking_to_async.<locals>.<lambda> at 0x7f263d781040>, args = (); kwargs = {}. def submit(self, fn, /, *args, **kwargs):; > with self._shutdown_lock, _global_shutdown_lock:; E Failed: Timeout >600.0s. /usr/lib/python3.9/concurrent/futures/thread.py:162: Failed; ---------------------------- Captured log teardown -----------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 352, in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 162, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/python3.9/asyncio/futures.py"", line 201, in result; raise self._exception; File ""/usr/lib/python3.9/concurrent/futures/thread.py"", li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:12099,concurren,concurrent,12099,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,time.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:7912,concurren,concurrent,7912,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['concurren'],['concurrent']
Performance,time.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.net.SocketTimeoutException: connect timed out; E 	at java.net.PlainSocketImpl.socketConnect(Native Method); E 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); E 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); E 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); E 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); E 	at java.net.Socket.connect(Socket.java:607); E 	at is.hail.relocated.org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75); E 	at is.hail.relocated.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142); E 	at is.hail.relocated.org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManag,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:7626,concurren,concurrent,7626,https://hail.is,https://github.com/hail-is/hail/issues/13074,1,['concurren'],['concurrent']
Performance,"ting D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadable=129.00 KiB, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.560 : INFO: RegionPool: FREE: 129.0K allocated (129.0K blocks / 0 chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.561 : ERROR: error while applying lowering 'LowerAndExecuteShuffles'; 2023-05-04 01:04:37.600 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:37.601 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.601 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.601 : INFO: RegionPool: FREE: 128.0K allocated (128.0K blocks / 0 chunks), regions.size = 2, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.603 : ERROR: SocketException: Connection reset; From javax.net.ssl.SSLException: Connection reset; 	at sun.security.ssl.Alert.createSSLException(Alert.java:127); 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:324); 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:267); 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:262); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:138); 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1400); 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1368); 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:23187,cache,cache,23187,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['cache'],['cache']
Performance,"ting decisions dynamically circumvents this limitation. However, this prevents usage of NGINX [upstream](http://nginx.org/en/docs/http/ngx_http_upstream_module.html) blocks that provide connection pooling, at least in the community edition, and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate TLS with internal-gateway and free up cycles in the batch-driver python process. By using proper persistent connections, we can reduce the TLS overhead to single-digit percents of a core. This leads to the first goal of this transition: configure our load balancers to know the full cluster configuration at any point in time so they can properly maintain connection pools with upstream services. However, this is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:2253,load,load,2253,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['load'],['load']
Performance,tingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.for,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:7839,Load,LoadVCF,7839,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,"tingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is.hail.utils.package$.using(package.scala:575); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:11716,Load,LoadVCF,11716,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Load'],['LoadVCF']
Performance,tion.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:9139,concurren,concurrent,9139,https://hail.is,https://github.com/hail-is/hail/issues/8545,1,['concurren'],['concurrent']
Performance,tion.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:81966,concurren,concurrent,81966,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['concurren'],['concurrent']
Performance,"tion.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-a1d6ecc; Error summary: NoSuchElementException: key not found: GT; ```; The file has `GT` in the format field, but it's missing the corresponding header line. Passing a custom `header_file=` fixes the problem, but it's unfortunate that that's required (especially on such a widely used publicly available dataset).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:13732,concurren,concurrent,13732,https://hail.is,https://github.com/hail-is/hail/issues/3467,2,['concurren'],['concurrent']
Performance,tion: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:5054,load,loadClass,5054,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['load'],['loadClass']
Performance,tionalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 12.144ms self 12.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 9.513ms self 3.957ms children 5.556ms %children 58.40%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 4.091ms self 4.091ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:41400,Optimiz,OptimizePass,41400,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,tions$.filterGenotypes$extension(VariantDataset.scala:463); at is.hail.variant.VariantDatasetFunctions.filterGenotypes(VariantDataset.scala:449); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: is.hail.asm4s.AsmFunction2; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:13599,load,loadClass,13599,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['loadClass']
Performance,"to a /opt/venv installation + added to PATH to be available to non-root users (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3202"">#3202</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>Change from deprecated <code>asyncio.get_event_loop()</code> to create our event loop which removes DeprecationWarning (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3164"">#3164</a>)</li>; <li>Remove logging from internal <code>blib2to3</code> library since it regularly emits error logs about failed caching that can and should be ignored (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3193"">#3193</a>)</li>; </ul>; <h3>Parser</h3>; <ul>; <li>Type comments are now included in the AST equivalence check consistently so accidental deletion raises an error. Though type comments can't be tracked when running on PyPy 3.7 due to standard library limitations. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2874"">#2874</a>)</li>; </ul>; <h3>Performance</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/psf/black/blob/main/CHANGES.md"">black's changelog</a>.</em></p>; <blockquote>; <h2>22.8.0</h2>; <h3>Highlights</h3>; <ul>; <li>Python 3.11 is now supported, except for <em>blackd</em> as aiohttp does not support 3.11 as; of publishing (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3234"">#3234</a>)</li>; <li>This is the last release that supports running <em>Black</em> on Python 3.6 (formatting 3.6; code will continue to be supported until further notice)</li>; <li>Reword the stability policy to say that we may, in rare cases, make changes that; affect code that was not previously formatted by <em>Black</em> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3155"">#3155</a>)</li>; </ul>; <h3>Stable style</h3>; <ul>; <li>Fix an infinite loop when using <code># fmt: on/off</code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12174:4975,Perform,Performance,4975,https://hail.is,https://github.com/hail-is/hail/pull/12174,1,['Perform'],['Performance']
Performance,"to complete before it starts the next iteration of the scheduler. This leaves the scheduler vulnerable to problematic workers or workers that happen to be preempted during the scheduling process. So, the driver sets a [2 second timeout](https://github.com/hail-is/hail/blob/b27737f67bf9e69f1abed2fec07fc7c921790ef8/batch/batch/driver/job.py#L585) on the call to `/api/v1alpha/batches/jobs/create`. Additionally, this general design means that in the event of a request timeout or transient error, Batch cannot guarantee that there is always at most one concurrent running attempt for a given job. This ends up being a fine (and intentional) concession in practice because the idempotent design of preemptible jobs tends to cover this scenario, but it is regardless wasted compute and cost to users. Nevertheless, we strive to minimize cases where we might halt the scheduling loop or double-schedule work, and one way to do that in the current design is to minimize the variance in latency of `/api/v1alpha/batches/jobs/create`. The largest source of this latency is the request to blob storage. While GCS and ABS are relatively fast and highly available, Batch in Azure Terra requires first obtaining SAS tokens from the Terra control plane, which can introduce much higher and more variable latency. There have also been occurrences in the past of corrupted or deleted specs, which introduce unexpected failure modes that should error the job but instead disrupt the scheduling loop. Many of these problems would be mitigated by moving the read from object storage outside of the `/api/v1alpha/batches/jobs/create` endpoint. The endpoint should push this read into the asynchronous task that ultimately runs the job and therefore return its acknowledgement to the driver faster. If the worker encounters errors later on while reading the spec, those should result in `error`ing the job instead of raising a 500 in the scheduling request. ### Version. 0.2.129. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14456:2001,latency,latency,2001,https://hail.is,https://github.com/hail-is/hail/issues/14456,2,['latency'],['latency']
Performance,"toBlockMatrixDense method did not specify size of matrix in constructor, meaning that querying its size later performed an RDD action.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1969:110,perform,performed,110,https://hail.is,https://github.com/hail-is/hail/pull/1969,1,['perform'],['performed']
Performance,"tools/htsjdk/issues/1525"">#1525</a>); d40fe5412 Beta implementation of Bundles. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1546"">#1546</a>)</p>; <p>CRAM; 489c4192d Support CRAM reference regions. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1605"">#1605</a>); 22aec6782 Fix decoding of CRAM Scores read feature during normalization. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1592"">#1592</a>); 6507249a4 Make the CRAM MD5 failure message more user friendly. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1607"">#1607</a>); b5af659e6 Fix restoration of read base feature code. <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1379"">#1379</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1590"">#1590</a>); e63c34a92 Ignore TC, TN on CRAM read (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1578"">#1578</a>)</p>; <p>BAM/SAM; 1449dec45 Support loading of CSI from URLs/streams. <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1507"">#1507</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1595"">#1595</a>); a38c78d6c Add an option to SAMFileWriter to disable checking of ordering of rec… (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1599"">#1599</a>); 51aa6ed2b Validate that SAM header tag keys are exactly 2 characters long (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1561"">#1561</a>); fbd9e96d5 Deprecate OTHER as a PL value (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1552"">#1552</a>); d5f7e106b Adding PL Tag 'DNBSEQ' as the Platform/Technology for BGI/MGI (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1547"">#1547</a>)</p>; <p>Misc Improvements; f461401e3 Silence AsciiLineReader warning when creating a FASTA sequence index (<a href=""https://github-redirect.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12229:2937,load,loading,2937,https://hail.is,https://github.com/hail-is/hail/pull/12229,2,['load'],['loading']
Performance,tor(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. Thanks!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:50926,concurren,concurrent,50926,https://hail.is,https://github.com/hail-is/hail/issues/7044,2,['concurren'],['concurrent']
Performance,tor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19279,Load,LoadVCF,19279,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"tor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6225,Load,LoadVCF,6225,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,"tors here: http://dev.hail.is/t/proposal-for-aggregators/93/3. Builds on: https://github.com/hail-is/hail/pull/3552. I still need to finish converting some tests from ExtractAggregatorSuite (they are currently commented out). Old-style aggregators (filter, map, flatMap) are expanded at toIR conversion time to ApplyAggOp and SeqOp. ApplyAggOp returns the result of the aggregation. ApplyAggOp's first argument, which must be of type TVoid, is the expression to run for each element being aggregated over. SeqOp merges a computed value into the RegionValueAggregator. I added a AggSignature that holds the AggOp, type being aggregated over and the RegionValueAggregator constructor argument types. This is stored by the ApplyAggOp and the SeqOp. @tpoterba I believe TAggregable is no longer used in the IR code and can go away when the AST gets ripped out. @danking I added some IR testing logic to TestUtils. Namely, `eval` evaluates an IR with environments, args and/or aggregations with a single call (and verifies that the interpret with and without optimization and compiler all agree). There are also functions for asserting the result of aggregations, for example:. ```; val aggSig = AggSignature(Sum(), TFloat64(), FastSeq()); assertEvalsTo(ApplyAggOp(; SeqOp(ApplyBinaryPrimOp(Multiply(), Ref(""a"", TFloat64()), Ref(""b"", TFloat64())), I32(0), aggSig),; FastSeq(), aggSig),; (FastIndexedSeq(Row(1.0, 10.0), Row(10.0, 10.0), Row(null, 10.0)), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),; 110.0); ```. The line:. > (FastIndexedSeq(Row(1.0, 10.0), ...), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),. is the IndexedSeq of values to aggregate over, along with their signature. The struct type is used to build the scope in which aggregators are evaluated. (A little noisy because of the aggregator syntax. It's noisier than I'd like it to be.). This nicely paves the way for aggregators with multi-argument seqOps (like takeBy) that were previously handled by lambdas and we didn't have a cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3570:1101,optimiz,optimization,1101,https://hail.is,https://github.com/hail-is/hail/pull/3570,1,['optimiz'],['optimization']
Performance,total 29.693ms self 9.419ms children 20.274ms %children 68.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 4.611ms self 3.590ms children 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.Opti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:3720,Optimiz,Optimize,3720,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,tpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:173238,Optimiz,Optimize,173238,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,tpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.154ms self 0.154ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.015ms self 0.015ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.056ms self 0.056ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:29921,Optimiz,Optimize,29921,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"tpStorageOptions deserialized (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2153"">#2153</a>) (<a href=""https://github.com/googleapis/java-storage/commit/68ad8e7357097e3dd161c2ab5f7a42a060a3702c"">68ad8e7</a>)</li>; <li>Update grpc default metadata projection to include acl same as json (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2150"">#2150</a>) (<a href=""https://github.com/googleapis/java-storage/commit/330e795040592e5df22d44fb5216ad7cf2448e81"">330e795</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.14.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2151"">#2151</a>) (<a href=""https://github.com/googleapis/java-storage/commit/eba8b6a235919a27d1f6dadf770140c7d143aa1a"">eba8b6a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.24.0...v2.25.0"">2.25.0</a> (2023-07-24)</h2>; <h3>Features</h3>; <ul>; <li>BlobWriteChannelV2 - same throughput less GC (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2110"">#2110</a>) (<a href=""https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b"">1b52a10</a>)</li>; <li>Update Storage.createFrom(BlobInfo, Path) to have 150% higher throughput (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2059"">#2059</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4c2f44e28a1ff19ffb2a02e3cefc062a1dd98fdc"">4c2f44e</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>Update BlobWriteChannelV2 to properly carry forward offset after incremental flush (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2125"">#2125</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c099a2f4f8ea9afa6953270876653916b021fd9f"">c099a2f</a>)</li>; <li>Update GrpcStorageImpl.createFrom(BlobInfo, Path) to use RewindableContent (<a href=""https://redirect.github.com/googleapis/java-storage/issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:7837,throughput,throughput,7837,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['throughput'],['throughput']
Performance,"tps://github-redirect.dependabot.com/psf/black/issues/3242"">#3242</a>)</li>; <li><a href=""https://github.com/psf/black/commit/767604e03f5e454ae5b5c268cd5831c672f46de8""><code>767604e</code></a> Use .gitignore files in the initial source directories (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3237"">#3237</a>)</li>; <li><a href=""https://github.com/psf/black/commit/2c90480e1a102ab0fac57737d2ba5143d82abed7""><code>2c90480</code></a> Use strict mypy checking (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3222"">#3222</a>)</li>; <li><a href=""https://github.com/psf/black/commit/ba618a307a30a119b4fafe526ebf7d5f092ba981""><code>ba618a3</code></a> Add parens around implicit string concatenations where it increases readabili...</li>; <li><a href=""https://github.com/psf/black/commit/c0cc19b5b3371842d696875897bebefebd5e1596""><code>c0cc19b</code></a> Delay worker count determination</li>; <li><a href=""https://github.com/psf/black/commit/afed2c01903465f9a486ac481a66aa3413cc1b01""><code>afed2c0</code></a> Load .gitignore and exclude regex at time of use</li>; <li><a href=""https://github.com/psf/black/commit/e269f44b25737360e0dc65379f889dfa931dc68a""><code>e269f44</code></a> Lazily import parallelized format modules</li>; <li><a href=""https://github.com/psf/black/commit/c47b91f513052cd39b818ea7c19716423c85c04e""><code>c47b91f</code></a> Fix misdetection of project root with <code>--stdin-filename</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3216"">#3216</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/psf/black/compare/22.3.0...22.8.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=black&package-manager=pip&previous-version=22.3.0&new-version=22.8.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12174:10639,Load,Load,10639,https://hail.is,https://github.com/hail-is/hail/pull/12174,1,['Load'],['Load']
Performance,"tps://github.com/psf/requests) from 2.31.0 to 2.32.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/psf/requests/releases"">requests's releases</a>.</em></p>; <blockquote>; <h2>v2.32.0</h2>; <h2>2.32.0 (2024-05-20)</h2>; <h2>🐍 PYCON US 2024 EDITION 🐍</h2>; <p><strong>Security</strong></p>; <ul>; <li>Fixed an issue where setting <code>verify=False</code> on the first request from a; Session will cause subsequent requests to the <em>same origin</em> to also ignore; cert verification, regardless of the value of <code>verify</code>.; (<a href=""https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56"">https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56</a>)</li>; </ul>; <p><strong>Improvements</strong></p>; <ul>; <li><code>verify=True</code> now reuses a global SSLContext which should improve; request time variance between first and subsequent requests. It should; also minimize certificate load time on Windows systems when using a Python; version built with OpenSSL 3.x. (<a href=""https://redirect.github.com/psf/requests/issues/6667"">#6667</a>)</li>; <li>Requests now supports optional use of character detection; (<code>chardet</code> or <code>charset_normalizer</code>) when repackaged or vendored.; This enables <code>pip</code> and other projects to minimize their vendoring; surface area. The <code>Response.text()</code> and <code>apparent_encoding</code> APIs; will default to <code>utf-8</code> if neither library is present. (<a href=""https://redirect.github.com/psf/requests/issues/6702"">#6702</a>)</li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed bug in length detection where emoji length was incorrectly; calculated in the request content-length. (<a href=""https://redirect.github.com/psf/requests/issues/6589"">#6589</a>)</li>; <li>Fixed deserialization bug in JSONDecodeError. (<a href=""https://redirect.github.com/psf/requests/issues/6629"">#6629</a>)</li>; <li>Fixed bug where an ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14555:999,load,load,999,https://hail.is,https://github.com/hail-is/hail/pull/14555,1,['load'],['load']
Performance,"traClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 13:51:03 WARN Utils: Your hostname, <my computer name> resolves to a loopback address: <my local IP>; using <my IP> instead (on interface enp3s0); 18/01/08 13:51:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; ```. And the other initialize hail like this (crashes with the stack trace/error in the issue):; ```; from pyspark import *; from hail import *; conf = SparkConf(); conf.set('spark.sql.files.maxPartitionBytes','60000000000') ; conf.set('spark.sql.files.openCostInBytes','60000000000') ; conf.set('spark.driver.cores','1') #test with 1 core; sc = SparkContext(conf=conf); hc = HailContext(sc); ```. With startup messages looking like this:; ```; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/01/08 15:16:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/01/08 15:16:23 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 18/01/08 15:16:23 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 18/01/08 15:16:23 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 15:16:23 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 15:16:23 WARN Utils: Your hostname, <my computer name> resolves to a l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:7756,load,load,7756,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['load'],['load']
Performance,"tring},entry:Struct{GT:Call,GP:+Array[+Float64],dosage:+Float64}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (TableMapRows; (TableMapGlobals; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}"")); (SelectFields (); (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:28530,optimiz,optimize,28530,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['optimiz'],['optimize']
Performance,"trout</code></a>.</li>; </ul>; </li>; <li>Prioritize binary builds for R dependencies.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2277"">#2277</a> PR by <a href=""https://github.com/lorenzwalthert""><code>@​lorenzwalthert</code></a>.</li>; </ul>; </li>; <li>Fix handling of git worktrees.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2252"">#2252</a> PR by <a href=""https://github.com/daschuer""><code>@​daschuer</code></a>.</li>; </ul>; </li>; <li>Fix handling of <code>$R_HOME</code> for R hooks.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2301"">#2301</a> PR by <a href=""https://github.com/jeff-m-sullivan""><code>@​jeff-m-sullivan</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2300"">#2300</a> issue by <a href=""https://github.com/jeff-m-sullivan""><code>@​jeff-m-sullivan</code></a>.</li>; </ul>; </li>; <li>Fix a rare race condition in change stashing.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2323"">#2323</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2287"">#2287</a> issue by <a href=""https://github.com/ian-h-chamberlain""><code>@​ian-h-chamberlain</code></a>.</li>; </ul>; </li>; </ul>; <h3>Updating</h3>; <ul>; <li>Remove python3.6 support. Note that pre-commit still supports running hooks written in older versions, but pre-commit itself requires python 3.7+.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2215"">#2215</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>pre-commit has migrated from the <code>master</code> branch to <code>main</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2302"">#2302</a> ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11731:4122,race condition,race condition,4122,https://hail.is,https://github.com/hail-is/hail/pull/11731,1,['race condition'],['race condition']
Performance,"trout</code></a>.</li>; </ul>; </li>; <li>Prioritize binary builds for R dependencies.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2277"">#2277</a> PR by <a href=""https://github.com/lorenzwalthert""><code>@​lorenzwalthert</code></a>.</li>; </ul>; </li>; <li>Fix handling of git worktrees.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2252"">#2252</a> PR by <a href=""https://github.com/daschuer""><code>@​daschuer</code></a>.</li>; </ul>; </li>; <li>Fix handling of <code>$R_HOME</code> for R hooks.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2301"">#2301</a> PR by <a href=""https://github.com/jeff-m-sullivan""><code>@​jeff-m-sullivan</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2300"">#2300</a> issue by <a href=""https://github.com/jeff-m-sullivan""><code>@​jeff-m-sullivan</code></a>.</li>; </ul>; </li>; <li>Fix a rare race condition in change stashing.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2323"">#2323</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2287"">#2287</a> issue by <a href=""https://github.com/ian-h-chamberlain""><code>@​ian-h-chamberlain</code></a>.</li>; </ul>; </li>; </ul>; <h3>Updating</h3>; <ul>; <li>Remove python3.6 support. Note that pre-commit still supports running hooks; written in older versions, but pre-commit itself requires python 3.7+.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2215"">#2215</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>pre-commit has migrated from the <code>master</code> branch to <code>main</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2302"">#2302</a>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11731:9301,race condition,race condition,9301,https://hail.is,https://github.com/hail-is/hail/pull/11731,1,['race condition'],['race condition']
Performance,"trying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$15(ServiceBackend.scala:225); 	at scala.util.Try$.apply(Try.scala:213); 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:995); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. The driver will have log output like this:; ```; 2023-09-22 19:11:13.051 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/8042383 response 200; 2023-09-22 19:11:13.052 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=: reading results; 2023-09-22 19:11:13.125 ServiceBackend$: INFO: all results read. 0.072746861 s. 0.0 result/s. 0.0 MiB/s.; 2023-09-22 19:11:13.125 : INFO: [collectDArray|table_native_writer]: executed 5 tasks in 1.822s; 2023-09-22 19:11:13.126 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.126 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.126 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:2344,concurren,concurrent,2344,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,trying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$15(ServiceBackend.scala:225); 	at scala.util.Try$.apply(Try.scala:213); 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:995); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. but the worker looks like this:; ```; 2023-09-22 19:11:12.125 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 2: /batch/fe537a243a3046d29d76861ffee94b92; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 3: /batch/fe537a243a3046d29d76861ffee94b92/log; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 4: gs://hail-query-ger0g/jars/b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 5: worker; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 6: gs://1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 7: 0; 2023-09-22 19:11:12.1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:6342,concurren,concurrent,6342,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"ts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4945,Latency,Latency,4945,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,1,['Latency'],['Latency']
Performance,"ts, like; <code>case Foo(bar=baz as quux)</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2749"">#2749</a>)</li>; <li>Tuple unpacking on <code>return</code> and <code>yield</code> constructs now implies 3.8+ (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2700"">#2700</a>)</li>; <li>Unparenthesized tuples on annotated assignments (e.g; <code>values: Tuple[int, ...] = 1, 2, 3</code>) now implies 3.8+ (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2708"">#2708</a>)</li>; <li>Fix handling of standalone <code>match()</code> or <code>case()</code> when there is a trailing newline or a; comment inside of the parentheses. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2760"">#2760</a>)</li>; <li><code>from __future__ import annotations</code> statement now implies Python 3.7+ (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2690"">#2690</a>)</li>; </ul>; <h3>Performance</h3>; <ul>; <li>Speed-up the new backtracking parser about 4X in general (enabled when; <code>--target-version</code> is set to 3.10 and higher). (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2728"">#2728</a>)</li>; <li><em>Black</em> is now compiled with <a href=""https://github.com/mypyc/mypyc"">mypyc</a> for an overall 2x; speed-up. 64-bit Windows, MacOS, and Linux (not including musl) are supported. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/1009"">#1009</a>,; <a href=""https://github-redirect.dependabot.com/psf/black/issues/2431"">#2431</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/psf/black/commits/22.1.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=black&package-manager=pip&previous-version=20.8b1&new-version=2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11468:10997,Perform,Performance,10997,https://hail.is,https://github.com/hail-is/hail/pull/11468,1,['Perform'],['Performance']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.184ms self 0.184ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.228ms self 1.353ms children 0.875ms %children 39.29%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.340ms self 0.340ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:153807,Optimiz,OptimizePass,153807,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.195ms self 0.195ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.409ms self 0.648ms children 0.762ms %children 54.04%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.463ms self 0.463ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:138020,Optimiz,OptimizePass,138020,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.445ms self 0.445ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.252ms self 0.765ms children 1.487ms %children 66.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.505ms self 0.505ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:122302,Optimiz,OptimizePass,122302,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.257ms self 0.257ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.074ms self 0.035ms children 0.040ms %children 53.33%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:96951,Optimiz,Optimize,96951,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.264ms self 0.264ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.073ms self 0.037ms children 0.036ms %children 49.79%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:111389,Optimiz,Optimize,111389,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.318ms self 0.318ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.089ms self 0.043ms children 0.047ms %children 52.21%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:75772,Optimiz,Optimize,75772,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.391ms self 0.391ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.098ms self 0.049ms children 0.049ms %children 50.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:61374,Optimiz,Optimize,61374,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.476ms self 0.476ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.138ms self 0.089ms children 0.049ms %children 35.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:52846,Optimiz,Optimize,52846,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 1.317ms self 1.317ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.144ms self 0.075ms children 0.069ms %children 48.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.069ms self 0.069ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:44318,Optimiz,Optimize,44318,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ttomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5606,Optimiz,Optimize,5606,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,"ttps://github.com/aio-libs/aiorwlock) from 1.0.0 to 1.3.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiorwlock/releases"">aiorwlock's releases</a>.</em></p>; <blockquote>; <h2>aiorwlock 1.2.0</h2>; <h1>Changes</h1>; <ul>; <li>Fix a bug that makes concurrent writes possible under some (rare) conjunctions (<a href=""https://github-redirect.dependabot.com/aio-libs/aiorwlock/issues/235"">#235</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiorwlock/blob/master/CHANGES.rst"">aiorwlock's changelog</a>.</em></p>; <blockquote>; <p>1.3.0 (2022-1-18); ^^^^^^^^^^^^^^^^^^</p>; <ul>; <li>Dropped Python 3.6 support</li>; <li>Python 3.10 is officially supported</li>; <li>Drop deprecated <code>loop</code> parameter from <code>RWLock</code> constructor</li>; </ul>; <p>1.2.0 (2021-11-09); ^^^^^^^^^^^^^^^^^^</p>; <ul>; <li>Fix a bug that makes concurrent writes possible under some (rare) conjunctions (<a href=""https://github-redirect.dependabot.com/aio-libs/aiorwlock/issues/235"">#235</a>)</li>; </ul>; <p>1.1.0 (2021-09-27); ^^^^^^^^^^^^^^^^^^</p>; <ul>; <li>Remove explicit loop usage in <code>asyncio.sleep()</code> call, make the library forward; compatible with Python 3.10</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/6599d10ba16f95f19d5b5963a00aa857bc98f656""><code>6599d10</code></a> Bump to 1.3.0</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/d4b41f54b57caf316c41c3973ab82bd53a418ff8""><code>d4b41f5</code></a> Drop deprecated 'loop' parameter from RWLock constructor</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/3edb2a1bc1636832df12671f035e21dd74440824""><code>3edb2a1</code></a> Fix tests</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/45a7418474a55defe9c53fd8e38df60af514cf84""><code>45a7418</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11514:1009,concurren,concurrent,1009,https://hail.is,https://github.com/hail-is/hail/pull/11514,1,['concurren'],['concurrent']
Performance,tune ImportVCFs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5507:0,tune,tune,0,https://hail.is,https://github.com/hail-is/hail/pull/5507,1,['tune'],['tune']
Performance,"turing things to get a; high level of re-use, but just to opportunistically exploit re-use which happens to occur -; e.g. if the early stages of an analysis involve reading an existing file and filtering in various; ways, then that may not be changed at all by changes to what happen in the real analysis; after the filtering. And this is also influenced by the medium-term goal of having a Hail service; which (amongst other things) can do simple analyses on small data in under ten seconds - in; that realm compilation time could become a critical factor as a serial bottleneck. [The place where persistent cacheing of compiled files helps most of all is in testing,; where you really are running the exact same queries over and over again on the same; small datasets, and in many cases after making small changes which only affect a few; of the queries]. b) We may actually have some version of the locking problem even if we don't try to reuse the files -; since we have several workers on a node, and possibly a master as well, all needing to put code; into a file (or wait for someone else to populate the file) so that they can load it. Depending on ; precisely how Spark manages things (which I wouldn't want to depend on too much anyway). In fact it's essential that all the workers share the same DLL file, because otherwise they'd be; trying to load multiple DLL's defining the same symbols. That aspect of it could be handled by; putting the files into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could manage it with nothing but atomic file-create; and atomic-rename, but that didn't quite pan out). As for writing LLVM IR, it can definitely be done, because that's what Endeca/Oracle did. But there; was such a huge learning curve that only 3 people ever did it successfully (I wasn't one of them),; and debugg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385:1444,load,load,1444,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385,2,['load'],['load']
Performance,"type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': None,; 'nominated_node_name': None,; 'phase': 'Pending',; 'pod_ip': None,; 'qos_class': 'Burstable',; 'reason': None,; 'start_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal())}}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 53, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_clie",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:9616,concurren,concurrent,9616,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['concurren'],['concurrent']
Performance,"u linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3960,Latency,Latency,3960,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,1,['Latency'],['Latency']
Performance,"ub-redirect.dependabot.com/chardet/chardet/issues/254"">#254</a> from chardet/master</li>; <li><a href=""https://github.com/chardet/chardet/commit/322229573173307e1380eb151ea446b8c6fe2c3b""><code>3222295</code></a> Linter fixes (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/253"">#253</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/85c96d3449afedf0e9fe57bd01eada92e0dd11b4""><code>85c96d3</code></a> Bump version to 5.0.0</li>; <li><a href=""https://github.com/chardet/chardet/commit/57abbca866a41758f7c26e1bb26a0126e28575c2""><code>57abbca</code></a> Rebased and cleaned up version of UTF-16/32 BE/LE PR (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/206"">#206</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/eca9558cf7569c1f7689bd66e5aaf965a56e903c""><code>eca9558</code></a> Fix missing black formatting</li>; <li><a href=""https://github.com/chardet/chardet/commit/f1f9d4280e11fb3a9b2d9eaf1827dac9263cb1cb""><code>f1f9d42</code></a> slight increase in performance (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/252"">#252</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/f9ef56cfd6c9b24b9c865eae6dc2285c67ffb75c""><code>f9ef56c</code></a> Use Python-3 super() syntax in Latin1Prober (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/240"">#240</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/c5e5d5a8f1b6e135a8bffd8d60b2f726bb168339""><code>c5e5d5a</code></a> Simple maintenance improvements (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/244"">#244</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/49b8341f507bed68f7d3ff7138bb97047a0e04f0""><code>49b8341</code></a> Configure setuptools using the declarative syntax in setup.cfg (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/239"">#239</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/5c73bfcdf819251d1a1d0de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12107:4261,perform,performance,4261,https://hail.is,https://github.com/hail-is/hail/pull/12107,1,['perform'],['performance']
Performance,"ue, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multiple methods.; - When using lots of methods, there's a lot of bookkeeping. I would like a way to define a ""staged class"" that wraps up some of the boilerplate. Not totally clear what I want here, just less boilerplate. Aside2: This is still pretty",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6987:1808,perform,performance,1808,https://hail.is,https://github.com/hail-is/hail/pull/6987,1,['perform'],['performance']
Performance,ulateConcordance.scala:108); 	at is.hail.methods.CalculateConcordance.apply(CalculateConcordance.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:7884,load,loadLength,7884,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['load'],['loadLength']
Performance,"uld remain open if the client did not close it. This; change ensures the transport is closed when the client does not close; it.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/master/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.9.4 (2024-04-11)</h1>; <h2>Bug fixes</h2>; <ul>; <li>; <p>The asynchronous internals now set the underlying causes; when assigning exceptions to the future objects; -- by :user:<code>webknjaz</code>.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8089</code>.</p>; </li>; <li>; <p>Treated values of <code>Accept-Encoding</code> header as case-insensitive when checking; for gzip files -- by :user:<code>steverep</code>.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8104</code>.</p>; </li>; <li>; <p>Improved the DNS resolution performance on cache hit -- by :user:<code>bdraco</code>.</p>; <p>This is achieved by avoiding an :mod:<code>asyncio</code> task creation in this case.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8163</code>.</p>; </li>; <li>; <p>Changed the type annotations to allow <code>dict</code> on :meth:<code>aiohttp.MultipartWriter.append</code>,; :meth:<code>aiohttp.MultipartWriter.append_json</code> and; :meth:<code>aiohttp.MultipartWriter.append_form</code> -- by :user:<code>cakemanny</code></p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>7741</code>.</p>; </li>; <li>; <p>Ensure websocket transport is closed when client does not close it; -- by :user:<code>bdraco</code>.</p>; <p>The transport could remain open if the client did not close it. This; change ensures the transport is closed when the client does not close; it.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14477:2782,perform,performance,2782,https://hail.is,https://github.com/hail-is/hail/pull/14477,12,"['cache', 'perform']","['cache', 'performance']"
Performance,"ule named 'pyspark'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_233/4275665471.py in <module>; ----> 1 combined_pandas = pd.read_pickle(gwas_pandas_file). /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 220 # ""No module named 'pandas.core.sparse.series'""; 221 # ""Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib""; --> 222 return pc.load(handles.handle, encoding=None); 223 except UnicodeDecodeError:; 224 # e.g. can occur for files written in py27; see GH#28645 and GH#31988. /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in load(fh, encoding, is_verbose); 272 up.is_verbose = is_verbose; 273 ; --> 274 return up.load(); 275 except (ValueError, TypeError):; 276 raise. /opt/conda/lib/python3.7/pickle.py in load(self); 1086 raise EOFError; 1087 assert isinstance(key, bytes_types); -> 1088 dispatch[key[0]](self); 1089 except _Stop as stopinst:; 1090 return stopinst.value. /opt/conda/lib/python3.7/pickle.py in load_stack_global(self); 1383 if type(name) is not str or type(module) is not str:; 1384 raise UnpicklingError(""STACK_GLOBAL requires str""); -> 1385 self.append(self.find_class(module, name)); 1386 dispatch[STACK_GLOBAL[0]] = load_stack_global; 1387 . /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in find_class(self, module, name); 204 key = (module, name); 205 module, name = _class_locations_map.get(key, key); --> 206 return super().find_class(module, name); 207 ; 208 . /opt/conda/lib/python3.7/pickle.py in find_class(self, module, name); 1424 elif module in _compat_pickle.IMPORT_MAPPING:; 1425 module = _compat_pickle.IMPORT_MAPPING[module]; -> 1426 __import__(module, level=0); 1427 if self.proto >= 4:; 1428 return _getattribute(sys.modules[module], name)[0]. /opt/conda/lib/python3.7/site-packages/hail/__init__.py in <module>; 31 # F401",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004:3110,load,load,3110,https://hail.is,https://github.com/hail-is/hail/issues/14004,1,['load'],['load']
Performance,uler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Opt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:2452,Load,LoadVCF,2452,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,uler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216010,concurren,concurrent,216010,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,uler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:21); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:17); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:498); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskSto,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:4625,Load,LoadVCF,4625,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['Load'],['LoadVCF']
Performance,uler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:29); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:25); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:557); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.Ra,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:10491,Load,LoadVCF,10491,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['Load'],['LoadVCF']
Performance,uler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:26); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:22); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:529); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:5010,Load,LoadVCF,5010,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['Load'],['LoadVCF']
Performance,un$main$6(ServiceBackend.scala:462); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:461); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:461); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:459); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); ```. ### Version. 0.2.116. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:15220,concurren,concurrent,15220,https://hail.is,https://github.com/hail-is/hail/issues/13074,6,['concurren'],['concurrent']
Performance,"un$parallelizeAndComputeWithIndex$4(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2191/716305671.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). ""pool-2-thread-1"" #26 prio=5 os_prio=0 tid=0x00007f502866e000 nid=0x88c waiting on condition [0x00007f50275fd000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.close(ServiceCacheableFS.scala:46); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 	at java.io.ObjectOutputStream$BlockDataOutputStream.close(ObjectOutputStream.java:1828); 	at java.io.ObjectOutputStream.close(ObjectOutputStream.java:742); 	at is.hail.utils.package$.using(pa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:3799,concurren,concurrent,3799,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,un$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(Loweri,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5935,Optimiz,Optimize,5935,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,un(Thread.java:745); Caused by: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:4985,load,loadClass,4985,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['load'],['loadClass']
Performance,"unWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$15(ServiceBackend.scala:225); 	at scala.util.Try$.apply(Try.scala:213); 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:995); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. The driver will have log output like this:; ```; 2023-09-22 19:11:13.051 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/8042383 response 200; 2023-09-22 19:11:13.052 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=: reading results; 2023-09-22 19:11:13.125 ServiceBackend$: INFO: all results read. 0.072746861 s. 0.0 result/s. 0.0 MiB/s.; 2023-09-22 19:11:13.125 : INFO: [collectDArray|table_native_writer]: executed 5 tasks in 1.822s; 2023-09-22 19:11:13.126 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.126 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.126 : INFO: RegionPool: FREE: 0 alloc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:2259,concurren,concurrent,2259,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,unWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$15(ServiceBackend.scala:225); 	at scala.util.Try$.apply(Try.scala:213); 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:995); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```. but the worker looks like this:; ```; 2023-09-22 19:11:12.125 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 2: /batch/fe537a243a3046d29d76861ffee94b92; 2023-09-22 19:11:12.125 JVMEntryway: INFO: 3: /batch/fe537a243a3046d29d76861ffee94b92/log; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 4: gs://hail-query-ger0g/jars/b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 5: worker; 2023-09-22 19:11:12.126 JVMEntryway: INFO: 6: gs://1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:6257,concurren,concurrent,6257,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"unch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:4260,concurren,concurrent,4260,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,3,['concurren'],['concurrent']
Performance,"unk$4(ServiceBackend.scala:664); at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$3(ServiceBackend.scala:650); at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:822); at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:447); at is.hail.backend.service.Main$.main(Main.scala:15); at is.hail.backend.service.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.write(get_aou_util_path('mt_sample_qc'), overwrite=args.overwrite); ```. Job lo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:6564,concurren,concurrent,6564,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['concurren'],['concurrent']
Performance,"uote>; <h2>1.26.5</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Python 3.10 ssl module</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d725a9b56bb8baf87c9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10544:1213,perform,performance,1213,https://hail.is,https://github.com/hail-is/hail/pull/10544,1,['perform'],['performance']
Performance,"up_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an excepti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1261,Tune,Tuned,1261,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,1,['Tune'],['Tuned']
Performance,ur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3644,concurren,concurrent,3644,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['concurren'],['concurrent']
Performance,us 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33769,cache,cached,33769,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,['cache'],"['cached', 'cachetools-']"
Performance,"us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:1136,Optimiz,Optimize,1136,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"ut. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line 36, in write_log_file; return await self.gcs.write_gs_file(path, data); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 37, in write_gs_file; return await self._wrapped_write_gs_file(self, uri, string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 56, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8053:3137,concurren,concurrent,3137,https://hail.is,https://github.com/hail-is/hail/issues/8053,2,['concurren'],['concurrent']
Performance,ute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at org.apache.hadoop.fs.Path.initialize(Path.java:205); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:5487,concurren,concurrent,5487,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,ute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.024ms self 0.021ms children 0.003ms %children 13.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:196929,Optimiz,Optimize,196929,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:209533,Optimiz,Optimize,209533,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.019ms self 0.016ms children 0.003ms %children 16.42%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:217554,Optimiz,Optimize,217554,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.067ms self 0.067ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:201581,Optimiz,Optimize,201581,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.641ms self 0.641ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.241ms self 0.241ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:112837,Optimiz,Optimize,112837,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.635ms self 0.635ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.295ms self 0.295ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:98399,Optimiz,Optimize,98399,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.830ms self 0.830ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.348ms self 0.348ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:77220,Optimiz,Optimize,77220,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.756ms self 0.756ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.409ms self 0.409ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:62822,Optimiz,Optimize,62822,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.885ms self 0.885ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.450ms self 0.450ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:54294,Optimiz,Optimize,54294,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.069ms self 0.069ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.703ms self 4.703ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.607ms self 0.607ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:45766,Optimiz,Optimize,45766,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"uth-library-python/compare/v2.5.0...v2.6.0"">2.6.0</a> (2022-01-31)</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/962"">#962</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/52c8ef90058120d7d04d3d201adc111664be526c"">52c8ef9</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>revert &quot;feat: add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>)&quot; (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/964"">#964</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/f9f23f4370f2a7a5b2c66ee56a5e700ef03b5b06"">f9f23f4</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/google-auth-library-python/compare/v2.4.1...v2.5.0"">2.5.0</a> (2022-01-25)</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/956"">#956</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/a8eb4c8693055a3420cfe9c3420aae2bc8cd465a"">a8eb4c8</a>)</li>; </ul>; <h3><a href=""https://github.com/googleapis/google-auth-library-python/compare/v2.4.0...v2.4.1"">2.4.1</a> (2022-01-21)</h3>; <h3>Bug Fixes</h3>; <ul>; <li>urllib3 import (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/953"">#953</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c8b5cae3da5eb9d40067d38dac51a4a8c1e0763e"">c8b5cae</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/google-auth-library-python/compare/v2.3.3...v2.4.0"">2.4.0</a> (2022-01-20)</h2>; <h3>Features</h3>; <ul>; <li>add 'py.typed' declaration (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:6219,load,load,6219,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['load'],['load']
Performance,"utor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 1.0 in stage 0.0 (TID 1, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:57662,concurren,concurrent,57662,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 21.0 in stage 0.0 (TID 21, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:59112,concurren,concurrent,59112,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 31.0 in stage 0.0 (TID 31, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:56210,concurren,concurrent,56210,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContaine,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:60564,concurren,concurrent,60564,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 6 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 6 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 6; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 7 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 7 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 7; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 4 on s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:82291,concurren,concurrent,82291,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 0.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:80841,concurren,concurrent,80841,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 10.0 in stage 0.0 (TID 10, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:77937,concurren,concurrent,77937,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 30.0 in stage 0.0 (TID 30, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:79389,concurren,concurrent,79389,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 3 requested; 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 3 from BlockManagerMaster.; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 3; 2019-01-22 13:11:57 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.186:56628) with ID 18; 2019-01-22 13:11:57 TaskSetManager: INFO: Starting task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18, partition 15, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:11:57 TaskSetManag",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:132835,concurren,concurrent,132835,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 15.0 in stage 0.0 (TID 15, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:131383,concurren,concurrent,131383,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 25.0 in stage 0.0 (TID 25, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:129931,concurren,concurrent,129931,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 5.0 in stage 0.0 (TID 5, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:128481,concurren,concurrent,128481,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 2 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 2 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 2; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 4 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 4 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 4; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 5 on s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:90025,concurren,concurrent,90025,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 13.0 in stage 0.0 (TID 13, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:85671,concurren,concurrent,85671,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 3.0 in stage 0.0 (TID 3, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:87123,concurren,concurrent,87123,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 33.0 in stage 0.0 (TID 33, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:88573,concurren,concurrent,88573,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 5 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 5 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:97759,concurren,concurrent,97759,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 12.0 in stage 0.0 (TID 12, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:96307,concurren,concurrent,96307,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 2.0 in stage 0.0 (TID 2, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:93405,concurren,concurrent,93405,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 22.0 in stage 0.0 (TID 22, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:94855,concurren,concurrent,94855,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 24.0 in stage 0.0 (TID 24, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:66256,concurren,concurrent,66256,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 34.0 in stage 0.0 (TID 34, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:64804,concurren,concurrent,64804,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 4.0 in stage 0.0 (TID 4, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:63354,concurren,concurrent,63354,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContaine,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:67708,concurren,concurrent,67708,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:2750,concurren,concurrent,2750,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['concurren'],['concurrent']
Performance,utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 1 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:74852,concurren,concurrent,74852,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 16.0 in stage 0.0 (TID 16, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:70498,concurren,concurrent,70498,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 36.0 in stage 0.0 (TID 36, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:71950,concurren,concurrent,71950,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 6.0 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:73402,concurren,concurrent,73402,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 8 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 8 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:115303,concurren,concurrent,115303,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 18.0 in stage 0.0 (TID 18, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:113851,concurren,concurrent,113851,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 28.0 in stage 0.0 (TID 28, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:112399,concurren,concurrent,112399,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 38.0 in stage 0.0 (TID 38, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:110947,concurren,concurrent,110947,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 27.0 in stage 0.0 (TID 27, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:122617,concurren,concurrent,122617,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 37.0 in stage 0.0 (TID 37, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:121165,concurren,concurrent,121165,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 7.0 in stage 0.0 (TID 7, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:119715,concurren,concurrent,119715,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:124069,concurren,concurrent,124069,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:2517,Load,LoadVCF,2517,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,utor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGSch,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9613,Load,LoadVCF,9613,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,ux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39106,cache,cached,39106,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,v already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215366,concurren,concurrent,215366,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:2108,load,loaded,2108,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251,1,['load'],['loaded']
Performance,"va.lang.Thread.State: WAITING (parking); 	at sun.misc.Unsafe.park(Native Method); 	- parking to wait for <0x00000000e8ddaea0> (a scala.concurrent.impl.Promise$CompletionLatch); 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); 	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242); 	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258); 	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263); 	at scala.concurrent.Await$.$anonfun$result$1(package.scala:220); 	at scala.concurrent.Await$$$Lambda$2201/1092639564.apply(Unknown Source); 	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:57); 	at scala.concurrent.Await$.result(package.scala:146); 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:145); ...; ```. This is the line that waits to upload the compiled code for the workers to Google Cloud Storage. The other threads appear to be waiting on the memory service:; ```; ""pool-2-thread-2"" #27 prio=5 os_prio=0 tid=0x00007f5028ad9000 nid=0x88d waiting on condition [0x00007f50274fc000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:1240,concurren,concurrent,1240,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,va.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.e,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19451,Load,LoadVCF,19451,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,valRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.641ms self 0.052ms children 3.589ms %children 98.56%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.326ms self 0.326ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:105703,Optimiz,Optimize,105703,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,valRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.847ms self 0.051ms children 3.797ms %children 98.69%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.342ms self 0.342ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:91265,Optimiz,Optimize,91265,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,valRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.479ms self 0.057ms children 4.422ms %children 98.72%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.355ms self 0.355ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:70086,Optimiz,Optimize,70086,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"veMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonClientException; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:419); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352); 	... 27 more. During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:218); 	at is.hail.backend.spark.SparkBackend.apply(SparkBackend.scala); 	at sun.reflect.Nat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:2442,load,loadClass,2442,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['load'],['loadClass']
Performance,"ver, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:8111/; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. # vim: syntax=apache ts=4 sw=4 sts=4 sr noet; </IfModule>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:2175,Load,LoadModule,2175,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,4,['Load'],['LoadModule']
Performance,verified this successfully loads natives on linux and osx. @jbloom22 FYI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3474:27,load,loads,27,https://hail.is,https://github.com/hail-is/hail/pull/3474,1,['load'],['loads']
Performance,"voke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:6138,Load,LoadVCF,6138,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,w on D's values; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.TupleExpression.count:: WARNING: py:class reference target not found: integer -- return number of occurrences of value; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.TupleExpression.index:: WARNING: py:class reference target not found: integer -- return first index of value.; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.annotate_cols:9: WARNING: py:class reference target not found: hail.Table; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.annotate_rows:11: WARNING: py:class reference target not found: TVariant; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.cache:11: WARNING: py:func reference target not found: hail.MatrixTable.persist; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:15: WARNING: py:data reference target not found: int64; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:17: WARNING: py:data reference target not found: int64; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:19: WARNING: py:data reference target not found: float32; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.index_cols:17: WARNING: py:meth reference target not found: index_cols(exprs); /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.index_rows:17: WARNING: py:meth reference target not found: index_rows(exprs); /Users/dking/projects/hail/hail/pyt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9403#issuecomment-685996666:14781,cache,cache,14781,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-685996666,2,['cache'],['cache']
Performance,ware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:15029,concurren,concurrent,15029,https://hail.is,https://github.com/hail-is/hail/issues/3342,2,['concurren'],['concurrent']
Performance,we can get the performance back in the short term with a 2-line change moving decompression from advance() to getValue() -- the `data` field isn't used anywhere in `advance`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3862#issuecomment-401760799:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/issues/3862#issuecomment-401760799,1,['perform'],['performance']
Performance,"we do for correctness, but not partitioning/performance it seems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11969#issuecomment-1168752606:44,perform,performance,44,https://hail.is,https://github.com/hail-is/hail/pull/11969#issuecomment-1168752606,1,['perform'],['performance']
Performance,"we need to make sure we keep the optimized joins, though. I'm not sure exactly how this'll look (special case for variant or locus keyed keytable)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1158#issuecomment-266392470:33,optimiz,optimized,33,https://hail.is,https://github.com/hail-is/hail/issues/1158#issuecomment-266392470,1,['optimiz'],['optimized']
Performance,"we now have our dev test environment running with hail 0.2.126 and this query took ~92 seconds. So faster, but we do still need some performance enhancements. @ehigham let me know what would be helpful for you to get you started on this effort",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1795785654:133,perform,performance,133,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1795785654,1,['perform'],['performance']
Performance,we're compiling like 20x fewer classes based on that. Every single cache hit is one fewer class compiled. The compilations there are value IRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7528#issuecomment-554563029:67,cache,cache,67,https://hail.is,https://github.com/hail-is/hail/pull/7528#issuecomment-554563029,1,['cache'],['cache']
Performance,"we; timeout substantially more frequently. I have observed this myself on my laptop. Just this; morning I saw it happen to Daniel. 2. When using an `aiohttp.AsyncIterablePayload`, it is *critical* to always check if the coroutine; which actually writes to GCS (which is stashed in the variable `request_task`) is still; alive. In the current `main`, we do not do this which causes hangs (in particular the timeout; exceptions are never thrown ergo we never retry). To understand the second problem, you must first recall how writing works in aiogoogle. There are; two Tasks and an `asyncio.Queue`. The terms ""writer"" and ""reader"" are somewhat confusing, so let's; use left and right. The left Task has the owning reference to both the source ""file"" and the; destination ""file"". In particular, it is the *left* Task which closes both ""files"". Moreover, the; left Task reads chunks from the source file and places those chunks on the `asyncio.Queue`. The; right Task takes chunks off the queue and writes those chunks to the destination file. This situation can go awry in two ways. First, if the right Task encounters any kind of failure, it will stop taking chunks off of the; queue. When the queue (which has a size limit of one) is full, the left Task will hang. The system; is stuck. The left Task will wait forever for the right Task to empty the queue. The second scenario is exactly the same except that the left Task is trying to add the ""stop""; message to the queue rather than a chunk. In either case, it is critical that the left Task waits simultaneously on the queue operation *and*; on the right Task completing. If the right Task has died, no further writes can occur and the left; Task must raise an exception. In the first scenario, we do not observe the right Task's exception; because that will be done when we close the `InsertObjectStream` (which represents the destination; ""file""). ---. I also added several types, assertions, and a few missing `async with ... as resp:` blocks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11830:1396,queue,queue,1396,https://hail.is,https://github.com/hail-is/hail/pull/11830,5,['queue'],['queue']
Performance,"werMatrixToTable/Verify total 0.019ms self 0.019ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.TypeCheck.apply total 0.132ms self 0.132ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable total 2.591ms self 0.010ms children 2.581ms %children 99.63%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Verify total 0.011ms self 0.011ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform total 2.548ms self 0.302ms children 2.246ms %children 88.15%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/FoldConstants, iteration: 0 total 0.307ms self 0.307ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ExtractIntervalFilters, iteration: 0 total 0.016ms self 0.016ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0 total 0.363ms self 0.004ms children 0.358ms %children 98.82%; timing is.hail.backend.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:10776,Optimiz,Optimize,10776,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,wering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 3.719ms self 0.009ms children 3.709ms %children 99.75%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.030ms self 0.030ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.641ms self 0.052ms children 3.589ms %children 98.56%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:104349,Optimiz,OptimizePass,104349,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,wering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 3.925ms self 0.009ms children 3.916ms %children 99.78%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.847ms self 0.051ms children 3.797ms %children 98.69%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:89911,Optimiz,OptimizePass,89911,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,wering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 4.618ms self 0.011ms children 4.607ms %children 99.77%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.479ms self 0.057ms children 4.422ms %children 98.72%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:68732,Optimiz,OptimizePass,68732,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,wering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.240ms self 0.240ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.087ms self 0.042ms children 0.045ms %children 51.66%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:141029,Optimiz,OptimizePass,141029,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,wering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.490ms self 0.490ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.083ms self 0.043ms children 0.040ms %children 48.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:156816,Optimiz,OptimizePass,156816,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,wering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.938ms self 0.938ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.101ms self 0.049ms children 0.052ms %children 51.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:125311,Optimiz,OptimizePass,125311,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJW",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:7024,cache,cached,7024,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4330,cache,cached,4330,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33320,cache,cached,33320,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2889,cache,cached,2889,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,why do you need the cache?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5305#issuecomment-462482330:20,cache,cache,20,https://hail.is,https://github.com/hail-is/hail/pull/5305#issuecomment-462482330,1,['cache'],['cache']
Performance,why? people can cache this themselves,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1759#issuecomment-299068258:16,cache,cache,16,https://hail.is,https://github.com/hail-is/hail/pull/1759#issuecomment-299068258,1,['cache'],['cache']
Performance,"wip. A few remaining tests fail. Includes a number of fixes to InferPType, InferType. Plan is to get this working before optimize, since that is the simple case, and then move to pre-simplify",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063:121,optimiz,optimize,121,https://hail.is,https://github.com/hail-is/hail/pull/8063,1,['optimiz'],['optimize']
Performance,"with a code change:; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}entriesIndex:3}; after: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}entriesIndex:7}. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4527#issuecomment-429057183:53,optimiz,optimization,53,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429057183,1,['optimiz'],['optimization']
Performance,within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRPar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6051,Load,LoadPlink,6051,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underlying pod IPs so Envoy can handle load-balancing instead of kube-proxy. This allows Envoy to make smart load-balancing decisions and correctly enforce rate-limiting when using connection pools. The namespace tracking in CI in Point 2 is possible before we make any changes to our networking, so that comes first in #12093. Point 3 is taken care of in #12094, and the rest of Point 2 and Point 1, everything to do with Envoy, is in this PR. ### Additional QoL improvements; - Envoy by default exposes Prometheus metrics that we can use to easily monitor things like rate-limiting, request failures and durations; - Since all Envoy configuration is in the configmap, we don't need to build any images. I suppose we could have done this with NGINX, so this isn't something to fault NGINX for. Just another small win buried in these changes. cc @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:5068,load,load-balancing,5068,https://hail.is,https://github.com/hail-is/hail/pull/12095,2,['load'],['load-balancing']
Performance,"wnload.cse.ucsc.edu/gbdb/hg19/1000Genomes/phase3/ALL.chrY.phase3_integrated_v1a.20130502.genotypes.vcf.gz"", force_bgz=True); ----------------------------------------------------------------------; Initializing Hail with default parameters...; 2022-10-06 15:56:03 WARN Utils:69 - Your hostname, nid resolves to a loopback address: 127.0.1.1; using 192.168.248.80 instead (on interface wlp0s20f3); 2022-10-06 15:56:03 WARN Utils:69 - Set SPARK_LOCAL_IP if you need to bind to another address; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/med/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 2022-10-06 15:56:03 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.3; SparkUI available at http://192.168.248.80:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.100-2ea2615a797a; LOGGING: writing to /; --------------------------------------------------------------------------; mt.filter_rows(mt.locus.position==2867101).count_rows(); ```; ### Expected ; Return a count of rows with that condition. ### Error ; ```; FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:208); at is.hail.expr.ir.LoweredTableReader$.makeCoercer(TableIR.scala:135); at is.hail.expr.ir.Gen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280:1326,load,load,1326,https://hail.is,https://github.com/hail-is/hail/issues/12280,1,['load'],['load']
Performance,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186:1383,concurren,concurrent,1383,https://hail.is,https://github.com/hail-is/hail/issues/1186,3,['concurren'],['concurrent']
Performance,"ws() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multiple methods.; - When using lots of methods, there's a lot of bookkeeping. I would like a way to define a ""staged class"" that wraps up some of the boilerplate. Not totally clear what I want here, just less boilerplate. Aside2: This is still pretty slow!? Splitting into multiple methods allowed me to interrogate where time was spent. The answer is ""method4"" which is `pa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6987:1930,perform,performance,1930,https://hail.is,https://github.com/hail-is/hail/pull/6987,1,['perform'],['performance']
Performance,x$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Ut,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:12549,load,loadClass,12549,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['load'],['loadClass']
Performance,x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply total 1.106s self 33.060ms children 1.072s %children 97.01%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 31.509ms self 1.445ms children 30.064ms %children 95.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.344ms self 0.344ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 29.693ms self 9.419ms children 20.274ms %children 68.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:2680,Optimiz,OptimizePass,2680,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"xecute(; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 2872, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: java.lang.IllegalArgumentException: bound must be positive; 	at java.util.Random.nextInt(Random.java:388); 	at scala.util.Random.nextInt(Random.scala:70); 	at is.hail.services.package$.delayMsForTry(package.scala:47); 	at is.hail.services.package$.retryTransientErrors(package.scala:186); 	at is.hail.io.fs.GoogleStorageFS$$anon$1.retryingRead(GoogleStorageFS.scala:220); 	at is.hail.io.fs.GoogleStorageFS$$anon$1.readHandlingRequesterPays(GoogleStorageFS.scala:2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888:1148,concurren,concurrent,1148,https://hail.is,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888,1,['concurren'],['concurrent']
Performance,"xecutor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 20.0 in stage 0.0 (TID 20, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:76485,concurren,concurrent,76485,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"xecutor 4; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 5 on scc-q08.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 32.0 in stage 0.0 (TID 32, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:91953,concurren,concurrent,91953,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"xecutor 7; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 4 on scc-q07.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 23.0 in stage 0.0 (TID 23, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:84219,concurren,concurrent,84219,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,xecutor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:116936,concurren,concurrent,116936,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"xecutor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 35.0 in stage 0.0 (TID 35, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:127029,concurren,concurrent,127029,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,xecutor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13390,Load,LoadMatrix,13390,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,xecutor: 3 (epoch 9); 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:46785,concurren,concurrent,46785,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"xecutors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadable=129.00 KiB, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.560 : INFO: RegionPool: FREE: 129.0K allocated (129.0K blocks / 0 chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.561 : ERROR: error while applying lowering 'LowerAndExecuteShuffles'; 2023-05-04 01:04:37.600 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:37.601 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.601 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.601 : INFO: RegionPool: FREE: 128.0K allocated (128.0K blocks / 0 chunks), regions.size = 2, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.603 : ERROR: SocketException: Connection reset; From javax.net.ssl.SSLException: Connection reset; 	at sun.security.ssl.Alert.createSSLException(Alert.java:127)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:22696,cache,cache,22696,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['cache'],['cache']
Performance,"xomes_v8/daly_atgu_finnish_swedish_exomes_99prosAndPSC.vds'). vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). intervals = KeyTable.import_interval_list('gs://ibd-exomes/5k.txt'); vds = vds.filter_variants_table(intervals, keep=True). vds = vds.variant_qc(); vds = vds.filter_variants_expr('va.qc.AF>0.01 && va.qc.callRate>0.99', keep=True). table = hc.import_table('gs://daly_atgu_finnish_swedish_exomes_v8/finnibdPSC_isCase.txt', impute=True).key_by('s'); vds = vds.annotate_samples_table(table, root='sa.pheno'). vds = vds.repartition(100); vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno.iscase) 1 else 0""). vds.export_samples('gs://daly_atgu_finnish_swedish_exomes_v8/99pros_psc_relBelowPIHAT35.tsv','s=s'). ```. But this results in an error:. ```hail: info: Reading table to impute column types; hail: info: Finished type imputation; Loading column `s' as type String (imputed); Loading column `case' as type Int (imputed); Struct{; pheno: Int; }; [Stage 6:====================================================>(3347 + 1) / 3348]Verify Output for is/hail/codegen/generated/C3:; Traceback (most recent call last):; File ""/tmp/0a89b0df-1299-4db1-9e90-0efc77501684/99pros_psc_relatedness_short.py"", line 26, in <module>; vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno) 1 else 0""); File ""<decorator-gen-322>"", line 2, in ibd_prune; File ""/home/ec2-user/BuildAgent/work/179f3a9ad532f105/python/hail/java.py"", line 112, in handle_py4j; hail.java.FatalError: IllegalStateException: Bytecode failed verification 2. Java stack trace:; java.lang.IllegalStateException: Bytecode failed verification 2; 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:196); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:208); 	at is.hail.expr.CM.runWithDelayedValues(CM.scala:78); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$evalNoTypeCheck(Parser.scala:49); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:62); 	at is.hail.expr.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2178:1082,Load,Loading,1082,https://hail.is,https://github.com/hail-is/hail/issues/2178,2,['Load'],['Loading']
Performance,xpr.ir.FoldConstants.apply total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.114ms self 0.048ms children 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:181577,Optimiz,OptimizePass,181577,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,xpr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.123ms self 0.054ms children 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:188602,Optimiz,OptimizePass,188602,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,xpr.ir.FoldConstants.apply total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.315ms self 0.116ms children 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.153ms self 0.153ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:15312,Optimiz,OptimizePass,15312,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,xpr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.328ms self 0.115ms children 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.164ms self 0.164ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:8328,Optimiz,OptimizePass,8328,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,xpr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 4.611ms self 3.590ms children 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.442ms self 0.442ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:4037,Optimiz,OptimizePass,4037,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,xt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32290,cache,cached,32290,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,xt$.$anonfun$scoped$3(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:350); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:495); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:494); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.120-f00f916faf78; Error summary: ClassCastException: class is.hail.types.physical.stypes.concrete.SIndexablePointer cannot be cast to class is.hail.types.physical.stypes.concrete.SJavaArrayString (is.hail.types.physical.stypes.concrete.SIndexablePointer and is.hail.types.physical.stypes.concrete.SJavaArrayString are in unnamed module of loader 'app'); ```. ### Version. 0.2.122. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:13912,load,loader,13912,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['load'],['loader']
Performance,xt(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-425cc84a9997; Error summary: HailException: array index out of bounds: 2 / 2; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3653:7815,concurren,concurrent,7815,https://hail.is,https://github.com/hail-is/hail/issues/3653,2,['concurren'],['concurrent']
Performance,xt(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1275:3439,concurren,concurrent,3439,https://hail.is,https://github.com/hail-is/hail/issues/1275,1,['concurren'],['concurrent']
Performance,xt(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508:10823,concurren,concurrent,10823,https://hail.is,https://github.com/hail-is/hail/issues/3508,2,['concurren'],['concurrent']
Performance,xt.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mut,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6248,Optimiz,OptimizePass,6248,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['OptimizePass']
Performance,xtRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:5579,concurren,concurrent,5579,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['concurren'],['concurrent']
Performance,xtras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3901:15109,concurren,concurrent,15109,https://hail.is,https://github.com/hail-is/hail/issues/3901,2,['concurren'],['concurrent']
Performance,"y but we; timeout substantially more frequently. I have observed this myself on my laptop. Just this; morning I saw it happen to Daniel. 2. When using an `aiohttp.AsyncIterablePayload`, it is *critical* to always check if the coroutine; which actually writes to GCS (which is stashed in the variable `request_task`) is still; alive. In the current `main`, we do not do this which causes hangs (in particular the timeout; exceptions are never thrown ergo we never retry). To understand the second problem, you must first recall how writing works in aiogoogle. There are; two Tasks and an `asyncio.Queue`. The terms ""writer"" and ""reader"" are somewhat confusing, so let's; use left and right. The left Task has the owning reference to both the source ""file"" and the; destination ""file"". In particular, it is the *left* Task which closes both ""files"". Moreover, the; left Task reads chunks from the source file and places those chunks on the `asyncio.Queue`. The; right Task takes chunks off the queue and writes those chunks to the destination file. This situation can go awry in two ways. First, if the right Task encounters any kind of failure, it will stop taking chunks off of the; queue. When the queue (which has a size limit of one) is full, the left Task will hang. The system; is stuck. The left Task will wait forever for the right Task to empty the queue. The second scenario is exactly the same except that the left Task is trying to add the ""stop""; message to the queue rather than a chunk. In either case, it is critical that the left Task waits simultaneously on the queue operation *and*; on the right Task completing. If the right Task has died, no further writes can occur and the left; Task must raise an exception. In the first scenario, we do not observe the right Task's exception; because that will be done when we close the `InsertObjectStream` (which represents the destination; ""file""). ---. I also added several types, assertions, and a few missing `async with ... as resp:` b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11830:1205,queue,queue,1205,https://hail.is,https://github.com/hail-is/hail/pull/11830,1,['queue'],['queue']
Performance,y dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Coll,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32379,cache,cached,32379,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,y execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.621ms self 0.621ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.245ms self 0.245ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.034ms self 0.034ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:159054,Optimiz,OptimizePass,159054,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,y execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.650ms self 0.650ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.254ms self 0.254ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:143267,Optimiz,OptimizePass,143267,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,y execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.283ms self 4.283ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.308ms self 0.308ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:127549,Optimiz,OptimizePass,127549,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"y key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:1750,perform,performance,1750,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['perform'],['performance']
Performance,"y keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions) to multi-way zip join? I don't see where that happens.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5424:1794,optimiz,optimization,1794,https://hail.is,https://github.com/hail-is/hail/pull/5424,1,['optimiz'],['optimization']
Performance,"y need to be changed for the full job group implementation. I don't know how to compute `@new_n_completed` grouped by job group and then `total_jobs_in_batch` would need to be computed per job group as well. I don't think you can use for loops in SQL. It might be possible to do this with temporary tables, but I thought it would be better to take a detour from adding job groups and get rid of how we currently do the batch update in MJC to allow for job groups in the future before putting in job groups tables so that I could slot in the appropriate state and time_completed updates to both batches and job_groups tables in the same place rather than relying on a trigger for the updates. I can think about which set of changes should go first (I'm not wedded to either PR coming first -- just thought this way was conceptually easier to understand when there was just a batches table). I haven't 100% convinced myself this change to tokenize the `batches_n_jobs_in_complete_states` table is absolutely necessary for nested job groups, but it seemed like we would want the performance improvements regardless. > 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. This is for performance reasons to avoid serialization and race conditions. If the update occurs in the same transaction, then each transaction will be serialized checking if `n_jobs == n_complete`. If we don't have a `SELECT ... FOR UPDATE`, I couldn't convince myself that there wouldn't be a race condition where a batch completion event isn't accidentally missed. . I think the healing loop would only happen if the batch driver got restarted in between:; 1. CALL mark_job_complete() in the database; 2. mark_batch_complete(). If 1. errors, the operation is aborted entirely. On second thought, it might be possible to use an optimistic locking strategy, but that's more complicated and I'd have to",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:2631,perform,performance,2631,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,1,['perform'],['performance']
Performance,y satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:35181,cache,cached,35181,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,y total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.114ms self 0.048ms children 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:181606,Optimiz,Optimize,181606,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.123ms self 0.054ms children 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:188631,Optimiz,Optimize,188631,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.315ms self 0.116ms children 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.153ms self 0.153ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:15341,Optimiz,Optimize,15341,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.328ms self 0.115ms children 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.164ms self 0.164ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:8357,Optimiz,Optimize,8357,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 0.575ms self 0.049ms children 0.527ms %children 91.53%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.114ms self 0.048ms children 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.Opti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:181260,Optimiz,Optimize,181260,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 0.577ms self 0.041ms children 0.536ms %children 92.90%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.123ms self 0.054ms children 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.Opti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:188285,Optimiz,Optimize,188285,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 1.096ms self 0.044ms children 1.052ms %children 96.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.315ms self 0.116ms children 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.Opti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:14995,Optimiz,Optimize,14995,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 4.611ms self 3.590ms children 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.442ms self 0.442ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:4066,Optimiz,Optimize,4066,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y$11.apply(ContextRDD.scala:235); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:235); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:234); 	at scala.Function1$$anonfun$andThen$1.apply(Function1.scala:52); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```. _All_ of my workers had this error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:6427,concurren,concurrent,6427,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['concurren'],['concurrent']
Performance,y$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-df317f3; Error summary: HailException: found non-left aligned variant: 18:76051965:C:G; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3040:14580,concurren,concurrent,14580,https://hail.is,https://github.com/hail-is/hail/issues/3040,2,['concurren'],['concurrent']
Performance,y(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5524,optimiz,optimizeIR,5524,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['optimiz'],['optimizeIR']
Performance,"y.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4417,cache,cached,4417,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"y.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; U",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3768,cache,cached,3768,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,y/is.hail.expr.ir.lowering.InlineApplyIR/is.hail.expr.ir.lowering.CompilableIRNoApply total 0.015ms self 0.015ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.518ms self 0.004ms children 0.514ms %children 99.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.496ms self 0.040ms children 0.456ms %children 91.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.228ms self 0.228ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.Lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:206128,Optimiz,OptimizePass,206128,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,y/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.938ms self 0.938ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.101ms self 0.049ms children 0.052ms %children 51.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:124582,Optimiz,Optimize,124582,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.490ms self 0.490ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.083ms self 0.043ms children 0.040ms %children 48.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:156087,Optimiz,Optimize,156087,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.058ms self 0.058ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.240ms self 0.240ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.087ms self 0.042ms children 0.045ms %children 51.66%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:140300,Optimiz,Optimize,140300,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:33.788 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/1-day/o?name=parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g%3D/result.7028&uploadType=resumable&upload_id=ADPycdv7y3A6GjTh6Kv7vrUu2ap2Kv0peLVWsVTAghIs7RCZk9X3fI1BDkeHag1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPqmOq13; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdv7y3A6GjTh6Kv7vrUu2ap2Kv0peLVWsVTAghIs7RCZk9X3fI1BDkeHag1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPqmOq13; 	| ; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:7200,concurren,concurrent,7200,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['concurren'],['concurrent']
Performance,y3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36534,cache,cached,36534,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"y3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5974,cache,cached,5974,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"y3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4500,cache,cached,4500,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"y4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: ClassFormatError: Too many arguments in method signature in class file __C2866stream. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 8.0 failed 20 times, most recent failure: Lost task 3.19 in stage 8.0 (TID 54368) (leo-test-w-8.australia-southeast1-a.c.ourdna-browser.internal executor 14): java.lang.ClassFormatError: Too many arguments in method signature in class file __C2866stream; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:3713,load,loadOrDefineClass,3713,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['load'],['loadOrDefineClass']
Performance,"yTransientErrors(package.scala:77); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$4(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2191/716305671.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). ""pool-2-thread-1"" #26 prio=5 os_prio=0 tid=0x00007f502866e000 nid=0x88c waiting on condition [0x00007f50275fd000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.close(ServiceCacheableFS.scala:46); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 	at java.io.ObjectOutputStream$BlockDataOutputStream.close(ObjectOutputStream.java:1828); 	at java.io.Objec",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:3714,concurren,concurrent,3714,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,yWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19220,Load,LoadVCF,19220,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,yasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39227,cache,cached,39227,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"yeah, I have a very low prior on this changing performance. This doesn't change the staged code generation at all anyway, and most of what we care about is staged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9989#issuecomment-773456445:47,perform,performance,47,https://hail.is,https://github.com/hail-is/hail/pull/9989#issuecomment-773456445,1,['perform'],['performance']
Performance,"yeah, the `keyed=False` path is for performance and I'm OK leaving that with duplicated nodes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5692#issuecomment-479643139:36,perform,performance,36,https://hail.is,https://github.com/hail-is/hail/pull/5692#issuecomment-479643139,1,['perform'],['performance']
Performance,"yeah, we have loads of tests go through this path. If Python tests pass, I'm quite satisfied this is correct!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8057#issuecomment-583559299:14,load,loads,14,https://hail.is,https://github.com/hail-is/hail/pull/8057#issuecomment-583559299,1,['load'],['loads']
Performance,"yes, sure. will do that after practicing talk once more. Also, this did remove the logic that prevents a remapping of entries if the field is a top level entry field. This will make UKBB regression performance worst in the very near term. Should we patch this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4535#issuecomment-430342969:198,perform,performance,198,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-430342969,1,['perform'],['performance']
Performance,"ylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a06270766887328bdb51066eccff2cd696a6; Stored in directory: /home/hadoop/.cache/pip/wheels/e3/a2/1e/5c1be0865f4170a89de34e0a798f32f674a7eaf63a93272c7f; Successfully built avro; Installing collected packages: sortedcontainers, pytz, py4j, commonmark, azure-common, xyzservices, wrapt, uvloop, urllib3, tzdata, typing-extensions, tornado, tenacity, tabulate, six, regex, pyyaml, python-json-logger, pyjwt, pygments, pycparser, pyasn1, protobuf, portalocker, pillow, packaging, orjs; on, oauthlib, numpy, nest-asyncio, multidict, markupsafe, jmespath, idna, humanize, google-crc32c, frozenlist, dill, decorator, charset-normalizer, certifi, cachetools, avro, attrs, asyncinit, async-timeout, yarl, typer, scipy, rsa, rich, requests, python-dateutil, pyasn1-modules, plotly, parsimonious; , jproperties, jinja2, janus, isodate, googleapis-common-protos, google-resumable-media, deprecated, contourpy, cffi, aiosignal, requests-oauthlib, pycares, pandas, google-auth, cryptography, botocore, azure-core, aiohttp, s3transfer, msrest, google-auth-oauthlib, google-api-core, bokeh, azure-storage; -blob, azure-mgmt-core, aiodns, msal, google-cloud-core, boto3, azure-mgmt-storage, msal-extensions, google-cloud-storage, azure-identity; Attempting uninstall: packaging; Found existing installation: packaging 23.2; Uninstalling packaging-23.2:; Successfully uninstalled packaging-23.2; Successfully installed aiodns-2.0.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 asyncinit-0.2.4 attrs-23.1.0 avro-1.11.2 azure-common-1.1.28 azure-core-1.29.3 azure-identity-1.14.0 azure-mgmt-core-1.4.0 azure-mgmt-storage-20.1.0 azure-storage-blob-12.17.0 bokeh-3.2.2 boto3-1.28.41 botocore-1.31.; 41 cache",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:41550,cache,cache,41550,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,['cache'],"['cache', 'cachetools']"
Performance,"you'll get even more benefit from staging, to get rid of the `loadField` overhead. please follow up with DSP for the 1kg gvcfs so we can add a benchmark!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7252#issuecomment-540783673:62,load,loadField,62,https://hail.is,https://github.com/hail-is/hail/pull/7252#issuecomment-540783673,2,['load'],['loadField']
Performance,"ypes are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non). PStruct. - Concrete implementations (canonical/non). PString. - Concrete implementations (canonical/non). PBinary. - Concrete implementations (canonical/non). ## <a name=""complex-ptypes""></a> Complex PTy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:1180,load,loading,1180,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loading']
Performance,"ython3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:3137,Load,LoadMatrix,3137,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"ze -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEval",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2158,Optimiz,Optimize,2158,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done.; ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08] failed: Initialization action failed. Failed action 'gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py', see output in: gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output.; Traceback (most recent call last):; File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6634:1983,perform,performance,1983,https://hail.is,https://github.com/hail-is/hail/issues/6634,1,['perform'],['performance']
Performance,"~Stacked on #10791~. A few high level changes got mixed up in this PR, since I couldn't predict where a thread would lead once I started pulling. If you would like, I can try to disentangle them. Here are the conceptual changes:; * add a CodeBuilder argument to `getEmitParam`, so that parameters which are pointers to region values can be loaded into `SValue`s with multiple locals; * make `EmitValue` a concrete class, consisting of an optional boolean value and an `SValue`; * add `valueTuple` to both `SValue` and `EmitValue`; * copy the `SCode` interface onto `SValue`, to make it easier to replace `SCode`s with `SValues` ; * add `loadToSValue` to `SingleCodeType`; * add `SStreamValue`. This loses the single use assertion, but that doesn't seem like it asserts much, since the stream producer can be freely accessed without memoizing the `SStreamCode`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10797:340,load,loaded,340,https://hail.is,https://github.com/hail-is/hail/pull/10797,2,['load'],"['loadToSValue', 'loaded']"
Performance,"~Stacked on #10906~. This PR refactors `MethodBuilder.invoke` and `EmitCodeBuilder.invoke(S)Code` to take/return values. * `invoke` now takes a `CodeBuilderLike`. It is used in places where there is only access to a `CodeBuilder` (not an `EmitCodeBuilder`), so I had to use the generic interface, and had to move a couple methods on `EmitCodeBuilder` to `CodeBuilderLike`. I have never understood this Emit/non-Emit split; would be a great simplification if we could collapse it.; * This change pushed some (S)Code->(S)Value refactorings inside some aggregator implementations, which generate and invoke internal methods.; * I had to keep a version of `MethodBuilder.invoke` that doesn't take a CodeBuilder, for use in `ThisLazyFieldRef.get`. Will have to think more about how this should work when Code is (mostly) gone. Maybe lazy fields should not subclass Value, and to access a lazy field requires an explicit `load(cb: CodeBuilder): Value[T]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10907:916,load,load,916,https://hail.is,https://github.com/hail-is/hail/pull/10907,1,['load'],['load']
Performance,"~Stacked on #12376~. Reworks the staged index reader query algorithm. Before, `queryInterval` did three traversals from root to leaf: one each for the start and end of the interval, and one to find the first record. Now it always does a single traversal, visiting one or two nodes at each level (two if the paths for the start and end keys diverge), and doing a single binary search at each visited node. This is probably not a significant performance improvement when doing a single query per partition, but could be beneficial when using the new `query_table` per row.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12399:440,perform,performance,440,https://hail.is,https://github.com/hail-is/hail/pull/12399,1,['perform'],['performance']
Performance,"~~Performance note: Was able to do 100k variants by 500k samples with 1000 partitions on 800 cores in 5m4s. We've never successfully done such a multiply on gCloud, but even 50k by 100k examples with the old method took a little more than 20 minutes, so safe to say this is an improvement.~~. Unfortunately, there was a bug that resulted in not all the work getting done and as such the method is not actually as performant as initial tests suggested.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1884#issuecomment-304080520:2,Perform,Performance,2,https://hail.is,https://github.com/hail-is/hail/pull/1884#issuecomment-304080520,2,"['Perform', 'perform']","['Performance', 'performant']"
Performance,"~~Stacked on #8172~~. Implement emitters for StreamScan, RunAggScan, and StreamLeftJoinDistinct. These complete the handling in the new emitter for non-root stream nodes (i.e. those which take stream children). Thus it is now safe to delete non-root nodes from the previous EmitStream. This also implements length tracking in the new EmitStream, and adds back the optimizations that take advantage of knowing the length, plus some we weren't doing before, like in ArraySort and CollectDistributedArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8204:364,optimiz,optimizations,364,https://hail.is,https://github.com/hail-is/hail/pull/8204,1,['optimiz'],['optimizations']
Performance,"~~Stacked on #9320~~. Use the framework introduced in #9320 to make the IR parser stack safe. This touches a lot of lines, but it is a completely mechanical refactoring. I did some preliminary benchmarking by timing the parse of the IR in `test_ld_prune`. (I chose that test fairly arbitrarily, and we can probably find better test cases, or generate synthetic large IRs.) Running a loop parsing the same IR repeatedly, with 10 burn-in rounds, and 60 timed rounds, I got the following results:; * On main; ```; quartiles = [4.55816, 5.209579, 5.647135]; avg = 5.332496950000001, std = 1.13761574944604; ```; * Using `StackSafe`, without the optimization in `repUntil`; ```; quantiles = [4.610798, 5.09793, 7.159075]; avg = 5.7519015000000016, std = 1.612397075594852; ```; * Using `StackSafe, with the optimization which reuses the `cont` closure, instead of allocating a new one for each token.; ```; quantiles = [4.466849, 4.826873, 5.719238]; avg = 5.2787357833333335, std = 1.272006325411254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9332:641,optimiz,optimization,641,https://hail.is,https://github.com/hail-is/hail/pull/9332,2,['optimiz'],['optimization']
Performance,…added optimized keyBy in scala / python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1572:7,optimiz,optimized,7,https://hail.is,https://github.com/hail-is/hail/pull/1572,1,['optimiz'],['optimized']
Performance,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:66,Load,Loaded,66,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,4,"['Load', 'Tune', 'cache']","['Loaded', 'Loading', 'Tuned', 'cached']"
Safety,	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:6336,abort,abortStage,6336,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['abort'],['abortStage']
Safety,	at is.hail.utils.richUtils.RichRDD$.writePartitions$extension(RichRDD.scala:209); 	at is.hail.io.RichRDDRegionValue$.writeRows$extension(RowStore.scala:526); 	at is.hail.variant.MatrixTable.write(MatrixTable.scala:2393); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.IndexOutOfBoundsException: 3; 	at is.hail.annotations.UnsafeRow.isNullAt(UnsafeRow.scala:294); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.uti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:5182,Unsafe,UnsafeRow,5182,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['Unsafe'],['UnsafeRow']
Safety, 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Ta,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3276:1396,Unsafe,UnsafeIndexedSeq,1396,https://hail.is,https://github.com/hail-is/hail/issues/3276,1,['Unsafe'],['UnsafeIndexedSeq']
Safety," ### What you did:. copied from gitter:. I'm using hail 0.1 on dataproc and trying to import a vcf from the local filesystem; run(""wget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh{0}/clinvar.vcf.gz -O /tmp/clinvar.vcf.gz"".format(args.genome_version)); run(""ls -l /tmp/clinvar.vcf.gz""); vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); The output is; ls -l /tmp/clinvar.vcf.gz; -rw-r--r-- 1 root root 16218805 Jun 14 20:21 /tmp/clinvar.vcf.gz; Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:1243,abort,aborted,1243,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['abort'],['aborted']
Safety," (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Itera",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2251,Unsafe,UnsafeRow,2251,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety," /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecting to localhost:9090 (127.0.0.1:9090); wget: server returned error: HTTP/1.1 503 Service Unavailable; /prometheus $ ; ```. https://github.com/prometheus/prometheus/issues/5727#issuecomment-510818825; https://github.com/prometheus/prometheus/issues/4324#issuecomment-460243182. ```; # k logs -n monitoring prometheus-0 ; level=info ts=2019-07-31T15:45:51.990Z caller=main.go:286 msg=""no time or size retention was set so using the default time retention"" duration=15d; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:322 msg=""Starting Prometheus"" version=""(version=2.10.0, branch=HEAD, revision=d20e84d0fb64aff2f62a977adc8cfb656da4e286)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:323 build_context=""(go=go1.12.5, user=root@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(L",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:1868,recover,recover,1868,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['recover'],['recover']
Safety," 0, 0, 2, 1, 0, 4, 2, 0]; ```. A library developer may want to remove keys while preserving order so as to implement the above methods. Because all sorts in Hail are unstable, this is a delicate feat. There are two cases: zero-length key, non-zero-length key. When the key is of zero-length, the data may be sorted in some unknown and arbitrary order. Consider for example:. ```; In [45]: import hail as hl ; ...: mt = hl.utils.range_matrix_table(3,3) ; ...: mt = mt.key_cols_by().choose_cols([1,2,0]) ; ...: mt.cols().collect() ; ...: ; Out[45]: [Struct(col_idx=1), Struct(col_idx=2), Struct(col_idx=0)]; ```. Or importing data with no key. In this case it is crucial to *not* call `order_by()` or `key_by()` because both permit hail to arbitrarily reorder the entire dataset (we are unstably sorting by an empty key, ergo, all values are equal). When the key is of non-zero-length, then data must be sorted by the key and the ordering of rows with equivalent keys is undefined. In this case, it is safe to `order_by(*t.key)`. # User Expectations. There is a subtle difference in how we treat zero-length keyed (ZLK) objects and non-zero-length keyed objects. We try to preserve the data ordering of ZLK objects. In particular, users would be pretty surprised if `import_table(..., key=[])` shuffled the rows. Additionally, users expect (and we document) that `mt.key_cols_by().cols()` does not shuffle the rows. In contrast, we treat a non-zero-length keyed object as if any ordering beyond that defined by the key is irrelevant. Is this surprising to a user? Suppose they had a text file ordered by `family id, sample id`, they might reasonably expect that `import_table(..., key=['family id'])` does not reorder the rows within a family. Hail doesn't guarantee this even though we do make pains to not reorder the same data imported by `import_table(..., key=[])`. # Ordering and the Optimizer. Currently, the Hail optimizer does not remove a `choose_cols` that precedes a `order_by()`. Nor does ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6929:4051,safe,safe,4051,https://hail.is,https://github.com/hail-is/hail/issues/6929,1,['safe'],['safe']
Safety," 13 mt.s.show(); 14. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py in count(self); 2129 Number of rows, number of cols.; 2130 """"""; -> 2131 r = self._jmt.count(); 2132 return r._1(), r._2(); 2133. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanag",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:1609,abort,aborted,1609,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['abort'],['aborted']
Safety," 233s, 233s; linreg with 10 PCs, 8 cores: 23s, 24s, 24s; pca, 8 cores: 179s. hail command:; ~/hail/build/install/hail/bin/hail read -i ~/data/profile75.vds linreg -c ~/data/profile75.cov -f ~/data/profile.fam -o ~/data/profile75.linreg. read: 1570.888546; linreg: 58496.508588. plink vcf command to create bed/bim/fam:; ./plink --vcf ~/data/profile75.vcf.bgz; - rename plink.bed/bim/fam to profile75.bed/bim/fam; - create covar with FID column by doubling first column of cov file (use cut and paste in bash). plink linreg command:; time ./plink --bfile profile75 --double-id --pheno ~/data/profile.pheno --allow-no-sex --covar ~/data/profile75.covar --linear --out ~/data/profile75.plink. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/profile75.plink.log.; Options in effect:; --allow-no-sex; --bfile profile75; --covar /Users/Jon/data/profile75.covar; --double-id; --linear; --out /Users/Jon/data/profile75.plink; --pheno /Users/Jon/data/profile.pheno. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 74885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/profile75.plink.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you may want; to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.914041.; 74885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/profile75.plink.assoc.linear ... done. real 0m38.837s; user 0m38.609s; sys 0m0.187s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/61:1222,detect,detected,1222,https://hail.is,https://github.com/hail-is/hail/pull/61,1,['detect'],['detected']
Safety, 3 * 10^16. ~~I can't find the referenced case analysis in Google's latest code. [It is present in this fork](https://github.com/leogamas/java-storage/blob/2af8dfd95cdebc9e4d8252b0bbe3f092844d9f2c/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java#L68-L198) from a few years ago.~~. Here's the [referenced case analysis in 2.17.1](https://github.com/googleapis/java-storage/blame/v2.17.1/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java). There seems to have been a rewrite [two months ago](https://github.com/googleapis/java-storage/blame/main/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java) (here's [the main commit](https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b)). That landed in [2.25.0](https://github.com/googleapis/java-storage/releases/tag/v2.25.0) which was released in July. ```; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:1311,recover,recover,1311,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['recover'],['recover']
Safety," 448 # Otherwise it looks like a bug in the code. File /opt/conda/lib/python3.10/http/client.py:1375, in HTTPConnection.getresponse(self); 1374 try:; -> 1375 response.begin(); 1376 except ConnectionError:. File /opt/conda/lib/python3.10/http/client.py:318, in HTTPResponse.begin(self); 317 while True:; --> 318 version, status, reason = self._read_status(); 319 if status != CONTINUE:. File /opt/conda/lib/python3.10/http/client.py:287, in HTTPResponse._read_status(self); 284 if not line:; 285 # Presumably, the server closed the connection before; 286 # sending a valid response.; --> 287 raise RemoteDisconnected(""Remote end closed connection without""; 288 "" response""); 289 try:. RemoteDisconnected: Remote end closed connection without response. During handling of the above exception, another exception occurred:. ProtocolError Traceback (most recent call last); File /opt/conda/lib/python3.10/site-packages/requests/adapters.py:487, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies); 486 try:; --> 487 resp = conn.urlopen(; 488 method=request.method,; 489 url=url,; 490 body=request.body,; 491 headers=request.headers,; 492 redirect=False,; 493 assert_same_host=False,; 494 preload_content=False,; 495 decode_content=False,; 496 retries=self.max_retries,; 497 timeout=timeout,; 498 chunked=chunked,; 499 ); 501 except (ProtocolError, OSError) as err:. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:787, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw); 785 e = ProtocolError(""Connection aborted."", e); --> 787 retries = retries.increment(; 788 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]; 789 ); 790 retries.sleep(). File /opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py:550, in Retry.increment(self, method, url, response, error, _pool, _stacktrace); 549 if read is False or not self._is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:6333,timeout,timeout,6333,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['timeout'],['timeout']
Safety," <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h2>3.9.11 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faste",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:1843,avoid,avoid,1843,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['avoid'],['avoid']
Safety," <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dropped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/5128f7f4ff73b165579006a8336978efaeeca07a""><code>5128f7f</code></a> Fix CHANGES</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/18e718a56f9def7dc40bb9ce3a2962a8fb0c883c""><code>18e718a</code></a> Bump to 4.0.2</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/48d1c4ce923b1da75976d7e2a6ca9234b3092c16""><code>48d1c4c</code></a> Setup towncrier</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/2ede7d73e55f8d1a2279d78861af0009d96219fb""><code>2ede7d7</code></a> Fix annotations on <code>__exit__</code> and <code>__aexit__</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/8685c60dc0e82ee246fbe3d1aa272d1dfe57c24c""><code>8685c60</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-time",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:4172,timeout,timeout,4172,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety," <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dropped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/5128f7f4ff73b165579006a8336978efaeeca07a""><code>5128f7f</code></a> Fix CHANGES</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/18e718a5",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:3167,timeout,timeout,3167,https://hail.is,https://github.com/hail-is/hail/pull/11465,2,['timeout'],['timeout']
Safety," <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Drooped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:2058,timeout,timeout,2058,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety," [<a href=""https://github.com/evanmiller""><code>@​evanmiller</code></a>]</li>; <li>Fixed reading FLI/FLC images with a prefix chunk <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7804"">#7804</a> [<a href=""https://github.com/twolife""><code>@​twolife</code></a>]</li>; <li>Updated package name for Tidelift <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7810"">#7810</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed unused code <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7744"">#7744</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst"">pillow's changelog</a>.</em></p>; <blockquote>; <h2>10.3.0 (2024-04-01)</h2>; <ul>; <li>; <p>CVE-2024-28219: Use <code>strncpy</code> to avoid buffer overflow <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7928"">#7928</a>; [radarhere, hugovk]</p>; </li>; <li>; <p>Deprecate <code>eval()</code>, replacing it with <code>lambda_eval()</code> and <code>unsafe_eval()</code> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7927"">#7927</a>; [radarhere, hugovk]</p>; </li>; <li>; <p>Raise <code>ValueError</code> if seeking to greater than offset-sized integer in TIFF <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7883"">#7883</a>; [radarhere]</p>; </li>; <li>; <p>Add <code>--report</code> argument to <code>__main__.py</code> to omit supported formats <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7818"">#7818</a>; [nulano, radarhere, hugovk]</p>; </li>; <li>; <p>Added RGB to I;16, I;16L, I;16B and I;16N conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7918"">#7918</a>, <a href=""https://redirect.github.com/python-pillow/Pi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:10201,avoid,avoid,10201,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['avoid'],['avoid']
Safety," _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; usr/local/lib/python3.9/dist-packages/hailtop/batch_client/client.py:84: in wait; return async_to_blocking(self._async_job.wait()); usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:156: in async_to_blocking; return loop.run_until_complete(task); usr/lib/python3.9/asyncio/base_events.py:634: in run_until_complete; self.run_forever(); usr/lib/python3.9/asyncio/base_events.py:601: in run_forever; self._run_once(); usr/lib/python3.9/asyncio/base_events.py:1869: in _run_once; event_list = self._selector.select(timeout); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <selectors.EpollSelector object at 0x7fae890f2d30>; timeout = 15.402000000000001. def select(self, timeout=None):; if timeout is None:; timeout = -1; elif timeout <= 0:; timeout = 0; else:; # epoll_wait() has a resolution of 1 millisecond, round away; # from zero to wait *at least* timeout seconds.; timeout = math.ceil(timeout * 1e3) * 1e-3; ; # epoll_wait() expects `maxevents` to be greater than zero;; # we want to make sure that `select()` can be called when no; # FD is registered.; max_ev = max(len(self._fd_to_key), 1); ; ready = []; try:; > fd_event_list = self._selector.poll(timeout, max_ev); E Failed: Timeout >360.0s. usr/lib/python3.9/selectors.py:469: Failed; ------------------------------ Captured log setup ------------------------------; 2023-09-06T21:45:24 INFO test.conftest conftest.py:14:log_before_after starting test; 2023-09-06T21:45:24 INFO hailtop.aiocloud.aioazure.credentials credentials.py:99:default_credentials using credentials file /test-gsa-key/key.json; ------------------------------ Captured log call -------------------------------; 2023-09-06T21:45:25 INFO azure.identity.aio._internal.get_token_mixin get_token_mixin.py:93:get_token ClientSecretCredential.get_token succeeded; 2023-09-06T21:45:25 INFO batch_client.aioclient aioclient.py:809:_submit created batch 1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:3293,timeout,timeout,3293,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['timeout'],['timeout']
Safety," a long or tuple of longs which is guaranteed to be distinct on every execution of `child`.; * Uids are typically created at the leaves of pipelines (`TableRead`, `StreamRange`, etc.), and propagated upwards. There was a phase-ordering conflict that had to be worked around:; * IRs must be given explict rng state and uid semantics as early as possible, to ensure determinism.; * The transformation to explicitly pass rng states and uids must happen during IR construction. If it happened later, it would create new IR objects, which would defeat the python CSE pass (which only recognizes equivalent subexpressions when they are represented by the same python object).; * The rng explication requires some type information.; * Types on the python IR are assigned after the IR is fully constructed. To fix this:; * `Ref`'s must be given a type at construction; * `TopLevelReference`s are the only case that needs to be constructed before a type is known. But they are always constructed wrapped in a `SelectFields` or `GetField`, whose type is known at construction. I added new IR classes `SelectedTopLevelReference` and `ProjectedTopLevelReference` for these two cases, which are thin wrappers which don't appear in the rendered IR.; * `construct_expr` always assigns a type to the ir. Bottom-up type construction will later assert equality with the assigned type. This caught some existing bugs, where expression type and ir type didn't agree.; * At construction of the root node of a stream/table/matrixtable pipeline (i.e. a non-stream value ir with at least one stream/table/matrixtable child), recursively rewrite the contained pipeline(s) to make rng states and uids explicit. This is safe, since stream/table/matrixtable IRs won't be CSE'd, because they may only be evaluated once. Contained value IRs are not rewritten, only wrapped with bindings which define the rng state. These are currently non-functional changes, as `ApplySeeded` still uses the old rng, and will ignore the rng state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11847:2160,safe,safe,2160,https://hail.is,https://github.com/hail-is/hail/pull/11847,1,['safe'],['safe']
Safety," as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be sl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:1873,avoid,avoid,1873,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['avoid'],['avoid']
Safety," book air (M1) , spark local mode; Rocky Linux 8.5 , spark local mode; Rocky Linux 8.5 , spark yarn cluster mode. - how to reproduce; ```; import os; os.environ['PYSPARK_SUBMIT_ARGS'] = ' \; --jars \; /Users/username/miniforge3/envs/hail/lib/python3.9/site-packages/hail/backend/hail-all-spark.jar \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; pyspark-shell '. from pyspark import SparkContext; sc=SparkContext.getOrCreate(). import hail as hl; hl.init(sc=sc); ```. - Error logs ; ```; 22/05/11 14:31:21 WARN Utils: Your hostname, spacerider.local resolves to a loopback address: 127.0.0.1; using 172.20.10.4 instead (on interface en6); 22/05/11 14:31:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/username/miniforge3/envs/hail/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 22/05/11 14:31:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Input In [2], in <cell line: 6>(); 3 sc = spark._sc; 5 import hail as h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11827:1210,unsafe,unsafe,1210,https://hail.is,https://github.com/hail-is/hail/issues/11827,1,['unsafe'],['unsafe']
Safety," client does not close; it.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/master/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.9.4 (2024-04-11)</h1>; <h2>Bug fixes</h2>; <ul>; <li>; <p>The asynchronous internals now set the underlying causes; when assigning exceptions to the future objects; -- by :user:<code>webknjaz</code>.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8089</code>.</p>; </li>; <li>; <p>Treated values of <code>Accept-Encoding</code> header as case-insensitive when checking; for gzip files -- by :user:<code>steverep</code>.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8104</code>.</p>; </li>; <li>; <p>Improved the DNS resolution performance on cache hit -- by :user:<code>bdraco</code>.</p>; <p>This is achieved by avoiding an :mod:<code>asyncio</code> task creation in this case.</p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>8163</code>.</p>; </li>; <li>; <p>Changed the type annotations to allow <code>dict</code> on :meth:<code>aiohttp.MultipartWriter.append</code>,; :meth:<code>aiohttp.MultipartWriter.append_json</code> and; :meth:<code>aiohttp.MultipartWriter.append_form</code> -- by :user:<code>cakemanny</code></p>; <p><em>Related issues and pull requests on GitHub:</em>; :issue:<code>7741</code>.</p>; </li>; <li>; <p>Ensure websocket transport is closed when client does not close it; -- by :user:<code>bdraco</code>.</p>; <p>The transport could remain open if the client did not close it. This; change ensures the transport is closed when the client does not close; it.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/b3397c7ac44fc80206d28",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14477:2868,avoid,avoiding,2868,https://hail.is,https://github.com/hail-is/hail/pull/14477,6,['avoid'],['avoiding']
Safety, com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: sun.reflect.generics.reflectiveObjects.NotImplementedException; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 10 more; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:1720,Unsafe,UnsafeRow,1720,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['Unsafe'],['UnsafeRow']
Safety," constructor `optional<T>`, i.e. base types don't encode missingness. Emits a single bit in the encoding. Can invert this bit to control whether missing values come first or last in the ordering. If missing, nothing is emitted after.; - sort-order; - treat reversing the default ordering as a type constructor `reverse<T>`; - simply inverts the encoding bitwise; - primitive types; - same as in datafusion, encoding has same size as original type; - signed integers - flip the sign bit; - floating point numbers - if sign bit is set, invert all bits, otherwise only flip the sign bit; - arrays; - before each element and after last element, emit continuation bit (0 if no more elements); - pad before each element. This prevents a variable number of missing bits packing into a byte; - strings and byte-arrays; - simply use null-terminated strings (being careful to do this in a unicode-safe way); - structs; - simply concatenate element encodings. safe because codes are prefix-free; - key structs; - support variable length ""interval endpoints""; - e.g. for a key type `struct<t1, t2>`, the interval `[{a}, {a, b})` contains all keys with first field `a` and second field less than `b`. We break it into two ""interval endpoints"", `({a}, -1)` and `({a, b}, -1)`, which consist of a struct value which is a prefix of the key struct type, and a ""sign"". In this case, both endpoints ""lean left"".; - needed for working with partitioners at runtime; - like an array with fixed but heterogenous types and a max length; - before each element and after last element, emit two continuation bits; - `00` - end of key, leans left (less than all longer keys with this prefix); - `01` - continue, or after last key field of actual key value (not interval endpoint); - unambiguous because key value can't terminate early, and can't continue past last key field (max length); - `11` - end of key, leans right (greater than all longer keys with this prefix); - after each element, pad. # Implementation sketch; Concr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14396:2254,safe,safe,2254,https://hail.is,https://github.com/hail-is/hail/issues/14396,1,['safe'],['safe']
Safety," count in genotypes, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=ExcessHet,Number=1,Type=Float,Description=""Phred-scaled p-value for exact test of excess heterozygosity"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=QD,Number=1,Type=Float,Description=""Variant Confidence/Quality by Depth"">; ##INFO=<ID=RAW_MQ,Number=1,Type=Float,Description=""Raw data for RMS Mapping Quality"">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">; ##INFO=<ID=SOR,Number=1,Type=Float,Description=""Symme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:8090,detect,detect,8090,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['detect'],['detect']
Safety," creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured out the rest of the terraform/bootstrap process should follow pretty quickly. Stacked on #10911",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10919:1235,redund,redundant,1235,https://hail.is,https://github.com/hail-is/hail/pull/10919,1,['redund'],['redundant']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI1ZDhmZDhmZC1mZGUxLTRiYmMtYWMzMi0xOTE1NmY0ZDFjZjIiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjVkOGZkOGZkLWZkZTEtNGJiYy1hYzMyLTE5MTU2ZjRkMWNmMiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/7fad328c-8d01-4768-8813-73d6c644e2d4?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/7fad328c-8d01-4768-8813-73d6c644e2d4?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""5d8fd8fd-fde1-4bbc-ac32-19156f4d1cf2"",""prPublicId"":""5d8fd8fd-fde1-4bbc-ac32-19156f4d1cf2"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""7fad328c-8d01-4768-8813-73d6c644e2d4"",""projectUrl"":""https://app.snyk.io/org/danking/project/7fad328c-8d01-4768-8813-73d6c644e2d4?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13112:3094,remediat,remediationStrategy,3094,https://hail.is,https://github.com/hail-is/hail/pull/13112,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2YzU3NmY1Yi1lNGM5LTQ4ZjctYmYxNy04YjEzOTIxODlmZDQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjZjNTc2ZjViLWU0YzktNDhmNy1iZjE3LThiMTM5MjE4OWZkNCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""6c576f5b-e4c9-48f7-bf17-8b1392189fd4"",""prPublicId"":""6c576f5b-e4c9-48f7-bf17-8b1392189fd4"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13097:3017,remediat,remediationStrategy,3017,https://hail.is,https://github.com/hail-is/hail/pull/13097,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3ZGVlZGFlMy1mZmE3LTQxYmUtOGY4MS1lNmYwZTA5YTczOTMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjdkZWVkYWUzLWZmYTctNDFiZS04ZjgxLWU2ZjBlMDlhNzM5MyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""7deedae3-ffa7-41be-8f81-e6f0e09a7393"",""prPublicId"":""7deedae3-ffa7-41be-8f81-e6f0e09a7393"",""dependencies"":[{""name"":""urllib3"",""from"":""1.26.16"",""to"":""1.26.17""}],""packageManager"":""pip"",""projectPublicId"":""701495b8-b53d-48af-82fe-1a6c57aa56cb"",""projectUrl"":""https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-URLLIB3-5926907""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[581],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13770:3109,remediat,remediationStrategy,3109,https://hail.is,https://github.com/hail-is/hail/pull/13770,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIwMzdiOGRmZS1hZDA4LTRmZjUtYTFkOC1hNGM4Nzg2N2NkYjAiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjAzN2I4ZGZlLWFkMDgtNGZmNS1hMWQ4LWE0Yzg3ODY3Y2RiMCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""037b8dfe-ad08-4ff5-a1d8-a4c87867cdb0"",""prPublicId"":""037b8dfe-ad08-4ff5-a1d8-a4c87867cdb0"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13159:3109,remediat,remediationStrategy,3109,https://hail.is,https://github.com/hail-is/hail/pull/13159,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyNWE2ZGYzMi1kYmEzLTQzOTctYmIyNC0zNjdlMzhmZWQ3ZmUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjI1YTZkZjMyLWRiYTMtNDM5Ny1iYjI0LTM2N2UzOGZlZDdmZSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""25a6df32-dba3-4397-bb24-367e38fed7fe"",""prPublicId"":""25a6df32-dba3-4397-bb24-367e38fed7fe"",""dependencies"":[{""name"":""urllib3"",""from"":""1.26.17"",""to"":""1.26.18""}],""packageManager"":""pip"",""projectPublicId"":""701495b8-b53d-48af-82fe-1a6c57aa56cb"",""projectUrl"":""https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-URLLIB3-6002459""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[496],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13848:3109,remediat,remediationStrategy,3109,https://hail.is,https://github.com/hail-is/hail/pull/13848,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIzM2VkMzM4Ny0zZTVmLTRkZDgtYjIxYy1iYzIyNzk4ODViZjMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjMzZWQzMzg3LTNlNWYtNGRkOC1iMjFjLWJjMjI3OTg4NWJmMyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""33ed3387-3e5f-4dd8-b21c-bc2279885bf3"",""prPublicId"":""33ed3387-3e5f-4dd8-b21c-bc2279885bf3"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""b72ce54d-5de3-48e5-a1d4-6f8967681a12"",""projectUrl"":""https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13107:3002,remediat,remediationStrategy,3002,https://hail.is,https://github.com/hail-is/hail/pull/13107,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIzYWEwNDk2OC02NDIxLTRmODktYTBjYy03MjE4MzExNDNiZGQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjNhYTA0OTY4LTY0MjEtNGY4OS1hMGNjLTcyMTgzMTE0M2JkZCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""3aa04968-6421-4f89-a0cc-721831143bdd"",""prPublicId"":""3aa04968-6421-4f89-a0cc-721831143bdd"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13100:3105,remediat,remediationStrategy,3105,https://hail.is,https://github.com/hail-is/hail/pull/13100,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiNzM2NzI0Yi1hY2RiLTRiOTUtYWQwMy1hYWI3MjkyZGNlYzQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImI3MzY3MjRiLWFjZGItNGI5NS1hZDAzLWFhYjcyOTJkY2VjNCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""b736724b-acdb-4b95-ad03-aab7292dcec4"",""prPublicId"":""b736724b-acdb-4b95-ad03-aab7292dcec4"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13116:3009,remediat,remediationStrategy,3009,https://hail.is,https://github.com/hail-is/hail/pull/13116,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlNDQxZTBmNS1jZDQ4LTQzZDUtYTdkMy1kMTM4YzQ2ZTc2NTgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImU0NDFlMGY1LWNkNDgtNDNkNS1hN2QzLWQxMzhjNDZlNzY1OCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""e441e0f5-cd48-43d5-a7d3-d138c46e7658"",""prPublicId"":""e441e0f5-cd48-43d5-a7d3-d138c46e7658"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13158:3101,remediat,remediationStrategy,3101,https://hail.is,https://github.com/hail-is/hail/pull/13158,1,['remediat'],['remediationStrategy']
Safety," dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlZjQxMWYxOC1hM2JiLTQ1YzgtODFjOS1hNmNhNjI4MWI1ZjMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImVmNDExZjE4LWEzYmItNDVjOC04MWM5LWE2Y2E2MjgxYjVmMyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""ef411f18-a3bb-45c8-81c9-a6ca6281b5f3"",""prPublicId"":""ef411f18-a3bb-45c8-81c9-a6ca6281b5f3"",""dependencies"":[{""name"":""requests"",""from"":""2.28.2"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""701495b8-b53d-48af-82fe-1a6c57aa56cb"",""projectUrl"":""https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[591],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13108:3092,remediat,remediationStrategy,3092,https://hail.is,https://github.com/hail-is/hail/pull/13108,1,['remediat'],['remediationStrategy']
Safety," devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:1239,abort,aborted,1239,https://hail.is,https://github.com/hail-is/hail/issues/3063,1,['abort'],['aborted']
Safety," directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of linux, we can reevaluate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414:1243,avoid,avoid,1243,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414,2,['avoid'],['avoid']
Safety," for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop from an empty deque. /usr/lib/python3.9/asyncio/base_events.py:1890: IndexError; ```. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13997:3581,timeout,timeout,3581,https://hail.is,https://github.com/hail-is/hail/issues/13997,2,"['safe', 'timeout']","['safe', 'timeout']"
Safety," full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop from an empty deque. /usr/lib/python3.9/asyncio/base_events.py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13997:3434,timeout,timeout,3434,https://hail.is,https://github.com/hail-is/hail/issues/13997,1,['timeout'],['timeout']
Safety," full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(hand",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10705:2523,timeout,timeout,2523,https://hail.is,https://github.com/hail-is/hail/pull/10705,1,['timeout'],['timeout']
Safety," in FastPath, not needed.</li>; </ul>; <h1>v4.11.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/369"">#369</a>: Fixed bug where <code>EntryPoint.extras</code> was returning; match objects and not the extras strings.</li>; </ul>; <h1>v4.11.1</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/367"">#367</a>: In <code>Distribution.requires</code> for egg-info, if <code>requires.txt</code>; is empty, return an empty list.</li>; </ul>; <h1>v4.11.0</h1>; <ul>; <li>bpo-46246: Added <code>__slots__</code> to <code>EntryPoints</code>.</li>; </ul>; <h1>v4.10.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/365"">#365</a> and bpo-46546: Avoid leaking <code>method_name</code> in; <code>DeprecatedList</code>.</li>; </ul>; <h1>v4.10.1</h1>; <h1>v2.1.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/361"">#361</a>: Avoid potential REDoS in <code>EntryPoint.pattern</code>.</li>; </ul>; <h1>v4.10.0</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/354"">#354</a>: Removed <code>Distribution._local</code> factory. This; functionality was created as a demonstration of the; possible implementation. Now, the; <code>pep517 &lt;https://pypi.org/project/pep517&gt;</code>_ package; provides this functionality directly through; <code>pep517.meta.load &lt;https://github.com/pypa/pep517/blob/a942316305395f8f757f210e2b16f738af73f8b8/pep517/meta.py#L63-L73&gt;</code>_.</li>; </ul>; <h1>v4.9.0</h1>; <ul>; <li>Require Python 3.7 or later.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python/importlib_metadata/commit/99a2ec4489da45407d8224be2804ff323a164ac0""><code>99a2ec4</code></a> Update changelog.</li>; <li><a href=""https://github.com/python/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11596:1479,Avoid,Avoid,1479,https://hail.is,https://github.com/hail-is/hail/pull/11596,1,['Avoid'],['Avoid']
Safety, is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:5956,abort,abortStage,5956,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['abort'],['abortStage']
Safety, is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:8088,abort,abortStage,8088,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['abort'],['abortStage']
Safety," more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiN2QwMTZlZS0zODA0LTQwMjItOWE0Yi01MzExNjZhNjBjMWQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImI3ZDAxNmVlLTM4MDQtNDAyMi05YTRiLTUzMTE2NmE2MGMxZCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""b7d016ee-3804-4022-9a4b-531166a60c1d"",""prPublicId"":""b7d016ee-3804-4022-9a4b-531166a60c1d"",""dependencies"":[{""name"":""cryptography"",""from"":""41.0.3"",""to"":""41.0.4""}],""packageManager"":""pip"",""projectPublicId"":""701495b8-b53d-48af-82fe-1a6c57aa56cb"",""projectUrl"":""https://app.snyk.io/org/danking/project/701495b8-b53d-48af-82fe-1a6c57aa56cb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CRYPTOGRAPHY-5914629""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[611],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13698:3115,remediat,remediationStrategy,3115,https://hail.is,https://github.com/hail-is/hail/pull/13698,1,['remediat'],['remediationStrategy']
Safety," of transient error. We pick 22 random characters from a 62 character alphabet. Odds of collision are minuscule:; ```; In [2]: (1/62)**22; Out[2]: 3.693029961058969e-40; ```; I verified `SecureRandom` with no constructor uses a randomly chosen seed. There's three exceptions there (all the same one). The deepest one came during a write. The next two came during closes. The outermost exception is from the `using` cleaning up. I'm not sure where the middle exception comes from, I can't imagine who would try to `close` the stream other than `using`. Regardless, it appears that the upload fails in some unrecoverable way. We're writing 2GiB in 256 8MiB chunks in this test, so we have more chances for something to go wrong. Maybe we just have to retry the entire partition when this happens?. https://ci.hail.is/batches/7404773/jobs/145; ```; starting test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt...; Exception:; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:1116,recover,recover,1116,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['recover'],['recover']
Safety," order in the builder (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2927"">#2927</a>).</li>; <li>[cu2quPen] Add Cu2QuMultiPen that converts multiple outlines at a time in interpolation compatible way; its methods take a list of tuples arguments that would normally be passed to individual segment pens, and at the end it dispatches the converted outlines to each pen (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2912"">#2912</a>).</li>; <li>[reverseContourPen/ttGlyphPen] Add outputImpliedClosingLine option (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2913"">#2913</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/2914"">#2914</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/2921"">#2921</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/2922"">#2922</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/2995"">#2995</a>).</li>; <li>[gvar] Avoid expanding all glyphs unnecessarily upon compile (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2918"">#2918</a>).</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/fonttools/fonttools/blob/main/NEWS.rst"">fonttools's changelog</a>.</em></p>; <blockquote>; <h2>4.39.3 (released 2023-03-28)</h2>; <ul>; <li>[sbix] Fixed TypeError when compiling empty glyphs whose imageData is None, regression; was introduced in v4.39 (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3059"">#3059</a>).</li>; <li>[ttFont] Fixed AttributeError on python &lt;= 3.10 when opening a TTFont from a tempfile; SpooledTemporaryFile, seekable method only added on python 3.11 (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3052"">#3052</a>).</li>; </ul>; <h2>4.39.2 (released 2023-03-16)</h2>; <ul>; <li>[varLib] Fixed regression introduced in 4.39.1 whereby an incomplete 'S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:9953,Avoid,Avoid,9953,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['Avoid'],['Avoid']
Safety," own `config` or `url`. `Dataset.from_name_and_json()` now calls `DatasetVersion.get_region()` method to retrieve the dataset from the bucket in the selected region if `custom_config` is `False`. ; - The `DatasetVersion.get_region()` method takes the dataset `name`, a list of `DatasetVersion` objects, and a `region`, and returns a list of the versions that are available for that region. This method calls the instance method `in_region()` to check if the dataset is available in the requested region.; - If `in_region()` determines the desired region is not available for some dataset that otherwise is available in another region, it will raise a warning. If user still tries to call `db.annotate_rows_db()` using a dataset unavailable in their region, then it will get caught by the `_check_availability` instance method in the `DB` class and raise a `ValueError`.; - Started to add documentation to the classes and methods, still a work in progress. Changes to datasets and datasets API site:; - Added the `ldsc_baselineLD_annotations`, `ldsc_baselineLD_ldscores`, and `ldsc_baseline_ldscores` datasets to the `annotation_db.json` configuration file. Now accessible via `load_dataset()` and `db.annotate_rows_db()` (for the annotations at least).; - New `.rst` files in `hail/python/hail/docs/datasets` have been generated to reflect the available datasets in the config file, and `hail/python/hail/docs/datasets.rst` has been updated with the new files as well. Future updates:; - In next PR can add the functionality to automatically determine the users region.; - Also considering modifying the `load_datasets()` function a bit to only require one `version` parameter, to be consistent with the way the version strings are formatted in `annotation_db.json`, and to avoid having to check if the version and reference genome are available separately. Something like this:. ```; mt_1kg = hl.experimental.load_dataset(name='1000_Genomes_autosomes',; version='phase_3-GRCh37', ; region='us'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9496:2924,avoid,avoid,2924,https://hail.is,https://github.com/hail-is/hail/pull/9496,1,['avoid'],['avoid']
Safety," remove redundant definition of npy_nextafter [wheel build]</li>; </ul>; <h2>Checksums</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/85f38ab180ece5290f64e8ddbd9cf06ad8fa4a5e""><code>85f38ab</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23159"">#23159</a> from charris/prepare-1.24.2-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/124252537f526a059b6a5ee3ac1e3bf1442bbc13""><code>1242525</code></a> REL: Prepare for the NumPy 1.24.2 release</li>; <li><a href=""https://github.com/numpy/numpy/commit/de0ee415e45b09c86d1ddc04f51c11192b1e2fe6""><code>de0ee41</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23161"">#23161</a> from mattip/npy_nextafter</li>; <li><a href=""https://github.com/numpy/numpy/commit/ed09037473581908f6b52ecc3cabc82a414e2a54""><code>ed09037</code></a> BLD: remove redundant definition of npy_nextafter [wheel build]</li>; <li><a href=""https://github.com/numpy/numpy/commit/bc47a5ba6798a942d4a76e38f1089fc38f81f50d""><code>bc47a5b</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23150"">#23150</a> from charris/backport-23144</li>; <li><a href=""https://github.com/numpy/numpy/commit/e5452b91b87523853b2e33c0f7f6788a9a22c1b4""><code>e5452b9</code></a> TYP,MAINT: Add a missing explicit <code>Any</code> parameter to the <code>npt.ArrayLike</code> defi...</li>; <li><a href=""https://github.com/numpy/numpy/commit/2433fe5b66016a640dd9337d9e114d7076f55861""><code>2433fe5</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23149"">#23149</a> from charris/backport-23128</li>; <li><a href=""https://github.com/numpy/numpy/commit/8dfa47db1eb6472490b33d5f380513308f3e1a2d""><code>8dfa47d</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23148"">#23148</a> from c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12898:4494,redund,redundant,4494,https://hail.is,https://github.com/hail-is/hail/pull/12898,1,['redund'],['redundant']
Safety," return decorator(_typecheck); /home/hail/hail.zip/hail/matrixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:2175,abort,aborted,2175,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['abort'],['aborted']
Safety," signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:6619,abort,abortStage,6619,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['abort'],['abortStage']
Safety," the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI5NmE4NGVhMS1hYzgxLTQxYmEtOGYzNC02MGU1ZTdhYzNjZTMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6Ijk2YTg0ZWExLWFjODEtNDFiYS04ZjM0LTYwZTVlN2FjM2NlMyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""96a84ea1-ac81-41ba-8f34-60e5e7ac3ce3"",""prPublicId"":""96a84ea1-ac81-41ba-8f34-60e5e7ac3ce3"",""dependencies"":[{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-SETUPTOOLS-3180412""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[509],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lessons/redos/javascript/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12896:3266,remediat,remediationStrategy,3266,https://hail.is,https://github.com/hail-is/hail/pull/12896,1,['remediat'],['remediationStrategy']
Safety," timed); 97 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 98 try:; ---> 99 result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); 100 (result, timings) = (result_tuple._1(), result_tuple._2()); 101 value = ir.typ._from_encoding(result). /opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1321 answer = self.gateway_client.send_command(command); 1322 return_value = get_return_value(; -> 1323 answer, self.gateway_client, self.target_id, self.name); 1324 ; 1325 for temp_arg in temp_args:. /opt/conda/lib/python3.7/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 29 tpl = Env.jutils().handleForPython(e.java_exception); 30 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad no",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:5006,abort,aborted,5006,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['abort'],['aborted']
Safety," timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/5128f7f4ff73b165579006a8336978efaeeca07a""><code>5128f7f</code></a> Fix CHANGES</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/18e718a56f9def7dc40bb9ce3a2962a8fb0c883c""><code>18e718a</code></a> Bump to 4.0.2</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/48d1c4ce923b1da75976d7e2a6ca9234b3092c16""><code>48d1c4c</code></a> Setup towncrier</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/2ede7d73e55f8d1a2279d78861af0009d96219fb""><code>2ede7d7</code></a> Fix annotations on <code>__exit__</code> and <code>__aexit__</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/8685c60dc0e82ee246fbe3d1aa272d1dfe57c24c""><code>8685c60</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:4472,timeout,timeout,4472,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety," to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--bucket=hail-ci-0-1-dataproc-staging-bucket', '--max-idle=40m']' returned non-zero exit status 1.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515:2047,timeout,timeout,2047,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515,2,['timeout'],['timeout']
Safety," use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); /hail/test/BRCA1.raw_indel.vcf MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2; >>> ; ```. ----------------; When I executed the command in local mode , there seems to hava some result:; ```; [root@tele-1 ~]# python; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext();; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel.vcf').write('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel_1.vds'); hail: info: No multiallelics detected.; hail: info: Coerced unsorted dataset; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; ```; ------------------------; How can I check if my spark configuration meet the requirement of the hail?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506:2448,detect,detected,2448,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506,1,['detect'],['detected']
Safety," │; │ 51 │ │ if LocalAsyncFS.valid_url(uri): │; │ 52 │ │ │ fs = LocalAsyncFS(**self._local_kwargs) │; │ 53 │ │ elif aiogoogle.GoogleStorageAsyncFS.valid_url(uri): │; │ ❱ 54 │ │ │ fs = aiogoogle.GoogleStorageAsyncFS( │; │ 55 │ │ │ │ **self._gcs_kwargs, │; │ 56 │ │ │ │ bucket_allow_list = self._gcs_bucket_allow_list.copy() │; │ 57 │ │ │ ) │; │ │; │ /Users/cvittal/src/hail/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:602 in │; │ __init__ │; │ │; │ 599 │ │ │ │ bucket_allow_list: Optional[List[str]] = None, │; │ 600 │ │ │ │ **kwargs): │; │ 601 │ │ if not storage_client: │; │ ❱ 602 │ │ │ storage_client = GoogleStorageClient(**kwargs) │; │ 603 │ │ self._storage_client = storage_client │; │ 604 │ │ if bucket_allow_list is None: │; │ 605 │ │ │ bucket_allow_list = [] │; │ │; │ /Users/cvittal/src/hail/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:309 in │; │ __init__ │; │ │; │ 306 │ │ if 'timeout' not in kwargs and 'http_session' not in kwargs: │; │ 307 │ │ │ # Around May 2022, GCS started timing out a lot with our default 5s timeout │; │ 308 │ │ │ kwargs['timeout'] = aiohttp.ClientTimeout(total=20) │; │ ❱ 309 │ │ super().__init__('https://storage.googleapis.com/storage/v1', **kwargs) │; │ 310 │ │ self._gcs_requester_pays_configuration = get_gcs_requester_pays_configuration( │; │ 311 │ │ │ gcs_requester_pays_configuration=gcs_requester_pays_configuration │; │ 312 │ │ ) │; │ │; │ /Users/cvittal/src/hail/hail/python/hailtop/aiocloud/aiogoogle/client/base_client.py:15 in │; │ __init__ │; │ │; │ 12 │ def __init__(self, base_url: str, *, session: Optional[GoogleSession] = None, │; │ 13 │ │ │ │ rate_limit: Optional[RateLimit] = None, **kwargs): │; │ 14 │ │ if session is None: │; │ ❱ 15 │ │ │ session = GoogleSession(**kwargs) │; │ 16 │ │ super().__init__(base_url, session, rate_limit=rate_limit) │; │ 17 │; │ │; │ /Users/cvittal/src/hail/hail/python/hailtop/aiocloud/aiogoogle/session.py:18 in __init__ │; │ │; │ 15 │ │ │ │ credentials = GoogleCredent",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13793:3839,timeout,timeout,3839,https://hail.is,https://github.com/hail-is/hail/issues/13793,3,['timeout'],['timeout']
Safety,"![Finally.](https://media.giphy.com/media/yIsbuPCEOgNHO/giphy.gif). - update endpoints to handle the ""zen"" that GitHub sends when a web hook is created. - update `make run-local` and friends for the new IP of the `dk-test` micro instance. - remove the unused `refresh_statuses` (this was intended to recover build state from github's commit statuses, but the commit status description is limited to like 120 characters, so I gave up on this a while ago, but never removed the code). - `.strip()` the GitHub token in case there are newlines. - print the SHA being deployed in the log statement. - add `hail-ci-build.sh` to CI, which just invokes `make test-in-cluster`(which in turn runs `test-in-cluster.sh`. - `test-in-cluster.sh` copies the secrets for testing to the expected locations and exposes the pod in which it is running with an internal service, recent changes to `site` [redirect sub URLs of ci.test.is to services named using this scheme](https://github.com/hail-is/hail/blob/master/site/hail.nginx.conf#L38-L41). GitHub uses these URLs to send updates to the CI under test about the watched repositories. - `test-locally.sh` now installs `../batch` into the currently running `pip` before testing (NB: if you edit batch and run the tests without committing the changes you've made to batch, this will pass tests but fail when pushed to a PR!). - `test-locally.sh` activates the `hail-ci` conda environment itself because it was not being propagated from the `Makefile`. I don't know why, but this is a simple fix. - `test-locally.sh` starts the ci after the repository is created. CI will print error messages if a watched repository doesn't exist. - `test/test-ci.py` now uses access tokens for all interaction with GitHub, previously it relied on the latent privileges that I and Cotton had in our environments. - `test/test-ci.py` uses a temporary, but not automatically deleted, directory when the environment variable `IN_CLUSTER` is set to `true` (to which it is set by `test-in-c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4474:300,recover,recover,300,https://hail.is,https://github.com/hail-is/hail/pull/4474,1,['recover'],['recover']
Safety,""" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""4d1e728e-269c-49a2-a2d0-bf1c04966e29"",""prPublicId"":""4d1e728e-269c-49a2-a2d0-bf1c04966e29"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494,539],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14026:5877,remediat,remediationStrategy,5877,https://hail.is,https://github.com/hail-is/hail/pull/14026,1,['remediat'],['remediationStrategy']
Safety,""",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13835:10413,remediat,remediationStrategy,10413,https://hail.is,https://github.com/hail-is/hail/pull/13835,1,['remediat'],['remediationStrategy']
Safety,""",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13866:10439,remediat,remediationStrategy,10439,https://hail.is,https://github.com/hail-is/hail/pull/13866,1,['remediat'],['remediationStrategy']
Safety,""":""13e2c460-fe06-4099-adab-e1f8fa931de0"",""prPublicId"":""13e2c460-fe06-4099-adab-e1f8fa931de0"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""42.0.2""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-CRYPTOGRAPHY-6036192"",""SNYK-PYTHON-CRYPTOGRAPHY-6050294"",""SNYK-PYTHON-CRYPTOGRAPHY-6092044"",""SNYK-PYTHON-CRYPTOGRAPHY-6126975"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,509,454,616,584,479,509,509,509,509,589,509,691,399,479,399,539,479,479,616,616,489,519],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr); 🦉 [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr); 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14329:13018,remediat,remediationStrategy,13018,https://hail.is,https://github.com/hail-is/hail/pull/14329,1,['remediat'],['remediationStrategy']
Safety,""":""9f2a0fe3-dbed-46c0-bd20-223771bc1497"",""prPublicId"":""9f2a0fe3-dbed-46c0-bd20-223771bc1497"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""42.0.2""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-CRYPTOGRAPHY-6036192"",""SNYK-PYTHON-CRYPTOGRAPHY-6050294"",""SNYK-PYTHON-CRYPTOGRAPHY-6092044"",""SNYK-PYTHON-CRYPTOGRAPHY-6126975"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,509,454,616,584,479,509,509,509,509,589,509,691,399,479,399,539,479,479,616,616,561,519],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr); 🦉 [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr); 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:13030,remediat,remediationStrategy,13030,https://hail.is,https://github.com/hail-is/hail/pull/14327,1,['remediat'],['remediationStrategy']
Safety,"""><code>18e718a</code></a> Bump to 4.0.2</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/48d1c4ce923b1da75976d7e2a6ca9234b3092c16""><code>48d1c4c</code></a> Setup towncrier</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/2ede7d73e55f8d1a2279d78861af0009d96219fb""><code>2ede7d7</code></a> Fix annotations on <code>__exit__</code> and <code>__aexit__</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/8685c60dc0e82ee246fbe3d1aa272d1dfe57c24c""><code>8685c60</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c7e10db3d0965422122bef263fb36f6fd7572330""><code>c7e10db</code></a> Fix linter configs</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/6af8bf421a799208b27d57c6bf69de0d998fedec""><code>6af8bf4</code></a> Bump pre-commit from 2.15 to 2.16.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/269"">#269</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c71bbb5b5d7330f6dabfde7a1adec30a4611c0be""><code>c71bbb5</code></a> Bump docutils from 0.18 to 0.18.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/266"">#266</a>)</li>; <li>Additional commits viewable in <a ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:5205,timeout,timeout,5205,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,"""analysis_type=ApplyRecalibration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false input=[(RodBinding name=input source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.unfiltered.vcf)] recal_file=(RodBinding name=recal_file source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recal) tranches_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.tranches out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub ts_filter_level=98.5 ignore_filter=null mode=SNP filter_mismatching_base_and_quals=false""; ##CombineVariants=""analysis_type=Combine",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:1085,unsafe,unsafe,1085,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['unsafe'],['unsafe']
Safety,"""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyN2MzNWY4NC0yNDIyLTRmNzUtYWMxYy1mODQxOGJmNzRlMzciLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjI3YzM1Zjg0LTI0MjItNGY3NS1hYzFjLWY4NDE4YmY3NGUzNyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""27c35f84-2422-4f75-ac1c-f8418bf74e37"",""prPublicId"":""27c35f84-2422-4f75-ac1c-f8418bf74e37"",""dependencies"":[{""name"":""cryptography"",""from"":""41.0.7"",""to"":""42.0.2""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CRYPTOGRAPHY-6149518"",""SNYK-PYTHON-CRYPTOGRAPHY-6157248"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[509,581,451],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use of a Broken or Risky Cryptographic Algorithm](https://learn.snyk.io/lesson/insecure-hash/?loc&#x3D;fix-pr); 🦉 [Uncontrolled Resource Consumption (&#x27;Resource Exhaustion&#x27;)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [NULL Pointer Dereference](https://learn.snyk.io/lesson/null-dereference/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14234:4302,remediat,remediationStrategy,4302,https://hail.is,https://github.com/hail-is/hail/pull/14234,2,"['Risk', 'remediat']","['Risky', 'remediationStrategy']"
Safety,"""https://github.com/aio-libs/aioredis-py/commit/224f843bd4b33d657770bded6f86ce33b881257c""><code>224f843</code></a> Release version 2.0.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1247"">#1247</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/a9825c2ac35939b9ad8928e9468335d8efab963f""><code>a9825c2</code></a> Bump py-actions/py-dependency-install from 2.1.0 to 3 (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1239"">#1239</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/7f65c4ccb0e954c17f2a3e1ecc665c62e4a1aaeb""><code>7f65c4c</code></a> Remove <strong>del</strong> from Redis (Fixes <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1115"">#1115</a>) (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1227"">#1227</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/5062740974e493c390fb8db33982f97d6e08df2d""><code>5062740</code></a> Fix typing on blpop (etc) timeout argument (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1224"">#1224</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/dbdd0add63f986f2ed2d56c9736303d133add23c""><code>dbdd0ad</code></a> fix socket.error raises (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1129"">#1129</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/2ba15fb6947fa2347d401ba436e362ad62ed38ff""><code>2ba15fb</code></a> Fix buffer is closed error when using PythonParser class (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1213"">#1213</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/0aa06df10b9531f4ba734ec7567f8621c00e65e9""><code>0aa06df</code></a> Fix typing on evalsha keys_and_args argument (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1215"">#1215</a>)</li>; <li><a href=""https://github.com/aio-lib",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11569:4746,timeout,timeout,4746,https://hail.is,https://github.com/hail-is/hail/pull/11569,1,['timeout'],['timeout']
Safety,"""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,624,531,604,589,726,434,589,449,696,589,479,519,509,711,701,586,586,384,494,539,589],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14108:12750,remediat,remediationStrategy,12750,https://hail.is,https://github.com/hail-is/hail/pull/14108,1,['remediat'],['remediationStrategy']
Safety,"""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JINJA2-6150717"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PROMPTTOOLKIT-6141120"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494,539,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Cross-site Scripting (XSS)](https://learn.snyk.io/lesson/xss/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14257:11863,remediat,remediationStrategy,11863,https://hail.is,https://github.com/hail-is/hail/pull/14257,1,['remediat'],['remediationStrategy']
Safety,"""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""40.5.0"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.32.2"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JINJA2-6150717"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PROMPTTOOLKIT-6141120"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494,539,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Cross-site Scripting (XSS)](https://learn.snyk.io/lesson/xss/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14364:12216,remediat,remediationStrategy,12216,https://hail.is,https://github.com/hail-is/hail/pull/14364,1,['remediat'],['remediationStrategy']
Safety,"# Notes On Container Security. Every pod has a [`PodSecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#podsecuritycontext-v1-core) which lets us restrict linux permissions, the linux user id, group id, SELinux policies, and permitted sysctl resources. I suspect for CI purposes non-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod y",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:440,safe,safe,440,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573,1,['safe'],['safe']
Safety,"## Change Description; - Added ability to check job status in individual jobs in a JobGroup; - Cancel JobGroup if any jobs in the partition fail; - Added test functionality for detecting cancelled, failed jobs; - Query batch for which jobs have failed within a JobGroup, rather than go through every job in the group. ## Security Assessment. Delete all except the correct answer:; - This change has no security impact. ### Impact Description; Mainly just code for testing, nothing security related. (Reviewers: please confirm the security impact before approving)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14751:177,detect,detecting,177,https://hail.is,https://github.com/hail-is/hail/pull/14751,1,['detect'],['detecting']
Safety,"## `JoinPoint` interface for nontrivial, type-safe control flow in the JVM backend. This PR implements a ""join-point"" abstraction for the JVM backend. Join-points are a primitive; control flow construct that allow sophisticated forms of branching to be implemented in a type safe; way, without having to directly manipulate labels and jumps at the JVM bytecode level. Importantly,; they will enable the kinds of branching needed by stream-deforestation techniques that; @patrick-schultz and I have been discussing, for which while loops and if's were not sufficient. We've also discussed plans for implementing join points as a feature in the IR. ### Notable examples:. * Implementation of `whileLoop` (emits bytecode identical to current version):; ```scala; def whileLoop(cond: Code[Boolean], body: Code[Unit]): Code[Unit] =; JoinPoint.CallCC[Unit] { (jb, break) =>; val continue = jb.joinPoint(); val loopBody = jb.joinPoint(); continue.define { _ => JoinPoint.mux(cond, loopBody, break) }; loopBody.define { _ => Code(body, continue(())) }; continue(()); }; ```. * Mutual recursion:; ```scala; def parity(; n: Code[Int],; even: Code[Ctrl],; odd: Code[Ctrl]; ): Code[Ctrl] = {; val isEven = jb.joinPoint[Code[Int]](mb); val isOdd = jb.joinPoint[Code[Int]](mb); isEven.define { i => (i ceq 0).mux(even, isOdd(i - 1)) }; isOdd.define { i => (i ceq 0).mux(odd, isEven(i - 1)) }; isEven(n); }; ```. ### Classes of interest (tl;dr). - `JoinPoint` - Non-returning function. Used to implement control flow in a type-safe, functional way.; - `ParameterPack` - Trait used for tuple deforesting. Allows join-points to be provided multiple; arguments.; - `JoinPointBuilder` - Used to define new join points.; - `CallCC` - Entry-point for expressions with complex control flow. Provides a `JoinPointBuilder`; and a `JoinPoint` to return a value from the expression. ### `JoinPoint`. A `JoinPoint[A]` acts like a non-returning function with an argument of type `A`. The type of an; applied join point is `Code[C",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7055:46,safe,safe,46,https://hail.is,https://github.com/hail-is/hail/pull/7055,2,['safe'],['safe']
Safety,"### Change Description. This change exists as part of larger refactoring work. Herein, I've exchanged; hard-coded contextual strings passed to `ExecutionTimer.time` with implict; contexts, drawing inspiration from scalatest. These contexts are now supplied after entering functions like `Compile` and; `Emit` instead of before (see `ExecuteContext.time`). By sprinking calls to ; `time` throughout the codebase after entering functions, we obtain a nice trace; of the timings with `sourcecode.Enclosing`, minus the previous verbosity. See [1] for more information about what pre-built macros are available. We can; always build our own later. See comments in [pull request id] for example output.; Note that `ExectionTimer.time` still accepts a string to support uses like; `Optimise` and `LoweringPass` where those contexts are provided already.; It is also exception-safe now. This change exposed many similarities between the implementations of query; execution across all three backends. I've stopped short of full unification; which is a greater work, I've instead simplified and moved duplicated result; encoding into the various backend api implementations. More interesting changes are to `ExecuteContext`, which now supports; - `time`, as discussed above; - `local`, a generalisation for temporarily overriding properties of an ; `ExecuteContext` (inspired by [2]). While I've long wanted this for testing,; we were doing some questionable things when reporting timings back to python,; for which locally overriding the `timer` of a `ctx` has been very useful.; We also follow this pattern for local regions. [1] https://github.com/com-lihaoyi/sourcecode; [2] https://hackage.haskell.org/package/mtl-2.3.1/docs/Control-Monad-Reader.html#v:local. ### Security Assessment. This change has no security impact as it's confined to refactoring of existing non-security-related code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14683:869,safe,safe,869,https://hail.is,https://github.com/hail-is/hail/pull/14683,1,['safe'],['safe']
Safety,"### Description. Today our APIs are ""documented"" only through the list of endpoint handlers in implementation code ([example](https://github.com/hail-is/hail/blob/main/batch/batch/front_end/front_end.py#L239)). We can and should:; - Create OpenAPI documentation for our APIs (maybe per-service, maybe once in the gateway?); - Host swagger page/pages for exploring and testing out APIs . ### Security Impact. High. ### Security Impact Description. ""None"" for the creation of documentation, since we do not believe that documenting our APIs is inherently risky. ""High"" for hosting a new functional component on our web endpoints. Mitigating factor: swagger pages are loaded as static html with no need (or ability) to interact with other functional components, except through the same public APIs as are already accessible. ### Appsec Signoff. - [ ] Reviewed and approved",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14725:553,risk,risky,553,https://hail.is,https://github.com/hail-is/hail/issues/14725,1,['risk'],['risky']
Safety,"### Hail version:. eb5d13fe97fc. ### What you did:. ```; t1 = hl.Table.parallelize([; {'a': 'foo', 'b': 1},; {'a': 'bar', 'b': 2},; {'a': 'bar', 'b': 2}],; hl.tstruct(a=hl.tstr, b=hl.tint32),; key='a'); t2 = hl.Table.parallelize([; {'t': 'foo', 'x': 3.14},; {'t': 'bar', 'x': 2.78},; {'t': 'bar', 'x': -1},; {'t': 'quam', 'x': 0}],; hl.tstruct(t=hl.tstr, x=hl.tfloat64),; key='t'). t1.join(t2, how='outer').show(). # or. t1.join(t2, how='right').show(); ```. ### What went wrong (all error messages here, including the full java stack trace):. FatalError: HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam]. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 24, localhost, executor driver): is.hail.utils.HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1012); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.RVD$$anonfun$4$$anon$1.hasNext(RVD.scala:226); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.utils.richUtils",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:758,abort,aborted,758,https://hail.is,https://github.com/hail-is/hail/issues/4055,1,['abort'],['aborted']
Safety,"### Hail version:. f2b0dca9f506. ### What you did:; ```; ht = hl.Table.parallelize([; {'a': '1', 'c': .5,'d': 'foo'},; {'a': '1', 'c': .6,'d': 'foo'},; ], hl.tstruct(a=hl.tstr,; c=hl.tfloat32, d=hl.tstr)); mt = ht.to_matrix_table(['a'], ['d']). mt.entries().show(); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 7, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.endArray(RegionValueBuilder.scala:167); 	at is.hail.expr.ir.TableToMatrixTable$$anonfun$75$$anonfun$apply$25.apply(MatrixIR.scala:1878); 	at is.hail.expr.ir.TableToMatrixTable$$anonfun$75$$anonfun$apply$25.apply(MatrixIR.scala:1849); 	at is.hail.utils.FlipbookIterator$$anon$4.<init>(FlipbookIterator.scala:133); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:131); 	at is.hail.expr.ir.TableToMatrixTable$$anonfun$75.apply(MatrixIR.scala:1849); 	at is.hail.expr.ir.TableToMatrixTable$$anonfun$75.apply(MatrixIR.scala:1840); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$18.apply(ContextRDD.scala:293); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$18.apply(ContextRDD.scala:293); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1014); 	at scala.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4114:463,abort,aborted,463,https://hail.is,https://github.com/hail-is/hail/issues/4114,1,['abort'],['aborted']
Safety,"### Hail version:; 18d0195e6; ### What you did:; ```; import hail as hl; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; mt = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr22_v3.bgen',; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; min_partitions=100); sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'); og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])); og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2); og_sample._force_count_rows(); ```; The important part is that I used `annotate_rows` on a sufficiently large dataset.; ### What went wrong (all error messages here, including the full java stack trace):; Container failures; ```; Job aborted due to stage failure: Task 5 in stage 9.0 failed 20 times, most recent failure: Lost task 5.19 in stage 9.0 (TID 603, dk-w-0.c.broad-ctsa.internal, executor 63): ExecutorLostFailure (executor 63 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 15.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; ```; This is due to `collectPerPartition` allowing regions to grow without bound.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3920:856,abort,aborted,856,https://hail.is,https://github.com/hail-is/hail/issues/3920,1,['abort'],['aborted']
Safety,"### Hail version:; `a230321`; ### What you did:; `mt.GT[1]` on a haploid call; ### What went wrong (all error messages here, including the full java stack trace):; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 3.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 3.0 (TID 10515, pbt-sw-nkpn.c.broad-mpg-gnomad.internal, executor 306): java.lang.IllegalArgumentException: requirement failed; at scala.Predef$.require(Predef.scala:212); at is.hail.variant.Call$.alleleByIndex(Call.scala:128); at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.BinaryFun.apply(Fun.scala:122); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3713:206,abort,aborted,206,https://hail.is,https://github.com/hail-is/hail/issues/3713,1,['abort'],['aborted']
Safety,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4938:1041,safe,safely,1041,https://hail.is,https://github.com/hail-is/hail/pull/4938,1,['safe'],['safely']
Safety,"### What happened?. After I ran the ""make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0"", I get the following error. > Configure project :; WARNING: Hail primarily tested with Spark 3.3.2, use other versions at your own risk. > Task :compileScala; [Error] /gpfs/fs1/home/jl/Hail2/hail/hail/src/main/scala/is/hail/HailContext.scala:127:21: value implOpMulMatrix_DMD_DVD_eq_DVD is not a member of object breeze.linalg.DenseMatrix; one error found. > Task :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; > Run with --info option to get more log output.; > Run with --scan to get full insights. BUILD FAILED in 4m 52s; 2 actionable tasks: 2 executed; make: *** [build/libs/hail-all-spark.jar] Error 1. ### Version. Hail 0.2.13. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14235:258,risk,risk,258,https://hail.is,https://github.com/hail-is/hail/issues/14235,1,['risk'],['risk']
Safety,"### What happened?. After sorting our costs into ""cost of goods"", ""operating expenses"", and ""capital expenses"", I realized there are four ""operating expenses"" that are not tracked and reported with the other expenses. I regressed these costs against the core-hours to estimate the cost per core-hour. resource | intercept (USD) | cost (USD/core-hour); -- | -- | --; GCP Support Variable fee | 3.46403 +- 0.49155 | 0.00123 +- 0.00007; System logs costs SKU#1 | 13.09991 +- 3.13991 | 0.00093 +- 0.00039; System logs costs SKU#2 | 7.87838 +- 0.81695 | 0.00027 +- 0.00012; Job specifications | 5.41150 +- 0.36608 | 0.00025 +- 0.00005; Firewall policy | 0.51216 +- 0.03185 | 0.00012 +- 0.00000. To fully recover the operating expenses at our current revenue, we need an additional 0.005 USD per core-hour (which is 0.002 more than the sum of intercepts). This issue is complete after we add a new product:. resource | cost (USD/core-hour); -- | --; support-logs-specs-and-firewall-fees/1 | 0.005. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13526:699,recover,recover,699,https://hail.is,https://github.com/hail-is/hail/issues/13526,1,['recover'],['recover']
Safety,"### What happened?. Although it is not possible to avoid all cross-region access (and thus costs), there are some obvious preventable misuses. For example, the following pipeline should error:. ```; b = hb.Batch(regions=['us-east1']); x = b.read_input('gs://bucket-in-central1/'); b.new_job(f'cat {x}'); b.run(); ```; But the following pipeline should not error:; ```; b = hb.Batch(regions=['us-east1']); x = b.read_input('gs://bucket-in-central1/'); j = b.new_job(f'cat {x}'); j.regions(['us-central1']); ```; The following should error because the job *could* be in us-east1:; ```; b = hb.Batch(regions=['us-east1', 'us-central1']); x = b.read_input('gs://bucket-in-central1/'); b.new_job(f'cat {x}'); b.run(); ```; The following should error:; ```; b = hb.Batch(regions=['us-east1']) # remote_tmpdir is set in config file as a us-centra1 bucket; j = b.new_job(f'echo hi > {j.f}'); j2 = b.new_job(f'cat {j.f}'); b.run(); ```. ### Version. 0.2.119. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13232:51,avoid,avoid,51,https://hail.is,https://github.com/hail-is/hail/issues/13232,1,['avoid'],['avoid']
Safety,"### What happened?. Double quote `""""` is frequently used to mean just one `""` when it appears inside a quoted field a la:. ```; a	b; 1	""""""""; ```; This contains one row whose value for column a is `1` and whose value for column b is `""`. The outer quotes are redundant indicators of the bounds of that column for that row. A less trivial case involves having a tab inside the column:. ```; a	b; 1	""	""""	""; ```; In this file, the b column's value in the first row has length three and consists of a tab, a quote character and a tab: `	""	`. Another test case. The `test.txt` contains:; ```; a	b	c; ""hello"",""a""""b"",""goodbye""; ```; This code,; ```python3; hl.import_table(""test.txt"", quote='""').collect(); ```; should return:; ```python3; [hl.Struct(a=""hello"", b=""a\""b"", c=""goodbye"")]; ```. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13563:258,redund,redundant,258,https://hail.is,https://github.com/hail-is/hail/issues/13563,1,['redund'],['redundant']
Safety,"### What happened?. Hail should support integer types like int8 & int16. In SEQR, we have sets of values that are known to range from 0 to a small fixed integer which would fit in 8 or 16 bytes. Using such a smaller type would avoid allocating unnecessary memory and may also improve size since the compressor need not clean up excess bytes. ### Version. 0.2.115. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13466:227,avoid,avoid,227,https://hail.is,https://github.com/hail-is/hail/issues/13466,1,['avoid'],['avoid']
Safety,"### What happened?. Hail's google/azure credential classes do not require the caller to specify scopes when requesting access tokens, and thus default to a [very wide set of scopes](https://github.com/hail-is/hail/blob/91f5a0bfc30927014b60b11a353a4d95db009427/hail/python/hailtop/aiocloud/aiogoogle/credentials.py#L140), making those access tokens excessively powerful. An access token does not need to have the `https://www.googleapis.com/auth/appengine.admin` scope to read a blob from GCS. This poses an unnecessary risk if such a token were leaked. These classes should instead require that scopes be specified when requesting an access token, and call sights should specify the minimum set of scopes necessary to perform their function. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13530:519,risk,risk,519,https://hail.is,https://github.com/hail-is/hail/issues/13530,1,['risk'],['risk']
Safety,"### What happened?. In particular, VDS is doubly sparse:; 1. Sparse columns. Reference blocks are a form of run-length compression of homozygous reference calls.; 2. Sparse alleles. Local alleles is a form of compressed-sparse-column (albeit on arrays rather than matrices). All sparsely encoded allele-indexed fields share the same index array (the ""local alleles"" (`LA`) field). `hl.vds.to_dense_mt` only desifies the sparse columns, it *does not* densify the sparse alleles. This ticket is complete when:; 1. We have written a careful analysis, for ourselves, both of (1) the possible terms for describing sparse columns, sparse allele-indexed fields, densification of columns, and densification of allele-indexed fields as well as of (2) how to structure the code to avoid foot guns, make clear what is being densified, and facilitating selective densification of only what is necessary.; 2. We have merged a (short) RFC realizing part (1).; 3. We have merged a PR the realizes part (2).; 4. We have improved the documentation of the densification and local alleles methods to use this new language and to clearly describe what is and is not densified and the growth of size caused by these densifications. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14009:771,avoid,avoid,771,https://hail.is,https://github.com/hail-is/hail/issues/14009,1,['avoid'],['avoid']
Safety,"### What happened?. It seems that the local filesystem can, infrequently, stall when executing `rmtree`. Note that the error about the directory being non-empty is because we have a bug in `rm_dir`: we try to remove the directory even if the children tasks failed. It oddly seems to have happened on both a deploy batch and a PR batch:; - PR: https://ci.hail.is/batches/7706444/jobs/170; - deploy: https://ci.hail.is/batches/7707793/jobs/172. ```; [2023-08-02 05:33:14] test/hail/utils/test_hl_hadoop_and_hail_fs.py::test_hadoop_methods_3[local] PASSED; +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_1 (139802083059456) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_0 (139802091452160) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:590,Timeout,Timeout,590,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['Timeout'],['Timeout']
Safety,"### What happened?. Julia Sealock reported this https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/vep.20issue/near/352790173. We also saw it in test_dataproc. Cal also reported it.; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 4.0 failed 20 times, most recent failure: Lost task 56.19 in stage 4.0 (TID 48622) (jsealock-schema-sw-43bq.c.daly-neale-sczmeta.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 125; VEP Error output:; docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.; See 'docker run --help'. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.methods.VEP$.waitFor(VEP.scala:73); 	at is.hail.methods.VEP.$anonfun$execute$5(VEP.scala:231); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:69); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collectio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:254,abort,aborted,254,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['abort'],['aborted']
Safety,"### What happened?. Most stored procedures take either a shared or exclusive lock on a relevant row of the `jobs` table near the start of the procedure, but not all. This appears to interact poorly with the `attempts_after_update` trigger as it attempts to take an exclusive lock on rows in the `jobs` table in the below join with the attempt resources tables. It's not clear exactly what the right fix is. It should be simple enough not to join on the jobs table in the `FOR UPDATE`, but we should also evaluate when in our various transactions a lock should be taken on the jobs table and whether it should be an X or S lock. ### Version. 0.2.128. ### Relevant log output. ```shell; ------------------------; LATEST DETECTED DEADLOCK; ------------------------; 2024-02-29 15:07:05 140331971655424; *** (1) TRANSACTION:; TRANSACTION 2486515, ACTIVE 0 sec inserting; mysql tables in use 27, locked 27; LOCK WAIT 16 lock struct(s), heap size 1128, 9 row lock(s), undo log entries 1; MySQL thread id 703, OS thread handle 140330830395136, query id 4745489 10.32.3.39 dgoldste-batch-user executing; INSERT INTO aggregated_billing_project_user_resources_v3 (billing_project, user, resource_id, token, `usage`); SELECT cur_billing_project, cur_user,; attempt_resources.deduped_resource_id,; rand_token,; msec_diff_rollup * quantity; FROM attempt_resources; WHERE attempt_resources.batch_id = NEW.batch_id AND attempt_resources.job_id = NEW.job_id AND attempt_id = NEW.attempt_id; FOR UPDATE; ON DUPLICATE KEY UPDATE `usage` = aggregated_billing_project_user_resources_v3.`usage` + msec_diff_rollup * quantity. *** (1) HOLDS THE LOCK(S):; RECORD LOCKS space id 351 page no 4 n bits 248 index PRIMARY of table `dgoldste-batch`.`instances_free_cores_mcpu` trx id 2486515 lock_mode X locks rec but not gap; Record lock, heap no 176 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (total ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14380:718,DETECT,DETECTED,718,https://hail.is,https://github.com/hail-is/hail/issues/14380,1,['DETECT'],['DETECTED']
Safety,"### What happened?. Notify these threads on completion:; - https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/exporting.20sites.20only.20VCF/near/376801844. Using QoB, reading out of GCS, we encounter corrupted blocks on this simple pipeline. ```; Caused by: com.github.luben.zstd.ZstdException: Corrupted block detected; at com.github.luben.zstd.ZstdDecompressCtx.decompressByteArray(ZstdDecompressCtx.java:157) ~[zstd-jni-1.5.2-1.jar:1.5.2-1]; at com.github.luben.zstd.Zstd.decompressByteArray(Zstd.java:409) ~[zstd-jni-1.5.2-1.jar:1.5.2-1]; at is.hail.io.ZstdInputBlockBuffer.readBlock(InputBuffers.scala:649) ~[gs:__hail-query-ger0g_jars_f00f916faf783b89cc2fc00bfc3e39df5485d8b0.jar.jar:0.0.1-SNAPSHOT]; at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:384) ~[gs:__hail-query-ger0g_jars_f00f916faf783b89cc2fc00bfc3e39df5485d8b0.jar.jar:0.0.1-SNAPSHOT]; at is.hail.io.BlockingInputBuffer.readByte(InputBuffers.scala:402) ~[gs:__hail-query-ger0g_jars_f00f916faf783b89cc2fc00bfc3e39df5485d8b0.jar.jar:0.0.1-SNAPSHOT]; ....; ```. A simplified version of the script:. ```python3; import hail as hl; import gnomad.utils.sparse_mt. tmp_dir = 'gs://bucket/'; vds_file = 'gs://neale-bge/bge-wave-1.vds'; out = 'gs://bucket/foo.vcf.bgz'. hl.init(default_reference = 'GRCh38',; tmp_dir = tmp_dir). vds = hl.vds.read_vds(vds_file); mt = hl.vds.to_dense_mt(vds); t = gnomad.utils.sparse_mt.default_compute_info(mt); t = t.annotate(info=t.info.drop('AS_SB_TABLE')); t = t.annotate(info = t.info.drop(; 'AS_QUALapprox', 'AS_VarDP', 'AS_SOR', 'AC_raw', 'AC', 'AS_SB'; )); t = t.drop('AS_lowqual'). hl.methods.export_vcf(dataset = t, output = out, tabix = True); ```. [batch-7751958-2713-main.log](https://github.com/hail-is/hail/files/12314207/batch-7751958-2713-main.log). ### Version. 0.2.120. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/Users/rye/Projects/VQSR/formatting-VQSR-vcf.py"", line 102, in <module>; main(args); File ""/Users/rye/Proj",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:335,detect,detected,335,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['detect'],['detected']
Safety,"### What happened?. On the driver (but this could happen anywhere), a `read` call failed in GoogleStorageFS. In particular line 205:; ```; if (reader != null) {; reader.read(bb); } else {; ```; We don't retry transient errors here or below in the other call to `read`. We only retry on the initial creation of the stream. I think we are concerned that the stream is in a bad state, possible advanced a few bytes. If we were to read from it, we might drop some data. The safe thing to do is to `seek` to the correct position. This will likely initiate a new HTTP request to GCS, which is fine, because we almost certainly lost the old connection due to the transient error. I also think we need to remove `lazyPosition`. I think we can achieve the requester pays nonsense by just relying on the `pos` from the parent class (see FS.scala). ### Version. 0.2.115-71fc978b5c22. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/usr/local/lib/python3.10/site-packages/reanalysis/summarise_clinvar_entries.py"", line 531, in <module>; main(subs=args.s, date=processed_date, variants=args.v, out=args.o); File ""/usr/local/lib/python3.10/site-packages/reanalysis/summarise_clinvar_entries.py"", line 505, in main; parse_into_table(json_path=temp_output, out_path=out); File ""/usr/local/lib/python3.10/site-packages/reanalysis/summarise_clinvar_entries.py"", line 439, in parse_into_table; ht.write(out_path, overwrite=True); File ""<decorator-gen-1106>"", line 2, in write; File ""/usr/local/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/usr/local/lib/python3.10/site-packages/hail/table.py"", line 1392, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/usr/local/lib/python3.10/site-packages/hail/backend/service_backend.py"", line 490, in execute; return self._cancel_on_ctrl_c(self._async_execute(ir, timed=timed)); File",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:470,safe,safe,470,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['safe'],['safe']
Safety,"### What happened?. One of our unit tests recently changed from taking around 20 seconds to being aborted by a time out after six hours — see populationgenomics/production-pipelines#352. This change turned out to coincide with the release of hail 0.2.113 and the unit test's `pip` selecting the new release. PR #12780 added a recursive `add_dependents` function to `LocalBackend`, that appears to be used to compute the transitive dependencies of each job. Profiling our unit test indicates that it is spending six hours inside this function with no end in sight. Running the job locally for a few seconds with more logging shows that it is calling `add_dependents` with the same `ancestor` and `child` millions of times. I'm not sure whether it's in an actual infinite loop or “merely” a combinatorial disaster than might terminate after a few months of runtime…. The following change, for example,. ```diff; --- a/hail/python/hailtop/batch/backend.py; +++ b/hail/python/hailtop/batch/backend.py; @@ -268,7 +268,7 @@ class LocalBackend(Backend[None]):; def add_dependents(ancestor, child):; dependent_jobs[ancestor].add(child); for ancestor_parent in ancestor._dependencies:; - add_dependents(ancestor_parent, child); + if child not in dependent_jobs[ancestor_parent]: add_dependents(ancestor_parent, child); ; for j in jobs:; for parent in j._dependencies:; ```. reduces it to calling it only once or twice for each `ancestor`/`child` combination, and returns the unit test to completing in ~20 seconds. I am not familiar enough with the data structure to say if that is a correct fix, but something of this nature appears to be needed to return this transitive dependency computation to a sensible runtime. ### Version. 0.2.113. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12915:98,abort,aborted,98,https://hail.is,https://github.com/hail-is/hail/issues/12915,1,['abort'],['aborted']
Safety,"### What happened?. See [Batch Metadata Server RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst) for background. The objective of this issue is to fully remove GSA key files from Batch job filesystems, preventing possible exfiltration of long-lived credentials. Each remaining task should get its own issue if there isn't already one. Breakdown of tasks:. - [X] Implement a Batch metadata server and expose it in GCP `DockerJob`s (#14019); - [ ] Add metadata server support for `JVMJob`s aka Query-on-Batch in GCP (#14487); - [ ] Add metadata server support in Azure; - [ ] Deprecate and remove support for key files in `DockerJob`s; - [ ] Deprecate and remove support for key files in `JVMJob`s. This requires dropping support for old versions of hail that depend on the key file (up to and including at least 0.2.130). These steps get us past the security milestone of not exposing GSA key files to jobs and risking exfiltration. We might be able to go even further and get rid of key files entirely, which would reduce our operational burden of securing and rotating them.; - [ ] In GCP, use Service Account Impersonation to have the Batch Worker identity impersonate user GSAs, allowing it to create metadata server access tokens without the key files themselves; - [ ] In Azure, investigate if something like the above is even possible. At time of writing, it does not appear that there is an alternative other than storing credentials or adding users to the VM's metadata server. It is unclear whether this can be done dynamically and with what frequency and feels like not their intended use case. ### Version. 0.2.130. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14486:943,risk,risking,943,https://hail.is,https://github.com/hail-is/hail/issues/14486,1,['risk'],['risking']
Safety,"### What happened?. Since we guarantee a job will run at least once, there are two issues that can happen:. 1. A user can write a pipeline in which two jobs race to write the same file, e.g.; ```; j = b.new_job(); j.command('echo hello > {j.out}'); j.write_output(j.out, ""gs://bucket/final-output""); ```; 2. Or, a clever user can avoid this race with some randomness:; ```; j = b.new_job(); j.command('echo hello gsutil cp - gs://bucket/final-output-$RANDOM'); ```. The former is a really common pattern and a bit of a footgun! The latter is rare (I don't know anyone who does it) and hard to work with: how would you know the output file of the *successful* attempt?. Hail should provide some mechanism for a user to get the list of successful attempts and their outputs. One simple option is to include some kind of seeded randomness which the user can access and to return either the seed or all the draws of the successful attempt for each job in `/jobs` or for the one job in `/job/{job_id}`. For example, consider:. ```; j = b.new_job(); j.command('echo hello gsutil cp - gs://bucket/final-output-$(/hail-random-str)'); ```. Where `/hail-random-str` is a binary we mount into the container that randomly generates numbers seeded by `(batch id, job id, attempt id)`. Hail should use the same randomness to ensure that `write_output` is reliable. We might also want a way to automatically remove the output files of the non-successful (e.g. preempted) attempts. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13502:330,avoid,avoid,330,https://hail.is,https://github.com/hail-is/hail/issues/13502,1,['avoid'],['avoid']
Safety,"### What happened?. Struct decoding currently uses `Region.loadBit` which:; 1. Calculates the address of the byte has this bit (e.g. the 65th bit is in the second byte).; 2. Loads the byte out of memory.; 3. Masks the bit out of the byte.; 4. Compares to zero. We don't have concrete data, but we suspect that the JVM can't avoid loading the byte out of memory 8 times. If we can instead load it once per 8 missing fields, there may be a speed up for structs that are frequently decoded (e.g. an entry struct). ### See also. - https://github.com/hail-is/hail/issues/13792#issuecomment-1761652107 . ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13811:324,avoid,avoid,324,https://hail.is,https://github.com/hail-is/hail/issues/13811,1,['avoid'],['avoid']
Safety,"### What happened?. The basic problem is:; 1. The type for `ArrayMaximalIndependentSet` is `TArray(...)` where `...` is whatever the node type is. ; 2. We choose a PType based on the Type.; 3. We choose an SType based on the PType.; 4. `unwrapReturn` makes an incorrect assumption about which SType corresponds to a `TArray(String)`. In particular,; ```; Code.invokeScalaObject1[UnsafeIndexedSeq, IndexedSeq[Any]](Graph.getClass, ""maximalIndependentSet"", jEdges); ```; returns a Java array of whatever `svalueToJavaValue` returns. In that case, that's a `String[]` which we call `SJavaArrayString`. However, the SType chosen for `TArray(TString)` is `SIndexablePointer(SBinary)`. **I think the real fix here is to just pass region pointers into MaximalIndependentSet.** Just get an `elementIterator` from `PCanonicalArray` and use `loadElement`, etc. to populate the `Graph`. ```; In [1]: import hail as hl; ...: ht = hl.Table.parallelize([hl.Struct(i='A', j='B', kin=0.25), hl.Struct(i='A', j='C', kin=0.25), hl.Struct(i='D', j='E', kin=0.5)]); ...: hl.maximal_independent_set(ht.i, ht.j, False).collect(); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); Cell In[1], line 3; 1 import hail as hl; 2 ht = hl.Table.parallelize([hl.Struct(i='A', j='B', kin=0.25), hl.Struct(i='A', j='C', kin=0.25), hl.Struct(i='D', j='E', kin=0.5)]); ----> 3 hl.maximal_independent_set(ht.i, ht.j, False).collect(). File <decorator-gen-1148>:2, in collect(self, _localize, _timed). File ~/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py:584, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 581 @decorator; 582 def wrapper(__original_func, *args, **kwargs):; 583 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 584 return __original_func(*args_, **kwargs_). File ~/miniconda3/lib/python3.10/site-packages/hail/table.py:2162, in Table.collect(self, _localize, _timed);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:379,Unsafe,UnsafeIndexedSeq,379,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['Unsafe'],['UnsafeIndexedSeq']
Safety,"### What happened?. The following fails:; ```; import hail as hl; hl.init(); ````; with the error:; ```; ImportError: cannot import name 'getargspec' from 'inspect' (/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py); ```; when running Python 3.11. The code importing `getargspec` is the Parsimonious library (see stacktrace below). ### Version. 0.2.109. ### Relevant log output. ```shell; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); Cell In[1], line 1; ----> 1 import hail as hl; 2 hl.init(). File /usr/local/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages/hail/__init__.py:33; 14 __doc__ = r""""""; 15 __ __ <>__; 16 / /_/ /__ __/ /; (...); 27 To report a bug, please open an issue: https://github.com/hail-is/hail/issues; 28 """"""; 30 # F403 'from .expr import *' used; unable to detect undefined names; 31 # F401 '.expr.*' imported but unused; 32 # E402 module level import not at top of file; ---> 33 from .table import Table, GroupedTable, asc, desc # noqa: E402; 34 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 35 from .expr import * # noqa: F401,F403,E402. File /usr/local/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages/hail/table.py:8; 5 import pyspark; 6 from typing import Optional, Dict, Callable, Sequence; ----> 8 from hail.expr.expressions import Expression, StructExpression, \; 9 BooleanExpression, expr_struct, expr_any, expr_bool, analyze, Indices, \; 10 construct_reference, to_expr, construct_expr, extract_refs_by_indices, \; 11 ExpressionException, TupleExpression, unify_all, NumericExpression, \; 12 StringExpression, CallExpression, CollectionExpression, DictExpression, \; 13 IntervalExpression, LocusExpression, NDArrayExpression, expr_stream; 14 from hail.expr.types import hail_type, tstruct, types_match, tarray, tset, dtypes_from_pandas; 15 from hail.expr.table_type import ttable. Fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12759:928,detect,detect,928,https://hail.is,https://github.com/hail-is/hail/issues/12759,1,['detect'],['detect']
Safety,"### What happened?. Try running a job with `_machine_type: 'n1-highmem-64'`. This is necessary to get enough memory for some larger jobs (> ~200GB). Startup on the batch worker fails because the job is calculating how many theoretical network namespaces it could support (4 per CPU, 64 CPUS, plus some for JVMs), but not considering that the IPv4 schema puts a hard limit of 255 on namespaces if only one subnet value is changing each time. ### Version. Live 7/30/24. ### Relevant log output. _No response_. ### Security considerations:. Low risk of impacting security. High CPU machine types are not materially different from others with respect to security considerations, and the bug is a simple logic error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14644:542,risk,risk,542,https://hail.is,https://github.com/hail-is/hail/issues/14644,1,['risk'],['risk']
Safety,### What happened?. Users need a way to control their risk tolerance for preemptions. ### Version. 0.2.120. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13395:54,risk,risk,54,https://hail.is,https://github.com/hail-is/hail/issues/13395,1,['risk'],['risk']
Safety,"### What happened?. We appear to have lost the run_until_done_or_deleted on [borrow_jvm](https://github.com/hail-is/hail/pull/11397/files#diff-f8ba97f763395908a5b67f47a630c98e8d223ca5914f18d588f405d629d52197L1703-R1811) and [download_jar](https://github.com/hail-is/hail/pull/11397/files#diff-f8ba97f763395908a5b67f47a630c98e8d223ca5914f18d588f405d629d52197L1724-R1832). We should understand why we made this change (is there some bad interaction?) and determine how to rectify it. It seems to me that, basically any `await` during the runtime of a job must be able to abort because the job could be cancelled at any time. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13950:569,abort,abort,569,https://hail.is,https://github.com/hail-is/hail/issues/13950,1,['abort'],['abort']
Safety,"### What happened?. We have reported to their GitHub, but we don't have a simple enough repro for them to make progress. https://github.com/Azure/azure-sdk-for-java/issues/35125. Personal correspondence with some MSFT researchers suggested there could be an issue with threading:; > It sort of reminds me of an issue we saw with Cromwell where their old akka pool code caused a bunch of unexpected network behavior that broke their API in certain cases. I've asked if BlobServiceClient is thread-safe or not. We share an object of that class, but none of the things it produces (e.g. blobs). We know that the java.io libraries can improperly drop an HTTP response if it is followed by a TCP RST. In particular, we've seen this happen when a server is load shedding and sends an HTTP ""429 Too Many Requests"" rapidly followed by a TCP RST. This might explain the ""Connection reset"" errors that we sometimes see. We have fewer intuitions about the ""Stream is already closed"". That specific error was reported to Azure in the aforementioned GitHub issue. We treat both stream is closed and connection reset as ""limited retry"" errors. We might retry too quickly. Our initial delay is `100ms * x` where `x` is drawn uniformly from `[0, 1]`. Perhaps we should try an initial delay of at least 1s? . For example, [Azure gives as an example retrying after 2s, 4s, 10s, and 30s](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-performance-checklist#timeout-and-server-busy-errors). Google's [code examples](https://cloud.google.com/storage/docs/retry-strategy#client-libraries_1) suggest an initial delay of 1s with a multiplier of 2. AWS seems to use 500ms as the [default base backoff for ""throttled"" exceptions](https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-core/src/main/java/com/amazonaws/retry/PredefinedBackoffStrategies.java#L39). ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13351:496,safe,safe,496,https://hail.is,https://github.com/hail-is/hail/issues/13351,2,"['safe', 'timeout']","['safe', 'timeout-and-server-busy-errors']"
Safety,"### What happened?. When the QoB client on a user's laptop sends a request to create a QoB job, it sends a `jar_spec` parameter as part of the job spec that is either:; - `git_revision`: the git SHA that the hail was built with. The Batch front end takes this and resolves a URL for the published JAR that was created when that commit was merged to `main`.; - `jar_url`: A blob storage URL that points directly to the JAR to use. The Batch front end ensures that this URL is trusted. The `jar_url` setting is mainly for development and debugging purposes, allowing a dev or user to set a URL to a development JAR instead of using a merged commit. In normal configuration fashion, it is possible to set `jar_url` in `hailctl config`. This is an enormous footgun, as users may forget to unset this configuration and continue using the dev jar *even after they install a different hail wheel*. We must do two things:; 1. Remove the ability to set the jar_url through `hailctl` so as to avoid this footgun. Batch should also fully remove support for `jar_url`s so that any users who might be inadvertently using it are loudly alerted (though I suspect there are few if any such users now).; 2. Remove entirely the ability to specify a JAR other than that which was built along with the installed wheel. The proposed plan is to always send `git_revision` for QoB jobs. In order to enable development JARs, Batch should be augmented to search first for production JARs matching a certain revision, and then if that fails search a specified `dev/` subdirectory for the requested revision. These development JARs should not be cached on workers so as to enable debugging development without constant committing. ### Version. 0.2.130. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14539:983,avoid,avoid,983,https://hail.is,https://github.com/hail-is/hail/issues/14539,1,['avoid'],['avoid']
Safety,"### What happened?. When the image cannot be pulled, the exception can trigger a FileNotFoundError reading the main container log. https://cloudlogging.app.goo.gl/5h9Q9MUG7KdZRVXN9. ### Version. 0.2.124. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 551, in pull; await docker_call_retry(; File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 840, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 460, in timed_out_f; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/lib/python3.9/asyncio/tasks.py"", line 479, in wait_for; return fut.result(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 484, in _pull_with_auth_refresh; return await docker.images.pull(image_ref_str, auth=credentials); File ""/usr/local/lib/python3.9/dist-packages/aiodocker/images.py"", line 133, in _handle_list; async with cm as response:; File ""/usr/local/lib/python3.9/dist-packages/aiodocker/utils.py"", line 309, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.9/dist-packages/aiodocker/docker.py"", line 275, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, 'Head ""https://us-docker.pkg.dev/v2/1/does-not-exist/manifests/latest"": denied: Permission ""artifactregistry.repositories.downloadArtifacts"" denied on resource ""projects/1/locations/us/repositories/does-not-exist"" (or it may not exist)'). The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 915, in run; await self.create(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 840, in create; await self._run_until_done_or_deleted(sel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13907:694,timeout,timeout,694,https://hail.is,https://github.com/hail-is/hail/issues/13907,1,['timeout'],['timeout']
Safety,"### What happened?. When using logistic regression, the null model tells me about the relationship between my covariates and the phenotype(s). In particular, if my covariates perfectly predict my phenotype, the model will fail to converge on every row. Investigating this situation demands access to the null model.; ```; import hail as hl; mt = hl.utils.range_matrix_table(3,3); mt = mt.annotate_entries(prod = mt.row_idx * mt.col_idx); hl.logistic_regression_rows('wald', y=[hl.bool(mt.col_idx)], x=mt.prod, covariates=[1.0]).describe(); ```. When using the Query-on-Spark backend, I receive no access to the null model parameters:; ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'row_idx': int32 ; 'logistic_regression': array<struct {; beta: float64, ; standard_error: float64, ; z_stat: float64, ; p_value: float64, ; fit: struct {; n_iterations: int32, ; converged: bool, ; exploded: bool; }; }> ; ----------------------------------------; Key: ['row_idx']; ----------------------------------------; ```. In contrast, the Query-on-Batch backend exposes this information:; ```; Global fields:; 'null_fits': array<struct {; b: ndarray<float64, 1>, ; score: ndarray<float64, 1>, ; fisher: ndarray<float64, 2>, ; mu: ndarray<float64, 1>, ; n_iterations: int32, ; log_lkhd: float64, ; converged: bool, ; exploded: bool; }> ; ```. The Query-on-Spark backend should expose the same information for the benefit of the user. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13789:185,predict,predict,185,https://hail.is,https://github.com/hail-is/hail/issues/13789,1,['predict'],['predict']
Safety,"### What happened?. `hailctl dataproc start` fails with an error message like the one below because [in Dataproc 2.2](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network#:~:text=Internal%20addresses%20only%20(no%2Daddress)%20is%20set%20by%20default%20when%20creating%20a%20Dataproc%202.2%20image%20version%20cluster.%20You%20can%20use%20the%20gcloud%20dataproc%20clusters%20create%20%2D%2Dpublic%2Dip%2Daddress%20flag%20to%20enable%20public%20IP%20addresses.), clusters are created without public internet access by default. A workaround is to pass the `--public-ip-address` flag to the command. Error message:. ```python; pip packages are ['setuptools', 'mkl<2020', 'lxml<5', 'https://github.com/hail-is/jgscm/archive/v0.1.13+hail.zip', 'ipykernel==6.22.0', 'ipywidgets==8.0.6', 'jupyter-console==6.6.3', 'nbconvert==7.3.1', 'notebook==6.5.6', 'qtconsole==5.4.2', 'aiodns==2.0.0', 'aiohttp==3.9.5', 'aiosignal==1.3.1', 'async-timeout==4.0.3', 'attrs==23.2.0', 'avro==1.11.3', 'azure-common==1.1.28', 'azure-core==1.30.2', 'azure-identity==1.17.1', 'azure-mgmt-core==1.4.0', 'azure-mgmt-storage==20.1.0', 'azure-storage-blob==12.20.0', 'bokeh==3.3.4', 'boto3==1.34.138', 'botocore==1.34.138', 'cachetools==5.3.3', 'certifi==2024.6.2', 'cffi==1.16.0', 'charset-normalizer==3.3.2', 'click==8.1.7', 'commonmark==0.9.1', 'contourpy==1.2.1', 'cryptography==42.0.8', 'decorator==4.4.2', 'deprecated==1.2.14', 'dill==0.3.8', 'frozenlist==1.4.1', 'google-auth==2.31.0', 'google-auth-oauthlib==0.8.0', 'humanize==1.1.0', 'idna==3.7', 'isodate==0.6.1', 'janus==1.0.0', 'jinja2==3.1.4', 'jmespath==1.0.1', 'jproperties==2.1.1', 'markupsafe==2.1.5', 'msal==1.29.0', 'msal-extensions==1.2.0', 'msrest==0.7.1', 'multidict==6.0.5', 'nest-asyncio==1.6.0', 'numpy==1.26.4', 'oauthlib==3.2.2', 'orjson==3.10.6', 'packaging==24.1', 'pandas==2.2.2', 'parsimonious==0.10.0', 'pillow==10.4.0', 'plotly==5.22.0', 'portalocker==2.10.0', 'protobuf==3.20.2', 'py4j==0.10.9.7', 'pyasn1==0.6.0', 'pyasn1-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14652:952,timeout,timeout,952,https://hail.is,https://github.com/hail-is/hail/issues/14652,1,['timeout'],['timeout']
Safety,"### What happened?. `hl.maximal_independent_set` should return the same independent set regardless of the ordering of the input table. gnomAD team reports that the returned set can differ depending on whether or not the input table had been written or came directly from PC-Relate. I have yet to create a simple reproducible example. Permuting the entries in this array does not change the output. I always get 'a' and 'b'. I suspect this is because what really matters is the order in which we traverse the entries of the multi map which depends on the hash of the nodes. I think a durable fix might be to eliminate the MultiMap, insert all the nodes into the binary heap, then increment priority for each edge detected. This will perform more reflows of the heap, but eliminates the non-determinism of MultiMap iteration order. ```; import hail as hl; ht = hl.Table.parallelize([; hl.Struct(i=hl.Struct(s=x[0]), j=hl.Struct(s=x[1])); for x in [('c', 'a'), ('a', 'b'), ('b', 'c'), ]; ]); hl.maximal_independent_set(ht.i, ht.j, False).collect(); ```. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13635:712,detect,detected,712,https://hail.is,https://github.com/hail-is/hail/issues/13635,1,['detect'],['detected']
Safety,"### What happened?. https://hail.zulipchat.com/#narrow/stream/128581-Cloud-support/topic/Inefficient.20computing.20in.20AoU.20workbench. At least the first one seems to be a genuine missed optimization in Hail; ```; import hail as hl; import os; bucket = os.getenv(""WORKSPACE_BUCKET""); vds_srwgs_path = os.getenv(""WGS_VDS_PATH""); vds = hl.vds.read_vds(vds_srwgs_path); vds = hl.vds.split_multi(vds, filter_changed_loci=False); vmt = vds.variant_data; vht = vmt.rows(); vht = vht.select('filters'); vht.write(f'{bucket}/aou_vat_with_filter_wlu.ht', overwrite=True); ```; The `vmt.rows()` should have avoided all entry-level work. This should really just explode the alleles array and write that to a file. That should be relatively quick. We should be able to reproduce this on any VDS we have, and see that the IR we actually execute still references the entry data. . ### Version. 0.2.107-2387bb00ceee. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13312:599,avoid,avoided,599,https://hail.is,https://github.com/hail-is/hail/issues/13312,1,['avoid'],['avoided']
Safety,"#13008 Started using the ci-utils from the CI pipeline for the database jobs, but we actually can't use it safely for the database cleanup step because we might untag the image before the database cleanup jobs run.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13014:107,safe,safely,107,https://hail.is,https://github.com/hail-is/hail/pull/13014,1,['safe'],['safely']
Safety,"#450</a>)</li>; <li><a href=""https://github.com/googleapis/python-logging/commit/97e32b67603553fe350b6327455fc9f80b8aa6ce""><code>97e32b6</code></a> fix: allow reading logs from non-project paths (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/444"">#444</a>)</li>; <li><a href=""https://github.com/googleapis/python-logging/commit/a760e02371a55d6262e42de9e0222fffa2c7192b""><code>a760e02</code></a> feat: add json_fields extras argument for adding to jsonPayload (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/447"">#447</a>)</li>; <li><a href=""https://github.com/googleapis/python-logging/commit/83d9ca8521fe7c470bb6755a48a97496515d7abc""><code>83d9ca8</code></a> feat!: make logging API more friendly to use (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/422"">#422</a>)</li>; <li><a href=""https://github.com/googleapis/python-logging/commit/818213e143d6a1941211a48e0b23069a426ac300""><code>818213e</code></a> feat: avoid importing grpc when explicitly disabled (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/416"">#416</a>)</li>; <li><a href=""https://github.com/googleapis/python-logging/commit/e1506fa9030776353878048ce562c53bf6ccf7bf""><code>e1506fa</code></a> fix!: api consistency between HTTP and Gapic layers (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/375"">#375</a>)</li>; <li><a href=""https://github.com/googleapis/python-logging/commit/6fa17735fe3edb45483ec5e3abd1f53c24ffa881""><code>6fa1773</code></a> feat!: support string-encoded json (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/339"">#339</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/python-logging/compare/v1.12.1...v3.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11574:13203,avoid,avoid,13203,https://hail.is,https://github.com/hail-is/hail/pull/11574,1,['avoid'],['avoid']
Safety,"#4659 teaches CI to recover from a build job gone missing, but; I neglected to teach CI how to recover from a deploy job gone; missing. This follows the same strategy but for deploy jobs. If a deploy job is not found in the list of refreshed jobs; it is simply removed from the deploy_jobs map. The next heal; stage of CI will kick off a new batch job for whatever the; latest undeployed SHA is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4683:20,recover,recover,20,https://hail.is,https://github.com/hail-is/hail/pull/4683,2,['recover'],['recover']
Safety,"#9634 Introduced a large performance regression in the `linear_regression_rows_nd` benchmark (making it about 4x slower). This PR fixes that by doing two things:. 1. Move all the global into one single `annotate_globals` expression, so that CSE can work properly. This required fixing a bug in some ndarray expressions that were not correctly tracking their source tables. To make sure I was only referencing the global versions of this computed things, rather accidentally recomputing, I wrapped the global setup in a function to scope the variables. This improvement was minor, didn't hit the real root of the problem. 2. Much more significantly, and not 100% clear why: `process_y_group` is now a function that returns a python dictionary, instead of a hail struct. I can guess that the allocation required by making a struct was wasteful, but it seems crazy that it was ""make the benchmark 4x slower"" amounts of wasteful. . While this is not user facing yet, would be good to get this in before an eventual 0.2.60 release if we want to avoid benchmarks regressing between versions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9666:1040,avoid,avoid,1040,https://hail.is,https://github.com/hail-is/hail/pull/9666,1,['avoid'],['avoid']
Safety,$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:3227,abort,abortStage,3227,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['abort'],['abortStage']
Safety,$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/rwalters-hail-tmp/o?name=merged_round2_sumstats.fix_lowconf.mt/entries/rows/parts/part-15801-2fde3786-67cb-42ed-8aac-f900cfcc4c00&uploadType=resumable&upload_id=ADPycduMEzX6d_uX4CiP6_XItJKmP8UnUnYBfyPoselMbyLUkxs1wDLPnxWl5gXr5LnBaVntYR_i7jchyxgVsRb_5PknvcCIcfDJ; chunkOffset: 16777216; chunkLength: 0; localOffset: 0; remoteOffset: 16777216; lastChunk: false. at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); at java.util.concurrent.Executo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:2296,recover,recover,2296,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['recover'],['recover']
Safety,$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/120:4475,abort,abortStage,4475,https://hail.is,https://github.com/hail-is/hail/issues/120,3,['abort'],['abortStage']
Safety,$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:5864,abort,abortStage,5864,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['abort'],['abortStage']
Safety,$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201647,abort,abortStage,201647,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['abort'],['abortStage']
Safety,"&quot;+&quot; by their names contributed a patch for the first time.; This list of names is automatically generated, and may not be fully complete.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/scipy/scipy/commit/656076ca6b490f587e9bd9c4cd10cb259a687c5b""><code>656076c</code></a> MAINT: wheel push 1.9.2 [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/ad0d0f907010fbc8b66cdbe8ce0af2683881a309""><code>ad0d0f9</code></a> REL: set 1.9.2 released [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/d9ad9801323653a2015b4d3e80d6d3ea93b6c021""><code>d9ad980</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17150"">#17150</a> from tylerjereddy/treddy_scipy_192_more_backports</li>; <li><a href=""https://github.com/scipy/scipy/commit/6b098c25223e224ff44101f86bbc86efecffe1d9""><code>6b098c2</code></a> TST: optimize.milp: remove problematic timeout/iteration test</li>; <li><a href=""https://github.com/scipy/scipy/commit/24dce9760b87934f1be046ec817c758b0f3952dc""><code>24dce97</code></a> DOC: stats.pearsonr: typo in coeffic<em>i</em>ent (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17153"">#17153</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/a6ba7cad3b54c35d2ccb55c595691689004742c1""><code>a6ba7ca</code></a> MAINT: misc 1.9.2 updates</li>; <li><a href=""https://github.com/scipy/scipy/commit/ed9760e60a28b8f13e5644494033e2dab9aafbcd""><code>ed9760e</code></a> MAINT: stats.pearson3: fix ppf for negative skew (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17055"">#17055</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/6fb67007dd7105755057f3379fb7ef423eae524e""><code>6fb6700</code></a> FIX: optimize.milp: return feasible solution if available on timeout/node lim...</li>; <li><a href=""https://github.com/scipy/scipy/commit/bcfce27fc061cbde6ac6531799362e0420ea4796""><code>bcfce27</cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12352:1965,timeout,timeout,1965,https://hail.is,https://github.com/hail-is/hail/pull/12352,1,['timeout'],['timeout']
Safety,"': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979cc310>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979cce10>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; ERROR: Could not install packages due to an OSError: HTTPSConnectionPool(host='github.com', port=443): Max retries exceeded with url: /hail-is/jgscm/archive/v0.1.13+hail.zip (Caused by ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979ce290>, 'Connection to github.com timed out. (connect timeout=15)')). Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 61, in <module>; safe_call(*command); File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 17, in safe_call; raise e; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 14, in safe_call; sp.check_output(args, stderr=sp.STDOUT, **kwargs); File ""/opt/conda/default/lib/python3.11/subprocess.py"", line 466, in check_output; return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/opt/conda/default/lib/python3.11/subprocess.py"", line 571, in run; raise CalledProcessError(retcode, process.args,; subprocess.CalledProcessError: Command '('pip', 'install', 'setuptools', 'mkl<2020', 'lxml<5', 'https://github.com/hail-is/jgscm/archive/v0.1.13+hail.zip', 'ipykernel==6.22.0', 'ipywidgets==8.0.6', 'jupyter-console==6.6.3', 'nbconvert==7",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14652:4566,timeout,timeout,4566,https://hail.is,https://github.com/hail-is/hail/issues/14652,1,['timeout'],['timeout']
Safety,'may or may not' is redundant phrasing. The word 'may' is sufficient to indicate the optional nature of glob expressions in the `path` argument to `import_vcf`. ## Security Assessment; - This change has no security impact. ### Impact Description; Docs only,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14746:20,redund,redundant,20,https://hail.is,https://github.com/hail-is/hail/pull/14746,1,['redund'],['redundant']
Safety,"'s the only event loop that will exist forever. Pytest (and newer version of IPython, afaict) violate this pretty liberally. ~~pytest_asyncio has [explicit instructions on how to run every test in the same event loop](https://pytest-asyncio.readthedocs.io/en/latest/how-to-guides/run_session_tests_in_same_loop.html). I've implemented those here.~~ [These instructions don't work](https://github.com/pytest-dev/pytest-asyncio/issues/744). It seems that the reliable way to ensure we're using one event loop everywhere is to use pytest-asyncio < 0.23 and to define an event_loop fixture with scope `'session'`. I also switched test_batch.py into pytest-only style. This allows me to use session-scoped fixtures so that they exist exactly once for the entire test suite execution. Also:; - `RouterAsyncFS` methods must either be a static method or an async method. We must not create an FS in a sync method. Both `parse_url` and `copy_part_size` now both do not allocate an FS.; - `httpx.py` now eagerly errors if the running event loop in `request` differs from that at allocation time. Annoying but much better error message than this nonsense about timeout context managers.; - `hail_event_loop` either gets the current thread's event loop (running or not, doesn't matter to us) or creates a fresh event loop and sets it as the current thread's event loop. The previous code didn't guarantee we'd get an event loop b/c `get_event_loop` fails if `set_event_loop` was previously called.; - `conftest.py` is inherited downward, so I lifted fixtures out of test_copy.py and friends and into a common `hailtop/conftest.py`; - I added `make -C hail pytest-inter-cloud` for testing the inter cloud directory. You still need appropriate permissions and authn.; - I removed extraneous pytest.mark.asyncio since we use auto mode everywhere.; - `FailureInjectingClientSession` creates an `aiohttp.ClientSession` and therefore must be used while an event loop is running. Easiest fix was to make the test async.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14097:1334,timeout,timeout,1334,https://hail.is,https://github.com/hail-is/hail/pull/14097,1,['timeout'],['timeout']
Safety,"().write(); ```. ```; Traceback (most recent call last):; File ""/tmp/a913d6ce5b814a63ad7af31060416237/pyscripts_Xr0D99.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/a913d6ce5b814a63ad7af31060416237/generate_qc_annotations.py"", line 247, in main; generate_call_stats(mt).write(annotations_mt_path(data_type, 'call_stats'), args.overwrite); File ""<decorator-gen-556>"", line 2, in write; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/matrixtable.py"", line 2027, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: IllegalArgumentException: requirement failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 1.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 1.0 (TID 10060, exomes3-sw-dfpw.c.broad-mpg-gnomad.internal, executor 134): java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:212); 	at is.hail.variant.Call$.alleleByIndex(Call.scala:128); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465:1606,abort,aborted,1606,https://hail.is,https://github.com/hail-is/hail/issues/3465,1,['abort'],['aborted']
Safety,(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RV,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:11091,abort,abortStage,11091,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['abort'],['abortStage']
Safety,(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:3167,abort,abortStage,3167,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['abort'],['abortStage']
Safety,(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:5128,abort,abortStage,5128,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['abort'],['abortStage']
Safety,(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2756,abort,abortStage,2756,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['abort'],['abortStage']
Safety,(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:9033,abort,abortStage,9033,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['abort'],['abortStage']
Safety,"(also safer, it was totally possible to inadvertently segfault with the previous version, this one guards against that)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9981#issuecomment-772624116:6,safe,safer,6,https://hail.is,https://github.com/hail-is/hail/pull/9981#issuecomment-772624116,1,['safe'],['safer']
Safety,"(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (139802248206080) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ------------------------------ live log teardown -------------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 352, in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 162, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/python3.9/asyncio/futures.py"", line 201, in result; raise self._exception; File ""/usr/lib/python3.9/concurrent/futures/th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:2564,Timeout,Timeout,2564,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['Timeout'],['Timeout']
Safety,"(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 168, in _new_conn\n self, \""Failed to establish a new connection: %s\"" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 449, in send\n timeout=timeout\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n _stacktrace=sys.exc_info()[2])\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\"", line 399, in increment\n raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6754:2523,timeout,timeout,2523,https://hail.is,https://github.com/hail-is/hail/issues/6754,2,['timeout'],['timeout']
Safety,"(https://github.com/pytest-dev/pytest) from 6.2.5 to 7.0.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pytest-dev/pytest/releases"">pytest's releases</a>.</em></p>; <blockquote>; <h2>7.0.1</h2>; <h1>pytest 7.0.1 (2022-02-11)</h1>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9608"">#9608</a>: Fix invalid importing of <code>importlib.readers</code> in Python 3.9.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9610"">#9610</a>: Restore [UnitTestFunction.obj]{.title-ref} to return unbound rather than bound method.; Fixes a crash during a failed teardown in unittest TestCases with non-default [__init__]{.title-ref}.; Regressed in pytest 7.0.0.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9636"">#9636</a>: The <code>pythonpath</code> plugin was renamed to <code>python_path</code>. This avoids a conflict with the <code>pytest-pythonpath</code> plugin.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9642"">#9642</a>: Fix running tests by id with <code>::</code> in the parametrize portion.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9643"">#9643</a>: Delay issuing a <code>~pytest.PytestWarning</code>{.interpreted-text role=&quot;class&quot;} about diamond inheritance involving <code>~pytest.Item</code>{.interpreted-text role=&quot;class&quot;} and; <code>~pytest.Collector</code>{.interpreted-text role=&quot;class&quot;} so it can be filtered using <code>standard warning filters &lt;warnings&gt;</code>{.interpreted-text role=&quot;ref&quot;}.</li>; </ul>; <h2>7.0.0</h2>; <h1>pytest 7.0.0 (2022-02-03)</h1>; <p>(<strong>Please see the full set of changes for this release also in the 7.0.0rc1 notes below</strong>)</p>; <h2>Deprecations</h2>; <ul>; <li>; <p><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9488",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:985,avoid,avoids,985,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['avoid'],['avoids']
Safety,"(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'adj'}). per_sample = per_sample.annotate(adj=adj_per_sample[per_sample.s]). mt = mt.annotate_rows(family_stats=mt.family_stats.append(family_stats_adj)); mt.write(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:394); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1433); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1421); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60$$anonfun$apply$4.apply$mcV$sp(MatrixTable.scala:1723); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:197); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:186); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1722); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1718); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3074:1919,unsafe,unsafeInsert,1919,https://hail.is,https://github.com/hail-is/hail/issues/3074,1,['unsafe'],['unsafeInsert']
Safety,"(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 220, ip-172-31-2-255.ec2.internal, executor 2): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:30138,abort,aborted,30138,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['abort'],['aborted']
Safety,"), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:9707,unsafe,unsafe,9707,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['unsafe'],['unsafe']
Safety,")._convert_to_j(index_file_map); 1956; -> 1957 Env.hc()._jhc.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: GC overhead limit exceeded. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.immutable.VectorBuilder.<init>(Vector.scala:713); at scala.collection.immutable.Vector$.newBuilder(Vector.scala:22); at scala.collection.immutable.IndexedSeq$.newBuilder(IndexedSeq.scala:46); at scala.collection.IndexedSeq$.newBuilder(IndexedSeq.scala:36); at scala.collection.IndexedSeq$$anon$1.apply(IndexedSeq.scala:34); at com.twitter.chill.TraversableSerializer.read(Traversable.scala:39); at com.twitter.chill.TraversableSerializer.read(Traversable.scala:21); at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396); at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307); at com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:6952,abort,aborted,6952,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['abort'],['aborted']
Safety,); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3970,abort,abortStage,3970,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['abort'],['abortStage']
Safety,); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:8283,Unsafe,UnsafeRow,8283,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety,); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5532,abort,abortStage,5532,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['abort'],['abortStage']
Safety,"); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10705:2722,timeout,timeout,2722,https://hail.is,https://github.com/hail-is/hail/pull/10705,1,['timeout'],['timeout']
Safety,"* Add a lightweight DSL for writing IR in Scala, which made the lowerings much easier to write, and read. It is implemented in `IRBuilder`, and can be used by importing `IRBuilder._` into scope. It's not complete, and I want to make it eagerly typecheck eventually, but we can build on it.; * Make `execute` protected on `MatrixIR` and `TableIR`, making `Interpret` the official place to execute IR.; * Add a compiler pass lowering some `MatrixIR` to `TableIR`. The `Interpret` gateway to `execute` always lowers, so we can safely remove the execute methods of IR nodes which are rewritten by the lowering.; * Fix `LoadBgen` to not create entries arrays when `dropCols` is true.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4707:524,safe,safely,524,https://hail.is,https://github.com/hail-is/hail/pull/4707,1,['safe'],['safely']
Safety,* Added a quick check in minRep to avoid copmuting things if ref is already minimal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1664:35,avoid,avoid,35,https://hail.is,https://github.com/hail-is/hail/pull/1664,1,['avoid'],['avoid']
Safety,"* Correct spelling of ""decommissioning"" in help for `--graceful-decommission-timeout`.; * Add space between ""match"" and ""the"" in help for `--update-hail-version`.; * Add punctuation to help for `--update-hail-version` for consistency with other arguments.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7335:77,timeout,timeout,77,https://hail.is,https://github.com/hail-is/hail/pull/7335,1,['timeout'],['timeout']
Safety,* Improves pruning of BlockMatrix.write_from_entry_expr to avoid large; allocation in the slow test.; * Delete the slow test.; * Add two benchmarks that should catch this,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9150:59,avoid,avoid,59,https://hail.is,https://github.com/hail-is/hail/pull/9150,1,['avoid'],['avoid']
Safety,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2329:1090,avoid,avoid,1090,https://hail.is,https://github.com/hail-is/hail/pull/2329,1,['avoid'],['avoid']
Safety,", <a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a></li>; </ul>; <h2>v4.0.1</h2>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h2>async-timeout 4.0.0</h2>; <h1>Changes</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Drooped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:1425,timeout,timeout,1425,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,", full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:5719,abort,aborted,5719,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['abort'],['aborted']
Safety,", only depending on the semantics. It also correctly handles missing key fields, where the previous implementation often produced an unsound transformation of the IR. Also adds a much more thorough test suite than we had before. At the top level, the analysis takes a boolean typed IR `cond` in an environment where there is a reference to some `key`, and produces a set `intervals`, such that `cond` is equivalent to `cond & intervals.contains(key)` (in other words `cond` implies `intervals.contains(key)`, or `intervals` contains all rows where `cond` is true). This means for instance it is safe to replace `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond)`. Then in a second pass it rewrites `cond` to `cond2`, such that `cond & (intervals.contains(key))` is equivalent to `cond2 & intervals.contains(key)` (in other words `cond` implies `cond2`, and `cond2 & intervals.contains(key)` implies `cond`). This means it is safe to replace the `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val extract = new ExtractIntervalFilters(ctx, ref.typ.asInstanceOf[TStruct].typeAfterSelectNames(key)); val trueSet = extract.analyze(cond, ref.name); if (trueSet == extract.KeySetLattice.top); None; else {; val rw = extract.Rewrites(mutable.Set.empty, mutable.Set.empty); extract.analyze(cond, ref.name, Some(rw), trueSet); Some((extract.rewrite(cond, rw), trueSet)); }; }; }; ```; `trueSet` is the set of intervals which contains all rows where `cond` is true. This set is passed back into `analyze` in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:1456,safe,safe,1456,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['safe'],['safe']
Safety,", output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$an",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:2483,abort,aborted,2483,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['abort'],['aborted']
Safety,", with only the core interface methods and without any runtime checking. It is intended to be used only for defining a new `FlipbookIterator` or `StagingIterator`, through the factory methods on their respective companion objects which take a `StateMachine`. I implemented `fliterWhere`, `map`, and `flatMap` on `FlipbookIterator`, allowing the use of for–comprehensions, and on top of that I defined all the varieties of join. Some of the join methods take `OrderingView` arguments. An `OrderingView` is a small abstraction on top of an `Ordering` which can take one element `a`, copy data (such as key-fields) from `a` if necessary, then later (after `a` might have been destroyed or mutated) compare `a` to other elements using the copied data. The potentially producting (non-distinct assuming) join methods also take a buffer argument, which is anything that can make a copy of an iterator and then iterate over the copy multiple times. To avoid allocating tuples in the output iterators of the join methods, I made `Muple`, which is just a mutable tuple. I turned the existing `JoinedRegionValue` into an alias of `Muple[RegionValue, RegionValue]`. All of this core `FlipbookIterator` and `StagingIterator` behavior has no dependencies on anything else in Hail, so I want to thoroughly test everything at this level, and treat it like a small external iterator library living inside the repo. As such, I think this level should be quite stable going forwards. At the higher level, I lifted all the join methods on `FlipbookIterator` to `OrderedRVIterator`, which is a `Iterator[RegionValue]` together with an `OrderedRVDType`. I think `OrderedRVIterator` should be replaced by something better soon: see future work below. I've replaced the old implementations of `innerJoinDistinct`, `leftJoinDistinct`, and `orderedZipJoin` using the new infrastructure (see OrderedRDD2.scala), which I think is a good example of the kind of simplifications possible. The existing JoinSuite tests also serve as",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3016:4116,avoid,avoid,4116,https://hail.is,https://github.com/hail-is/hail/pull/3016,1,['avoid'],['avoid']
Safety,",A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String; }; [u'va.varid']; +--------+-------+--------------------+--------------------+--------------------+; |v.contig|v.start| v.ref| v.altAlleles| va.varid|; +--------+-------+--------------------+--------------------+--------------------+; | 01| 10013| A| [[A,C]]| 1:10013_A_C|; | 01| 10179| G| [[G,T]]| 1:10179_G_T|; | 01| 10259| C| [[C,A]]| 1:10259_C_A|; | 01| 10292| C| [[C,T]]| 1:10292_C_T|; | 01| 10402| G| [[G,A]]| 1:10402_G_A|; | 01| 10527| T| [[T,A]]| 1:10527_T_A|; | 01| 10611| G| [[G,A]]| 1:10611_G_A|; | 01| 10754| G| [[G,C]]| 1:10754_G_C|; | 01| 11099| T| [[T,G]]| 1:11099_T_G|; | 01| 11115| C| [[C,A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String,; C1: Double,; C2: Double; }; [u'va.varid']; [Stage 6:====================================================>(1640 + 1) / 1641]Traceback (most recent call last):; File ""/tmp/ec5f6e42-0ea7-404d-8311-f97f7ec26ad6/kt_troubleshooting_issue_042617.py"", line 31, in <module>; kt2.to_dataframe().show(10); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o448.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 20 times, most recent failure: Lost task 0.19 in stage 8.0 (TID 3406, cluster-mh-sw-xn3h.c.practice.internal): java.lang.ClassCastException: java.lang.String cannot be cast to is.hail.variant.Variant; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1725:2974,abort,aborted,2974,https://hail.is,https://github.com/hail-is/hail/issues/1725,1,['abort'],['aborted']
Safety,"- **Requires the python modules ** `nbsphinx`, `matplotlib`, `pandas`, `numpy`, and `seaborn`.; - Use property `-Dtutorial.home=/path/to/tutorial/files` with `gradle` to avoid downloading tutorial files with `wget`.; - Added new tgz file with tutorial files (reduced number of samples to 248 from 2535) https://storage.googleapis.com/hail-tutorial/Hail_Tutorial_Data-v2.tgz; - Edited tutorial to reflect smaller input file.; - Added iPython notebook to repository (this should be edited from now on); - Added tutorial to Sphinx docs.; - Changed tutorial location on website.; - Removed old tutorial infrastructure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1374:170,avoid,avoid,170,https://hail.is,https://github.com/hail-is/hail/pull/1374,1,['avoid'],['avoid']
Safety,"- Add a flush after writing the first log statement. This log statement is; displayed before any network requests, the flush ensures we always see it.; - Set the retries for in-cluster synchronous requests to 1.; - Change all external (ones that go through the gateway) HTTP(S) requests to use; a centrally defined session. This session improves the situation in two ways:; 1. It prevents urllib from retrying requests, which ensures Hail's retry; infrastructure is the only retry infrastructure.; 2. It sets a timeout, ensuring that all requests will timeout. Previously,; requests could hang forever.; 3. It permits setting headers that are used for all requests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9478:511,timeout,timeout,511,https://hail.is,https://github.com/hail-is/hail/pull/9478,2,['timeout'],['timeout']
Safety,"- Also moved the location of where buckets are mounted to not be in /batch so as to avoid accidentally deleting entire buckets.; - The file mode didn't do what I expected (allowed you to write to a bucket), but now that I think about it, we probably do want to expose this and my first intuition was right. We probably want files to be specified as read only when they're created on the local file system. I can make this a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8979:84,avoid,avoid,84,https://hail.is,https://github.com/hail-is/hail/pull/8979,1,['avoid'],['avoid']
Safety,"- Create a cache that stores an instance's token which can be looked up by the instance's name; - Use this cache in the active_instances_only decorator to avoid making DB request on every invocation; - Add monitoring of caches' hits, misses, evictions, and load latencies",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11346:155,avoid,avoid,155,https://hail.is,https://github.com/hail-is/hail/pull/11346,1,['avoid'],['avoid']
Safety,"- Eigen, EigenDistributed in Scala and Python; - EigenSuite; - assertVectorEqualityUpToSignDouble in TestUtils for comparing eigenvectors; - eigen methods in LDMatrix and KinshipMatrix; - toLocalMatrix on IRM avoiding BlockMatrix and test; - RichSparkMatrix with asBreeze to avoid copy; - asSpark and toArrayShallow on RichBreezeDenseMatrix. I think there is opportunity to have Eigen and EigenDistributed abstract a common class to replication. And for Kinship and LDMatrix as well, so that Kinship can also use the read and write that LDMatrix has.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2160:209,avoid,avoiding,209,https://hail.is,https://github.com/hail-is/hail/pull/2160,2,['avoid'],"['avoid', 'avoiding']"
Safety,"- On *deploys*, makes sure that whatever is in our third-party images is in our private registry before starting builds like hail-ubuntu that might depend on those images. This means that we can update our ubuntu base image without the australians needing to deploy any images by hand. However, this does not run in PRs because I 1) didn't want to add that kind of latency for PRs and 2) we don't do any kind of namespacing for our images so if we did include this for a PR that ultimately wasn't merged we would have to manually remove the image anyway so why not manually add it if you're going to PR it… I think point 2 is a little weak but I recall this being what we agreed on a couple months back when we discussed this. I'm wondering if we should just eat the minute or so latency at the beginning of PRs to be safe but it also feels like a shame for something that changes so infrequently. . - Again on deploys, upload the hailgenetics/* images to the private registry if they don't already exist there. This way any deployments that aren't hail team's GCP deployment can get these images automatically when they deploy a new SHA instead of uploading them manually. It won't backfill skipped versions, but we decided that was ok. This seems less relevant for testing on PRs as it will get triggered on releases and we can easily dev deploy to rectify the image if this breaks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12818:818,safe,safe,818,https://hail.is,https://github.com/hail-is/hail/pull/12818,1,['safe'],['safe']
Safety,"- Signup page with web socket and spinner while waiting for account to create; - Upon account creation, a billing project named `{username}-trial` is created with $10 limit and a user `{username}`; - When deleting an account, the billing project is reopened if it's closed, then remove the user, and finally close the billing project. This behavior might be debatable. We may not need to remove the user from the billing project. I think it's better to be safe in case the billing project is reopened. Auth-driver is implicitly dependent on the batch front end, but this dependency isn't stated in build.yaml. An event queue is a future solution. I can get rid of my personal email being whitelisted, but I think it will be useful if we need to debug later and for a possible demo on Monday (although I'll probably just use some screenshots). If you want to test it in your namespace, make sure to comment out all the create and delete steps that are not related to billing projects.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9658:456,safe,safe,456,https://hail.is,https://github.com/hail-is/hail/pull/9658,1,['safe'],['safe']
Safety,"- Values of types other than `Array` and `Boolean` get output in VCF format (e.g. `.` instead of `NA` for missing values); - `NaN` values are converted to missing (`.`) when exporting VCF since VCF doesn't handle `NaN`; - Changes to handling of filters:; - `.` <=> `NA:Set[String]`; - `PASS` <=> `{}:Set[String]`; - `other` <=> `{""other""}:Set[String]""`; - Removed `va.pass` entirely (redundant with `va.filters` and needs constant synchronization)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1517:384,redund,redundant,384,https://hail.is,https://github.com/hail-is/hail/pull/1517,1,['redund'],['redundant']
Safety,"- [ ] (@tpoterba) caf1e1e673 add fails_service_backend; - [ ] (@tpoterba, @cseed) a979dfba58 [hail] introduce and use mktemp and mktempd; - [ ] (@tpoterba) dcf026b01c [hail] make is.hail.expr.ir.functions threadsafe; - [ ] (@tpoterba) 807f38c20e [hail] fix use of row requiredness in lowerDistributedSort; - [ ] (@catoverdrive) 12df8eb456 [query-service] handle void-typed IRs in query-service; - [ ] (@catoverdrive) 03357ee83d [query-service] make user cache thread-safe; - [ ] (@tpoterba) 6c6734bc71 [query-service] bugfix: preserve globals through a shuffle; - [ ] (@catoverdrive) a3d2572ce7 [shuffler] log ShuffleCodecSpec anytime it is created; - [ ] (@daniel-goldstein) 8949dfec3c [scala-lsm] bugfix: least key may equal greatest key; - [ ] (@daniel-goldstein) 6067bd8e51 [services] discovered new transient error; - [ ] (@daniel-goldstein) c8356d30bb [shuffler] more assertions in ShuffleClient; - [ ] (@daniel-goldstein) 9991da90f0 [shuffler] bugfix: shuffler needs a HailContext to decode loci; - [ ] (@daniel-goldstein) bc0140ab6f [query-service] move hail.jar earlier in Dockerfile; - [ ] (@daniel-goldstein) f96c28174d [query-service] permit pod scaling and remove cpu limit; - [ ] (@catoverdrive) 6ae26339fe [query-service] simplify socket handling; - [ ] (@jigold) f3db30e23f [batch] teach JVMJob where to find the hail configuration files; - [ ] (@daniel-goldstein) b5c6d85554 [query-service] switch to services team approved logging; - [ ] (@tpoterba) 35a306c066 [query-service] query workers need a hail context; - [ ] (@daniel-goldstein, @catoverdrive) 051c89b8e7 [query-service] use a UNIX Domain Socket for Py-Scala communication; - [ ] (@daniel-goldstein, @catoverdrive) ad9ea73d7a [query-service] run tests against query service",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10072:467,safe,safe,467,https://hail.is,https://github.com/hail-is/hail/pull/10072,1,['safe'],['safe']
Safety,- [ ] attributes PR (per-batch attributes); - [ ] per-job attributes; - [ ] use queue and concurrent worker pool for all k8s communication; - [ ] batch avoids scheduling more than 150k pods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6493:152,avoid,avoids,152,https://hail.is,https://github.com/hail-is/hail/issues/6493,1,['avoid'],['avoids']
Safety,"- [x] Width of navbar doesn't match #body; * Caused by using em to determine max-width of a child where the parent (#body) had a font-size set differently from root (html). Fixed by using rem, and to avoid changing so many em's, removing font-size on #body. - [x] Better dropdown: width, box shadow, padding. - [x] Some apparently unnecessary styles. Before (narrow):; <img width=""677"" alt=""Screenshot 2019-08-01 17 03 06"" src=""https://user-images.githubusercontent.com/5543229/62328170-61bb7700-b480-11e9-838a-43229ee955c3.png"">. After (narrow):; <img width=""712"" alt=""Screenshot 2019-08-01 17 02 53"" src=""https://user-images.githubusercontent.com/5543229/62328172-641dd100-b480-11e9-9be1-f3ff67035cd0.png"">. (more views):; ; <img width=""894"" alt=""Screenshot 2019-08-01 17 37 00"" src=""https://user-images.githubusercontent.com/5543229/62329162-19518880-b483-11e9-9cfd-12ca8c1a52dc.png"">; <img width=""854"" alt=""Screenshot 2019-08-01 17 37 05"" src=""https://user-images.githubusercontent.com/5543229/62329163-19518880-b483-11e9-8a70-90faa3dcc685.png"">; <img width=""923"" alt=""Screenshot 2019-08-01 17 37 11"" src=""https://user-images.githubusercontent.com/5543229/62329164-19518880-b483-11e9-9871-0ce39c6c7c53.png"">. I did this quickly, so didn't set up local server. If anything becomes ugly I'll fix today. Also didn't test in IE, just chrome, safari, and I expect Firefox to be fine as well. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6789:200,avoid,avoid,200,https://hail.is,https://github.com/hail-is/hail/pull/6789,1,['avoid'],['avoid']
Safety,"- `TContainer.loadLength` is now on the object so creating byte code to; read the length does not require knowledge of the element type. - in `elementsOffset`, use the non-Code pattern of delegating to; `roundUpAlignment`. - add staged `TContainer.elementOffset`, `TContainer.loadElement`,; `TStruct.fieldOffset`, `UnsafeUtils.roundUpAlignment`. - fix bug: loadElement reads addresses using `loadAddress` instead of; `loadInt`. - add several `loadX` methods to `RichCodeMemoryBuffer`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2347:315,Unsafe,UnsafeUtils,315,https://hail.is,https://github.com/hail-is/hail/pull/2347,1,['Unsafe'],['UnsafeUtils']
Safety,"- created is.hail.services package; - added DeployConfig, Tokens with the necessary functionality to get BatchClient working; - BatchClient is built on Apache HttpComponents; - Synchronous, thread safe. HttpClient is thread safe, BatchClient should be, too.; - Simple hello, world! test; - Added build step for Java services tests. FYI @jigold this might be a possible model if we ever rework the Python BatchClient. Also, if there are Batch changes going forward this code will also need to updated. The client is incredibly light weight, so that shouldn't be often, similar to the aiogoogle clients I wrote recently. Next up: Query Batch backend!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8779:197,safe,safe,197,https://hail.is,https://github.com/hail-is/hail/pull/8779,2,['safe'],['safe']
Safety,"- improved and fleshed out documentation of current BlockMatrix python functionality. Note`from_matrix_table` renamed to `from_entry_expr`, `from_numpy_matrix(numpy_matrix, ...)` renamed to `from_numpy(ndarray, ...)`, and similarly for `to_numpy_matrix`.; - renamed `matrix.py` as `blockmatrix.py`; - renamed `toLocalMatrix` to the more specific `toBreezeMatrix` on BlockMatrix and RowMatrix. This is prep for filling out the BlockMatrix interface w/ NumPy broadcast rules (model is LocalMatrix) and speeding up `to_numpy` and `from_numpy` (for now, by passing bytes via temp files rather than trough py4j) so that NumPy ndarrays serve as local matrix on Python side and interact predictably with BlockMatrices.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3072:680,predict,predictably,680,https://hail.is,https://github.com/hail-is/hail/pull/3072,1,['predict'],['predictably']
Safety,"- methods to go from `BlockMatrix` to `Table` and `MatrixTable` in a row-major representation. The table has fields for row index and array of entries for a row, and the matrix table has a row index, col index and the value at those indexes as the entry. This operation goes through disk to avoid a shuffle.; - method to go from a `Table` to a `MatrixTable` with a similar representation, where selected fields of the same type from the `Table` become column fields in the matrix table, with their values making the fields of the matrix table. Motivation for these conversions can be found at the issue below.; Resolves #5504. Big thanks to @patrick-schultz and @jigold for taking the time to teach me about region-based mem management!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5568:291,avoid,avoid,291,https://hail.is,https://github.com/hail-is/hail/pull/5568,1,['avoid'],['avoid']
Safety,- remove some redundant project git ignores; - add a few more file types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8192:14,redund,redundant,14,https://hail.is,https://github.com/hail-is/hail/pull/8192,1,['redund'],['redundant']
Safety,- two network requests instead of four now that we know the name of both the service name and pod name; - use try-catch to avoid 500ing if a user hits `GET /new` and their service or pod is already gone (for whatever reason),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4863:123,avoid,avoid,123,https://hail.is,https://github.com/hail-is/hail/pull/4863,1,['avoid'],['avoid']
Safety,"------------------- live log setup --------------------------------; 2023-09-06T21:45:24 INFO test.conftest conftest.py:14:log_before_after starting test; 2023-09-06T21:45:24 INFO hailtop.aiocloud.aioazure.credentials credentials.py:99:default_credentials using credentials file /test-gsa-key/key.json; -------------------------------- live log call ---------------------------------; 2023-09-06T21:45:25 INFO azure.identity.aio._internal.get_token_mixin get_token_mixin.py:93:get_token ClientSecretCredential.get_token succeeded; 2023-09-06T21:45:25 INFO batch_client.aioclient aioclient.py:809:_submit created batch 191; submit job bunches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 1/1 0:00:00 0:00:00; 2023-09-06T21:47:17 WARNING hailtop.utils utils.py:842:retry_transient_errors_with_debug_string A transient error occured. We will automatically retry. Do not be alarmed. We have thus far seen 2 transient errors (next delay: 3.794s). The most recent error was <class 'asyncio.exceptions.TimeoutError'> . . +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (140387515627072) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++; FAILED; _________________ test_always_run_job_private_instance_cancel __________________. client = <hailtop.batch_client.client.BatchClient object at 0x7fae899806a0>. def test_always_run_job_private_instance_cancel(client: BatchClient):; b = create_batch(client); resources = {'machine_type': smallest_machine_type()}; j = b.create_job(DOCKER_ROOT_IMAG",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:1153,Timeout,TimeoutError,1153,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['Timeout'],['TimeoutError']
Safety,"------------------------------------------------------------------------; To reproduce; ```python; import hail as hl; mt = hl.import_vcf(""http://hgdownload.cse.ucsc.edu/gbdb/hg19/1000Genomes/phase3/ALL.chrY.phase3_integrated_v1a.20130502.genotypes.vcf.gz"", force_bgz=True); ----------------------------------------------------------------------; Initializing Hail with default parameters...; 2022-10-06 15:56:03 WARN Utils:69 - Your hostname, nid resolves to a loopback address: 127.0.1.1; using 192.168.248.80 instead (on interface wlp0s20f3); 2022-10-06 15:56:03 WARN Utils:69 - Set SPARK_LOCAL_IP if you need to bind to another address; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/med/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 2022-10-06 15:56:03 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.3; SparkUI available at http://192.168.248.80:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.100-2ea2615a797a; LOGGING: writing to /; --------------------------------------------------------------------------; mt.filter_rows(mt.locus.position==2867101).count_rows(); ```; ### Expected ; Return a count of rows with that condition. ### Error ; ```; FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assert",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280:1074,unsafe,unsafe,1074,https://hail.is,https://github.com/hail-is/hail/issues/12280,1,['unsafe'],['unsafe']
Safety,"--------------------------; RemoteDisconnected Traceback (most recent call last); File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:703, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw); 702 # Make the request on the httplib connection object.; --> 703 httplib_response = self._make_request(; 704 conn,; 705 method,; 706 url,; 707 timeout=timeout_obj,; 708 body=body,; 709 headers=headers,; 710 chunked=chunked,; 711 ); 713 # If we're going to release the connection in ``finally:``, then; 714 # the response doesn't need to know about the connection. Otherwise; 715 # it will also try to release it and we'll have a double-release; 716 # mess. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:449, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code.; --> 449 six.raise_from(e, None); 450 except (SocketTimeout, BaseSSLError, SocketError) as e:. File <string>:3, in raise_from(value, from_value). File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:444, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 443 try:; --> 444 httplib_response = conn.getresponse(); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code. File /opt/conda/lib/python3.10/http/client.py:1375, in HTTPConnection.getresponse(self); 1374 try:; -> 1375 response.begin(); 1376 except ConnectionError:. File /opt/conda/lib/python3.10/http/client.py:318, in HTTPResponse.begin(self); 317 while True:; --> 3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:4584,timeout,timeout,4584,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['timeout'],['timeout']
Safety,"----------------; 2023-09-06T21:45:24 INFO test.conftest conftest.py:14:log_before_after starting test; 2023-09-06T21:45:24 INFO hailtop.aiocloud.aioazure.credentials credentials.py:99:default_credentials using credentials file /test-gsa-key/key.json; -------------------------------- live log call ---------------------------------; 2023-09-06T21:45:25 INFO azure.identity.aio._internal.get_token_mixin get_token_mixin.py:93:get_token ClientSecretCredential.get_token succeeded; 2023-09-06T21:45:25 INFO batch_client.aioclient aioclient.py:809:_submit created batch 191; submit job bunches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 1/1 0:00:00 0:00:00; 2023-09-06T21:47:17 WARNING hailtop.utils utils.py:842:retry_transient_errors_with_debug_string A transient error occured. We will automatically retry. Do not be alarmed. We have thus far seen 2 transient errors (next delay: 3.794s). The most recent error was <class 'asyncio.exceptions.TimeoutError'> . . +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (140387515627072) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++; FAILED; _________________ test_always_run_job_private_instance_cancel __________________. client = <hailtop.batch_client.client.BatchClient object at 0x7fae899806a0>. def test_always_run_job_private_instance_cancel(client: BatchClient):; b = create_batch(client); resources = {'machine_type': smallest_machine_type()}; j = b.create_job(DOCKER_ROOT_IMAGE, ['true'], resources=resources, always_run=True);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:1208,Timeout,Timeout,1208,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['Timeout'],['Timeout']
Safety,"--------. ### Hail version: 0.1-6e815ac. ### What you did: hc.import_bgen('*.bgen) X chromosome cannot be imported, which is a major issue when working on phenotypes linked to blood coagulation, for example. ### What went wrong (all error messages here, including the full java stack trace):. [Stage 1:===============================================> (747 + 9) / 871]Traceback (most recent call last):; File ""regression1.py"", line 22, in <module>; hc.import_bgen('/mnt/volume/imputed_genotypes/*.bgen', sample_file='/mnt/volume/imputed_genotypes/MT.sample').split_multi().write('/mnt/volume/imputed_genotypes/MT_intersect_imputed.vds'); File ""<decorator-gen-285>"", line 2, in write; File ""/usr/local/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101); at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58); at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56); at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74); at org.apac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:1231,abort,aborted,1231,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['abort'],['aborted']
Safety,"---. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap-southeast-1.compute.internal, executor 3): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5643:1859,abort,aborted,1859,https://hail.is,https://github.com/hail-is/hail/issues/5643,1,['abort'],['aborted']
Safety,"-0-18-2021-10-26</a></p>; <h2>Incompatible changes</h2>; <p>5.0.0 b1</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10031"">#10031</a>: autosummary: <code>sphinx.ext.autosummary.import_by_name()</code> now raises; <code>ImportExceptionGroup</code> instead of <code>ImportError</code> when it failed to import; target object. Please handle the exception if your extension uses the; function to import Python object. As a workaround, you can disable the; behavior via <code>grouped_exception=False</code> keyword argument until v7.0.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9962"">#9962</a>: texinfo: Customizing styles of emphasized text via <code>@definfoenclose</code>; command was not supported because the command was deprecated since texinfo 6.8</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/2068"">#2068</a>: :confval:<code>intersphinx_disabled_reftypes</code> has changed default value; from an empty list to <code>['std:doc']</code> as avoid too surprising silent; intersphinx resolutions.; To migrate: either add an explicit inventory name to the references; intersphinx should resolve, or explicitly set the value of this configuration; variable to an empty list.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10197"">#10197</a>: html theme: Reduce <code>body_min_width</code> setting in basic theme to 360px</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9999"">#9999</a>: LaTeX: separate terms from their definitions by a CR (refs: <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9985"">#9985</a>)</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10062"">#10062</a>: Change the default language to <code>'en'</code> if any language is not set in; <code>conf.py</code></li>; </ul>; <p>5.0.0 final</p>; <ul>; <li><a href=""https://github-redirect",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11871:2435,avoid,avoid,2435,https://hail.is,https://github.com/hail-is/hail/pull/11871,2,['avoid'],['avoid']
Safety,"-jinja2/blob/master/CHANGES.rst"">aiohttp-jinja2's changelog</a>.</em></p>; <blockquote>; <h2>1.5 (2021-08-21)</h2>; <ul>; <li>Drop support for jinaj2 &lt;3. Add support for 3+.</li>; <li>Don't require <code>typing_extensions</code> on Python 3.8+.</li>; </ul>; <h2>1.4.2 (2020-11-23)</h2>; <ul>; <li>Add CHANGES.rst to MANIFEST.in and sdist <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/402"">#402</a></li>; </ul>; <h2>1.4.1 (2020-11-22)</h2>; <ul>; <li>Document async rendering functions <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/396"">#396</a></li>; </ul>; <h2>1.4.0 (2020-11-12)</h2>; <ul>; <li>; <p>Fix type annotation for <code>context_processors</code> argument <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/354"">#354</a></p>; </li>; <li>; <p>Bump the minimal supported <code>aiohttp</code> version to 3.6.3 to avoid problems; with uncompatibility between <code>aiohttp</code> and <code>yarl</code></p>; </li>; <li>; <p>Add async rendering support <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/393"">#393</a></p>; </li>; </ul>; <h2>1.3.0 (2020-10-30)</h2>; <ul>; <li>; <p>Remove Any from template annotations <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/343"">#343</a></p>; </li>; <li>; <p>Fix type annotation for filters in <code>aiohttp_jinja2.setup</code> <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/330"">#330</a></p>; </li>; <li>; <p>Drop Python 3.5, support Python 3.9</p>; </li>; </ul>; <h2>1.2.0 (2019-10-21)</h2>; <ul>; <li>Add type hints <a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp_jinja2/issues/285"">#285</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp-jinja2/commit/d83f081c4c1b10102b53d4b973225c190b91652f""><code>d83f081</code></a> Release 1.5</li>; <li><a href=""https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11576:2912,avoid,avoid,2912,https://hail.is,https://github.com/hail-is/hail/pull/11576,1,['avoid'],['avoid']
Safety,"-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c7e10db3d0965422122bef263fb36f6fd7572330""><code>c7e10db</code></a> Fix linter configs</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/6af8bf421a799208b27d57c6bf69de0d998fedec""><code>6af8bf4</code></a> Bump pre-commit from 2.15 to 2.16.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/269"">#269</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c71bbb5b5d7330f6dabfde7a1adec30a4611c0be""><code>c71bbb5</code></a> Bump docutils from 0.18 to 0.18.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/266"">#266</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/async-timeout/compare/v3.0.1...v4.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=async-timeout&package-manager=pip&previous-version=3.0.1&new-version=4.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:5974,timeout,timeout,5974,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,"-preview</code> header from :meth:<code>gidgethub.apps.get_installation_access_token</code>; because it is out of preview. The <code>machine-man-preview</code> is <code>no longer required &lt;https://developer.github.com/changes/#--machine-man-and-sailor-v-previews-graduate&gt;</code>_; as of August 20, 2020.</li>; </ul>; <h2>5.0.0</h2>; <ul>; <li>Add :meth:<code>gidgethub.routing.Router.fetch</code> for obtaining a frozenset of functions; registered to the router that the event would be called on.; (<code>Issue [#74](https://github.com/brettcannon/gidgethub/issues/74) &lt;https://github.com/brettcannon/gidgethub/issues/74&gt;</code>_).</li>; <li>Add support for GitHub Actions Environment Files with :meth:<code>gidgethub.actions.setenv</code>; and :meth:<code>gidgethub.actions.addpath</code>.; (<code>Issue [#137](https://github.com/brettcannon/gidgethub/issues/137) &lt;https://github.com/brettcannon/gidgethub/issues/132&gt;</code>_).</li>; <li>Make router callback execution order non-deterministic to avoid relying on; registration order.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/9660d1e1c0187d9def32c473c8ceefcd130fe26f""><code>9660d1e</code></a> Add .DS_Store to .gitignore file</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/ef0368998fe40769f4f20a6c4b6ccfea27fe8ca9""><code>ef03689</code></a> Bump the version number</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/1f80a51670555acda0db0e42189d00bb58bb3b45""><code>1f80a51</code></a> Release 5.2.1</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/89ade8859539212e0663e91f0777ad8a39ecf323""><code>89ade88</code></a> Fix cgi and importlib_resources deprecations (<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/185"">#185</a>)</li>; <li><a href=""https://github.com/brettcannon/gidgethub/commit/6488",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12328:6911,avoid,avoid,6911,https://hail.is,https://github.com/hail-is/hail/pull/12328,1,['avoid'],['avoid']
Safety,". Then the paper computes a $p \times n$ matrix $W$ called the **SNP weight matrix**:. $$W \coloneqq X^T U.$$. Suppose that there are $n_r$ individuals in the related set and let $Y$ be the $n_r \times p$ standardized genotype matrix for the related individuals. The paper computes the principal components associated with the related samples with. $$ \frac{1}{p} Y W (\Sigma^2)^{-1}.$$. ### Simplifications. The first simplification that I noticed is that we can do away with the $\frac{1}{p}$ terms. Because $\Psi$ is scaled by $p^{-1}$, the inverse of the eigenvalues, $(\Sigma^2)^{-1}$ is scaled by $p$, which cancels out the $1/p$ term in the calculation of the principal components for the related individuals. From here on, let us redefine $\Sigma^2$ as the diagonal matrix containing the eigenvalues of $XX^T$ (not $\frac{1}{p} XX^T$). Next, by examining the relationship between singular value decomposition (SVD) and eigendecomposition ([Wikipedia link](https://en.m.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition)), I realized that it is not necessary to compute $\Psi$. Instead, we can get $U$ and $\Sigma$ from the SVD of $X$:. $$X = U\Sigma V^T,$$. where $V$ is a $p \times p$ basis of the new PCA coordinate space. Then while investigating the meaning of $W$, I realized that $W = X^T U = V \Sigma^T U^T U = V \Sigma^T$. Taking these simplifications into account, I realized that the paper is, in essence, computing $Y V$ to get the predicted scores associated with the related individuals. (Technically, I think that the paper is computing $Y V \Sigma^{-1}$. I am not sure why they scale the columns by the reciprocal of the eigenvalues here.). ### Simplified Approach. As I understand it, $V$ is a change-of-basis matrix from the original coordinates to the PCA scores. Letting $G$ denote the full standardized genotype matrix of all the individuals, I think we can just return $GV$, where $V$ is defined by the SVD of $X$:. $$X = U\Sigma V^T.$$",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14326#issuecomment-1962279184:2045,predict,predicted,2045,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1962279184,2,['predict'],['predicted']
Safety,". debian8), and systems; based on g++-5.x and later with new-ABI std::string by default (e.g. debian9). That; incompatibility is the problem. >Not having access to the standard library seems problematic. You absolutely have access to the full C++11 standard library in dynamically-generated code -; you're compiling with the master node's default compiler, and header files, and using; the default libstdc++.o (libc++.dylib), and it's all fine. And you'll be using whichever flavor; of std::string is the default for that system, which will presumably also be interoperable with any; third-party library packages on that system. The issues we're getting round are:. a) If libhail.so is prebuilt on a new-ABI system *and* uses std::string, then it can't run against; the libstdc++ on an old-ABI system. b) If libhail.so is prebuilt on an old-ABI system, then it can run against either old-ABI or new-ABI; libstdc++, but if it then gets linked against third-party libraries compiled against new-ABI; headers, you'll have two different flavors of std::string floating around in the same program,; which causes confusion at any interfaceswhich pass std::string around. Now if you want to go further in shipping more of the system, then the question of whether to; ship your own libstdc++ (or libc++) is independent of the choice of compiler version. *If* you ; ship your own libstdc++, then you potentially introduce the problems of interoperability with; other libraries on the system). And there's a slight question about whether your libstdc++ will; work against the other systems libc.so, though I think the ABI at that level has been stable enough for long enough that it probably doesn't cause trouble.; ; In short, it's a can of worms. Avoiding std::string in libhail.so keeps the can closed for now.; And I believe dataproc will move to using debian9 images as the default in November, so at; some point the need to support old-ABI systems (debian8) will diminish and possibly go away completely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941:2053,Avoid,Avoiding,2053,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941,1,['Avoid'],['Avoiding']
Safety,"... in the sense that they always aggregate in partition order. This is achieved by a `AssociativeCombiner` which greedily combOp's aggregator state from adjacent partitions. I think this is the best you can do. Later we should have a `CommutativeCombiner` and choose between the two based on the whether the user-level aggregators are (duh) associative (like collect and prev_nonnull) or commutative (like collectAsSet or count). This is slightly conservative as I converted, with slavish consistency, all aggregators and reducers, included one only used by concordance, which I think is commutative. I'm OK with that mostly because this is safer and concordance really needs to get rewritten in Python (I think @tpoterba has a draft but it needed some performance work). FYI @chrisvittal this should fix any prev_nonnull aggregator/scan issues.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5420:642,safe,safer,642,https://hail.is,https://github.com/hail-is/hail/pull/5420,1,['safe'],['safer']
Safety,".0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3452,timeout,timeout,3452,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['timeout'],['timeout']
Safety,".9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/futures.py"", line 287, in __await__; return self.result() # May raise too.; File ""/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/futures.py"", line 201, in result; raise self._exception; File ""/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py"", line 256, in __step; result = coro.send(None); File ""/usr/local/lib/python3.9/site-packages/hailtop/aiocloud/common/session.py"", line 21, in post; return await self.request('POST', url, **kwargs); File ""/usr/local/lib/python3.9/site-packages/hailtop/aiocloud/common/session.py"", line 105, in request; return await self._http_session.request(method, url, **kwargs); File ""/usr/local/lib/python3.9/site-packages/hailtop/httpx.py"", line 137, in request_and_raise_for_status; resp = await self.client_session._request(method, url, **kwargs); File ""/usr/local/lib/python3.9/site-packages/aiohttp/client.py"", line 535, in _request; conn = await self._connector.connect(; File ""/usr/local/lib/python3.9/site-packages/aiohttp/connector.py"", line 542, in connect; proto = await self._create_connection(req, traces, timeout); File ""/usr/local/lib/python3.9/site-packages/aiohttp/connector.py"", line 907, in _create_connection; _, proto = await self._create_direct_connection(req, traces, timeout); File ""/usr/local/lib/python3.9/site-packages/aiohttp/connector.py"", line 1206, in _create_direct_connection; raise last_exc; File ""/usr/local/lib/python3.9/site-packages/aiohttp/connector.py"", line 1175, in _create_direct_connection; transp, proto = await self._wrap_create_connection(; File ""/usr/local/lib/python3.9/site-packages/aiohttp/connector.py"", line 992, in _wrap_create_connection; raise client_error(req.connection_key, exc) from exc; aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host storage.googleapis.com:443 ssl:default [Too many open files]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13940:6266,timeout,timeout,6266,https://hail.is,https://github.com/hail-is/hail/issues/13940,2,['timeout'],['timeout']
Safety,".; It's just one step short of using containers - but since it doesn't require; a containerized OS, I think it works; for laptops etc. I believe the package could have all the stuff we currently manage my; manual install, viz JDK, Spark, Python-3.6,; R, R packages, as well as Hail and a friendly-C++17-capable compiler. All; without perturbing anything else; on the system. See https://bitnami.com. I took a similar approach at PhysicsSpeed, though without using any bitnami; tools because we had less than; zero dollars :-(. I don't know if this adds any value in the containerized/cloud environment,; where custom machine images; are presumably the way to go. But it makes setup easy for standalone use. Regards; Richard. On Thu, Aug 2, 2018 at 10:44 PM Richard Cownie <rcownie@broadinstitute.org>; wrote:. > We have a difference of opinion about the risks involved in using whatever; > compiler happens to show up as $(CXX); > to try to compile arbitrarily large auto-generated C++ files, and maybe; > about what happens when that fails; > and gives an error message about something in the middle of 12000 lines of; > code that bears no obvious relationship; > to what the user is doing. Or when that compiler takes 15 minutes to; > compile it. It's the C++ equivalent of; > the JVM ""no, that's just too much bytecode"". Or worst of all, it compiles; > it but the code gives the wrong answers; > because that particular compiler has a bug, and we never tested the; > combination of our codegen with *that*; > compiler/version.; >; > A couple of years ago I was seeing g++ take 40-60 seconds to compile; > something that clang did in 2 seconds; > (fairly heavily templated code generated for an SQL query, so very much in; > the same ballpark as parts of Hail),; > which contributes to my concern about this, especially on linux where g++; > is the default.; >; > So in the long run I expect we'll ship a compiler, or specify a compiler.; > But that becomes a problem in itself; > if we want the sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287:1177,risk,risks,1177,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287,1,['risk'],['risks']
Safety,".</em></p>; <blockquote>; <h2>v2.32.0</h2>; <h2>2.32.0 (2024-05-20)</h2>; <h2>🐍 PYCON US 2024 EDITION 🐍</h2>; <p><strong>Security</strong></p>; <ul>; <li>Fixed an issue where setting <code>verify=False</code> on the first request from a; Session will cause subsequent requests to the <em>same origin</em> to also ignore; cert verification, regardless of the value of <code>verify</code>.; (<a href=""https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56"">https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56</a>)</li>; </ul>; <p><strong>Improvements</strong></p>; <ul>; <li><code>verify=True</code> now reuses a global SSLContext which should improve; request time variance between first and subsequent requests. It should; also minimize certificate load time on Windows systems when using a Python; version built with OpenSSL 3.x. (<a href=""https://redirect.github.com/psf/requests/issues/6667"">#6667</a>)</li>; <li>Requests now supports optional use of character detection; (<code>chardet</code> or <code>charset_normalizer</code>) when repackaged or vendored.; This enables <code>pip</code> and other projects to minimize their vendoring; surface area. The <code>Response.text()</code> and <code>apparent_encoding</code> APIs; will default to <code>utf-8</code> if neither library is present. (<a href=""https://redirect.github.com/psf/requests/issues/6702"">#6702</a>)</li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed bug in length detection where emoji length was incorrectly; calculated in the request content-length. (<a href=""https://redirect.github.com/psf/requests/issues/6589"">#6589</a>)</li>; <li>Fixed deserialization bug in JSONDecodeError. (<a href=""https://redirect.github.com/psf/requests/issues/6629"">#6629</a>)</li>; <li>Fixed bug where an extra leading <code>/</code> (path separator) could lead; urllib3 to unnecessarily reparse the request URI. (<a href=""https://redirect.github.com/psf/requests/issues/6644"">#6644</a>)</li>; </ul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14555:1214,detect,detection,1214,https://hail.is,https://github.com/hail-is/hail/pull/14555,1,['detect'],['detection']
Safety,.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:5977,abort,abortStage,5977,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['abort'],['abortStage']
Safety,".RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but ¯\_(ツ)_/¯). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:5078,abort,aborted,5078,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['abort'],['aborted']
Safety,.SparkBackend.withExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); 			at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:8418,abort,aborted,8418,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['abort'],['aborted']
Safety,".__version__, deepest)) from None; 229 except pyspark.sql.utils.CapturedException as e:; 230 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 20 times, most recent failure: Lost task 0.19 in stage 24.0 (TID 1813, lfrani-sw-hqb8.c.broad-mpg-gnomad.internal, executor 159): is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:2782,abort,aborted,2782,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['abort'],['aborted']
Safety,".apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8Str",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:7506,unsafe,unsafe,7506,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['unsafe'],['unsafe']
Safety,".com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c7e10db3d0965422122bef263fb36f6fd7572330""><code>c7e10db</code></a> Fix linter configs</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/6af8bf421a799208b27d57c6bf69de0d998fedec""><code>6af8bf4</code></a> Bump pre-commit from 2.15 to 2.16.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/269"">#269</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c71bbb5b5d7330f6dabfde7a1adec30a4611c0be""><code>c71bbb5</code></a> Bump docutils from 0.18 to 0.18.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/266"">#266</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/async-timeout/compare/v3.0.1...v4.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=async-timeout&package-manager=pip&previous-version=3.0.1&new-version=4.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:6268,timeout,timeout,6268,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:8221,Unsafe,UnsafeRow,8221,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety,.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748). ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4886,Timeout,TimeoutTimeoutSubscriber,4886,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['Timeout'],['TimeoutTimeoutSubscriber']
Safety,.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:12111,abort,abortStage,12111,https://hail.is,https://github.com/hail-is/hail/issues/7044,2,['abort'],['abortStage']
Safety,".nt2) &; (mt.alleles[1] == mt.ss.nt1)) |; ((flip_text(mt.alleles[0]) == mt.ss.nt2) &; (flip_text(mt.alleles[1]) == mt.ss.nt1)),; mt.ss.ldpred_inf_beta); .or_missing()). # filter bgen matrixtable down to only SNPs with betas; mt = mt.filter_rows(hl.is_defined(mt.beta)). # filter bgen matrixtable to only include people in scoring sample; mt = mt.filter_cols(hl.is_defined(sampleids[mt.s])). # score sample; mt = mt.annotate_cols(prs=hl.agg.sum(mt.beta * mt.dosage)). # write out table with sample IDs and PRS scores; mt.cols().export('gs://ukbb_prs/prs/UKB_'+pheno+'_PRS_22.txt'). parser = argparse.ArgumentParser(); parser.add_argument(""--phenotype"", help=""name of the sumstat phenotype""); args = parser.parse_args(). try:; start = time.time(); main(args.phenotype); end = time.time(); message = ""Success! Job was completed in %s"" % time.strftime(""%H:%M:%S"", time.gmtime(end - start)); send_message(message); except Exception as e:; send_message(""Fail.""); ```. ""Failure Reason"":; ```; Job aborted due to stage failure: Task 98 in stage 13.0 failed 20 times, most recent failure: Lost task 98.19 in stage 13.0 (TID 22699, ccarey-sw-xt4j.c.ukbb-robinson.internal, executor 68): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rv",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:2826,abort,aborted,2826,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['abort'],['aborted']
Safety,.scala:86); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855); 	at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:8337,abort,abortStage,8337,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['abort'],['abortStage']
Safety,.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:5248,abort,abortStage,5248,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['abort'],['abortStage']
Safety,.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)sun.reflect.generics.reflectiveObjects.NotImplementedException: null; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:6615,Unsafe,UnsafeRow,6615,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['Unsafe'],['UnsafeRow']
Safety,"/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:3409,abort,aborted,3409,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['abort'],['aborted']
Safety,"/Library/Python/3.9/lib/python/site-packages/hail/backend/py4j_backend.py:218, in Py4JBackend._rpc(self, action, payload); 216 path = action_routes[action]; 217 port = self._backend_server_port; → 218 resp = self._requests_session.post(f’http://localhost:{port}{path}', data=data); 219 if resp.status_code >= 400:; 220 error_json = orjson.loads(resp.content). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:637, in Session.post(self, url, data, json, **kwargs); 626 def post(self, url, data=None, json=None, **kwargs):; 627 r""""“Sends a POST request. Returns :class:Response object.; 628; 629 :param url: URL for the new :class:Request object.; (…); 634 :rtype: requests.Response; 635 “””; → 637 return self.request(“POST”, url, data=data, json=json, **kwargs). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:589, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json); 584 send_kwargs = {; 585 “timeout”: timeout,; 586 “allow_redirects”: allow_redirects,; 587 }; 588 send_kwargs.update(settings); → 589 resp = self.send(prep, **send_kwargs); 591 return resp. File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:703, in Session.send(self, request, **kwargs); 700 start = preferred_clock(); 702 # Send the request; → 703 r = adapter.send(request, **kwargs); 705 # Total elapsed time of the request (approximately); 706 elapsed = preferred_clock() - start. File ~/Library/Python/3.9/lib/python/site-packages/requests/adapters.py:501, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies); 486 resp = conn.urlopen(; 487 method=request.method,; 488 url=url,; (…); 497 chunked=chunked,; 498 ); 500 except (ProtocolError, OSError) as err:; → 501 raise ConnectionError(err, request=request); 503 except MaxRetryError as e:; 504 if isinstance(e.reason, ConnectTimeoutError):; 505 # TODO: Remove this in 3.0.0: see #2811. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14557:2134,timeout,timeout,2134,https://hail.is,https://github.com/hail-is/hail/issues/14557,3,['timeout'],['timeout']
Safety,"/a>) (<a href=""https://github.com/vitejs/vite/commit/1afc1c2"">1afc1c2</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8668"">#8668</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.12 (2022-06-10)<!-- raw HTML omitted --></h2>; <ul>; <li>fix: outdated optimized dep removed from module graph (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8534"">#8534</a>) (<a href=""https://github.com/vitejs/vite/commit/c0d6c60"">c0d6c60</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8534"">#8534</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.11 (2022-06-10)<!-- raw HTML omitted --></h2>; <ul>; <li>fix: respect server.headers in static middlewares (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8481"">#8481</a>) (<a href=""https://github.com/vitejs/vite/commit/ab7dc1c"">ab7dc1c</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8481"">#8481</a></li>; <li>fix(dev): avoid FOUC when swapping out link tag (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7973"">#7973</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8495"">#8495</a>) (<a href=""https://github.com/vitejs/vite/commit/01fa807"">01fa807</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7973"">#7973</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8495"">#8495</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.10 (2022-06-06)<!-- raw HTML omitted --></h2>; <ul>; <li>feat: treat Astro file scripts as TS (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8151"">#8151</a>) (<a href=""https://github.com/vitejs/vite/commit/9fdd0a3"">9fdd0a3</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8151"">#8151</a></li>; <li>feat: new hook <code>configurePreviewServer</code> (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7658"">#7658</a>) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12142:3070,avoid,avoid,3070,https://hail.is,https://github.com/hail-is/hail/pull/12142,2,['avoid'],['avoid']
Safety,"/compare/4.0.0...4.0.1"">https://github.com/samtools/htsjdk/compare/4.0.0...4.0.1</a></p>; <h2>4.0.0</h2>; <h2>Moving forward</h2>; <p>This is the first release to be built exclusively for java 17. Java 17 features are now allowed in our source code and we will no longer support older versions of java. We've also updated dependencies to fix security issues. There are several small bug fixes as well.</p>; <h3>JSON dependency:</h3>; <p>We've dropped the MJSON library which was no longer being updated and replaced it with a similarly small json library from org.json</p>; <h2>What's Changed</h2>; <ul>; <li>Migrate to Java 17 by <a href=""https://github.com/lbergelson""><code>@​lbergelson</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1649"">samtools/htsjdk#1649</a></li>; <li>Remove low-value progress logging message by <a href=""https://github.com/nh13""><code>@​nh13</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1659"">samtools/htsjdk#1659</a></li>; <li>removed redundant code by <a href=""https://github.com/KleinSamuel""><code>@​KleinSamuel</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1664"">samtools/htsjdk#1664</a></li>; <li>Update snappy-java and migrate mjson to org.json to address CVEs by <a href=""https://github.com/bbimber""><code>@​bbimber</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1670"">samtools/htsjdk#1670</a></li>; <li>Remove incorrect zero-length-B-array checks <a href=""https://github.com/gileshall""><code>@​gileshall</code></a> and <a href=""https://github.com/jmarshall""><code>@​jmarshall</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1674"">samtools/htsjdk#1674</a></li>; <li>add SINGULAR platform to read group by <a href=""https://github.com/omicsorama""><code>@​omicsorama</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1635"">samtools/htsjdk#1635</a></li>; </ul>; <h2>New Contributors</h2>; <ul>; <li><a href=""https://gith",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13576:2222,redund,redundant,2222,https://hail.is,https://github.com/hail-is/hail/pull/13576,1,['redund'],['redundant']
Safety,"/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2387,Unsafe,UnsafeRow,2387,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety,"/github.com/aio-libs/aiohttp/issues/6396&gt;</code>_</p>; </li>; <li>; <p>Remove a deprecated usage of pytest.warns(None); <code>[#6663](https://github.com/aio-libs/aiohttp/issues/6663) &lt;https://github.com/aio-libs/aiohttp/issues/6663&gt;</code>_</p>; </li>; <li>; <p>Fix regression where <code>asyncio.CancelledError</code> occurs on client disconnection.; <code>[#6719](https://github.com/aio-libs/aiohttp/issues/6719) &lt;https://github.com/aio-libs/aiohttp/issues/6719&gt;</code>_</p>; </li>; <li>; <p>Export :py:class:<code>~aiohttp.web.PrefixedSubAppResource</code> under; :py:mod:<code>aiohttp.web</code> -- by :user:<code>Dreamsorcerer</code>.</p>; <p>This fixes a regression introduced by :pr:<code>3469</code>.; <code>[#6889](https://github.com/aio-libs/aiohttp/issues/6889) &lt;https://github.com/aio-libs/aiohttp/issues/6889&gt;</code>_</p>; </li>; <li>; <p>Dropped the :class:<code>object</code> type possibility from; the :py:attr:<code>aiohttp.ClientSession.timeout</code>; property return type declaration.; <code>[#6917](https://github.com/aio-libs/aiohttp/issues/6917) &lt;https://github.com/aio-libs/aiohttp/issues/6917&gt;</code>_,</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/30b7a4e99677b4014dda2372504343bb05fc983e""><code>30b7a4e</code></a> Add a yanking caution message to v3.8.2 changelog</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/18279443d4081a02585739d52c5822340068a13f""><code>1827944</code></a> Stop including an empty changelog draft in Sphinx</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/58a4733a17f7d1a29ceda6d8cabd8d4204039038""><code>58a4733</code></a> Mention that v3.8.2 has been yanked</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/13f50f949b8eca81c3809bc79f106e2336d49781""><code>13f50f9</code></a> Move the Python 3.6 attention box to v3.8.3</li>; <li><a h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12296:5289,timeout,timeout,5289,https://hail.is,https://github.com/hail-is/hail/pull/12296,1,['timeout'],['timeout']
Safety,"/importlib_metadata/commit/c8d7285af792d6851227212d4261ce7ae180a87c""><code>c8d7285</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/391"">#391</a> from python/ghpython-93259/from-name-arg-validation-s...</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/91b71494226a95251134c4fe6ea65a1dd25f495c""><code>91b7149</code></a> Update changelog</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/c96dc1e77f032315bfc78f0c1d13c9a61fb68c3f""><code>c96dc1e</code></a> Merge branch 'main' into ghpython-93259/from-name-arg-validation-simple</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/f52757d0c8a9a555d0591a86b334a17028e2ead9""><code>f52757d</code></a> In Distribution.from_name, re-use discover.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/344a6ffc612eec611592e7686264ced72f64da5a""><code>344a6ff</code></a> Refactor Distribution.from_name to avoid return in loop and unnecessary None ...</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/eb19c647519c754dd93b42a0c421101af73cf7a4""><code>eb19c64</code></a> In Distribution.from_name, require a non-empty string. Fixes <a href=""https://github-redirect.dependabot.com/python/cpython/issues/9"">python/cpython#9</a>...</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/d3fe031dbad4590896829f18ecbd8d9d8a132f53""><code>d3fe031</code></a> Add comment about the compatibility factor.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/a4ae953dca38da768bcd1786aeba84bada32efb4""><code>a4ae953</code></a> Add xfail test capturing new expectation.</li>; <li><a href=""https://github.com/python/importlib_metadata/commit/e5b7d8759214feedd0c49a7859ebb124473bcfc3""><code>e5b7d87</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/390"">#390</a> from python/bugfix/noisy-coverage</li>; <li>Additional comm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12000:3422,avoid,avoid,3422,https://hail.is,https://github.com/hail-is/hail/pull/12000,1,['avoid'],['avoid']
Safety,"/li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.24 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2158"">#2158</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f5682a4f6d6d5372a2d382ae3e47dace490ca0d"">4f5682a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.25.0...v2.26.0"">2.26.0</a> (2023-08-03)</h2>; <h3>Features</h3>; <ul>; <li>Implement BufferToDiskThenUpload BlobWriteSessionConfig (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2139"">#2139</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4dad2d5c3a81eda7190ad4f95316471e7fa30f66"">4dad2d5</a>)</li>; <li>Introduce new BlobWriteSession (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2123"">#2123</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e0191b518e50a49fae0691894b50f0c5f33fc6af"">e0191b5</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li><strong>grpc:</strong> Return error if credentials are detected to be null (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2142"">#2142</a>) (<a href=""https://github.com/googleapis/java-storage/commit/b61a9764a9d953d2b214edb2b543b8df42fbfa06"">b61a976</a>)</li>; <li>Possible NPE when HttpStorageOptions deserialized (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2153"">#2153</a>) (<a href=""https://github.com/googleapis/java-storage/commit/68ad8e7357097e3dd161c2ab5f7a42a060a3702c"">68ad8e7</a>)</li>; <li>Update grpc default metadata projection to include acl same as json (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2150"">#2150</a>) (<a href=""https://github.com/googleapis/java-storage/commit/330e795040592e5df22d44fb5216ad7cf2448e81"">330e795</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.14.0 (<a href=""https://redirect.github.com/google",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:6569,detect,detected,6569,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['detect'],['detected']
Safety,"/mrabarnett/mrab-regex) from 2023.3.23 to 2023.5.5.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/mrabarnett/mrab-regex/blob/hg/changelog.txt"">regex's changelog</a>.</em></p>; <blockquote>; <p>Version: 2023.5.5</p>; <pre><code>Removed semicolon after 'else' in 'munge_name'.; </code></pre>; <p>Version: 2023.5.4</p>; <pre><code>Fixed pyproject.toml and setup.py.; </code></pre>; <p>Version: 2023.5.3</p>; <pre><code>pyproject.toml was missing.; </code></pre>; <p>Version: 2023.5.2</p>; <pre><code>Added pyproject.toml.; </code></pre>; <p>Version: 2023.3.23</p>; <pre><code>Git issue 495: Running time for failing fullmatch increases rapidly with input length; Re-enabled modified repeat guards due to regression in speed caused by excessive backtracking.; </code></pre>; <p>Version: 2023.3.22</p>; <pre><code>Git issue 494: Backtracking failure matching regex `^a?(a?)b?c\1$` against string `abca`; Disabled repeat guards. They keep causing issues, and it's just simpler to rely on timeouts.; </code></pre>; <p>Version: 2022.10.31</p>; <pre><code>Updated text for supported Unicode and Python versions.; </code></pre>; <p>Version: 2022.9.13</p>; <pre><code>Updated to Unicode 15.0.0.; </code></pre>; <p>Version: 2022.9.11</p>; <pre><code>Updated version.; </code></pre>; <p>Version: 2022.8.17</p>; <pre><code>Git issue 477: \v for vertical spacing; <p>Added \p{HorizSpace} (\p{H}) and \p{VertSpace} (\p{V}).; </code></pre></p>; <p>Version: 2022.7.25</p>; <pre><code>Git issue 475: 2022.7.24 improperly released; <p>The file <a href=""https://pypi.org/pypi/regex/2022.7.24/json"">https://pypi.org/pypi/regex/2022.7.24/json</a> was missing references to most of the wheels, so this is a new release in the hope that it was just a glitch in GitHub Actions.; </code></pre></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/mrabarnett/mrab-regex/commit/9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12989:1058,timeout,timeouts,1058,https://hail.is,https://github.com/hail-is/hail/pull/12989,1,['timeout'],['timeouts']
Safety,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8083:5262,timeout,timeout,5262,https://hail.is,https://github.com/hail-is/hail/issues/8083,1,['timeout'],['timeout']
Safety,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8053:5272,timeout,timeout,5272,https://hail.is,https://github.com/hail-is/hail/issues/8053,2,['timeout'],['timeout']
Safety,"00 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_statuses': {k: None for k in tasks},; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078:1156,timeout,timeout,1156,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078,6,"['abort', 'timeout']","['aborting', 'timeout']"
Safety,"1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEven",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:2416,safe,safeSetFailure,2416,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['safe'],['safeSetFailure']
Safety,1); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)sun.reflect.generics.reflectiveObjects.NotImplementedException: null; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.ja,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:6554,Unsafe,UnsafeRow,6554,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['Unsafe'],['UnsafeRow']
Safety,"1. Add options to store the scores as sample annotations and the loadings as variant annotations (or, eventually, store by default and write out optionally); 2. Once LD-pruning is implemented, should it be performed first automatically? Probably not, but perhaps an option and the doc should mention the issue.; 3. PLINK has an option to use X-chromosome variants. What is it doing exactly? There are several decisions around encoding hemizygous sites for males. More importantly, does anyone use it? Should we support it?; 4. What about PCA of things other than genotypes, such as missingness? Analysts have mentioned applications to QC and flagged the latter specifically, which is implemented in GCTA.; 5. Extension to multiallelics? Probably not so important as few variants have more than two common alleles and each individual variant generally contributes little. If we did it, a good approach is probably a one-hot encoding although the variance normalization needs some care. For microsatellites/STRs a quantitative rather than categorical encoding may be better.; 6. Support for outlier detection a la SmartPCA and/or EIGENSTRAT?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/205:1097,detect,detection,1097,https://hail.is,https://github.com/hail-is/hail/issues/205,1,['detect'],['detection']
Safety,"1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only found two other places that use hailtop. One of them was a completely unused Dockerfile. I deleted that (`Dockerfile.hailtop`). The other was Dockerfile.ci-test, which I updated to copy the hail_version file just like service-base. https://github.com/hail-is/hail/compare/main...danking:add-version-endpoint. Do you want to cherry-pick that change onto your add-version-endpoint branch?. One more change request: can we remove the try-catch? I don't expect any errors in that call and I tend to avoid revealing anything about errors to our users. Aiohttp will log the error and the stack trace if you let it rise all the way. Sorry for all the complication! Our build system is the second service we built and is clearly showing its age.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401:2562,avoid,avoid,2562,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401,2,['avoid'],['avoid']
Safety,"1. I'm not sure why we don't throw an error. My bash isn't good enough to run both commands and then detect if either failed if the exit code is indeed not equal to 0. My only thought is that maybe the exit code isn't 0 if there are no VMs or disks to delete. 2. Yes, I'll see if I can PR the fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1737459046:101,detect,detect,101,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1737459046,1,['detect'],['detect']
Safety,"1. If we receive an error other than 404 from Google when asking about an instance, we should raise. This is unexpected. (The later lines will fail anyway because spec is `None`); 2. (the main issue) if the instance is not active, do not bother contacting it and, crucially, continue `check_on_instance` eventually learning the instance does not exist.; 3. Drop timeout to 5s to talk to a batch agent. Fixes the zombie instance issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8023:362,timeout,timeout,362,https://hail.is,https://github.com/hail-is/hail/pull/8023,1,['timeout'],['timeout']
Safety,"1. It's feasible to build and ship the compiler + libraries for a limited number of known platforms; (at Physics Speed I did this for Ubuntu-16.04 and one particular version of CentOS). It gets nuts if; you have many different OS'es each of which needs its own compiler build (and it then becomes; another build-system/packaging issue to get all those compilers built correctly for each OS).; Possibly a good thing to do in the long run. Probably not something I could do in the limited time; available. 2. If you build your own compiler + library, then you risk becoming incompatible with other ; libraries on the target system which were built against that system's ""standard"" compiler; and library and header files. e.g. BLAS. [Though this only applies to libraries compiled from C++,; not libraries in C, which might conatin the damage]. So it seemed like the least disruptive path in the short term was to excise the few uses of; std::string and std::stringstream, so that we can build a libhail.so which should work across; a wide variety of Linux systems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424744733:558,risk,risk,558,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424744733,1,['risk'],['risk']
Safety,"1. Makefile is a bit more resilient to changes in the `dk-test` instance that is used to route traffic from GitHub to a local laptop test. It now looks up the ip. The zone is still hardcoded and it's moved to zone `us-central1-a`. The name is also hardcoded to `dk-test`.; 2. I renamed `is_running` to `is_building`; 3. When a job refresh happens, it is now `PRS` responsibility to determine what to do. It starts the same as it always does, updating existing PRs with new job information. The difference is that it tracks which (believed to be) currently building jobs are not seen in the job list. All such jobs are re-built, under the assumption that the job must have failed. cc: @cseed . This should allow CI to recover from the loss of batch. Fixes #4654.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4659:717,recover,recover,717,https://hail.is,https://github.com/hail-is/hail/pull/4659,1,['recover'],['recover']
Safety,"1. The default log path includes the version and a; timestamp. This will help people avoid overwriting; log files, which will help us.; 2. Echo the full path to the log after the hail logo; 3. Add a function `hl.copy_log` which can be used to; copy the session log to a hadoop-api-compliant; location.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4421:85,avoid,avoid,85,https://hail.is,https://github.com/hail-is/hail/pull/4421,1,['avoid'],['avoid']
Safety,"1. Treat any 500 from Docker as a retryable error.; 2. Move DockerError transiency to is_transient_errors and use retry_transient_errors instead of a hand rolled transient wrapper. The first change also makes us robust to changes in error messages on the GCR side. In particular, we started seeing this error message:. ```; Head https://gcr.io/v2/hail-vdc/ubuntu/manifests/18.04: Get https://gcr.io/v2/token?account=_json_key&scope=repository%3Ahail-vdc%2Fubuntu%3Apull&service=gcr.io: net/http: request canceled (Client.Timeout exceeded while awaiting headers); ```. which is slightly different from the extant messages we check for.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11943:521,Timeout,Timeout,521,https://hail.is,https://github.com/hail-is/hail/pull/11943,1,['Timeout'],['Timeout']
Safety,"1. Until we scale up the memory service's throughput, avoid use on the client; and the worker if there are more than 50 partitions. 2. On the driver, open no more than 100 concurrent connections to Google Cloud; Storage. 3. Set a timeout of five seconds to connect or read from Google Cloud Storage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11688:54,avoid,avoid,54,https://hail.is,https://github.com/hail-is/hail/pull/11688,2,"['avoid', 'timeout']","['avoid', 'timeout']"
Safety,1. Use encoded bytes to transfer result from Scala to Python; 2. Use encoded bytes for RelationalLet literals; 3. Optimize after lifting relational operations to eliminate trivial lets; 4. Avoid the memory service for large jobs (eventually we need to scale memory),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11670:189,Avoid,Avoid,189,https://hail.is,https://github.com/hail-is/hail/pull/11670,1,['Avoid'],['Avoid']
Safety,"1. `test_from_entry_expr_simple` does too much in one test. I split into multiple.; 2. `get_dataset` was fine when we had single-threaded tests, but now we are probably executing that, like, 40 times. I just ran the code and saved it as an MT. We have separate tests for split-multi and vcf import.; 3. `test_big_driver_has_big_memory` might have to spin up a machine to service this request.; 4. `test_billing_project_accrued_costs` can take a long time because its adding up billing info.; 5. split some more randomness tests (should be no mega tests left now); 6. relax pc relate timeouts even further (14 minutes!!)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13147:583,timeout,timeouts,583,https://hail.is,https://github.com/hail-is/hail/pull/13147,1,['timeout'],['timeouts']
Safety,"1. log should include job id not job; 2. `client_session` is only used for k8s-internal requests to worker pods, so; use a very aggressive timeout of 10s; 3. reduce refresh delay to two minutes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7026:139,timeout,timeout,139,https://hail.is,https://github.com/hail-is/hail/pull/7026,1,['timeout'],['timeout']
Safety,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5571,timeout,timeout,5571,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,6,['timeout'],['timeout']
Safety,1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:5346,abort,abortStage,5346,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['abort'],['abortStage']
Safety,"13""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JINJA2-6150717"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PROMPTTOOLKIT-6141120"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,624,531,556,604,589,726,434,589,449,399,696,589,479,519,509,711,701,586,586,384,494,539,589],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Cross-site Scripting (XSS)](https://learn.snyk.io/lesson/xss/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:13942,remediat,remediationStrategy,13942,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['remediat'],['remediationStrategy']
Safety,"17]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:6527,abort,abortStage,6527,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['abort'],['abortStage']
Safety,"181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2427,detect,detector,2427,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['detect'],['detector']
Safety,"1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/97"">#97</a>, <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/284"">#284</a>, <a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/300"">#300</a>: Removed compatibility shims for deprecated entry; point interfaces.</li>; </ul>; <h1>v4.13.0</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/396"">#396</a>: Added compatibility for <code>PathDistributions</code> originating; from Python 3.8 and 3.9.</li>; </ul>; <h1>v4.12.0</h1>; <ul>; <li>py-93259: Now raise <code>ValueError</code> when <code>None</code> or an empty; string are passed to <code>Distribution.from_name</code> (and other; callers).</li>; </ul>; <h1>v4.11.4</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/379"">#379</a>: In <code>PathDistribution._name_from_stem</code>, avoid including; parts of the extension in the result.</li>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/381"">#381</a>: In <code>PathDistribution._normalized_name</code>, ensure names; loaded from the stem of the filename are also normalized, ensuring; duplicate entry points by packages varying only by non-normalized; name are hidden.</li>; </ul>; <h1>v4.11.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/372"">#372</a>: Removed cast of path items in FastPath, not needed.</li>; </ul>; <h1>v4.11.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/369"">#369</a>: Fixed bug where <code>EntryPoint.extras</code> was returning; match objects and not the extras strings.</li>; </ul>; <h1>v4.11.1</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/367"">#367</a>: In <code>Distribution.requires</code> for egg-info, if <code>requires.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12391:1292,avoid,avoid,1292,https://hail.is,https://github.com/hail-is/hail/pull/12391,1,['avoid'],['avoid']
Safety,"2, mt.col_idx % 2)); hl.export_vcf(mt, '/tmp/foo.vcf'); ```; It generates a 15GiB file. My initial tests, which used the balding nichols model, had write times of ~8MiB/s. With all my changes, I once saw 177 MIB/s but I think that may have been a fluke. I see pretty consistent ~110MiB/s in the profiler's estimate of bandwidth to the FileOutputStream. When measured by `time python3 test.py` this script writes at ~93MiB/s. Ideally we would hit 250MiB/s (1/8th of an n1-standard-8's network bandwidth), but, considering that we have to split that bandwidth with reading in most cases, ~91 MiB/s ain't so bad. On main, this pipeline writes at 32 MiB/s. The wins in decreasing order of importance were:; 1. Use buffered I/O. All of our exporters should now use buffered I/O because I changed it in the EmitMethodBuilder. I didn't change it in HadoopFS because (a) Hail's native I/O has buffering and (b) buffering and position tracking requires work.; 2. Avoid String allocation, String to UTF8 conversion, and Array[Byte] allocation in VCF writing. In particular, for the most common types of Calls, I just return the UTF8 byte array in a switch statement.; 3. Use a fast path for diploid genotypes. In the worst case, we did 5 branches and now we do 2 which should be well predicted.; 4. Remove an allocation of a lambda in a hot method in Genotype.scala. Future Work:; 1. The randomness stuff still has a lot of low hanging fruit. NumPy can generate random numbers at bandwidths far above what we're managing here.; 2. For VCFs with more entry fields, we should not write ints and floats by generating strings and getting their UTF8 encoding; 3. Invert the writing control flow: serialization methods should take an OutputStream and write bytes directly into it. Contrast that with passing around arrays which are memcopied into a buffer. ---. Range table test:; ```; import hail as hl; hl.init(master='local[1]'); hl._set_flags(write_ir_files='1'); mt = hl.utils.range_matrix_table(n_rows=1000_000,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12733:1791,Avoid,Avoid,1791,https://hail.is,https://github.com/hail-is/hail/pull/12733,1,['Avoid'],['Avoid']
Safety,209); 	at is.hail.io.RichRDDRegionValue$.writeRows$extension(RowStore.scala:526); 	at is.hail.variant.MatrixTable.write(MatrixTable.scala:2393); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.IndexOutOfBoundsException: 3; 	at is.hail.annotations.UnsafeRow.isNullAt(UnsafeRow.scala:294); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spar,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:5261,Unsafe,UnsafeRow,5261,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['Unsafe'],['UnsafeRow']
Safety,"22); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1559.1 in stage 8.0 (TID 21702, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:3343,safe,safeExecute,3343,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['safe'],['safeExecute']
Safety,23) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 			at is.hail.expr.ir.In,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:11342,abort,abortStage,11342,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['abort'],['abortStage']
Safety,"27745>; ##contig=<ID=chrUn_KI270753v1,length=62944>; ##contig=<ID=chrUn_KI270754v1,length=40191>; ##contig=<ID=chrUn_KI270755v1,length=36723>; ##contig=<ID=chrUn_KI270756v1,length=79590>; ##contig=<ID=chrUn_KI270757v1,length=71251>; ##contig=<ID=chrUn_GL000214v1,length=137718>; ##contig=<ID=chrUn_KI270742v1,length=186739>; ##contig=<ID=chrUn_GL000216v2,length=176608>; ##contig=<ID=chrUn_GL000218v1,length=161147>; ##contig=<ID=chrEBV,length=171823>; ##contig=<ID=hs38d1,length=10560522>; ##bcftools_pluginVersion=1.9+htslib-1.9; ##bcftools_pluginCommand=plugin fill-AN-AC; Date=Sat Dec 29 14:52:44 2018; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=RGQ,Number=1,Type=Integer,Description=""Unconditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output 3P5CH.new.vcf --use-new-qual-calculator true --annotation-group StandardAnnotation --annotation-group StandardHCAnnotation --dbsnp /home/fgc3/dbsnp/150/GRCh38/All_20170710.vcf.gz --variant 3P5CH.new.g.vcf.gz --reference /home/fgc3/10x/refdata-GRCh38-2.1.0/fasta/genome.fa --create-output-variant-index false --verbosity ERROR --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:22877,detect,detect,22877,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['detect'],['detect']
Safety,"2>; <h2>Misc</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/259"">#259</a>, <a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a></li>; </ul>; <h2>v4.0.1</h2>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h2>async-timeout 4.0.0</h2>; <h1>Changes</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Drooped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:1289,timeout,timeout,1289,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,"3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13932:10799,remediat,remediationStrategy,10799,https://hail.is,https://github.com/hail-is/hail/pull/13932,1,['remediat'],['remediationStrategy']
Safety,339); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1431); 	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2276); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2861); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2803); 	at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:5161,abort,abortStage,5161,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['abort'],['abortStage']
Safety,348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.r,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:24077,abort,abortStage,24077,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['abort'],['abortStage']
Safety,36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision false --debug false --use-filtered-reads-for-annotations false --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --capture-assembly-failure-bam false --error-correct-reads false --do-not-run-physical-phasing false --min-base-quality-score 10 --smith-waterman JAVA --use-new-qual-calculator false --annotate-with-num-discovered-alleles false --heterozygosity 0.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:2605,recover,recover-dangling-heads,2605,https://hail.is,https://github.com/hail-is/hail/issues/8469,2,['recover'],"['recover-dangling-branches', 'recover-dangling-heads']"
Safety,"36 ms; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:22",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:2680,abort,abortStage,2680,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['abort'],['abortStage']
Safety,"38-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:33,456	hail_logging.py	log:40	https POST /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/activate done in 3.2369999999998527s: 200; ERROR	2022-03-02 19:06:33,492	job.py	schedule_job:473	error while scheduling job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aioht",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:3701,timeout,timeout,3701,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['timeout'],['timeout']
Safety,"3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a hre",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:2298,avoid,avoid,2298,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['avoid'],['avoid']
Safety,"3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""50e1cce8-d68e-4133-a591-e2d1a6257337"",""prPublicId"":""50e1cce8-d68e-4133-a591-e2d1a6257337"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""41.0.4""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr); 🦉 [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr); 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13887:9621,remediat,remediationStrategy,9621,https://hail.is,https://github.com/hail-is/hail/pull/13887,1,['remediat'],['remediationStrategy']
Safety,"3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""d900f7ca-fce7-41d4-a16d-bad109338beb"",""prPublicId"":""d900f7ca-fce7-41d4-a16d-bad109338beb"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""41.0.4""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr); 🦉 [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr); 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13736:9621,remediat,remediationStrategy,9621,https://hail.is,https://github.com/hail-is/hail/pull/13736,1,['remediat'],['remediationStrategy']
Safety,"3e55f8d1a2279d78861af0009d96219fb""><code>2ede7d7</code></a> Fix annotations on <code>__exit__</code> and <code>__aexit__</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/8685c60dc0e82ee246fbe3d1aa272d1dfe57c24c""><code>8685c60</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c7e10db3d0965422122bef263fb36f6fd7572330""><code>c7e10db</code></a> Fix linter configs</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/6af8bf421a799208b27d57c6bf69de0d998fedec""><code>6af8bf4</code></a> Bump pre-commit from 2.15 to 2.16.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/269"">#269</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c71bbb5b5d7330f6dabfde7a1adec30a4611c0be""><code>c71bbb5</code></a> Bump docutils from 0.18 to 0.18.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/266"">#266</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/async-timeout/compare/v3.0.1...v4.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=async-timeout&package-mana",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:5472,timeout,timeout,5472,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,4); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:5227,abort,abortStage,5227,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['abort'],['abortStage']
Safety,"42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh \; --master-machine-type=n1-standard-1 \; --master-boot-disk-size=40GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-1 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --bucket=hail-ci-0-1-dataproc-staging-bucket \; --max-idle=10m; Starting cluster 'ci-test-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 210, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-6boype3d', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:9406,timeout,timeout,9406,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518,2,['timeout'],['timeout']
Safety,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8473:3269,timeout,timeout,3269,https://hail.is,https://github.com/hail-is/hail/issues/8473,1,['timeout'],['timeout']
Safety,"4:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5232,abort,abortStage,5232,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['abort'],['abortStage']
Safety,"4; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:1798,abort,aborted,1798,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['abort'],['aborted']
Safety,5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:9563,abort,abortStage,9563,https://hail.is,https://github.com/hail-is/hail/issues/8545,1,['abort'],['abortStage']
Safety,5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:82390,abort,abortStage,82390,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['abort'],['abortStage']
Safety,"541 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:27:48] ""POST /pod_changed HTTP/1.1"" 204 -; ...skipping...; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1222,timeout,timeout,1222,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038,1,['timeout'],['timeout']
Safety,"56"">https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56</a>)</li>; </ul>; <p><strong>Improvements</strong></p>; <ul>; <li><code>verify=True</code> now reuses a global SSLContext which should improve; request time variance between first and subsequent requests. It should; also minimize certificate load time on Windows systems when using a Python; version built with OpenSSL 3.x. (<a href=""https://redirect.github.com/psf/requests/issues/6667"">#6667</a>)</li>; <li>Requests now supports optional use of character detection; (<code>chardet</code> or <code>charset_normalizer</code>) when repackaged or vendored.; This enables <code>pip</code> and other projects to minimize their vendoring; surface area. The <code>Response.text()</code> and <code>apparent_encoding</code> APIs; will default to <code>utf-8</code> if neither library is present. (<a href=""https://redirect.github.com/psf/requests/issues/6702"">#6702</a>)</li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed bug in length detection where emoji length was incorrectly; calculated in the request content-length. (<a href=""https://redirect.github.com/psf/requests/issues/6589"">#6589</a>)</li>; <li>Fixed deserialization bug in JSONDecodeError. (<a href=""https://redirect.github.com/psf/requests/issues/6629"">#6629</a>)</li>; <li>Fixed bug where an extra leading <code>/</code> (path separator) could lead; urllib3 to unnecessarily reparse the request URI. (<a href=""https://redirect.github.com/psf/requests/issues/6644"">#6644</a>)</li>; </ul>; <p><strong>Deprecations</strong></p>; <ul>; <li>Requests has officially added support for CPython 3.12 (<a href=""https://redirect.github.com/psf/requests/issues/6503"">#6503</a>)</li>; <li>Requests has officially added support for PyPy 3.9 and 3.10 (<a href=""https://redirect.github.com/psf/requests/issues/6641"">#6641</a>)</li>; <li>Requests has officially dropped support for CPython 3.7 (<a href=""https://redirect.github.com/psf/requests/issues/6642"">#6642</a>)</l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14555:1697,detect,detection,1697,https://hail.is,https://github.com/hail-is/hail/pull/14555,2,['detect'],['detection']
Safety,"6); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:107); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:77); at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:552); at is.hail.variant.MatrixTable.count(MatrixTable.scala:550); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.4-d602a3d7472d; Error summary: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:11447,abort,aborted,11447,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['abort'],['aborted']
Safety,"61cd8""><code>acada32</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/readthedocs/sphinx_rtd_theme/issues/1220"">#1220</a> from readthedocs/nienn/fix-sphinx-4-pre-overflow</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/e319184c89b0e11dba77441241fe9a735855fedb""><code>e319184</code></a> Merge branch 'master' into nienn/fix-sphinx-4-pre-overflow</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/20f205fc2a5c7dc813faa0b05ff1152e6ba5a4a6""><code>20f205f</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/readthedocs/sphinx_rtd_theme/issues/1217"">#1217</a> from readthedocs/agj/release-labels</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/2774670572c44555c6d6ecc280dfe34827ef62d4""><code>2774670</code></a> Add CSS max-width to dl.property</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/b557851511509c75cfffd10e1ede1e2266249c0a""><code>b557851</code></a> Make section labels verbose to avoid numeric labels</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/73d1707e791712efb837167065c4173ce9b380f8""><code>73d1707</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/readthedocs/sphinx_rtd_theme/issues/1088"">#1088</a> from readthedocs/Blendify/fix-717</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/3a031121ed86bc2b857f734eede0b48d8164545b""><code>3a03112</code></a> Fix build</li>; <li>Additional commits viewable in <a href=""https://github.com/readthedocs/sphinx_rtd_theme/compare/0.4.2...1.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx-rtd-theme&package-manager=pip&previous-version=0.4.2&new-version=1.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11464:6323,avoid,avoid,6323,https://hail.is,https://github.com/hail-is/hail/pull/11464,2,['avoid'],['avoid']
Safety,"627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; /usr/local/lib/python3.5/site-packages/IPython/core/history.py:228: UserWarning: IPython History requires SQLite, your history will not be saved; warn(""IPython History requires SQLite, your history will not be saved""); Python 3.5.2 (default, Jul 12 2017, 14:00:23) ; Type ""copyright"", ""credits"" or ""license"" for more information. IPython 5.1.0 -- An enhanced Interactive Python.; ? -> Introduction and overview of IPython's features.; %quickref -> Quick reference.; help -> Python's own help system.; object? -> Details about 'object', use 'object??' for extra details.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/09 12:51:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/09 12:51:55 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 12 2017 14:00:23); SparkSession available as 'spark'. In [1]: from hail import *; ---------------------------------------------------------------------------; Impo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609:1298,detect,detected,1298,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609,1,['detect'],['detected']
Safety,"627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; /usr/local/lib/python3.5/site-packages/IPython/core/history.py:228: UserWarning: IPython History requires SQLite, your history will not be saved; warn(""IPython History requires SQLite, your history will not be saved""); Python 3.5.2 (default, Jul 12 2017, 14:00:23) ; Type ""copyright"", ""credits"" or ""license"" for more information. IPython 5.1.0 -- An enhanced Interactive Python.; ? -> Introduction and overview of IPython's features.; %quickref -> Quick reference.; help -> Python's own help system.; object? -> Details about 'object', use 'object??' for extra details.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/10 08:41:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/10 08:41:32 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/10 08:41:32 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/10 08:41:32 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 12 2017 14:00:23); SparkSession available as 'spark'. In [1]: ; ```; -----------------------------; Step2 : read the file with sc.textFile; ```; In [1]: rdd = sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); ```; -----------------------------; Step3, import hail an",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160:1282,detect,detected,1282,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160,1,['detect'],['detected']
Safety,"64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040; dependencies [0x00007fe4a8b83340,0x00007fe4a8b83348] = 8; nul chk table [0x00007fe4a8b83348,0x00007fe4a8b83430] = 232; #; FATAL: caught signal 6 SIGABRT; # If you would like to submit a bug report, please visit:; # http://bugreport.sun.com/bugreport/; #; /tmp/libhail8122447512081932366.so(+0x18f5f)[0x7fe3a7bf0f5f]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fe4be507e97]; /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fe4be509801]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e80b9)[0x7fe4bd7f00b9]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0xaaed23)[0x7fe4bd9b6d23]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(JVM_handle_linux_signal+0x1b4)[0x7fe4bd7fa694]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4718:10553,Unsafe,UnsafeRow,10553,https://hail.is,https://github.com/hail-is/hail/issues/4718,1,['Unsafe'],['UnsafeRow']
Safety,"6eb448e-db52-4a21-b596-3e3ad42aaea1"",""prPublicId"":""46eb448e-db52-4a21-b596-3e3ad42aaea1"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""42.0.2""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-CRYPTOGRAPHY-6036192"",""SNYK-PYTHON-CRYPTOGRAPHY-6050294"",""SNYK-PYTHON-CRYPTOGRAPHY-6092044"",""SNYK-PYTHON-CRYPTOGRAPHY-6126975"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr); 🦉 [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr); 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14296:11356,remediat,remediationStrategy,11356,https://hail.is,https://github.com/hail-is/hail/pull/14296,1,['remediat'],['remediationStrategy']
Safety,"7,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2266,Detect,Detected,2266,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['Detect'],['Detected']
Safety,78); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 11 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:3307,abort,abortStage,3307,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['abort'],['abortStage']
Safety,"7819/icon/m.png ""medium severity"") | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Missing Cryptographic Step <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6036192](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6092044](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | Use of a Broken or Risky Cryptographic Algorithm <br/>[SNYK-PYTHON-PYJWT-2840625](https://snyk.io/vuln/SNYK-PYTHON-PYJWT-2840625) | `pyjwt:` <br> `1.7.1 -> 2.4.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Timing Attack <br/>[SNYK-PYTHON-RSA-1038401](https://snyk.io/vuln/SNYK-PYTHON-RSA-1038401) | `rsa:` <br> `4.5 -> 4.7` <br> | No | No Known Exploit . Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14134:7040,Risk,Risky,7040,https://hail.is,https://github.com/hail-is/hail/pull/14134,1,['Risk'],['Risky']
Safety,"78efaeeca07a""><code>5128f7f</code></a> Fix CHANGES</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/18e718a56f9def7dc40bb9ce3a2962a8fb0c883c""><code>18e718a</code></a> Bump to 4.0.2</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/48d1c4ce923b1da75976d7e2a6ca9234b3092c16""><code>48d1c4c</code></a> Setup towncrier</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/2ede7d73e55f8d1a2279d78861af0009d96219fb""><code>2ede7d7</code></a> Fix annotations on <code>__exit__</code> and <code>__aexit__</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/8685c60dc0e82ee246fbe3d1aa272d1dfe57c24c""><code>8685c60</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c7e10db3d0965422122bef263fb36f6fd7572330""><code>c7e10db</code></a> Fix linter configs</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/6af8bf421a799208b27d57c6bf69de0d998fedec""><code>6af8bf4</code></a> Bump pre-commit from 2.15 to 2.16.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/269"">#269</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c71bbb5b5d7330f6dabfde7a1adec30a4611c0be""><code>c71bbb5</code></a> Bump docutils",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:5028,timeout,timeout,5028,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,"869: in _run_once; event_list = self._selector.select(timeout); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <selectors.EpollSelector object at 0x7fae890f2d30>; timeout = 15.402000000000001. def select(self, timeout=None):; if timeout is None:; timeout = -1; elif timeout <= 0:; timeout = 0; else:; # epoll_wait() has a resolution of 1 millisecond, round away; # from zero to wait *at least* timeout seconds.; timeout = math.ceil(timeout * 1e3) * 1e-3; ; # epoll_wait() expects `maxevents` to be greater than zero;; # we want to make sure that `select()` can be called when no; # FD is registered.; max_ev = max(len(self._fd_to_key), 1); ; ready = []; try:; > fd_event_list = self._selector.poll(timeout, max_ev); E Failed: Timeout >360.0s. usr/lib/python3.9/selectors.py:469: Failed; ------------------------------ Captured log setup ------------------------------; 2023-09-06T21:45:24 INFO test.conftest conftest.py:14:log_before_after starting test; 2023-09-06T21:45:24 INFO hailtop.aiocloud.aioazure.credentials credentials.py:99:default_credentials using credentials file /test-gsa-key/key.json; ------------------------------ Captured log call -------------------------------; 2023-09-06T21:45:25 INFO azure.identity.aio._internal.get_token_mixin get_token_mixin.py:93:get_token ClientSecretCredential.get_token succeeded; 2023-09-06T21:45:25 INFO batch_client.aioclient aioclient.py:809:_submit created batch 191; 2023-09-06T21:47:17 WARNING hailtop.utils utils.py:842:retry_transient_errors_with_debug_string A transient error occured. We will automatically retry. Do not be alarmed. We have thus far seen 2 transient errors (next delay: 3.794s). The most recent error was <class 'asyncio.exceptions.TimeoutError'> . ------------------------------ live log teardown -------------------------------; 2023-09-06T21:51:25 INFO test.conftest conftest.py:16:log_before_after ending test. ```. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:4591,Timeout,TimeoutError,4591,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['Timeout'],['TimeoutError']
Safety,"8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (TID 296, nid00004.urika.com): java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:3308,abort,aborted,3308,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['abort'],['aborted']
Safety,"88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.Or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2319,Unsafe,UnsafeRow,2319,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety,"89 ); 790 retries.sleep(). File /opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py:550, in Retry.increment(self, method, url, response, error, _pool, _stacktrace); 549 if read is False or not self._is_method_retryable(method):; --> 550 raise six.reraise(type(error), error, _stacktrace); 551 elif read is not None:. File /opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py:769, in reraise(tp, value, tb); 768 if value.__traceback__ is not tb:; --> 769 raise value.with_traceback(tb); 770 raise value. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:703, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw); 702 # Make the request on the httplib connection object.; --> 703 httplib_response = self._make_request(; 704 conn,; 705 method,; 706 url,; 707 timeout=timeout_obj,; 708 body=body,; 709 headers=headers,; 710 chunked=chunked,; 711 ); 713 # If we're going to release the connection in ``finally:``, then; 714 # the response doesn't need to know about the connection. Otherwise; 715 # it will also try to release it and we'll have a double-release; 716 # mess. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:449, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code.; --> 449 six.raise_from(e, None); 450 except (SocketTimeout, BaseSSLError, SocketError) as e:. File <string>:3, in raise_from(value, from_value). File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:444, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 443 try:; --> 444 httplib_response = conn.getresponse(); 445 except Ba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:8057,timeout,timeout,8057,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['timeout'],['timeout']
Safety,"9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2412,Unsafe,UnsafeRow,2412,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety,": 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CLI commands to build.yaml to clean up VMs, disks, nics, public ip addresses, and network security groups based on a tag; - Fix batch tests to be cloud agnostic. ## Infrastructure. - Create a shared SSH public key on the VMs for development purposes; - Consider having every PR / namespace deploy resources in a separate resource group rather than having one resource group for all Batch resources! We'd need something to name the resource groups something like `hail-dev_jigold` and `jigold_jigold` for example to handle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10970:2078,redund,redundancy,2078,https://hail.is,https://github.com/hail-is/hail/pull/10970,1,['redund'],['redundancy']
Safety,": 10}. # Group by the array directly (gives an expected error); > mt.aggregate_rows(hl.agg.counter(mt.alleles)); TypeError: unhashable type: 'list'. # Aggregate sorted arrays (works but gives wrong result); > mt.aggregate_rows(hl.agg.counter(hl.delimit(hl.sorted(mt.alleles), '|'))); {'A|A|A|C|\x0b\x00\x00': 2, 'A|A|A|C|C|C': 8}. # Aggregate the sorted arrays directly (segfault); # *This should probably throw ""unhashable type list"" like it does without the sort*; mt.aggregate_rows(hl.agg.counter(hl.sorted(mt.alleles))); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ...; Py4JError: An error occurred while calling o59.executeJSON; ```. Here is the full [stack trace](https://github.com/hail-is/hail/files/4187400/stacktrace.txt) and [core dump](https://github.com/hail-is/hail/files/4187399/coredump.txt). I think some related questions that arise from this are:. 1. What's the best way to group by an array to avoid the conversion to a delimited string? In this case I could do something like ```mt.aggregate_rows(hl.agg.counter(hl.tuple([mt.alleles[0], mt.alleles[1]])))``` but I can't find a solution for getting a tuple from an array without knowing the length of it beforehand for every row. Is there a more fundamental reason why the API doesn't allow aggregation by arrays even if Spark does?; 2. When the Py4J server crashes, it's no longer reachable from the python clients so I have to restart my process and re-initialize Hail. Is there already functionality implemented for bringing that server up if it's down? I'd imagine segfaults aren't the only reason it could down, so it would be nice if there was a way to bring it back up either automatically or manually. Hail version: 0.2.30-2ae07d872f43; Spark version: 2.4.4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8076:1535,avoid,avoid,1535,https://hail.is,https://github.com/hail-is/hail/issues/8076,1,['avoid'],['avoid']
Safety,": BatchClient):; b = create_batch(client); resources = {'machine_type': smallest_machine_type()}; j = b.create_job(DOCKER_ROOT_IMAGE, ['true'], resources=resources, always_run=True); b.submit(); b.cancel(); > status = j.wait(). io/test/test_batch.py:1487: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; usr/local/lib/python3.9/dist-packages/hailtop/batch_client/client.py:84: in wait; return async_to_blocking(self._async_job.wait()); usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:156: in async_to_blocking; return loop.run_until_complete(task); usr/lib/python3.9/asyncio/base_events.py:634: in run_until_complete; self.run_forever(); usr/lib/python3.9/asyncio/base_events.py:601: in run_forever; self._run_once(); usr/lib/python3.9/asyncio/base_events.py:1869: in _run_once; event_list = self._selector.select(timeout); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <selectors.EpollSelector object at 0x7fae890f2d30>; timeout = 15.402000000000001. def select(self, timeout=None):; if timeout is None:; timeout = -1; elif timeout <= 0:; timeout = 0; else:; # epoll_wait() has a resolution of 1 millisecond, round away; # from zero to wait *at least* timeout seconds.; timeout = math.ceil(timeout * 1e3) * 1e-3; ; # epoll_wait() expects `maxevents` to be greater than zero;; # we want to make sure that `select()` can be called when no; # FD is registered.; max_ev = max(len(self._fd_to_key), 1); ; ready = []; try:; > fd_event_list = self._selector.poll(timeout, max_ev); E Failed: Timeout >360.0s. usr/lib/python3.9/selectors.py:469: Failed; ------------------------------ Captured log setup ------------------------------; 2023-09-06T21:45:24 INFO test.conftest conftest.py:14:log_before_after starting test; 2023-09-06T21:45:24 INFO hailtop.aiocloud.aioazure.credentials credentials.py:99:default_credentials using credentials file /test-gsa-key/key.json; ------------------------------ Captured log ca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:3044,timeout,timeout,3044,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['timeout'],['timeout']
Safety,": INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 36) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:1995,abort,aborted,1995,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['abort'],['aborted']
Safety,": global model fit: beta = Map(intercept -> 0.0370042272400176, sa.cov -> -0.012886009824596447); 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: sigmaG2 = 0.13829390418697945; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: sigmaE2 = 0.8304138510277874; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: delta = 6.004703214575758; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: h2 = 0.1427612233333665; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: seH2 = 0.13770872661270844; 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Cov1' as type `Float64' (user-specified); Loading column `Cov2' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Pheno' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: No multiallelics detected.; 2017-08-28 21:47:48 Hail: INFO: Coerced sorted dataset; 2017-08-28 21:47:48 Hail: WARN: called redundant `filtermulti' on an already split or multiallelic-filtered VDS; 2017-08-28 21:47:48 Hail: INFO: rrm: Computing Realized Relationship Matrix...; 2017-08-28 21:47:49 Hail: INFO: rrm: RRM computed using 3 variants.; 2017-08-28 21:47:49 Hail: WARN: 1 of 8 samples have a missing phenotype or covariate.; 2017-08-28 21:47:49 Hail: INFO: lmmreg: running lmmreg on 7 samples with 3 sample covariates including intercept...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Computing eigendecomposition of kinship matrix...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Estimating delta using REML... ; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 1 to 7: 3.09757, 2.66667, 2.23576, 0.00000, 0.00000, -0.00000, -0.00000; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 7 to 1: -0.00000, -0.00000, 0.00000, 0.00000, 2.23576, 2.66667, 3.09757; 2017-08-28 21:47:50 Hail: INFO: lmmreg: globa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:5635,detect,detected,5635,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,1,['detect'],['detected']
Safety,:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12422,abort,abortStage,12422,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['abort'],['abortStage']
Safety,":issue:<code>2235</code></li>; </ul>; <h2>Version 8.1.1</h2>; <p>Released 2022-03-30</p>; <ul>; <li>Fix an issue with decorator typing that caused type checking to; report that a command was not callable. :issue:<code>2227</code></li>; </ul>; <h2>Version 8.1.0</h2>; <p>Released 2022-03-28</p>; <ul>; <li>; <p>Drop support for Python 3.6. :pr:<code>2129</code></p>; </li>; <li>; <p>Remove previously deprecated code. :pr:<code>2130</code></p>; <ul>; <li><code>Group.resultcallback</code> is renamed to <code>result_callback</code>.</li>; <li><code>autocompletion</code> parameter to <code>Command</code> is renamed to; <code>shell_complete</code>.</li>; <li><code>get_terminal_size</code> is removed, use; <code>shutil.get_terminal_size</code> instead.</li>; <li><code>get_os_args</code> is removed, use <code>sys.argv[1:]</code> instead.</li>; </ul>; </li>; <li>; <p>Rely on :pep:<code>538</code> and :pep:<code>540</code> to handle selecting UTF-8 encoding; instead of ASCII. Click's locale encoding detection is removed.; :issue:<code>2198</code></p>; </li>; <li>; <p>Single options boolean flags with <code>show_default=True</code> only show; the default if it is <code>True</code>. :issue:<code>1971</code></p>; </li>; <li>; <p>The <code>command</code> and <code>group</code> decorators can be applied with or; without parentheses. :issue:<code>1359</code></p>; </li>; <li>; <p>The <code>Path</code> type can check whether the target is executable.; :issue:<code>1961</code></p>; </li>; <li>; <p><code>Command.show_default</code> overrides <code>Context.show_default</code>, instead; of the other way around. :issue:<code>1963</code></p>; </li>; <li>; <p>Parameter decorators and <code>@group</code> handles <code>cls=None</code> the same as; not passing <code>cls</code>. <code>@option</code> handles <code>help=None</code> the same as; not passing <code>help</code>. :issue:<code>[#1959](https://github.com/pallets/click/issues/1959)</code></p>; </li>; </ul>; <!-- raw HTML omitted -->; </bloc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11801:3474,detect,detection,3474,https://hail.is,https://github.com/hail-is/hail/pull/11801,1,['detect'],['detection']
Safety,":issue:<code>2235</code></li>; </ul>; <h2>Version 8.1.1</h2>; <p>Released 2022-03-30</p>; <ul>; <li>Fix an issue with decorator typing that caused type checking to; report that a command was not callable. :issue:<code>2227</code></li>; </ul>; <h2>Version 8.1.0</h2>; <p>Released 2022-03-28</p>; <ul>; <li>; <p>Drop support for Python 3.6. :pr:<code>2129</code></p>; </li>; <li>; <p>Remove previously deprecated code. :pr:<code>2130</code></p>; <ul>; <li><code>Group.resultcallback</code> is renamed to <code>result_callback</code>.</li>; <li><code>autocompletion</code> parameter to <code>Command</code> is renamed to; <code>shell_complete</code>.</li>; <li><code>get_terminal_size</code> is removed, use; <code>shutil.get_terminal_size</code> instead.</li>; <li><code>get_os_args</code> is removed, use <code>sys.argv[1:]</code> instead.</li>; </ul>; </li>; <li>; <p>Rely on :pep:<code>538</code> and :pep:<code>540</code> to handle selecting UTF-8 encoding; instead of ASCII. Click's locale encoding detection is removed.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/click/commit/9c6f4c8e1bb8670ce827c98559f57f6ee5935cd0""><code>9c6f4c8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/click/issues/2262"">#2262</a> from pallets/release-8.1.3</li>; <li><a href=""https://github.com/pallets/click/commit/5ec77494bdf2c294d3b082bed429ebce78321431""><code>5ec7749</code></a> release version 8.1.3</li>; <li><a href=""https://github.com/pallets/click/commit/2ac3211cb79a63bae8e6f0441136b432ec2126bc""><code>2ac3211</code></a> update requirements</li>; <li><a href=""https://github.com/pallets/click/commit/5fd87bdf80ed450334b37344f6c99890c217d3db""><code>5fd87bd</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/click/issues/2248"">#2248</a> from jreese/8.1.x</li>; <li><a href=""https://github.com/pal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11808:4304,detect,detection,4304,https://hail.is,https://github.com/hail-is/hail/pull/11808,1,['detect'],['detection']
Safety,; 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:3129,abort,abortStage,3129,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['abort'],['abortStage']
Safety,"; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently schedu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10705:2654,timeout,timeout,2654,https://hail.is,https://github.com/hail-is/hail/pull/10705,1,['timeout'],['timeout']
Safety,"; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:7656,abort,aborted,7656,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['abort'],['aborted']
Safety,"; 707 timeout=timeout_obj,; 708 body=body,; 709 headers=headers,; 710 chunked=chunked,; 711 ); 713 # If we're going to release the connection in ``finally:``, then; 714 # the response doesn't need to know about the connection. Otherwise; 715 # it will also try to release it and we'll have a double-release; 716 # mess. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:449, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code.; --> 449 six.raise_from(e, None); 450 except (SocketTimeout, BaseSSLError, SocketError) as e:. File <string>:3, in raise_from(value, from_value). File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:444, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 443 try:; --> 444 httplib_response = conn.getresponse(); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code. File /opt/conda/lib/python3.10/http/client.py:1375, in HTTPConnection.getresponse(self); 1374 try:; -> 1375 response.begin(); 1376 except ConnectionError:. File /opt/conda/lib/python3.10/http/client.py:318, in HTTPResponse.begin(self); 317 while True:; --> 318 version, status, reason = self._read_status(); 319 if status != CONTINUE:. File /opt/conda/lib/python3.10/http/client.py:287, in HTTPResponse._read_status(self); 284 if not line:; 285 # Presumably, the server closed the connection before; 286 # sending a valid response.; --> 287 raise RemoteDisconnected(""Remote end closed connection without""; 288 "" response""); 289 try:. ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection witho",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:9029,timeout,timeout,9029,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['timeout'],['timeout']
Safety,"; 707 timeout=timeout_obj,; 708 body=body,; 709 headers=headers,; 710 chunked=chunked,; 711 ); 713 # If we're going to release the connection in ``finally:``, then; 714 # the response doesn't need to know about the connection. Otherwise; 715 # it will also try to release it and we'll have a double-release; 716 # mess. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:449, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code.; --> 449 six.raise_from(e, None); 450 except (SocketTimeout, BaseSSLError, SocketError) as e:. File <string>:3, in raise_from(value, from_value). File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:444, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw); 443 try:; --> 444 httplib_response = conn.getresponse(); 445 except BaseException as e:; 446 # Remove the TypeError from the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code. File /opt/conda/lib/python3.10/http/client.py:1375, in HTTPConnection.getresponse(self); 1374 try:; -> 1375 response.begin(); 1376 except ConnectionError:. File /opt/conda/lib/python3.10/http/client.py:318, in HTTPResponse.begin(self); 317 while True:; --> 318 version, status, reason = self._read_status(); 319 if status != CONTINUE:. File /opt/conda/lib/python3.10/http/client.py:287, in HTTPResponse._read_status(self); 284 if not line:; 285 # Presumably, the server closed the connection before; 286 # sending a valid response.; --> 287 raise RemoteDisconnected(""Remote end closed connection without""; 288 "" response""); 289 try:. RemoteDisconnected: Remote end closed connection without response. During handling of the abo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:5105,timeout,timeout,5105,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['timeout'],['timeout']
Safety,"; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dropped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:2876,timeout,timeout,2876,https://hail.is,https://github.com/hail-is/hail/pull/11465,2,"['Timeout', 'timeout']","['Timeout', 'timeout']"
Safety,"; </li>; <li>; <p>Drooped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:2305,timeout,timeout,2305,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,"; <li>1598_, [Windows]: <code>disk_partitions()</code>_ only returns mountpoints on drives; where it first finds one.</li>; <li>1874_, [SunOS]: swap output error due to incorrect range.</li>; <li>1892_, [macOS]: <code>cpu_freq()</code>_ broken on Apple M1.</li>; <li>1901_, [macOS]: different functions, especially <code>Process.open_files()</code>_ and; <code>Process.connections()</code><em>, could randomly raise <code>AccessDenied</code></em> because the; internal buffer of <code>proc_pidinfo(PROC_PIDLISTFDS)</code> syscall was not big enough.; We now dynamically increase the buffer size until it's big enough instead of; giving up and raising <code>AccessDenied</code>_, which was a fallback to avoid crashing.</li>; <li>1904_, [Windows]: <code>OpenProcess</code> fails with <code>ERROR_SUCCESS</code> due to; <code>GetLastError()</code> called after <code>sprintf()</code>. (patch by alxchk)</li>; <li>1913_, [Linux]: <code>wait_procs()</code>_ should catch <code>subprocess.TimeoutExpired</code>; exception.</li>; <li>1919_, [Linux]: <code>sensors_battery()</code>_ can raise <code>TypeError</code> on PureOS.</li>; <li>1921_, [Windows]: <code>swap_memory()</code>_ shows committed memory instead of swap.</li>; <li>1940_, [Linux]: psutil does not handle <code>ENAMETOOLONG</code> when accessing process; file descriptors in procfs. (patch by Nikita Radchenko)</li>; <li>1948_, <strong>[critical]</strong>: <code>memoize_when_activated</code> decorator is not thread-safe.; (patch by Xuehai Pan)</li>; <li>1953_, [Windows], <strong>[critical]</strong>: <code>disk_partitions()</code>_ crashes due to; insufficient buffer len. (patch by MaWe2019)</li>; <li>1965_, [Windows], <strong>[critical]</strong>: fix &quot;Fatal Python error: deallocating None&quot;; when calling <code>users()</code>_ multiple times.</li>; <li>1980_, [Windows]: 32bit / WoW64 processes fails to read <code>Process.name()</code>_ longer</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11459:2698,Timeout,TimeoutExpired,2698,https://hail.is,https://github.com/hail-is/hail/pull/11459,1,['Timeout'],['TimeoutExpired']
Safety,"; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-nam",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2478,timeout,timeoutSeconds,2478,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611,2,['timeout'],['timeoutSeconds']
Safety,"; at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:7); at is.hail.backend.Backend.execute(Backend.scala:86); at is.hail.backend.Backend.executeJSON(Backend.scala:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the maximum alloreamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_15657888296Exception.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOExcee.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.netty.channel.Abst",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:16186,abort,aborted,16186,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['abort'],['aborted']
Safety,; at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:4287,abort,abortStage,4287,https://hail.is,https://github.com/hail-is/hail/issues/3413,1,['abort'],['abortStage']
Safety,"; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2458,Unsafe,UnsafeRow,2458,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['Unsafe'],['UnsafeRow']
Safety,"</a>, <a href=""https://github.com/mattcary""><code>@​mattcary</code></a>)</li>; <li>Client-go impersonation config can specify a UID to pass impersonated uid information through in requests. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104483"">kubernetes/kubernetes#104483</a>, <a href=""https://github.com/margocrawf""><code>@​margocrawf</code></a>)</li>; <li>Create HPA v2 from v2beta2 with some fields changed. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/102534"">kubernetes/kubernetes#102534</a>, <a href=""https://github.com/wangyysde""><code>@​wangyysde</code></a>) [SIG API Machinery, Apps, Auth, Autoscaling and Testing]</li>; <li>Ephemeral containers graduated to beta and are now available by default. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105405"">kubernetes/kubernetes#105405</a>, <a href=""https://github.com/verb""><code>@​verb</code></a>)</li>; <li>Fix kube-proxy regression on UDP services because the logic to detect stale connections was not considering if the endpoint was ready. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106163"">kubernetes/kubernetes#106163</a>, <a href=""https://github.com/aojea""><code>@​aojea</code></a>) [SIG API Machinery, Apps, Architecture, Auth, Autoscaling, CLI, Cloud Provider, Contributor Experience, Instrumentation, Network, Node, Release, Scalability, Scheduling, Storage, Testing and Windows]</li>; <li>If a conflict occurs when creating an object with <code>generateName</code>, the server now returns an &quot;AlreadyExists&quot; error with a retry option. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104699"">kubernetes/kubernetes#104699</a>, <a href=""https://github.com/vincepri""><code>@​vincepri</code></a>)</li>; <li>Implement support for recovering from volume expansion failures (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106154"">kubernetes/kubernetes#",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:3747,detect,detect,3747,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['detect'],['detect']
Safety,"</code></a>) [SIG Apps, Auth, Node, Release, Scheduling and Testing]</li>; <li>ServiceAppProtocol feature gate is now beta and enabled by default, adding new AppProtocol field to Services and Endpoints. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/90023"">kubernetes/kubernetes#90023</a>, <a href=""https://github.com/robscott""><code>@​robscott</code></a>) [SIG Apps and Network]</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/bcfd4ed2ec3b2f503adc4f2e681f9404216d302c""><code>bcfd4ed</code></a> chore: update version</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/37f5d63425976b463bb83348d592859a82f2b5af""><code>37f5d63</code></a> chore: update changelog</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/ef2fe15d3473a38f1c13558acf05631d909560ce""><code>ef2fe15</code></a> fix: watch returns raw_object if detection of returned objects fail (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/177"">#177</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/59a6e2a60fb7592e596447555c2da2797b7273a9""><code>59a6e2a</code></a> chore(deps): update sphinx requirement (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/175"">#175</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/f4e11b7223e546515e99c984f9948b6caa06622a""><code>f4e11b7</code></a> chore: update version</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/bf4a9a8eb24149cd68efbc6ae61a6445121f4b70""><code>bf4a9a8</code></a> chore: update setup.py</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/f028cc793e3a2c519be6a52a49fb77ff0b014c9b""><code>f028cc7</code></a> [feat] regenerate client for v1.19.15 (<a href=""https://github-redirect.dependabot.com/tomplus/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:14795,detect,detection,14795,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['detect'],['detection']
Safety,"</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dropped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:2585,Timeout,TimeoutError,2585,https://hail.is,https://github.com/hail-is/hail/pull/11465,3,"['Timeout', 'timeout']","['TimeoutError', 'timeout']"
Safety,"</p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dropped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:2445,timeout,timeout,2445,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['timeout'],['timeout']
Safety,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Use of a Broken or Risky Cryptographic Algorithm <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6149518](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6149518) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Uncontrolled Resource Consumption (&#x27;Resource Exhaustion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6157248](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6157248) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **451/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.3 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14236:872,Risk,Risky,872,https://hail.is,https://github.com/hail-is/hail/pull/14236,1,['Risk'],['Risky']
Safety,"= 3.15.0, &lt;4.0.0dev (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-core/issues/385"">#385</a>) (<a href=""https://github.com/googleapis/python-api-core/commit/d84d66c2a4107f5f9a20c53e870a27fb1250ea3d"">d84d66c</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/python-api-core/compare/v2.7.3...v2.8.0"">2.8.0</a> (2022-05-18)</h2>; <h3>Features</h3>; <ul>; <li>adds support for audience in client_options (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-core/issues/379"">#379</a>) (<a href=""https://github.com/googleapis/python-api-core/commit/c97c4980125a86f384cdf12720df7bb1a2adf9d2"">c97c498</a>)</li>; <li>adds support for audience in client_options. (<a href=""https://github.com/googleapis/python-api-core/commit/c97c4980125a86f384cdf12720df7bb1a2adf9d2"">c97c498</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/python-api-core/compare/v2.7.2...v2.7.3"">2.7.3</a> (2022-04-29)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Avoid AttributeError if grpcio-status is not installed (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-core/issues/370"">#370</a>) (<a href=""https://github.com/googleapis/python-api-core/commit/022add16266f9c07f0f88eea13472cc2e0bfc991"">022add1</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/python-api-core/compare/v2.7.1...v2.7.2"">2.7.2</a> (2022-04-13)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>allow grpc without grpcio-status (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-core/issues/355"">#355</a>) (<a href=""https://github.com/googleapis/python-api-core/commit/112049e79f5a5b0a989d85d438a1bd29485f46f7"">112049e</a>)</li>; <li>remove dependency on pkg_resources (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-core/issues/361"">#361</a>) (<a href=""https://github.com/googleapis/python-api-core/commit/523dbd0b10d37ffcf83fa751f0bad313f162abf1"">523dbd0</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/python-api-core/compare/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11970:6834,Avoid,Avoid,6834,https://hail.is,https://github.com/hail-is/hail/pull/11970,1,['Avoid'],['Avoid']
Safety,"=""analysis_type=CombineVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=[(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recalibrated.vcf), (RodBinding name=variant2 source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.filtered.vcf)] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub genotypemergeoption=UNSORTED filteredrecordsmergetype=KEEP_IF_ANY_UNFILTERED multipleallelesmergetype=BY_TYPE rod_priority_list=null printComplexMerges=false filteredAreUncalled=false minimalVCF=false",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:3061,unsafe,unsafe,3061,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['unsafe'],['unsafe']
Safety,"=""https://github-redirect.dependabot.com/psf/black/issues/2951"">GH-2951</a>)</li>; <li><a href=""https://github.com/psf/black/commit/bd1e98034907463f5d86f4d87e89202dc6c34dd4""><code>bd1e980</code></a> Remove unnecessary parentheses from <code>except</code> clauses (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2939"">#2939</a>)</li>; <li><a href=""https://github.com/psf/black/commit/14d84ba2e96c5ca1351b8fe4d0d415cc148f4117""><code>14d84ba</code></a> Resolve new flake8-bugbear errors (B020) (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2950"">GH-2950</a>)</li>; <li><a href=""https://github.com/psf/black/commit/14e5ce5412efa53438df0180e735b3834df3b579""><code>14e5ce5</code></a> Remove unnecessary parentheses from tuple unpacking in <code>for</code> loops (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2945"">#2945</a>)</li>; <li><a href=""https://github.com/psf/black/commit/3800ebd81df6a1c31d1eac8cc15899537b9cbb61""><code>3800ebd</code></a> Avoid magic-trailing-comma in single-element subscripts (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2942"">#2942</a>)</li>; <li><a href=""https://github.com/psf/black/commit/062b54931dc3ea35f673e755893fe28ff1f5a889""><code>062b549</code></a> Github now supports .git-blame-ignore-revs (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2948"">GH-2948</a>)</li>; <li><a href=""https://github.com/psf/black/commit/5379d4f3f460ec9b7063dd1cc10f437b0edf9ae3""><code>5379d4f</code></a> stub style: remove some possible future changes (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2940"">#2940</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/psf/black/compare/22.1.0...22.3.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=black&package-manager=pip&previous-version=22.1.0&new-version=22.3.0)](https://docs.git",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696:8294,Avoid,Avoid,8294,https://hail.is,https://github.com/hail-is/hail/pull/11696,1,['Avoid'],['Avoid']
Safety,"=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527:2061,abort,abortStage,2061,https://hail.is,https://github.com/hail-is/hail/issues/2527,1,['abort'],['abortStage']
Safety,"==2024.6.0', 'yarl==1.9.4']; Collecting https://github.com/hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979a53d0>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef9797e050>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979ba910>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979cc310>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979cce10>, 'Connection to github.com timed out. (connect timeout=15)')': /hail-is/jgscm/archive/v0.1.13+hail.zip; ERROR: Could not install packages due to an OSError: HTTPSConnectionPool(host='github.com', port=443): Max retries exceeded with url: /hail-is/jgscm/archive/v0.1.13+hail.zip (Caused by ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef979ce290>, 'Connection to github.com timed out. (connect timeo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14652:3555,timeout,timeout,3555,https://hail.is,https://github.com/hail-is/hail/issues/14652,1,['timeout'],['timeout']
Safety,"> > I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.; > ; > Can you elaborate more? I'm not sure which code paths you are referring to. Mainly that the commit procedure branches on whether the start id is 1 and that we sometimes grab the update id from the batch token and sometimes from the update table. Not very different code paths but slightly different, which could lead to some confusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403:159,avoid,avoid,159,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403,2,['avoid'],['avoid']
Safety,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391:78,avoid,avoid,78,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391,2,['avoid'],['avoid']
Safety,"> A couple notes. I'm not sure what this should ultimately look like. I think I want `hailctl create user`, which probably creates a batch that does all the creation? That avoids setting up tunnels to the sql server, etc. Sure, if that's the way we want this to work, I will modify to do that. Seems more elegant than having tunnels, which predates our current deployment solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6619#issuecomment-515501206:172,avoid,avoids,172,https://hail.is,https://github.com/hail-is/hail/pull/6619#issuecomment-515501206,1,['avoid'],['avoids']
Safety,"> A user reported this error concurrent.futures._base.TimeoutError with no stack trace while copying files in a batch job. . This. No stack trace. If you look at this output, the previous stack trace is part of the WARNING message. ```; INFO:deploy_config:deploy config file not found: None; INFO:hailtop.aiocloud.aiogoogle.credentials:using credentials file /gsa-key/key.json: GoogleServiceAccountCredentials for XXXXX@PROJECT.iam.gserviceaccount.com; WARNING:hailtop.utils:Encountered 2 errors (current delay: 0.2). My stack trace is File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 128, in <module>; asyncio.run(main()); File ""/usr/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 735, in retry_transient_errors; st = ''.join(traceback.format_stack()); . Most recent error was; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 729, in retry_transient_errors; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 134, in request_and_raise_for_status; resp = await self.client_session._request(method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 634, in _request; break; File ""/usr/local/lib/python3.7/dist-packages/aiohttp/helpers.py"", line 721, in __exit__; raise asyncio.TimeoutError from None; concurrent.futures._base.TimeoutError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330:54,Timeout,TimeoutError,54,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330,3,['Timeout'],['TimeoutError']
Safety,"> Am I strange in that I want to name something what it is (ci, batch, etc.); > rather than give everything codenames? The purpose of codenames is to hide and; > obscure, you know. Good point. > I think this should be called tutorial. And when it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:477,avoid,avoid,477,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,4,"['avoid', 'timeout']","['avoid', 'timeouts']"
Safety,"> And there's a slight question about whether your libstdc++ will work against the other systems libc.so. Is there something special about libstdc++ here? Our code will certainly call libc directly, too. As far as I know there no proposal on the table handles incompatible libc's. I think we'd have to make multiple builds. Luckily, as far as I know, recent distributions have compatible libc so this shouldn't be an issue. > Avoiding std::string in libhail.so keeps the can closed for now. Can you explain why std::string is special here vs the rest of the standard library? What assumption are you working under?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424794222:426,Avoid,Avoiding,426,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424794222,1,['Avoid'],['Avoiding']
Safety,"> Banning versions completely is a little tricky because the user can specify a JAR url directly instead of a version. JARs don't currently have a simple way to report pip version to the worker, though we could cook something up. We could also just delete the old JARs. I feel like we should make a (cached) request to ensure that the JAR exists in the front-end upon job submission and return a 400 if it doesn't exist instead of waiting for the worker to error. It would:. - Allow us to remove support by deleting old jars; - Fail fast (I know I have accidentally messed up deploying a dev jar and had to wait until a worker came online to find out); - Avoid alerts from workers that can't find dev jars due to mistakes like I mention above",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664:655,Avoid,Avoid,655,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664,1,['Avoid'],['Avoid']
Safety,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:296,timeout,timeouts,296,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659,2,['timeout'],['timeouts']
Safety,"> Forks would indeed need to overwrite ours, but since the file wouldn't change much it seems like that's not much of a hassle to maintain, right Leo? And ya this seems like a fine change but we would need to follow up right afterward with our own credentials. Yes, alternatively we could also use the GitHub organization name or something similar when constructing the file path to encrypted credentials, to avoid collisions completely. (Forks like the CPG one would only add files to their deployments.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11724#issuecomment-1171065130:409,avoid,avoid,409,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1171065130,1,['avoid'],['avoid']
Safety,"> I do not know why the retries setting in pip.conf did not catch https://ci.hail.is/batches/167314/jobs/27, but more retries never hurt anyone. Is the whole pip process crashing because of the exception? Perhaps a different kind of crash that pip can't recover from and retry. > Another CI-related PR. This one changes the base image of everything else: hail-ubuntu. It's an ubuntu image with two scripts that make pip and apt more resilient. Take a look at docker/hail-ubuntu. Should we do this for `apt` too?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906#issuecomment-767033129:254,recover,recover,254,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-767033129,1,['recover'],['recover']
Safety,"> I don't see how a hard coded list in the repo is any different than a compare and swap with the SKU stored in the database. I don't think we can get around parsing the description in some form when we initialize the product. . I think I'm missing context here, especially surrounding what the source of truth is for a ""product"". Maybe we should have a meeting or discussion in a different forum at some point? I don't know how volatile the product descriptions are, but my current understanding is that this is all pretty low risk/priority. For posterity, my main concern is around reproducibility. Assume a description change happens tomorrow and now however we're parsing the description ends up pointing at a different SKU. Now at any point in the future, if we have some data loss and have to rebuild these tables, the SKU will suddenly change, but only because we rebuilt the database not because of some actual change in GCP. Or if we or anyone else stands up a new batch cluster, they will get a SKU that differs from what we have currently in `hail-vdc`, just because they started up their cluster at a different point in time. With my current understanding that is an undesirable scenario, but I don't actually know what the implications of such a scenario are.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13607#issuecomment-1736105248:528,risk,risk,528,https://hail.is,https://github.com/hail-is/hail/pull/13607#issuecomment-1736105248,1,['risk'],['risk']
Safety,"> I haven't checked if new bokeh supports old pandas. Nor do I know if we have old pandas usage lurking in the codebase. Can we make our pinned-requirements.txt use pandas 2.0, fix whatever issues arise, but leave requirements.txt flexible for folks?. I think there are compromises either way, but I would be surprised if this just worked. It seems very easy to accidentally adopt new functionality so at that point why even have a lower-bound? I think that, while it's very hard to make sure that all our dependencies are compatible at all possible version combinations, and these things will happen, it just feels like an easily-avoidable lie to say we support 1.x and 2.x but then use functionality exclusive to 2.x. So I'm ok keeping the bounds more relaxed if we can guarantee that *our* usage of pandas is compatible with both. FWIW, I think that our primary dependencies release major versions infrequently enough that it is reasonable to only support the most recent major version, in much the same way that we don't support python versions indefinitely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520500573:631,avoid,avoidable,631,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520500573,1,['avoid'],['avoidable']
Safety,"> I kinda prefer a fresh file with freshly written tests? I feel like it's a bit hard to get a total view of what is and is not tests when we're using annotations. I agree a single file feels nice, but I'm a little hesitant to copy paste tests. Unless you think I should move where these tests are? That also feels weird. Does the following pytest invocation make you feel better about markers?. ```; (hail) dgoldste@wmce3-cb7 hail % pytest --collect-only -m qobtest hail/python/test; ============================================================================== test session starts ===============================================================================; platform darwin -- Python 3.9.17, pytest-7.4.0, pluggy-1.3.0; rootdir: /Users/dgoldste/hail/hail/python/test; configfile: pytest.ini; plugins: anyio-4.0.0, xdist-2.5.0, instafail-0.5.0, timeout-2.1.0, devtools-0.12.2, asyncio-0.21.1, timestamper-0.0.9, metadata-3.0.0, html-1.22.1, forked-1.6.0; asyncio: mode=auto; collected 8218 items / 8133 deselected / 85 selected. <Package hail>; <Module test_context.py>; <Function test_init_hail_context_twice>; <Function test_top_level_functions_are_do_not_error>; <Function test_tmpdir_runs>; <Module test_randomness.py>; <Function test_table_explode>; <Package backend>; <Module test_service_backend.py>; <Function test_tiny_driver_has_tiny_memory>; <Function test_big_driver_has_big_memory>; <Function test_tiny_worker_has_tiny_memory>; <Function test_big_worker_has_big_memory>; <Function test_regions>; <Package expr>; <Module test_expr.py>; <UnitTestCase Tests>; <TestCaseFunction test_aggregators>; <TestCaseFunction test_densify_table>; <TestCaseFunction test_scan>; <Package genetics>; <Module test_reference_genome.py>; <Function test_reference_genome>; <Function test_reference_genome_sequence>; <Function test_reference_genome_liftover>; <Function test_read_custom_reference_genome>; <Package matrixtable>; <Module test_grouped_matrix_table.py>; <UnitTestCase Tests>; <TestCaseFunct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13620#issuecomment-1720268851:851,timeout,timeout-,851,https://hail.is,https://github.com/hail-is/hail/pull/13620#issuecomment-1720268851,1,['timeout'],['timeout-']
Safety,"> I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first. Can you elaborate more? I'm not sure which code paths you are referring to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494:157,avoid,avoid,157,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494,2,['avoid'],['avoid']
Safety,"> I think if it's not too hard of a change, we should add the files with encoded secrets to something like `infra/gcp/data/...`. This makes it clear that these files have a different purpose and gives some indication that they're specific to your repo. If you want to also add prefixing the file name with the repo, then that would make it even clearer. But if it's too much work, don't bother. Maybe something like `infra/gcp/data/hail-is/` etc. I've put all deployment-specific configs in an `$ORGANIZATION_DOMAIN` subfolder now, which hopefully should avoid any collisions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11724#issuecomment-1173211731:555,avoid,avoid,555,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1173211731,2,['avoid'],['avoid']
Safety,"> I think we should change the taints on the node pools before we merge this PR. The GCP UI has changed and I don't see where we can change the taints. I think that's backwards. This PR shouldn't change the scheduling (it just adds tolerations to non-existent taints), so it should go in first, then we should add the taint and let the preemptible workload on non-preemptible nodes get rescheduled to preemptible nodes. However, I think what we should do is create a new tainted non-preemptible pool, merge this PR, and then delete the old pool. Yeah, looks like you can't edit labels or taints from the UI. Maybe you can from the command line? ; Anyway, with the above strategy which seems upgrade safe, it doesn't matter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039:699,safe,safe,699,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039,1,['safe'],['safe']
Safety,"> I'm not sure how this is unsafe. Could you explain?. Partitioner just says what elements are in what partitions. OrderedRDD also guarantees those elements are in a specific order. It is not safe to take an RDD partitioned with an OrderedPartitioner and just ""make"" it an OrderedRDD. Case where it fails: in read, union, which creates a partition-aware RDD that unions corresponding partition from a set of identically partitioned RDDs. > i think the better solution is to check that the ordered key inside the partitioner is the same as the implicit ordered key supplied. this also could have caught the errors. The error was making it into an OrderedRDD without sorting. It is perfectly valid to have an OrderedPartitioner-partitioned RDD that is sorted. (Above read case.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1864#issuecomment-303237349:27,unsafe,unsafe,27,https://hail.is,https://github.com/hail-is/hail/pull/1864#issuecomment-303237349,2,"['safe', 'unsafe']","['safe', 'unsafe']"
Safety,"> If there's no requester pays, is it just impossible to have ""public"" data in Azure storage safely? Like anything in there could be downloaded infinity times to drive up a bill?. That's correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11187#issuecomment-1004244889:93,safe,safely,93,https://hail.is,https://github.com/hail-is/hail/pull/11187#issuecomment-1004244889,1,['safe'],['safely']
Safety,"> In general, I think it is safest not to rely on the order of evaluation. I'm not aware of where we do. Unless I'm missing something, this code splitter does rely on the evaluation order, which is why I asked about it. If a statement `x` has large children, it moves them to execute before `x`, evaluated in the `children` array order (more precisely, in the post-order traversal order), storing their results in locals. I think it's possible to avoid relying on order of evaluation, if instead of only splitting out sequences of statements, we can also split out a `ValueX` directly, replacing it by a call to new method which returns the result. I don't have strong feelings about whether the order should be guaranteed, just want to be clear which it is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8958#issuecomment-645546246:28,safe,safest,28,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645546246,4,"['avoid', 'safe']","['avoid', 'safest']"
Safety,> Mainly that the commit procedure branches on whether the start id is 1. I was trying to avoid an expensive query to update all of the n_pending_parents when we know it's the first update being committed. I think I'd prefer to keep this branching there.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1221599755:90,avoid,avoid,90,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1221599755,1,['avoid'],['avoid']
Safety,"> No tests for this right now, although I can write . I think an UnsafeSuite test for this should be easy and effective.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6500#issuecomment-506490357:65,Unsafe,UnsafeSuite,65,https://hail.is,https://github.com/hail-is/hail/pull/6500#issuecomment-506490357,1,['Unsafe'],['UnsafeSuite']
Safety,"> One conceptual comment: you described SUnreachable as a type representing a value that will never be constructed at runtime, i.e. a bottom type. But I think what you've actually built is something different, that may have other uses. Each virtual type has a designated ""default value"", and the corresponding SUnreachable type is a subtype which can only have that default value, with a compact (usually 0 bytes) runtime representation. I do think the usages right now do fit this description - any usage of a SUnreachableCode will never be on a live path at runtime. I think any of the methods on SUnreachableCode/Value could safely be replaced with thrown exceptions. That's not to say there might be other applications this helps streamline! Do you have any specific ideas?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10539#issuecomment-870772732:628,safe,safely,628,https://hail.is,https://github.com/hail-is/hail/pull/10539#issuecomment-870772732,1,['safe'],['safely']
Safety,"> Only external requests go to the gateway. Why doesn't this do that (where baseurl is `http://{service}.namespace`. ```; base_url = internal_base_url(namespace, name, port); async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5.0)) as session:; while True:; try:; async with session.get(f'{base_url}/healthcheck') as resp:; ```. edit: Ah it's just routing through Kubernetes. I thought there would need to be something that specified the dns to use... Ok.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548105738:208,timeout,timeout,208,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548105738,1,['timeout'],['timeout']
Safety,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:417,avoid,avoid,417,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641,1,['avoid'],['avoid']
Safety,"> So I just put back the dependencies on `native-lib-prebuilt`. Since that just calls make recursively, it would probably be better to let mill invoke make, but I didn't want to deal with that, and this is a pretty uncommon use case (I think). That's sane to me, and avoids needing to deal with the make jobserver at the mill level. Feel free to un-WIP this whenever you're ready to merge.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147#issuecomment-1930537252:267,avoid,avoids,267,https://hail.is,https://github.com/hail-is/hail/pull/14147#issuecomment-1930537252,1,['avoid'],['avoids']
Safety,"> Struct decoding (the fourth generated code one, with 399 own time and 229 samples) is pretty branchy: it checks a bit for each field. I'm not sure how to speed this up. Consider a struct of 8 optional fields. There are 2^8 possible missingness pattern. Each pattern corresponds to a different sequence of field-decoders. I suppose we could generate 256 different patterns and jump to them? That seems excessive. We could maybe generate 16 patterns but that only saves 3/4 of the branches. Maybe that's enough for a substantial speedup?. With the array decoding, I suspect a lot of the speedup wasn't from avoiding branches, but from avoiding a bunch of extra operations handling missing bits one at a time, especially computing the address of the containing byte and loading it from memory every time. We should be able to do something similar for structs, though it will be more complicated. I think that's worth trying independently of trying to reduce branches.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13792#issuecomment-1761652107:607,avoid,avoiding,607,https://hail.is,https://github.com/hail-is/hail/issues/13792#issuecomment-1761652107,2,['avoid'],['avoiding']
Safety,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375:1202,avoid,avoid,1202,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375,2,['avoid'],['avoid']
Safety,"> The thinking behind this is that there's a huge amount of re-use of code for an individual from one Hail analysis to the next. So I'm not sure I buy this. Hail is a exploratory data analysis platform, it isn't a SQL engine running nightly billing reports. In particular, as we do whole stage optimization and code generation. It isn't clear to me how you do code-reuse when we're specializing each operation into the global context (happy to hear the plan). For example, tables that have many fields, you will need to load different ones for different queries and in general it is infeasible (exponential) to generate them all. Also, to get sharing you need to break the code up and now you're running the compiler multiple times which also seems bad. Given our focus on large-scale analysis, introducing optimization boundaries for code reuse seems like a bad trade off to me. A significant amount of analysis happens in the cloud where $HOME is ephemeral so you won't get savings between sessions. Finally, there are pipelines that are more standardized but it is my impression they are run on extremely large datasets (hours, overnight) in which case compilation speed isn't important. Finally, there's the complexity around locking that isn't easy and have real technical risk. A compiler cache potentially becomes more appealing in the context of an always-on service. There's no locking issue, and you can start to do things like speculative compilation (e.g. immediately start compiling the decoder (the full decoder? Hmm.) when a user opens the dataset.). I would say getting in a 3x decoder improvement is way more important than this. I would have punted it down the road and instrumented to estimate cache hit rates before building this out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596:1278,risk,risk,1278,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596,2,['risk'],['risk']
Safety,"> This suggests to me that the dataframe created by hail maintains reference to hail objects and pandas is attempting to recreate these objects when unpickling. I suspect this is not intentional. Hi @anh151, you are correct that `to_pandas` is creating dataframes that contain hail objects. In your example, the hail type in question is the [`Locus`](https://hail.is/docs/0.2/genetics/hail.genetics.Locus.html#locus), but we also have a couple auxiliary classes like `Interval` and `Call` that could end up in the pandas table. I see how this can be unintuitive especially with your point of round-tripping through CSV (which uses the `str` of the object by default and thus will avoid the class lookup on read), but I hesitate to call it unintentional. I'll broach this question with the team as to what would be the least confusing behavior, but I suspect many users are using `to_pandas` results in the same hail session in which case it might be expected to get small hail objects in their result. In the meantime, you can look at the schema of your resultant table and translate within pandas to your desired representation, i.e. convert Locus entries to dicts.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004#issuecomment-1813149330:680,avoid,avoid,680,https://hail.is,https://github.com/hail-is/hail/issues/14004#issuecomment-1813149330,1,['avoid'],['avoid']
Safety,"> ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. Without a concrete example, I'm having some trouble understanding what this problem is. What IR nodes can return a container type that aren't included in the streamify match? Isn't it just ToSet/ToDict/GroupByKey? Calling `boundary` on a ToSet is no problem at all, I think. > ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). Can you write such an IR, and what it should return after streamify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159:107,safe,safe,107,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159,2,['safe'],['safe']
Safety,"> Using the automatic Json deserializer kind of makes us write the wrong code here, because we really want one LZ4CodecSpec which takes the appropriate parameters, and then deserializer can construct it with the proper arguments based on the specific case being derserialized (a legacy format, or the current one with parameters). Yeah, I agree. I think it would be a couple days of work to write ser/deser stuff that avoids the json4s case class extraction, but I'm not sure how high-priority that is right now since this stuff is somewhat stable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9292#issuecomment-675510814:418,avoid,avoids,418,https://hail.is,https://github.com/hail-is/hail/pull/9292#issuecomment-675510814,1,['avoid'],['avoids']
Safety,"> Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. We take special care to ensure our system is as efficient as possible when reading or writing to this native format. So, it's partly a sociological thing. On the practical end of things, Hail's native formats (for Tables and Matrix Tables) are a partitioned binary format. The partitioned part means Hail can use many cores in parallel to process and write the dataset. The binary part means that Hail need not use unnecessarily large (in terms of bytes) representations of values. These three things together make writing the native formats use less time, use less memory, and be more reliable. ---. > One thing I noticed is the mt_hwe_vals variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. Yes, after you aggregate you get back an MT with a different column key. ---. The `entries` method converts your matrix table from a compact and efficient matrix into a ""long"" and inefficient table. I generally recommend avoiding it if you can. However, if you only have a handful of ancestries, I wouldn't expect this to be *that* bad. You can just write the MT itself:. ```python3; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)); mt_hwe_vals = mt_hwe_vals.select_rows().select_cols() # drop irrelevant row and column fields; mt_hwe_vals.write(bucket + '/hwe.ht'); ```. ---. > I tried modifying the code to what is shown below but I'm still having the same issue. Just to be clear it's the exact same error ""Container exited with a non-zero exit code 137. ""? This makes me think we have an issue with `entries`, because, even though it's not great, it shouldn't be blowing RAM here. Can you share the log file from your previous or next attempt?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636:1067,avoid,avoiding,1067,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636,2,['avoid'],['avoiding']
