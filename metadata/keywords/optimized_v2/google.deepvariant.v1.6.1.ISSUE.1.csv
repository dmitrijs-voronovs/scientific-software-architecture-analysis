quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Deployability,"Hello, . I was wondering ... I know you have a mosquito model that seems to work for when there is a high heterozygosity, but I was wondering if you planned to release those mdels... We, who work with non-model organisms, would be really happy to use DeepVariant. . Sorry if this is out of place here, I didn't really know where to ask. Feel free to lock . Cheers. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661:160,release,release,160,,https://github.com/google/deepvariant/issues/661,1,['release'],['release']
Deployability,"Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:556,install,install,556,,https://github.com/google/deepvariant/issues/793,4,['install'],"['install', 'installed']"
Deployability,"Hello, . I've been attempting to use the customized_classes_labeler to train a DeepVariant model. Specifically, I've been trying use the ""callsets"" field from the INFO field of a Genome In A Bottle VCF file. I've been working with NA12878, VCF/BED files available here: ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh38/. At first, I could not make training examples using this as that field is an integer, but by making a copy of the VCF where I changed that field to be a string, I was able to make examples (using the `--labeler algorithm`, `--customized_classes_labeler_info_field_name`, and `--customized_classes_labeler_classes_list` options) and train the model. However, when I use the best model from training to predict variants, this class label information is not included in the VCF file. Am I misinterpreting how to use this customized class labeling? Any suggestions on how to incorporate this field into training and variant prediction? Thank you for your time!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/454:312,release,release,312,,https://github.com/google/deepvariant/issues/454,1,['release'],['release']
Deployability,"Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed; . /etc/profile.d/modules.sh; module load xxxxx/singularity/3.5.3. # inputs; reference=$2; bam=$1.final.bam; sampleid=$1; outdir=deepvar. # Create output directories; if [ ! -e deepvar ]; then mkdir deepvar; fi; if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches; if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi; export SINGULARITY_TMPDIR=$PWD/.singularity; export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image; if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant; singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${reference} \; --reads=${bam} \; --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \; --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \; --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/543:422,install,installed,422,,https://github.com/google/deepvariant/issues/543,1,['install'],['installed']
Deployability,"Hello, ; Operatin system: Linux HPC ; Version: 1.3.0 ; Installation: Singularity ; Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**; **Command**; ```; `#!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p compute; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-23; #SBATCH --time=12:00:00; #SBATCH --mem-per-cpu=128GB. module purge; module load parallel; module load singularity; EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27; HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna; PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam; BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed; OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz; OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz; INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$BED_REGIONS \; --output_vcf=$OUTPUT_VCF \; --output_gvcf=$OUTPUT_GVCF \; --intermediate_results_dir=$INTERMEDIATE_RESULTS""; ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/542:55,Install,Installation,55,,https://github.com/google/deepvariant/issues/542,1,['Install'],['Installation']
Deployability,"Hello, I am a pharmacy student and I am trying to use your tool. ; I have in my possession a pc with i7-6700, 32GB RAM and a RTX 2070 with CUDA cores and Tensor Cores.; 1) Can I somehow use the Tensor cores of my GPU and how?; 2) Is 32 GB enough? If I upgrade to 64gb will be better?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/157:252,upgrade,upgrade,252,,https://github.com/google/deepvariant/issues/157,1,['upgrade'],['upgrade']
Deployability,"Hello, I am attempting to compile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -f",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/189:108,Install,Installing,108,,https://github.com/google/deepvariant/issues/189,3,['Install'],"['Install', 'Installing']"
Deployability,"Hello, I am trying to install DeepVariant on an IBM Power 8 machine within a docker container. I get the following error during ./build_and_test.sh, which I understand is tied to Intel SSE2 instruction set. `external/libssw/src/ssw.c:38:23: fatal error: emmintrin.h: No such file or directory`. I did `export DV_USE_GCP_OPTIMIZED_TF_WHL=0` from the command line before running the compile. I also changed `DV_COPT_FLAGS` to `--copt=-Wno-sign-compare --copt=-Wno-write-strings` within settings.sh (removing the corei7 option). I am using bazel version '0.15.0-' (settings.sh is changed to reflect this). I am using scikit-learn=0.20 (run-prereq.sh changed to reflect this). pyclif was compiled from source. Is there a way to circumvent this error? The complete error message is as follows. ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/libssw/BUILD.bazel:11:1: C++ compilation of rule '@libssw//:ssw' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction; -sections -fdata-sections -MD -MF bazel-out/ppc-opt/bin/external/libssw/_objs/ssw/external/libssw/src/ssw.pic.d -fPIC -iquote external/libssw -iquote bazel-out/ppc-opt/genfiles/external/libssw -iquote ext; ernal/bazel_tools -iquote bazel-out/ppc-opt/genfiles/external/bazel_tools -Wno-maybe-uninitial",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123:22,install,install,22,,https://github.com/google/deepvariant/issues/123,1,['install'],['install']
Deployability,"Hello, I am trying to make a variant calling analysis with ONT data for Homo Sapiens. However, it is still running even though I started this analysis 6 days ago and it is still making examples. Could anyone help me to understand whether it is normal or I should re-run the analysis? The computer has 64-core Linux. It should be able to run the analysis for nearly one to two days based on my experiences with other variant callers. I would be very happy to get feedbacks from you! ; Thanks a lot!. Deep Variant/ Variant Calling; BIN_VERSION=1.6.1; Installation via Docker; Homo Sapiens Oxford Nanopore Whole Genome; ![Screenshot 2024-05-01 220709](https://github.com/google/deepvariant/assets/74244954/93bf098a-083d-40f5-ba41-9d876a836be3)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/814:549,Install,Installation,549,,https://github.com/google/deepvariant/issues/814,1,['Install'],['Installation']
Deployability,"Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists.; I've googled all over and still can't solve the problem. please help me!. commandï¼š; ```; INPUT_DIR=/path1/4_Test/qingjiang/dpv; OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \; --num_shards=3 \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/QJref.fa \; --reads=""${INPUT_DIR}""/input.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; ```; errorï¼š; ```; I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s; user 0m0.211s; sys 0m0.371s; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/613:1321,install,installed,1321,,https://github.com/google/deepvariant/issues/613,2,['install'],['installed']
Deployability,"Hello, I have installed all the binaries and ran all the shell scripts to install tensorflow and bazel, but after that I could not follow how to actually train the model or how to identify the snps for my files. I am sorry I am very new to deep learning. Any help would be greatly appreciate",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/63:14,install,installed,14,,https://github.com/google/deepvariant/issues/63,2,['install'],"['install', 'installed']"
Deployability,"Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:; ```; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/...; [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:; Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2; [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1; ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/511:109,patch,patch,109,,https://github.com/google/deepvariant/issues/511,2,"['patch', 'update']","['patch', 'update']"
Deployability,"Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it?. **Setup**; - DeepVariant version: 1.1.0 and 1.3.0; - Installation method: Singularity; - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** ; ```; W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 ; W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 ; W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 ; W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804; W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 ; W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 ; ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/512:199,Install,Installation,199,,https://github.com/google/deepvariant/issues/512,1,['Install'],['Installation']
Deployability,"Hello, when running DeepVariant on a machine with a GPU, we get ; [the attached error](https://github.com/google/deepvariant/files/5947987/DeepVariantError.txt); which seems to indicate that DeepVariant cannot find the samples in the working directory which is a solid state drive contained within the node. Oddly enough, when we rerun without removing the files in the /tmp directory, DeepVariant completes without error. Do you have any explanation for this? The submit command is below as system information. **Setup**; - Operating system: CentOS7, cuda/11.0; - DeepVariant version: v1.1.0; - Installation method: Singularity; - Type of data: PacBio HiFi from SQII with hg38. **Steps to reproduce:**; - Command: `singularity run --nv --bind $(readlink -f dv_wd):/wd /path/to/deepvariant/images/deepvariant_1.1.0-gpu.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/wd/hg38.ref.fasta --reads=/wd/${sample}.bam --output_vcf=/wd/${sample}.vcf --output_gvcf=/wd/${sample}.gvcf --novcf_stats_report --intermediate_results_dir=/tmp/deepvariant_tmp/$( whoami )_${sample}/ --num_shards=${threads}`; - Error trace: Included above; - We have also tried out a similar process running on another machine without a GPU, and we do not see this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/422:596,Install,Installation,596,,https://github.com/google/deepvariant/issues/422,1,['Install'],['Installation']
Deployability,"Hello,. Deepvariant is reported to work well with WGS data from the Element AVITIâ„¢ System.; #623 ; https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data?; Is it possible to use the current WES model or is it still required to update the WES model?. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/703:291,update,update,291,,https://github.com/google/deepvariant/issues/703,1,['update'],['update']
Deployability,"Hello,. I am facing the below error when running built_and_test.sh. I have protocol buf built and installed in my home directory. (19:11:12) ERROR: /uufs/chpc.utah.edu/common/home/u1142888/deepvariant/bazel_tmp/_bazel_u1142888/bc41070ad1d30708841b968fbd6bc540/external/com_google_protobuf/BUILD:104:1: C++ compilation of rule '@com_google_protobuf//:protobuf' failed (Exit 1): gcc failed: error executing command . I have built the latest protobuf from source following the instructions [here](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md) and have installed it under $HOME/protobuf and updated my PATH and LD_LIBRARY_PATH accordingly. Looking for some pointers on how to resolve this issue by tweaking the build procedure. Thank you,; Ram",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94:98,install,installed,98,,https://github.com/google/deepvariant/issues/94,3,"['install', 'update']","['installed', 'updated']"
Deployability,"Hello,. I am trying to build deepavariant on a HPC node on which all the required dependency is met except pyclif. I do not have root privileges to install it under /usr/local/clif. Hence I downloaded pyclif source code and ran the INSTALL.sh to get it successfully built and installed under $HOME and activated the pyclif virtualenv. . (clif) [test-node]$ which pyclif; ~/opt/clif/bin/pyclif. However build_and_test.sh fails with the below error. Any changes required to deepvariant build setup to pick up the pyclif installation in my home directory?; (18:02:14) ERROR: missing input file '@clif//:clif/bin/pyclif'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/93:148,install,install,148,,https://github.com/google/deepvariant/issues/93,4,"['INSTALL', 'install']","['INSTALL', 'install', 'installation', 'installed']"
Deployability,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/443:23,install,install,23,,https://github.com/google/deepvariant/issues/443,1,['install'],['install']
Deployability,"Hello,. I am working on a next-flow pipeline and have a question about using the deep variant tool. Would it still work correctly if I split the bam file into chromosomes, or if it's not designed to be used with a splitter? Additionally, I would like to know if there would be any speed issues between the two options. Using the splitter chromosomes would be faster, but I am unsure.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/744:36,pipeline,pipeline,36,,https://github.com/google/deepvariant/issues/744,1,['pipeline'],['pipeline']
Deployability,"Hello,. I apologise if my question is naive I am a beginner with neural networks. ; Do you plan to release models trained with non-humans? Like the mosquito analysis published on your blog? And would it made sense to have a kind of ""universal model""? . thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/204:99,release,release,99,,https://github.com/google/deepvariant/issues/204,1,['release'],['release']
Deployability,"Hello,. I tried running ""_run_deepvariant_keras.py_"" script from the latest release of deepvaraint and faced some issues while running the keras-based call variant module. . I used the following command to run:. `; python bazel-out/k8-opt/bin/deepvariant/run_deepvariant_keras.py --model_type=WGS --ref=ref.fa --reads=reads.bam --regions ""chr19"" --output_vcf=${OUTPUT_DIR}/output.vcf.gz --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz --intermediate_results_dir ${OUTPUT_DIR}/intermediate_results_dir --num_shards=10; `. The above command successfully ran for _make_example_ module and failed at _call_varaint_keras.py_ with the following error message.. -------------. _Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 399, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 312, in run_; _run_main(main, args); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 387, in main; call_variants(; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 344, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/636:76,release,release,76,,https://github.com/google/deepvariant/issues/636,1,['release'],['release']
Deployability,"Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? ; I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)?. Here's the code I'm running from the Quickstart:; ```; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules; module load singularity; module load cuda-dcgm/2.2.9.1; module load cuda11.4/toolkit; module load cuda11.4/blas; module load cuda11.4/nsight; module load cuda11.4/profiler; module load cuda11.4/fft; source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh; conda activate TensorFlow_GPU. # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. And here's my error:; ```; 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514:31,install,installation,31,,https://github.com/google/deepvariant/issues/514,1,['install'],['installation']
Deployability,"Hello,. I'm trying to run DeepVariant using ultima data (cram file).; I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page.; But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/711:181,release,release,181,,https://github.com/google/deepvariant/issues/711,1,['release'],['release']
Deployability,"Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following; ```; Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx; ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/524:837,pipeline,pipelines,837,,https://github.com/google/deepvariant/issues/524,1,['pipeline'],['pipelines']
Deployability,"Hello,. I've been trying to set up the **google/deepvariant:1.6.1-gpu** or **google/deepvariant:latest-gpu** image on a GPU instance, but I've encountered the error message mentioned below when running the **run_deepvariant** or **train** scripts, and despite generating the flags (screenshot) as expected, I believe those incompatible/missing TensorRT libraries are preventing these scripts from using the GPU. **Command used:** ; ` sudo docker run --runtime=nvidia --gpus 1 google/deepvariant:1.6.1-gpu train --help; `. **Error message:**; ```; 2024-05-08 15:11:26.358196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-05-08 15:11:26.358229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; ```. ![image](https://github.com/google/deepvariant/assets/169280348/fd17bf4e-0b6c-46b7-b5e8-74a3525d07a5)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819:1107,install,installed,1107,,https://github.com/google/deepvariant/issues/819,1,['install'],['installed']
Deployability,"Hello,. I've been using DeepTrio for de novo variant analysis, and it's been performing excellently. However, I noticed from the logs that DeepTrio uses the CPU to prepare data, which is quite time-consuming. In my case, it took 12 hours for one trio analysis, with an additional 4 hours on the GPU. Given that renting GPU servers( usually with less CPU) is more expensive than CPU servers and access to privately owned GPU servers is limited, it seems inefficient to run lengthy CPU processes on a GPU server. It feels like a bit of a waste, and sometimes I half-jokingly feel there might be someone out there with murderous intent because of it!. Would it be possible in future updates to partition the DeepTrio analysis into separate steps? This way, CPU-intensive tasks could be completed on a CPU server, and then the job could be transferred to a GPU server for the remaining tasks. Alternatively, could the data preparation (CPU) and analysis (GPU) be run at the same time? This would help optimize resource usage and reduce costs. Thank you for considering these suggestions.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/873:680,update,updates,680,,https://github.com/google/deepvariant/issues/873,1,['update'],['updates']
Deployability,"Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:; ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; FAMILY_ID=Case1; PROBAND_ID=Case1_proband; MOTHER_ID=Case1_mother; FATHER_ID=Case1_father; SIBLING_ID=.; PED=$FAMILY_ID.ped. MOTHER_PRESENT=true; FATHER_PRESENT=true; SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam; FATHER_BAM=${FATHER_ID}.sorted.bam; MOTHER_BAM=${MOTHER_ID}.sorted.bam; SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz; FATHER_VCF=${FATHER_ID}.vcf.gz; MOTHER_VCF=${MOTHER_ID}.vcf.gz; SIBL",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:599,pipeline,pipeline,599,,https://github.com/google/deepvariant/issues/518,2,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']"
Deployability,"Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio.; Thanks for considering this request; Fred-07. **Describe the issue:**; Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created.; https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md; All other expected files are created. **Setup**; - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64; - DeepVariant version: 1.4.0; - Installation method: singularity pull from docker, LSF as batch system; - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**; - Command: additional flag; `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/544:534,Install,Installation,534,,https://github.com/google/deepvariant/issues/544,1,['Install'],['Installation']
Deployability,"Hello,. We have found that a known de novo variant was missed when using DeepTrio and GLnexus in our pipeline (WGS, hg38). I know that a similar issue has already been raised and appreciate the interesting discussion on this (i.e. https://github.com/google/deepvariant/issues/440), but to recap for others this was the result of two contributing factors:. 1. DeepTrio being less confident in the de novo call for the proband than when DeepVariant is run in singleton mode on the proband. In our case, comparing the output VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepT",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475:101,pipeline,pipeline,101,,https://github.com/google/deepvariant/issues/475,3,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"Hello,. We noticed that adjacent variants of the same haplotype (i.e. MNVs) are being called as separate variants in the DeepVariant and DeepTrio outputs with VCF and gVCF files. During downstream processing these MNVs are then treated as two individual SNVs at two different loci, leading to faulty assessments. . For example two variants for a site of interest (reference TCG -> Serine) were separated between two lines in the DeepVariant/DeepTrio output VCF and then categorised as containing a nonsynonymous (T**G**G -> Tryptophan) and synonymous mutation (TC**A** -> Serine). Whereas the correct and desired way to handle this, at least for us but I imagine others too, would seem to be to recognise both mutations on a single line in the VCF as a combined substitution, which could then be identified as resulting in a stopgain (T**GA** -> Nonsense mutation). Are there plans to support these MNV calls in the DeepVariant/DeepTrio outputs? Or alternatively are there any current post-processing approaches that you may be using and can recommend to handle these cases? Can understand these may be challenging to manage in some aspects but could be important to flag given some recent literature around this topic. For reference this was using hg38 with WGS. We initially identified this using the original DeepTrio release (docker image deeptrio:1.0.1rc), but then also using the most recent DeepVariant release (via docker, v1.2). We also tested this to see whether the change to the unfiltered GLnexus config could be contributing to this for processing of DeepTrio gVCFs due to the joint genotyping parameter, but reverting back to the WGS config did not result in a merged MNV in this instance. . Many thanks,; Macabe.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/486:1321,release,release,1321,,https://github.com/google/deepvariant/issues/486,2,['release'],['release']
Deployability,"Hello,. Will DeepTrio be updated for use with Oxford Nanopore data ?. Thank you, regards.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/715:25,update,updated,25,,https://github.com/google/deepvariant/issues/715,1,['update'],['updated']
Deployability,"Hello,; I'm writing you because I'm trying to install deepvariant, but I'm encountering several difficulties in doing so.; I've tried at first to install through anaconda (```conda install -c bioconda deepvariant```), but I alway get the same problem:; ```. Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 497, in run_script; subprocess_call(command_args, env=env, path=dirname(path)); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/gateways/subprocess.py"", line 56, in subprocess_call; output=_format_output(command_str, path, rc, stdout, stderr)); subprocess.CalledProcessError: Command '['/bin/bash', '-x', '/PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh']' returned non-zero exit status 1.; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 327, in _execute_actions; run_script(target_prefix, Dist(pkg_data), 'post-unlink' if is_unlink else 'post-link'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 513, in run_script; raise LinkError(message); conda.exceptions.LinkError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 281, in execute; pkg_data, actions); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/cond",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:46,install,install,46,,https://github.com/google/deepvariant/issues/252,3,['install'],['install']
Deployability,"Hello,; first of all ,i do not have the permission to run docker on my machine or sudo update or install. And as i work on cluster service,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/391:87,update,update,87,,https://github.com/google/deepvariant/issues/391,7,"['install', 'update']","['install', 'update']"
Deployability,Hello. ; Thanks for long awaited update to Python3.; I'm trying to update our pipeline and getting error when installing CLIF.; Looks like https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-18.latest.tgz is missing while https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-16.latest.tgz exists.; Can you fix this please?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/289:33,update,update,33,,https://github.com/google/deepvariant/issues/289,4,"['install', 'pipeline', 'update']","['installing', 'pipeline', 'update']"
Deployability,"Hello. OS: Scicore Cluster, Linux; Deep Variant version:1.2.0; Installation: Singularity; Instrument: Ilumina; Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:; ```; #!/bin/bash. #SBATCH --job-name=Deepvariant_debug; #SBATCH --cpus-per-task=2 # change this according to your needs; #SBATCH --mem=8G # change this according to your needs; #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes; #SBATCH --output=myrun.o%j; #SBATCH --error=myrun.e%j. mkdir -p output; mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000; BIN_VERSION=""1.2.0""; # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder; export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output ; export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata ; # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add; # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container; # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif); singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \; /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \; --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \; --regions=/scicore/home/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/515:63,Install,Installation,63,,https://github.com/google/deepvariant/issues/515,1,['Install'],['Installation']
Deployability,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:127,install,install,127,,https://github.com/google/deepvariant/issues/489,5,['install'],"['install', 'installable', 'installed']"
Deployability,"Hello; I download 40 sample in 1kgp, first I use oqfe to BWA; Then I send the CRAM to Deepvariant to call variants.; But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis?; So if someone give me some explain or advice?; very Thanks!; ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/670:192,pipeline,pipeline,192,,https://github.com/google/deepvariant/issues/670,1,['pipeline'],['pipeline']
Deployability,"Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:1323,pipeline,pipeline,1323,,https://github.com/google/deepvariant/issues/91,1,['pipeline'],['pipeline']
Deployability,"Hey guys,. I got deepvariant installed with conda fine, but my run failed by needing glibc, when I installed glibc all processes get a segmentation fault, if I remove glibc it works until failing needing the dependency. Any advice? We can't install docker images on our HPC, I haven't tried converting docker to singularity, as I have no experience with that but I'm comfortable compiling from source, but couldn't find the right files/instructions. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/179:29,install,installed,29,,https://github.com/google/deepvariant/issues/179,3,['install'],"['install', 'installed']"
Deployability,"Hi , when i run call_variant , it arises this warn which means can't use the gpu,but i can make sure that the tensorflow can use the gpu.There are the screen shots of the warn and the existence of the gpu. - the gpu existence; ![image](https://github.com/google/deepvariant/assets/71956115/367b1a98-123c-48fb-b170-3f8e4aae7d30). ```python; tensorflow.test.is_gpu_available(); WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.config.list_physical_devices('GPU')` instead.; 2024-05-12 21:36:00.744470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 15089 MB memory: -> device: 0, name: Vega 20, pci bus id: 0000:26:00.0; True; ```. - the warn ; ![image](https://github.com/google/deepvariant/assets/71956115/246d5cfd-a9b3-4ac5-aea7-bf4c89401c76). ```shell; warnings.warn(; 2024-05-12 21:43:29.067332: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ```. - Operating system: Linux ; - DeepVariant version: 1.6.1-gpu; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820:1200,Install,Installation,1200,,https://github.com/google/deepvariant/issues/820,1,['Install'],['Installation']
Deployability,Hi ; I recently installed docker version of deep variant for cancer exome analysis. I like to know if I don't use paired cancer samples for variant calling with deep variant will it be right approach,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/211:16,install,installed,16,,https://github.com/google/deepvariant/issues/211,1,['install'],['installed']
Deployability,"Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. Iâ€™m looking forward to using this tool for my research.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/235:1289,release,release,1289,,https://github.com/google/deepvariant/issues/235,1,['release'],['release']
Deployability,"Hi DeepVariant team,. I have been using DeepVariant ï¼ˆv1.1.0ï¼‰to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:; ```; chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43; chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34; chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17; ```; The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): Docker; - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,; Peng Jia",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660:1364,Install,Installation,1364,,https://github.com/google/deepvariant/issues/660,1,['Install'],['Installation']
Deployability,"Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:; ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```; rule deepvariant:; input:; bam=rules.apply_bqsr.output.bam,; ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'; output:; vcf=""results/deepvariant/{sample}.vcf.gz""; params:; model=""WES""; threads: ; 64; resources:; mem_mb=163840; log:; ""logs/deepvariant/{sample}/stdout.log""; singularity:; ""singularity/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:156,pipeline,pipeline,156,,https://github.com/google/deepvariant/issues/602,1,['pipeline'],['pipeline']
Deployability,"Hi Developers,. I get the following error when I am trying to use the latest version (V0.7.0) of deepvariant for wes analysis. I encounter the same error even I used the example given [here](https://cloud.google.com/genomics/docs/tutorials/deepvariant). ; ./deepvariant_v0.7.0_UDN644883_wes_09202018.sh; ERROR: (gcloud.alpha.genomics.pipelines.run) INVALID_ARGUMENT: Error: validating pipeline: zones and regions cannot be specified together. I have attached my script for the reference. ; [deepvariant_v0.7.0_UDN644883_wes_09202018.sh.txt](https://github.com/google/deepvariant/files/2403088/deepvariant_v0.7.0_UDN644883_wes_09202018.sh.txt). I was able to run deepvariant for wes using V0.6.1 successfully however looking at the output vcf I 'think' the script is not restricting the variants to the bed file regions. ; [deepvariant_v0.6.1_UDN644883_wes_09132018.sh.txt](https://github.com/google/deepvariant/files/2403091/deepvariant_v0.6.1_UDN644883_wes_09132018.sh.txt); [deepvariant_v0.6.1_UDN644883_wes_09132018.yaml.txt](https://github.com/google/deepvariant/files/2403092/deepvariant_v0.6.1_UDN644883_wes_09132018.yaml.txt). Thanks,; Shruti. Shruti Marwaha, PhD.; Research Engineer,; Stanford Center for Undiagnosed Diseases; Stanford University",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/96:334,pipeline,pipelines,334,,https://github.com/google/deepvariant/issues/96,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Hi all,. The DeepVariant case study scripts for running via binaries on CPU install the `intel-tensorflow` package. We have noticed the below error when installing this package and are looking into how this can be fixed. ```; $ pip install intel-tensorflow; ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info; ```; If you run into this issue, we recommend one of the following options in the meantime:; * Use the Docker scripts instead of the binaries scripts.; * Set the [`DV_USE_GCP_OPTIMIZED_TF_WHL`](https://github.com/google/deepvariant/blob/r0.9/settings.sh#L90) variable to 0 prior to setting up DeepVariant and running the case study scripts for binaries. `intel-tensorflow` is only installed when this variable is set.; * Use the GPU scripts instead of the CPU scripts. Best,; The DeepVariant Team",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/263:76,install,install,76,,https://github.com/google/deepvariant/issues/263,4,['install'],"['install', 'installed', 'installing']"
Deployability,"Hi all;; Thanks for all the help getting an initial conda package in place for DeepVariant (#9) through bioconda. I wanted to follow up with some suggestions that would help make the pre-built binaries more portable as part of this process, in order of helpfulness for portability:. - Currently the binaries need a recent kernel with GLIBC > 2.23 due to pre-built htslib and other libraries. Would it be possible to build the DeepVariant libraries on an older machine to allow a wider range of system support? We build on CentOS 6 in conda to provide wider compatibility.; - main.py in the zip files hardcodes python to use `/usr/bin/python`. Would it be possible to generalize this by using the python that the zip file gets called with (`sys.executable`)? I currently patch this in the conda build: https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/recipes/deepvariant/build.sh#L22; - This is currently built against numpy 1.13 and ideally we'd want to sync with CONDA_NPY (1.12: https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9). I believe building against 1.12 would make it forward compatible. An alternative to points 1 and 3 is making it easier to build DeepVariant as part of the conda build process. The major blocker here is the `clif` dependency which is difficult to build and the pre-built binaries require unpacking into `/usr`. If we could make this relocatable and easier to install globally we could build with portable binaries and adjustable numpy as part of the bioconda preparation process. Thanks again for all the help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29:770,patch,patch,770,,https://github.com/google/deepvariant/issues/29,2,"['install', 'patch']","['install', 'patch']"
Deployability,"Hi folks,. Our users are running deepvariant on our HPC and resource requests are proving quite tricky for them as different stages of the pipeline seem to have different resource needs. This leaves execution nodes essentially idle for much of the time of the jobs' running. In the two days runtime you can see below that a user who has asked for 48 cpus doesn't use most of the cores for most of the time:. ![Image](https://github.com/user-attachments/assets/c631a80d-a7b3-435a-ac50-97a1743f7638). ```; singularity run deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref /path/to/their/reference.fasta \; --reads /path/to/their/input.bam \; --sample_name unique_name \; --output_vcf /path/to/their/unique_name.vcf.gz \; --output_gvcf /path/to/their/unique_name.g.vcf.gz \; --num_shards 48; ```; Is there a way that each stage of the pipeline with discreet resource requests so that the bits that can be cleanly parallelized can go in one job submission and the stages that don't can be in separate jobs/commands, to avoid having requested and reserved resources being idle?. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/905:139,pipeline,pipeline,139,,https://github.com/google/deepvariant/issues/905,2,['pipeline'],['pipeline']
Deployability,"Hi guys, . I have been doing some test on DeepVariant genotyping call. I run the suit and got very good results, but as part of experiments I need to generate a cohort VCF (multi-sample). As suggested here on github, I generated GVCF for all my samples, but when I tried to use GATK's CombineGVCFs or GenotypeGVCFs neither worked because they don't recognize the alternative allele `<*>`, instead GATK moved to `<NON_REF>` on the more recent versions.; After identified the problem, I run a simple substitution using `sed` to replace all occurrences of `<*>` for `<NON_REF>` and the commands ran fine. . `zcat SAMPLE.deepvar.g.vcf.gz | sed 's/<*>/<NON_REF>/g' | bgzip -c > SAMPLE.gvcf.gz`. May I suggest this update for your software to keep the compatibility with GATK?. Best,; AndrÃ© Santos",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/83:709,update,update,709,,https://github.com/google/deepvariant/issues/83,1,['update'],['update']
Deployability,"Hi! The current bioconda recipe requires precompiled binaries to be available at `https://github.com/google/deepvariant/releases/download/v{{ version }}/deepvariant.zip`. I've updated the bioconda recipe to work with v1.0.0, but for a smooth update to v1.1.0, it would be excellent if you could provide zipped binaries with the release. The bioconda recipe: https://github.com/bioconda/bioconda-recipes/tree/master/recipes/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/436:120,release,releases,120,,https://github.com/google/deepvariant/issues/436,4,"['release', 'update']","['release', 'releases', 'update', 'updated']"
Deployability,"Hi!. Is there a way to emit all sites in given regions (using a bed file) in the final VCF even if they are the same as reference? I want to use it in a pipeline in which any position not in the input VCF is assumed to be a ""no call"". Missing positions will not be interpreted as reference.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/484:153,pipeline,pipeline,153,,https://github.com/google/deepvariant/issues/484,1,['pipeline'],['pipeline']
Deployability,"Hi, . I had trouble running Deepvariant using conda. I ran the following command.; ```; dv_make_examples.py --sample {MY_SAMPLE} --ref {MY_FASTA}.fasta --reads {MY_BAM}.bam --logdir ./log/ --examples examples/; ```. and I got an error like this:; ```; ETA: 0s Left: 1 AVG: 0.00s local:1/0/100%/0.0s sh: 1: unzip: not found; Traceback (most recent call last):; File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 252, in <module>; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 187, in Main; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 138, in GetRepositoriesImports; FileNotFoundError: [Errno 2] No such file or directory: '/tmp/Bazel.runfiles_qwsw52c7/runfiles'; parallel: This job failed:; /opt/conda/envs/deepvariant/bin/python /opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip --mode calling --ref E.coli_K12_MG1655.fa --reads SRR1770413.bam --examples examples//SRR1770413.tfrecord@1.gz --task 0; ```. When I installed unzip by `conda install`, the command worked fine. When using conda to install deepvariant, should it not have to be installed together?. Thanks,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/314:1499,install,installed,1499,,https://github.com/google/deepvariant/issues/314,4,['install'],"['install', 'installed']"
Deployability,"Hi, . This is more of a support question, but I wasn't sure where else to get help. I'm trying to build and test deepvariant inside of a docker image. I know that there is already an image published to google cloud, but for my purposes I prefer to build my own image. My docker file looks like this. ; ```; FROM ubuntu:16.04. RUN set -ex \; && buildDependencies=' \; ca-certificates \; curl \; wget \; git \; apt-transport-https \; xz-utils \; bzip2 \; make \; ' \; && apt-get update \; && apt-get install -y --no-install-recommends $buildDependencies \; # gsutil; && wget https://storage.googleapis.com/pub/gsutil.tar.gz \; && tar xfz gsutil.tar.gz -C $HOME && rm gsutil.tar.gz \; && export PATH=$PATH:$HOME/gsutil \; # deepvariant; && git clone https://github.com/google/deepvariant.git \; && cd deepvariant \; && git checkout v0.4.1 \; && ./build-prereq.sh \; && ./build_and_test.sh; ```; The `build_and_test.sh` script fails with these errors:; ```; + ./build_and_test.sh; + source settings.sh; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_P",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:477,update,update,477,,https://github.com/google/deepvariant/issues/19,3,"['install', 'update']","['install', 'install-recommends', 'update']"
Deployability,"Hi, . Would it be possible to use Deepvariant's training pipeline to create a model that is able to call larger variants (SVs), or is there something that would fundamentally limit the use of Deepvariant's algorithm for this case? . Thanks a lot in advance for any comments!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/298:57,pipeline,pipeline,57,,https://github.com/google/deepvariant/issues/298,1,['pipeline'],['pipeline']
Deployability,"Hi, ; when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:. CondaError: Downloaded bytes did not match Content-Length; url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; Content-Length: 229846992; downloaded bytes: 217650750. Best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228:13,install,install,13,,https://github.com/google/deepvariant/issues/228,4,['install'],"['install', 'installation', 'installed']"
Deployability,"Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/355:423,configurat,configuration,423,,https://github.com/google/deepvariant/issues/355,1,['configurat'],['configuration']
Deployability,"Hi, I want to know is there any way to make deepvariant support command line pipeline, I'm expecting it could accept read from stdin and pipe result to stdout. I tried explicitly using /dev/stdout to pipe result to stdout, ; ```bash; docker run \; --gpus all \; --rm \; -v ${INPUT_DIR}:/input \; -v ${OUTPUT_DIR}:/output \; google/deepvariant:latest-gpu \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,000,100"" \; --output_vcf=/dev/stdout \; --num_shards=4 \; 2> stderr.txt; ```; it could work, but print some debug information to stdout and pollute the result; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --regions ""chr20:10,000,000-10,000,100"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --outfile ""/dev/stdout"". ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/288:77,pipeline,pipeline,77,,https://github.com/google/deepvariant/issues/288,1,['pipeline'],['pipeline']
Deployability,"Hi, I was working on update the deepvariant source code from ubuntu 16.04 to 18.04 and python 3.6 to python 3.8. Now I met a problem in build_release_binaries.shell scripts. . bazel build -c opt \; --output_filter=DONT_MATCH_ANYTHING \; --noshow_loading_progress \; --show_result=0 \; ${DV_COPT_FLAGS} \; --build_python_zip \; :binaries. The error is below:; [1,442 / 1,802] Compiling third_party/nucleus/protos/struct.pb.cc; 1s local ... (128 actions, 48 running); (17:42:57) [1,544 / 1,802] Compiling external/org_tensorflow/tensorflow/core/util/test_log.pb.cc; 6s local ... (128 actions, 47 running); (17:43:03) ERROR: /opt/deepvariant/deepvariant/realigner/python/BUILD:54:1: C++ compilation of rule '//deepvariant/realigner/python:ssw_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python3.8 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; TF_CONFIGURE_IOS=0 \; TF_ENABLE_XLA=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.d '-frandom-seed=bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o' -fPIC -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DHAVE_SYS_UIO_H -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/libssw -iquote bazel-out/k8-opt/bin/external/libssw -iquote external/org_tensorflow -iquote bazel-out/k8-opt/bin/external/org_tensorflow -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441:21,update,update,21,,https://github.com/google/deepvariant/issues/441,1,['update'],['update']
Deployability,"Hi, I'm running DeepVariant with BQSR -adjusted bam files. I have sequencing data for hg002 and hg005 and I have to say the validation results are very impressive!. I wanted to test the option for using the original base quality scores with:. --parse_sam_aux_fields ; --use_original_quality_scores. but get the following error: . FATAL Flags parsing error: Unknown command line flag 'parse_sam_aux_fields'; Pass --helpshort or --helpfull to see help on flags. I was running DeepVariant with docker by following the whole genome sequencing case study -tutorial, but will next test the pipeline for multi-sample variant calling for my cohort of 50 samples. I'm wondering should I realign the reads or is it possible to use the original base quality scores from BQSR adjusted bam files? I was previously using the GATK4 pipeline, but the results are so much better with DeepVariant and as a bonus, it's a million times easier (and quicker). Thanks. Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/595:584,pipeline,pipeline,584,,https://github.com/google/deepvariant/issues/595,2,['pipeline'],['pipeline']
Deployability,"Hi, I'm trying to visualize the pileup images generated by DeepVariant. The images for SNP sites and deletions seem to be straightforward, but I found those for insertions are rather confusing. The reference lines for insertion sites are still continuous, and at the point where the insertion happens, the bases on the sequenced reads are set to 0. Here's part of an example of a homozygous ""A->AATAAAAT"" variant, the top 5 lines are the reference lines. 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250. The problem is these images are not presenting detailed infomation for the inserted sequence, and on sites where multiple insertions happen, the ""supports variant"" channel might become the only useful infomation to distinguish them.; Also, on the ""base quality"" channel, the qualities for these 0-bases are not zeros, how are these values determined?. I'm wondering if other structures of pileup images on these sites can achieve better performance, like adding 0s on the reference lines?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/306:244,continuous,continuous,244,,https://github.com/google/deepvariant/issues/306,1,['continuous'],['continuous']
Deployability,"Hi, it's getting harder to build deepvariant, even using bioconda as everything it moving to python3.7 or higher.; Would it be possible to get the build and Dockerfile updated to 3.7? And/or could you provide some guidance on what is needed?. Using the docker container works perfectly. But I want to add bcftools and samtools (for example) to the container and also have it work on singularity.; thanks,; -B",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/445:168,update,updated,168,,https://github.com/google/deepvariant/issues/445,1,['update'],['updated']
Deployability,"Hi, so sorry for asking something again. But I really want to use mostly DeepVariant for variant calling. . **Setup**; - Operating system: Red Hat Enterprise Linux 9; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: WES mapped to hg19. **My code:**; - Commands: ; ```; #!/bin/bash; #$ -l m_mem_free=200G; #$ -l os=rhel9; #$ -m bea; #$ -cwd; #$ -pe smp 2; #$ -o deepvariant_output.log; #$ -e deepvariant_error.log. cd path/to/deepvariant. BAM_DIR=.; VCF_DIR=deepvariant_output/; REFERENCE=Reference_HLA/human_g1k_v37_decoy.fasta. export SINGULARITY_CACHEDIR=""path/to/deepvariant/.singularity-$(whoami)""; export SINGULARITY_TMPDIR=""path/to/deepvariant/.singularity-$(whoami)"". BIN_VERSION=""1.6.1"". for BAM_FILE in ""${BAM_DIR}""/*.bam; do; # Extract the base name of the BAM file (without the directory and extension); BASE_NAME=$(basename ""${BAM_FILE}"" .bam). # Define the output VCF file name; VCF_FILE=""${VCF_DIR}/${BASE_NAME}.vcf.gz""; echo $BAM_FILE; echo $VCF_FILE; singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref $REFERENCE \; --reads $BAM_FILE \; --regions 6:32509320-32669663 \; --output_vcf $VCF_FILE \; --num_shards 12; done; ``` . - Error trace: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference_HLA/chr6_hg19.fa"" --reads ""./MDC05_1463_3.final.bam"" --examples ""/tmp/7361351.1.gpu.q/tmpzsp9g_vq/make_examples.tfrecord@12.gz"" --channels ""insert_size"" --regions ""chr6:32509320-32669663"" --task {}. [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when serializing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; [libprotobuf ERROR external/com_google_protobuf/src/g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/870:199,Install,Installation,199,,https://github.com/google/deepvariant/issues/870,1,['Install'],['Installation']
Deployability,"Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads bamlink \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo. ```; With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrUn_GL456370', u'chrUn_GL456379', u'chrUn_GL456366', u'chrUn_GL456368', u'chr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150:712,update,update,712,,https://github.com/google/deepvariant/issues/150,1,['update'],['update']
Deployability,"Hi,. Are there any plans to extend DeepVariant to somatic variant calling? The current model seems to be inherently diploid. What is the training time for the released versions of DeepVariant? The Supplementary information from the Nature paper mentions something about ""80 hours"" but does not specify which kind of hardware was used?. Do you have any numbers on how much the neural network improves the accuracy as compared to the raw (over-sensitive) variant calls output after the haplotype-aware realignment step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/241:159,release,released,159,,https://github.com/google/deepvariant/issues/241,1,['release'],['released']
Deployability,"Hi,. Are there any plans to upgrade to python3? Py2 has been deprecated, which makes deepvariant kind of obsolete.. Thanks; M",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/285:28,upgrade,upgrade,28,,https://github.com/google/deepvariant/issues/285,1,['upgrade'],['upgrade']
Deployability,"Hi,. I am trying to get the Docker image of DeepVariant to work on CentOS 7.; The same image works just fine on Ubuntu. Many thanks, Alf. $ cat /etc/redhat-release; CentOS Linux release 7.4.1708 (Core); $ docker --version; Docker version 1.13.1, build 6e3bb8e/1.13.1. $ docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.0. root@dade8141a904:/# /opt/deepvariant/bin/make_examples \; > --mode calling \; > --ref /dv2/input/ucsc.hg19.chr20.unittest.fasta.gz \; > --reads /dv2/input/NA12878_S1.chr20.10_10p1mb.bam \; > --examples output.examples.tfrecord \; > --regions ""chr20:10,000,000-10,010,000""; [W::hts_idx_load2] The index file is older than the data file: /dv2/input/NA12878_S1.chr20.10_10p1mb.bam.bai; 2018-10-12 09:57:22.558003: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring:; I1012 09:57:22.558365 139655512114944 genomics_reader.py:213] Reading /dv2/input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1074, in make_examples_runner; resource_monitor = resources.ResourceMonitor().start(); File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 59, in __init__; self.metrics_pb = self._initial_metrics_protobuf(); File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 72, in _initial_metrics_protobuf; cpu_frequency_mhz=_get_cpu_frequency(),; File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/104:156,release,release,156,,https://github.com/google/deepvariant/issues/104,2,['release'],['release']
Deployability,"Hi,. I am trying to run DeepVariant 1.2.0 on a few human samples PacBio HiFi data (about 30x coverage per sample). I first ran my samples through the [PEPPER-Margin pipeline r0.4](https://github.com/kishwarshafin/pepper) to get a haplotagged BAM file. Then I ran DeepVariant as follows:; ```; singularity exec -B ${SOME_PATHS} deepvariant_1.2.0.sif bash /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref ${PATH_TO_REF} --reads MARGIN_PHASED.PEPPER_SNP_MARGIN.happlotagged.bam --output_vcf sample.vcf.gz --output_gvcf sample.g.vcf.gz --num_shards 24 --make_examples_extra_args=""realign_reads=false,min_mapping_quality=5"" --sample_name MYSAMPLE --use-hp-information;; ```. I have two problems:; 1. Right from the beginning (`CALL VARIANT MODULE SELECTED`), for each interval processed. I get thousands of `READ TAG: n_elements is zero` messages in the console. What does it mean and is it a problem or just a warning?; 2. I allocate 200GB of RAM for per job and they all seem to systematically fail on memory. I do not recall DeepVariant using that much memory in the past but I might be wrong. Is 200GB too light for a human genome PacBio Hifi 30x coverage dataset?. Thank you for your help,; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/490:165,pipeline,pipeline,165,,https://github.com/google/deepvariant/issues/490,1,['pipeline'],['pipeline']
Deployability,"Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:; ```; CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990; ```; We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:; ```; CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV; ```; The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here?. Thank you for your help,; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/521:180,release,release,180,,https://github.com/google/deepvariant/issues/521,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"Hi,. I am working on a [deepvariant pipeline](https://github.com/nf-core/deepvariant/tree/dev) written in nextflow which installs deepvariant via conda. The command for `make_examples` will evaluate to something like this:; ```bash; time seq 0 !{numberShardsMinusOne} | \; parallel --eta --halt 2 \; python /opt/conda/pkgs/deepvariant-0.7.0-py27h5d9141f_0/share/deepvariant-0.7.0-0/binaries/DeepVariant/0.7.0/DeepVariant-0.7.0+cl-208818123/make_examples.zip \; --mode calling \; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz; --task {}; ```. I noticed that there is `dv_make_examples.py` on the PATH. Does this executable carry out the same/similar function as `python make_examples.zip`. If so how could I modify the code above to make it work?. Many thanks in advance",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/101:36,pipeline,pipeline,36,,https://github.com/google/deepvariant/issues/101,2,"['install', 'pipeline']","['installs', 'pipeline']"
Deployability,"Hi,. I don't know if this is the place to report issues with running the docker pipeline on the google cloud, I have been following instructions at [https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1) and it mostly works, but the second command within the big docker call dies with some python error. . That page indicates I should email google directly at google-genomics-contact@google.com, but this address bounces, which is why I came here. Anyway I have all the commands and error messages etc, so let me know if this is the right place for that and I will post. Thanks,; Ariel",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399:80,pipeline,pipeline,80,,https://github.com/google/deepvariant/issues/399,1,['pipeline'],['pipeline']
Deployability,"Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**; Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**; - Operating system: CentOS; - DeepVariant version: v1.2; - Installation method (Docker, built from source, etc.): Singularity v3.5.2; - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**; - Command:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=""PACBIO"" \; --regions ""$CHROM""; [ ... otherwise default options ...]; ```; - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`); - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**; can't be used to reproduce the problem. **Any additional context:**; In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this; ```; I0112 10:31:04.917984 47443049531200 \; make_examples_core.py:236] \; Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]; ```. Best,; Peter",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/603:395,Install,Installation,395,,https://github.com/google/deepvariant/issues/603,1,['Install'],['Installation']
Deployability,"Hi,. The singularity containers are on version 0.9.0, can these be updated to 1.0.0?. Cheers,. Max H.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/353:67,update,updated,67,,https://github.com/google/deepvariant/issues/353,1,['update'],['updated']
Deployability,"Hi,; I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>; from etils import epath; ModuleNotFoundError: No module named 'etils' . ```; Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/669:7,install,installed,7,,https://github.com/google/deepvariant/issues/669,1,['install'],['installed']
Deployability,"Hi. . I recently ran deep variant with a collection of WGS samples. It seems to have run through the whole pipeline producing the g.vcf.gz and vcf.gz files, however there was no visual_report.html file? . /nrnb/opt/singularity-3.3.0/bin/singularity run --nv /nrnb/opt/singularity-containers/deepvariant_gpu_0.8.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref --reads=$bam --regions $chr --output_vcf=$x.$chr.vcf.gz --output_gvcf=$x.$chr.g.vcf.gz --num_shards=3. Do I have to include a specific output tag in order to get the file? Based on the docs, it should automatically be produced?. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/290:107,pipeline,pipeline,107,,https://github.com/google/deepvariant/issues/290,1,['pipeline'],['pipeline']
Deployability,"Hi. I'm running a pipeline but get an error on ""make_examples"" stage. Could you help me to debug possible cause, please?. Here is a log:; ```; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:51.456603: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0814 12:36:53.581777 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:36:54.201131 140158089049856 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:58.286794: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0814 12:36:59.914532 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:45:36.568115 140158089049856 make_examples.py:946] Common contigs are [u'LKUA01000001.1', u'LKUA01000002.1', ...<ANOTHER 300k NAMES>..., u'LKUA01311038.1', u'LKUA01311039.1']; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/cannabis-3k-vcf/staging/SRS1107973_LKUA01/staging/examples/0/examples_output.tfrecord@512.gz --reads /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam --ref /mnt/google/.google/input/cannabis-3k/reference/LKUA01/LKUA01.fa --task 8; ```. For me it looks that the error message is `parallel: This job failed:` and failure doesn't relate to the warnings at the beginning of the file?. Regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/207:18,pipeline,pipeline,18,,https://github.com/google/deepvariant/issues/207,1,['pipeline'],['pipeline']
Deployability,"Hi. when I try to pull the deepvariant docker image via singularity using the following command:; singularity pull docker://google/deepvariant:""1.3.0""; it return the following error:; WARNING: pull for Docker Hub is not guaranteed to produce the; WARNING: same image on repeated pull. Use Singularity Registry; WARNING: (shub://) to pull exactly equivalent images.; ERROR Authentication error, exiting.; Cleaning up...; ERROR: pulling container failed!. could you please help. I don't have the permission to install docker.; Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/513:508,install,install,508,,https://github.com/google/deepvariant/issues/513,1,['install'],['install']
Deployability,"Hi.; I read this post and came here. [https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction). I'd like to apply the process to my pipeline. But the above example ingores indel errors and considers only regions with no known variants. Does this repo has sequencing error correction part?; If then, can I use the part only?. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/760:315,pipeline,pipeline,315,,https://github.com/google/deepvariant/issues/760,1,['pipeline'],['pipeline']
Deployability,"Hi.; I'm running the pipeline on a CRAM file. I read that the pipeline works with CRAM files, so I guess that's not the issue.; Can you assist in any way?; Thanks. **Setup**; - Operating system: Ubuntu 20.04.5 LTS; - DeepVariant version: 1.4.0; - Installation method: Docker; - Type of data: I have no information about the sequencing instrument, the reference genome was GRCh38. **Steps to reproduce:**; - Command:; ```; BIN_VERSION=""1.4.0"". sudo docker run \; -v ""input"":""/input"" \; -v ""output"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/input/GCF_000001405.26_GRCh38_genomic1.fa.gz \; --reads=/input/1115492_23181_0_0.cram \; --regions ""chr3:10,049,322-10,156,156"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=5 ; ```; - Error trace:; ; > parallel: This job failed:; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; retu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/588:21,pipeline,pipeline,21,,https://github.com/google/deepvariant/issues/588,3,"['Install', 'pipeline']","['Installation', 'pipeline']"
Deployability,"Hi;; I would like to build DeepVariant on CentOS 7. I have installed dependencies of the Centos version corresponding to run-prereq.sh. But I also cann't run copying binaries on my local machines which have installed CentOS 7. The wrror message indicate it cann't find some dependent package in environment.; the error message:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 41, in <module>; from deepvariant import pileup_image; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 42, in <module>; from third_party.nucleus.util import ranges; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 42, in <module>; from third_party.nucleus.io import bed; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 79, in <module>; from third_party.nucleus.io.python import bed_reader; ImportError: libbz2.so.1.0: cannot open shared object file: No such file or directory; ```; I know we can use docker to run DeepVariant on CentOS 7. But There are some reason why I cann't use docker to run DeepVariant. Did you try to build DeepVariant for CentOS 7. Or, you know who have build DeepVariant with some way on CentOS 7. If you know, can give me some adviceï¼Ÿ; Thanks a lot,; Simon.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/95:59,install,installed,59,,https://github.com/google/deepvariant/issues/95,2,['install'],['installed']
Deployability,"I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/208:53,install,installing,53,,https://github.com/google/deepvariant/issues/208,2,"['install', 'pipeline']","['installing', 'pipeline']"
Deployability,"I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10.; I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed.; On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ?. Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not; unavailable for this platform. Please install CLIF at; https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**.; A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/160:25,install,install,25,,https://github.com/google/deepvariant/issues/160,6,['install'],"['install', 'installed']"
Deployability,"I am following the [Building from sources](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-build-test.md) tutorial. I cloned the repository, then started building and got the following errors:. ```; viniws@woese:~/Code/deepvariant$ ./build-prereq.sh ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Install the runtime packages' starting ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Misc setup' starting ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Update package list' starting ; E: The repository 'http://apt.postgresql.org/pub/repos/apt YOUR_UBUNTU_VERSION_HERE-pgdg Release' does not have a Release file. ; E: The repository 'http://ppa.launchpad.net/gnome-terminator/ppa/ubuntu bionic Release' does not have a Release file ; ```. And after:. ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ DV_BAZEL_VERSION=0.15.0; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98:353,Install,Install,353,,https://github.com/google/deepvariant/issues/98,6,"['Install', 'Release', 'Update']","['Install', 'Release', 'Update']"
Deployability,"I am running DeepVariant on google cloud following the wiki (Cost-optimized configuration): https://cloud.google.com/genomics/deepvariant, and it works for one of our sample. . Now I have several bam files for several samples. I can run them one by one. But I wonder is there some options to set a list of bam files? Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/33:76,configurat,configuration,76,,https://github.com/google/deepvariant/issues/33,1,['configurat'],['configuration']
Deployability,"I am trying to build DeepVariant from source, and **trying to use a custom python installation rather than the standard one.** However, ```bazel test ``` fails because it tries to use the standard library python. The requisite python is accessible as ""python"" because it is in the PATH variable, but bazel seems to ignore that and looks for python in the standard location. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/loca",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356:82,install,installation,82,,https://github.com/google/deepvariant/issues/356,2,['install'],"['installation', 'installed']"
Deployability,"I am trying to install DeepVariant on an IBM Power 8 system within a docker container. The docker container has the following Bazel version installed: 0.15.0- (https://github.com/bazelbuild/bazel/releases/tag/0.15.0) . I installed tensorflow r1.11 from source inside the docker container for CPU-only execution. This same source-code is placed so that it is seen by the build-prereq.sh script. I set the `export DV_USE_PREINSTALLED_TF=1`. In settings.sh, I changed DV_BAZEL_VERSION to DV_BAZEL_VERSION=""0.15.0-"" (to match the bazel version above). I also removed the corei7 option in DV_COPT_FLAGS. . In build-prereq.sh, I hard-coded the following in: `DV_PLATFORM=""ubuntu-16""`, since `lsb_release` didn't match the case statement conditions there. The following is the result `lsb_release`.; root@1f07cee05809:~/deepvariant# lsb_release; LSB Version: core-9.20160110ubuntu0.2-noarch:core-9.20160110ubuntu0.2-ppc64el:security-9.20160110ubuntu0.2-noarch:security-9.20160110ubuntu0.2-ppc64el. After these changes, build-prereq.sh runs fine. However, build_and_test.sh fails with the following error:; (03:21:40) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:424:1: ClifProtoLibraryGeneration third_party/nucleus/protos/reads_pyclif.h failed (Exit 2): proto failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.cc -h bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.h '--strip_dir=bazel; -out/ppc-opt/genfiles' '--source_dir='\''.'\''' third_party/nucleus/protos/reads.proto); bazel-out/host/bin/external/clif/proto: 3: bazel-out/host/bin/external/clif/proto: __requires__: not found; bazel-out/host/bin/external/clif/proto: 4: bazel-out/host/bin/external/clif/proto: import: not found; bazel-out/host/bin/external/clif/proto: 5: bazel-out/host/bin/external/clif/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/122:15,install,install,15,,https://github.com/google/deepvariant/issues/122,4,"['install', 'release']","['install', 'installed', 'releases']"
Deployability,"I am trying to install deepvariant within a singularity container using recipe build. I have successfully complied google-sdk within the container, when it comes to the point where its runs ""./build-prereq.sh"" script, it terminates with this error: . **Setting up unzip (6.0-9ubuntu1) ...; Setting up zip (3.0-8) ...; Setting up zlib1g-dev:amd64 (1:1.2.8.dfsg-1ubuntu1) ...; Processing triggers for libc-bin (2.19-0ubuntu6) ...; Processing triggers for sgml-base (1.26+nmu4ubuntu1) ...; ========== [Tue Apr 17 00:09:23 UTC 2018] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; E: Unable to locate package python-wheel; ABORT: Aborting with RETVAL=255; Cleaning up...**. I am using singularity within in vagrant on my mac and here is my recipe file:. **Bootstrap: shub; From: singularityhub/ubuntu. %runscript; exec echo ""The runscript is the containers default runtime command!"". %files; # /home/vanessa/Desktop/hello-kitty.txt # copied to root of container; # /home/vanessa/Desktop/party_dinosaur.gif /opt/the-party-dino.gif #. %environment. export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)"". %labels; AUTHOR mnoon@email.arizona.edu. %post; apt-get update && apt-get -y install python2.7 git wget curl ; mkdir /data; echo ""The post section is where you can install, and configure your container."". echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" > /etc/apt/sources.list.d/google-cloud-sdk.list; curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -. apt-get update && apt-get install -y google-cloud-sdk; ##download deepVariant scripts and run them; git clone https://github.com/google/deepvariant.git; cd deepvariant; ; ./build-prereq.sh. ./build_and_test.sh. ./run-prereq.sh**. I have no idea whats causing this error. any help will be appreciated. -M",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/66:15,install,install,15,,https://github.com/google/deepvariant/issues/66,7,"['Install', 'install', 'update']","['Install', 'install', 'update']"
Deployability,I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137:199,install,installing,199,,https://github.com/google/deepvariant/issues/137,1,['install'],['installing']
Deployability,"I am using Ubuntu 16. I got binaries from file:; https://github.com/google/deepvariant/releases/download/v0.7.0/deepvariant.zip. **I run run-prereq.sh first and warning message appears:** ; Cloning into 'tensorflow'...; Switched to a new branch 'r1.9'; Extracting Bazel installation...; WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"". **When I run ./build_and_test.sh command, an error appears:**; ./build_and_test.sh: line 54: bazel: command not found. **When I run ./run-prereq.sh command, it stops at ""unable to re-open stdin:""** ; debconf: unable to initialize frontend: Dialog; debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.); debconf: falling back to frontend: Readline; debconf: unable to initialize frontend: Readline; debconf: (This frontend requires a controlling tty.); debconf: falling back to frontend: Teletype; dpkg-preconfigure: unable to re-open stdin: ; debconf: unable to initialize frontend: Dialog; debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.); debconf: falling back to frontend: Readline; debconf: unable to initialize frontend: Readline; debconf: (This frontend requires a controlling tty.); debconf: falling back to frontend: Teletype; dpkg-preconfigure: unable to re-open stdin: . As I can see the problem is bazel installation and already some ways of resolving the problem were suggested - one of suggestion was to change .txt.sh file, another one to manually install bazel package (which seems to me regarding instructions on bazel site not a straightforward approach). I am running DeepVariant on a cluster, therefore would be very grateful for any more straightforward; suggestion. Thank you very much.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/92:87,release,releases,87,,https://github.com/google/deepvariant/issues/92,4,"['install', 'release']","['install', 'installation', 'releases']"
Deployability,"I currently tried to write some script to train the new model. The make_example script have created labeled data. But the model_train script have some problem. I made some labeled examples using the WGS case study data of Deepvariant on chr20. I used the 0.6.0 version of released DeepVariant model as a started training model. **The make_examples script is:**; `python ../bin/make_examples.zip \; --mode training \; --ref ""file/ucsc.hg19.chr20.unittest.fasta.gz"" \; --reads ""file/NA12878_S1.chr20.10_10p1mb.bam"" \; --confident_regions ""file/test_nist.b37_chr20_100kbp_at_10mb.bed"" \; --truth_variants ""file/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz"" \; --examples ""output/examples.tfrecord.gz""; `; **my-training-dataset.pbtxt file:**; `name: ""my-training-dataset""; tfrecord_path: ""/home/suanfa/Documents/wangpeng/testmake_examples/output/examples.tfrecord.gz""; num_examples: 1`. **The model_train script is:**; `python ../bin/model_train.zip \; --dataset_config_pbtxt ""./my-training-dataset.pbtxt"" \; --start_from_checkpoint ""/my/path/of/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt""; `. **The following error have happened while the model_train.zip is invoked:**; > I0502 10:58:51.903573 139632719935232 model_train.py:182] Initializing model from checkpoint at /home/suanfa/Documents/source/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt; 2018-05-02 10:58:56.347500: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA; 2018-05-02 10:58:57.263635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:; name: Tesla P100-PCIE-12GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285; pciBusID: 0000:3b:00.0; totalMemory: 11.91GiB freeMemory: 11.62GiB; 2018-05-02 10:58:57.263682: I tensorflow/core/common_runtim",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69:272,release,released,272,,https://github.com/google/deepvariant/issues/69,1,['release'],['released']
Deployability,"I found is for version 0.9, and I am unsure how to proceed with version 1.6.1. The command I used is the following: . ```; docker run \; -v ${HOME}:${HOME} \; google/deepvariant:1.6.1 \; train \; --config=${HOME}/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.num_epochs=10 \; --config.learning_rate=0.001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=tpu \; --config.batch_size=1024; ```. However, I am not an expert with TPUs, and this is entirely new to me. Below is the error I encountered. Do you have any suggestions or can you direct me to an updated tutorial to follow?. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0627 21:18:43.707066 139683296487232 train.py:92] Running with debug=False; I0627 21:18:43.707488 139683296487232 train.py:100] Use TPU at local; I0627 21:18:43.707705 139683296487232 train.py:103] experiment_dir: /home/gambardella/training_chk; INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.; I0627 21:18:43.707828 139683296487232 tpu_strategy_util.py:57] Deallocate tpu buffers before initializing tpu system.; INFO:tensorflow:Initializing the TPU system: local; I0627 21:18:43.846984 139683296487232 tpu_strategy_util.py:81] Initializing the TPU system: local; 2024-06-27 21:18:43.848149: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized wi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:1155,release,release,1155,,https://github.com/google/deepvariant/issues/841,1,['release'],['release']
Deployability,"I get the following error when running the example provided in the quick start document:. merge_overlaps() got an unexpected keyword argument 'strict'. Any advice as to how I can resolve the issue is greatly appreciated. ```; This is the context of the error.; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/data/hs37d5.fa.gz"" --reads ""/input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" `--gvcf` ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""20"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I1220 08:40:22.262234 46912496321664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1220 08:40:22.268675 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.272100 46912496321664 make_examples.py:1324] Preparing inputs; I1220 08:40:22.280786 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.292714 46912496321664 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_UJ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/255:811,install,installed,811,,https://github.com/google/deepvariant/issues/255,1,['install'],['installed']
Deployability,I had tried install the soft using condaï¼Œ but it was failed finally. Besides dose GPU necessary ï¼Ÿ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/169:12,install,install,12,,https://github.com/google/deepvariant/issues/169,1,['install'],['install']
Deployability,"I have WGS data (about 200x) and WES data (about 1000x) of the same individual.; Ideally I would like to merge the 2 datasets and run DeepVariant with --model_type=WGS on the merged data and obtain one VCF file. Or is the model behind ""--model_type=WES"" really a different machine learning model (ML) trained on real Exome data?; I could imagine that such a ML model would learn a slightly different sequencing error model specific for sequencing data derived from target enrichment (hybridization probes) as the ones used for WES. Thank you for your advice. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: r0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Illumina WGS and WES data",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/338:636,Install,Installation,636,,https://github.com/google/deepvariant/issues/338,1,['Install'],['Installation']
Deployability,"I have try to install the DeepVariant, but failed. I cannot install the GoogleCloud at first, and then cannot also install the software. I come from China. Any solutions to the problem. I hope you can give some help. Thank you. I downloaded a versioned archive for Cloud SDK. When installing googlecloud, some module error pops. ; File ""xxxxx/install.py"", line 8, in <module> import bootstrapping; File ""xxxxx/install.py"", line 9, in <module> import setup; File ""xxxxx/install.py"", line 38, in <module> from googlecloudsdk.core.util import platforms; ImportError: No module named googlecloudsdk.core.util; I also tried the apt-get install, however due to network striction, I cannot install it by online command.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/17:14,install,install,14,,https://github.com/google/deepvariant/issues/17,9,['install'],"['install', 'installing']"
Deployability,"I managed to start training on a GPU, but it took too much time. Now, I am attempting to train DeepVariant on a TPU v3-8 VM. However, the most recent tutorial I found is for version 0.9, and I am unsure how to proceed with version 1.6.1. The command I used is the following: . ```; docker run \; -v ${HOME}:${HOME} \; google/deepvariant:1.6.1 \; train \; --config=${HOME}/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.num_epochs=10 \; --config.learning_rate=0.001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=tpu \; --config.batch_size=1024; ```. However, I am not an expert with TPUs, and this is entirely new to me. Below is the error I encountered. Do you have any suggestions or can you direct me to an updated tutorial to follow?. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0627 21:18:43.707066 139683296487232 train.py:92] Running with debug=False; I0627 21:18:43.707488 139683296487232 train.py:100] Use TPU at local; I0627 21:18:43.707705 139683296487232 train.py:103] experiment_dir: /home/gambardella/training_chk; INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.; I0627 21:18:43.707828 139683296487232 tpu_strategy_util.py:57] Deallocate tpu buffers before initializing tpu system.; INFO:tensorflow:Initializing the TPU system: local; I0627 21:18:43.846984 139683296487232 tpu_strategy_util.py:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:901,update,updated,901,,https://github.com/google/deepvariant/issues/841,1,['update'],['updated']
Deployability,"I noticed a mention of VG giraffe evaluation in the 1.5 changelog. . Has your team evaluated the impact of performing indel realignment prior to variant calling VG giraffe-generated bamfiles? This is what was done in the vg giraffe-DeepVariant paper. . In my hands, the indel realignment step significantly adds to run time. If your team does not think it meaningfully improves accuracy beyond make_example's built in realignment algorithm, then I could remove the step from my pipeline. That would be welcome news!. -Joe Lalli",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/629:478,pipeline,pipeline,478,,https://github.com/google/deepvariant/issues/629,1,['pipeline'],['pipeline']
Deployability,"I ran deeptrio on a trio WGS data. I got the gvcf and vcf for parent 1 and 2 but I didn't get output from child. There were no error messages that I could find as to why. The output seems complete. **Setup**; - Operating system: Windows 10; - DeepVariant version: DeepTrio version 1.1.0; - Installation method: Docker; - Type of data: Illumina, GRCh38, trio WGS. **Steps to reproduce:**; - Command:; `/opt/deepvariant/bin/deeptrio/run_deeptrio . - --model_type=WGS ; - --ref=GRCh38_full_analysis_set_plus_decoy_hla.fa ; - --reads_child=20A0012672_P_GRCh38.bam ; - --reads_parent1=20A0012673_M_GRCh38.bam ; - --reads_parent2=NBVY8432_GRCh38.bam; - --output_vcf_child 20A0012672_P_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent1 20A0012673_M_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent2 NBVY8432_GRCh38_deeptrio.vcf.gz ; - --sample_name_child '20A0012672_P' ; - --sample_name_parent1 '20A0012673_M' ; - --sample_name_parent2 'NBVY8432' ; - --num_shards $(nproc) ; - --intermediate_results_dir ../home/tmp ; - --output_gvcf_child 20A0012672_P_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent1 20A0012673_M_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent2 NBVY8432_GRCh38_deeptrio.gvcf.gz`. - Error trace: NA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/431:290,Install,Installation,290,,https://github.com/google/deepvariant/issues/431,1,['Install'],['Installation']
Deployability,"I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions!. **Setup**; - Operating system: linux; - DeepVariant version: 1.5.0 (in Deeptrio as well); - Installation method: Singularity version; - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704:34,pipeline,pipeline,34,,https://github.com/google/deepvariant/issues/704,2,"['Install', 'pipeline']","['Installation', 'pipeline']"
Deployability,"I rebuilt docker images from instruction on https://github.com/google/deepvariant/issues/99#issuecomment-428366972. ```; gcloud builds submit \; --project ""${PROJECT_ID}"" \; --config cloudbuild.yaml \; --substitutions TAG_NAME=""${VERSION_NUMBER}"" \; --timeout 2h .; ```. I see three images on GCP Container Registry:. 1. **deepvariant**; 1. **deepvariant_gpu**; 1. **deepvariant_runner**. After finishing make_examples, now I am running calll_variant, but seems my rebuilt deepvariant_gpu image doesn't have CUDA installed, seeing such error. ```; ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory; ```. The command I used:; ```; ( time nvidia-docker run -v /home/${USER}:/home/${USER} gcr.io/my_project/deepvariant_gpu:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) | tee ""${LOG_DIR}/call_variants.log"" 2>&1; ```. I confirmed this is NOT an issue with gcr.io/deepvariant-docker/deepvariant_gpu, which means that it's just my rebuilt image missing CUDA driver. How should I modify the build command to build an image with CUDA driver, please?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102:513,install,installed,513,,https://github.com/google/deepvariant/issues/102,1,['install'],['installed']
Deployability,"I recently took four cram files (one family) and put them through the deep variant pipeline in accordance with the documentation. I then followed the instructions for ""Best practices for multi-sample variant calling with DeepVariant (WES trio demonstration)"" and tried two sets of of ""trios"" and one comparison containing all four. Each of them however came up with a substanial amount of mendelian constraints which I'll list below:. Checking: /home/username/deepvariant-run/output/RBAs.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2115432]; 12 non-pass records were skipped; Concordance 2115432: F:127216/127862 (99.49%) M:121292/121882 (99.52%) F+M:79650/80917 (98.43%); Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 4249/338863 (1.25%) records did not conform to expected call ploidy; 228759/338863 (67.51%) records were variant in at least 1 family member and checked for Mendelian constraints; 147067/228759 (64.29%) records had indeterminate consistency status due to incomplete calls; 1838/228759 (0.80%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNs.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617]; 18 non-pass records were skipped; Concordance 2009617: F:124581/125097 (99.59%) M:120027/120545 (99.57%) F+M:79289/80523 (98.47%); Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 3705/295693 (1.25%) records did not conform to expected call ploidy; 184648/295693 (62.45%) records were variant in at least 1 family member and checked for Mendelian constraints; 103541/184648 (56.07%) records had indeterminate consistency status due to incomplete calls; 1603/184648 (0.87%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNAs.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617, 2115432]; 24 non-pass records were ski",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/311:83,pipeline,pipeline,83,,https://github.com/google/deepvariant/issues/311,1,['pipeline'],['pipeline']
Deployability,"I tried to build deepvariant through my own docker image. I installed deepvariant and openvino toolkit. But, when I run _call_variants.zip_ script, I get the error **_name 'optimize_for_inference_lib' is not defined_**. I was retracing steps and came to conclusion that `from openvino.inference_engine import StatusCode` part of the code is failing. StatusCode cannot be imported. Have you ever encountered the same problem ?. I installed DeepVariant1.1.0 version via Docker using Ubuntu 18.04. The command I run was ; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32 ` . and got this error:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/432:60,install,installed,60,,https://github.com/google/deepvariant/issues/432,2,['install'],['installed']
Deployability,"I used DeepVariant to call SNPs and Indels with the HG002 Pacbio Revio benchmark data download from https://human-pangenomics.s3.amazonaws.com/submissions/80d00e88-7a92-46d8-88c7-48f1486e11ed--HG002_PACBIO_REVIO/, while the hap.py result showed Indels have very low precision (0.653927) and recall (0.884985) and SNPs seemed to be normal having 0.998 precision and recall. Type	Filter	TRUTH.TOTAL	TRUTH.TP	TRUTH.FN	QUERY.TOTAL	QUERY.FP	QUERY.UNK	FP.gtMETRIC.Recall	METRIC.Precision	METRIC.Frac_NA	METRIC.F1_Score; INDEL	ALL	523034	462877	60157	1215487	252834	484908	16813	0.884985	0.653927	0.398941	0.75211; INDEL	PASS	523034	462877	60157	1215487	252834	484908	16813	0.884985	0.653927	0.398941	0.75211; SNP	ALL	3352818	3349190	3628	3960152	8899	597545	864	0.998918	0.997354	0.150889	0.998135; SNP	PASS	3352818	3349190	3628	3960152	8899	597545	864	0.998918	0.997354	0.150889	0.998135. I running the pipeline with aligner minimap2 v2.24 with parameters ""-L --MD -Y -a -x map-hifi --secondary=no"" and calling SNVs with deepVariant v1.3.0 with --model_type=PACBIO.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/641:898,pipeline,pipeline,898,,https://github.com/google/deepvariant/issues/641,1,['pipeline'],['pipeline']
Deployability,"I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04; - DeepVariant version: 1.4.0; - Building docker image locally. **Steps to reproduce:**; - Command: `docker build .` in source directory (no modifications); - Error trace: ; ; ```; #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting; #16 1484.7 Extracting Bazel installation...; #16 1487.8 Starting local Bazel server and connecting to it...; #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc); #16 1489.8 (21:51:01) INFO: Options provided by the client:; #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:479,install,installation,479,,https://github.com/google/deepvariant/issues/608,1,['install'],['installation']
Deployability,"I was planning to analyse runs of homozygosity in a genome assembly with Plink using DeepVariant for the variant calling. Unfortunately Plink needs basepair resolution in the input vcf file, e.g. a vcf/gvcf file that includes homozygous reference calls as well. I tried different options in DeepVariant but there seems to be no option for basepair resolution. GATK has the option to output this (-ERC BP_RESOLUTION) but since DeepVariant is more accurate and much faster I was wondering if would it be possible to add such a feature in the future? . **Setup**; - Operating system: CentOS; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Genome assembly plus PacBio HIFI or Illumina WGS",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/571:621,Install,Installation,621,,https://github.com/google/deepvariant/issues/571,1,['Install'],['Installation']
Deployability,"I was trying to follow the quick start guide. While running the run-prereq.sh file, I got; `========== Load config settings.`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Misc setup' starting`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Update package list' starting`; `sudo: apt-get: command not found`; Then, I realize it is because I am running it on mac. Is there any quick fix to this problem?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/44:250,Update,Update,250,,https://github.com/google/deepvariant/issues/44,1,['Update'],['Update']
Deployability,"I would like to try to make a custom singularity version of deepvariant in which is integrated with some custom scripts, do you know where I could find the singularity recepie of deepvariant? It would be a good starting point for me. Thank you in advance for any suggestion,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/862:84,integrat,integrated,84,,https://github.com/google/deepvariant/issues/862,1,['integrat'],['integrated']
Deployability,"I would rather not install the DeepVariant dependencies into my global python environment. When the python dependencies are installed into a virtual environment, make_examples.zip cannot find tensorflow. The steps below show the error. Any suggestions?. ```; $ mkvirtualenv -p /usr/bin/python2.7 DeepVariant.2.7; (DeepVariant.2.7) $ cd bin; bash run-prereq.sh; cd -. (DeepVariant.2.7) $ python bin/make_examples.zip --mode calling --ref ""${REF}"" --reads ""${BAM}"" --regions ""chr20:10,000,000-10,010,000"" --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_r1oZvM/runfiles/genomics/deepvariant/make_examples.py"", line 38, in <module>; import tensorflow as tf; ImportError: No module named tensorflow; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/25:19,install,install,19,,https://github.com/google/deepvariant/issues/25,2,['install'],"['install', 'installed']"
Deployability,"I'm opening another issue to follow up on:; https://github.com/google/deepvariant/issues/132#issuecomment-482956117; (The original thread is getting a bit too long, and this issue seems more specific). Relevant system information in another comment:; https://github.com/google/deepvariant/issues/132#issuecomment-483551683; ""The super computer has an OS CentOS Linux release 7.6.1810 (LSB Version: core-4.1-amd64:core-4.1-noarch) and singularity version 2.5.2; I created the image on Amazon instance with Ubuntu 16.04. I tried using singularity version 2.5.2 & 2.6.0 but both did not help"". Adding @drtamermansour here as well.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/178:367,release,release,367,,https://github.com/google/deepvariant/issues/178,1,['release'],['release']
Deployability,"I'm trying to build a scatter gather implementation of make_calls -> call_variants -> post_processing, without using GNU parallel, and since multiple shards of call_variants, even with num_readers set to 1, increases the system load well beyond the number of cores, I'd like to try to limit this to a 1:1 ratio where one shard produces a system load of 1. Is this possible?. This is essentially what my pipeline looks like today: https://github.com/oskarvid/wdl_deepvariant/blob/master/deepvar-simple-SG.wdl; I say essentially because I've made insignificant changes, like added --num_readers for example. . One way would be to try to combine all tfrecord files into one, because wdl cannot use your method of using all output files from make_calls as input files for call_variants, inputFile@#shards.gz doesn't compute for wdl, and using wdl's normal way of handling multiple input files, i.e ""--examples ${sep="" --examples "" InputFile}"" doesn't work either since call_variants only takes the last ""--examples"" as input when there are many ""--examples"" in the command. Regarding combining the tfrecord files before they're used as input for call_variants, I'm not familiar enough with tensorflow to know if it's at all possible, and a quick google search didn't return anything fruitful. Is it possible to combine many tfrecord files into one?. Is it easier to try to limit the number of threads per process instead of trying to combine the tfrecord files? Or is there a third method that solves this problem better?. And thanks for a great tool!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49:403,pipeline,pipeline,403,,https://github.com/google/deepvariant/issues/49,1,['pipeline'],['pipeline']
Deployability,"I'm trying to output gVCF's via DeepVariant as described by the tutorial here: https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-gvcf-support.md . Is there a way that I could just modify the gcp_deepvariant_runner.py script (linked below) rather than having to manually run the commands step by step? I have many BAM files to process and running the pipeline manually is intractable. https://github.com/googlegenomics/gcp-deepvariant-runner/blob/master/gcp_deepvariant_runner.py. I'm guessing I would need to fork the gcp-deepvariant-runner repo, edit the python file, then push the new repo to some sort of container registry? Any guidance here would be much appreciated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/193:365,pipeline,pipeline,365,,https://github.com/google/deepvariant/issues/193,1,['pipeline'],['pipeline']
Deployability,"I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook ; n1-standard-64 - 64v CPUs - 240GB RAM; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): docker deepvariant ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:; export BIN_VERSION=""1.5.0""; export INPUT_DIR=""/home/jupyter/input""; export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""; export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam""; export OUTPUT_DIR=""/home/jupyter/output""; export OUTPUT_VCF=""HG003.deepvariant.vcf.gz""; export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md; ; I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. ; Is there a reason for the differences in runtime? ; ; Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/735:328,Install,Installation,328,,https://github.com/google/deepvariant/issues/735,2,"['Install', 'configurat']","['Installation', 'configuration']"
Deployability,"If I generated a VCF file for a trio (with a father using deeptrio) or a solo male (using deepvariant) without --haploid_contigs=""chrX,chrY"" and/or --par_regions_bed parameters. Can I fix the VCF after the run is finished, let's say by targeted calling?. And as a suggestion, it would be nice if the algorithm takes care of this automatically :). - Operating system: Linux; - DeepVariant version: 1.6.0 and 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PacBio, Hg38",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/825:416,Install,Installation,416,,https://github.com/google/deepvariant/issues/825,1,['Install'],['Installation']
Deployability,Install intel-tensorflow with custom wheel.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/264:0,Install,Install,0,,https://github.com/google/deepvariant/pull/264,1,['Install'],['Install']
Deployability,Install precompiled binaries,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:0,Install,Install,0,,https://github.com/google/deepvariant/issues/252,1,['Install'],['Install']
Deployability,Installation error with intel-tensorflow,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/263:0,Install,Installation,0,,https://github.com/google/deepvariant/issues/263,1,['Install'],['Installation']
Deployability,Installation of 0.8.0 using bioconda:; ```; conda create -n deepvariant deepvariant; ```. fails on CentOS 6.6 with: . ```; CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/.travis.yml'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/README.rst'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/documentation/Makefile'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/documentation/conf.pyc'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/documentation/index.rst'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/setup.cfg'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177:0,Install,Installation,0,,https://github.com/google/deepvariant/issues/177,1,['Install'],['Installation']
Deployability,"Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:; ```; micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; Traceback (most recent call last):; File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call; with Popen(*popenargs, **kwargs) as p:; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__; restore_signals, start_new_session); File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'; ```. I noticed inside the bazel .zip files the python binary is hard-coded:; /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```; micro",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664:0,Install,Installed,0,,https://github.com/google/deepvariant/issues/664,1,['Install'],['Installed']
Deployability,Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2),MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657:0,Install,Installing,0,,https://github.com/google/deepvariant/issues/657,1,['Install'],['Installing']
Deployability,Is there a document release for DeepVariant used in PrecisionFDA v2 (the hybrid version)?. https://precision.fda.gov/challenges/10/view/results,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/336:20,release,release,20,,https://github.com/google/deepvariant/issues/336,1,['release'],['release']
Deployability,"Iâ€™m new to working with computers tools like DeepVariant. Iâ€™m trying to build DeepVariant using Docker on a Mac M1 and am encountering issues with the Dockerfile during the Bazel build process. I want to ensure compatibility with ARM64 architecture. **Docker version**: Docker version 27.1.1, build 6312585; **Bazel Version**: 7.3.1; **MacBook Model**: M1 chip (ARM64 architecture). **Error**: ; ![IMG_3267](https://github.com/user-attachments/assets/11e28824-b941-42cc-9d33-7e9155a03543); ![IMG_3268](https://github.com/user-attachments/assets/4e923de6-99d5-43ee-80c6-29b32504527d). **My Dockerfilee code**:. ```; # Base image suitable for ARM64 architecture; FROM arm64v8/ubuntu:latest AS base. # Prevent interactive prompts; ENV DEBIAN_FRONTEND=noninteractive. # Install necessary packages; RUN apt-get update && \; apt-get install -y \; git \; curl \; unzip \; wget \; openjdk-17-jdk \; build-essential \; bzip2 \; python3-pip \; parallel && \; apt-get clean && \; rm -rf /var/lib/apt/lists/*. # Install Bazel (adjust version as needed); RUN curl -LO ""https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to ski",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:766,Install,Install,766,,https://github.com/google/deepvariant/issues/871,3,"['Install', 'install', 'update']","['Install', 'install', 'update']"
Deployability,"M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****; ```; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader; W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs; I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/542:3051,install,installed,3051,,https://github.com/google/deepvariant/issues/542,1,['install'],['installed']
Deployability,"Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/13939489157244551677"" failed: executing pipeline: Execution failed: action 5: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); details:; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116:2289,pipeline,pipeline,2289,,https://github.com/google/deepvariant/issues/116,1,['pipeline'],['pipeline']
Deployability,"My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do?. ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737:94,install,installing,94,,https://github.com/google/deepvariant/issues/737,2,['install'],"['installed', 'installing']"
Deployability,"Note: this PR is on the gh-pages branch, meant for an update for the goo.gl/deepvariant blog. It's submitted by a DeepVariant team member.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/244:54,update,update,54,,https://github.com/google/deepvariant/pull/244,3,['update'],['update']
Deployability,Numpy installation problem,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/394:6,install,installation,6,,https://github.com/google/deepvariant/issues/394,1,['install'],['installation']
Deployability,OpenVINO integration,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363:9,integrat,integration,9,,https://github.com/google/deepvariant/pull/363,1,['integrat'],['integration']
Deployability,"Our cluster environment only allows us to run Singularity containers (due to not running as root) instead of Docker containers. I've been converting the DeepVariant provided docker images to Singularity using version 0.7.0 which worked well. With version 0.7.2 the python imports break due to the location of where the python packages are installed. In version 0.7.0 the python requirements were installed in /usr/local/lib/python2.7/dist-packages/, but now the requirements are installed into /root/.local/lib/python2.7/site-packages/ which does not get copied over into the Singularity container. This might be because of Singularity being designed to run not as root. Would you be able to change the install location back to /usr/local/lib/python2.7/dist-packages/ or provide working Singularity containers?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132:339,install,installed,339,,https://github.com/google/deepvariant/issues/132,4,['install'],"['install', 'installed']"
Deployability,Questions about GLnexus integration and DeepTrio training data for config evaluation,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475:24,integrat,integration,24,,https://github.com/google/deepvariant/issues/475,1,['integrat'],['integration']
Deployability,"R_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, since apt-utils is not installed; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 663 100 663 0 0 5012 0 --:--:-- --:--:-- --:--:-- 5022; debconf: delaying package configuration, since apt-utils is not installed; WARNING: Logging before flag parsing goes to stderr.; ```. But I then see many messages about candidate variants it's found. . The directory `staging-folder/examples/0/` also includes 8 `.gz` files like `examples_output.tfrecord-00007-of-00008.gz`. . Can you help me figure out what I'm doing wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:3499,configurat,configuration,3499,,https://github.com/google/deepvariant/issues/60,4,"['configurat', 'install']","['configuration', 'installed']"
Deployability,Rebuilt deepvariant_gpu docker image doesn't seem to have CUDA driver installed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102:70,install,installed,70,,https://github.com/google/deepvariant/issues/102,1,['install'],['installed']
Deployability,Run OpenVINO processing in separate thread which let's to use common logging (https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 + https://github.com/google/deepvariant/pull/363/commits/9e69c4096fac8ddb788c3d29e4405fc50e85d1e3) . Please ignore test scripts from `.github/workflows` - they are not a part of patch but just used for validation.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/393:352,patch,patch,352,,https://github.com/google/deepvariant/pull/393,1,['patch'],['patch']
Deployability,Soooo hard to install,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/169:14,install,install,14,,https://github.com/google/deepvariant/issues/169,1,['install'],['install']
Deployability,"Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh; build-prereq.sh; build_release_binaries.sh; run-prereq.sh; settings.sh",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/740:163,install,install,163,,https://github.com/google/deepvariant/issues/740,1,['install'],['install']
Deployability,"TF_CUDA_VERSION=""10.0""; export CUDA_TOOLKIT_PATH=""/usr/local/cuda""; export TF_CUDNN_VERSION=""7""; export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant.; DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that; # `bazel` will find the latest version of bazel installed in the user's home; # directory. This is set in setting.sh as all DeepVariant scripts source; # settings.sh and assume that `bazel` will find the right version.; export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts.; export DEEPVARIANT_BUCKET=""gs://deepvariant""; export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages""; export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a; # named release version. Set it to an already existing value in the environment; # (allowing command line control of the build), defaulting to 0 (release build).; # Note that setting this to 1 implies that the C++ code in DeepVariant will be; # build using the master branch and not the pinned version to avoid; # incompatibilities between TensorFlow C++ used to build DeepVariant and the; # tf-nightly wheel.; export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not; # the same as the python version of TensorFlow we use, but should be similar or; # we risk having version incompatibilities between our C++ code and the Python; # code we use at runtime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an alr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:4223,release,release,4223,,https://github.com/google/deepvariant/issues/145,1,['release'],['release']
Deployability,Tensorflow .whl is not installing during build,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/30:23,install,installing,23,,https://github.com/google/deepvariant/issues/30,1,['install'],['installing']
Deployability,The deep variant wrapper dv_call_variants.py crushing when installed using conda.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/573:59,install,installed,59,,https://github.com/google/deepvariant/issues/573,1,['install'],['installed']
Deployability,"The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is; https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/789:61,install,install,61,,https://github.com/google/deepvariant/issues/789,1,['install'],['install']
Deployability,"The very_sensitive_caller seems to be a new feature in the release 0.9.0, what's the difference between the very sensitive caller and the usual one?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/256:59,release,release,59,,https://github.com/google/deepvariant/issues/256,1,['release'],['release']
Deployability,"There are some problems while running the ./build-prereq.sh:; ```; + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0; Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0); Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0); Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:86,install,install,86,,https://github.com/google/deepvariant/issues/739,3,"['Install', 'install']","['Installing', 'install', 'installation']"
Deployability,This is a blog update by the DeepVariant team. Note: we are not taking pull requests at this time.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/276:15,update,update,15,,https://github.com/google/deepvariant/pull/276,1,['update'],['update']
Deployability,This is an internal pull request that is meant to only update the `gh_pages` branch.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/621:55,update,update,55,,https://github.com/google/deepvariant/pull/621,1,['update'],['update']
Deployability,"Traceback (most recent call last):; File ""get-pip.py"", line 32992, in <module>; main(); File ""get-pip.py"", line 135, in main; bootstrap(tmpdir=tmpdir); File ""get-pip.py"", line 111, in bootstrap; monkeypatch_for_cert(tmpdir); File ""get-pip.py"", line 92, in monkeypatch_for_cert; from pip._internal.commands.install import InstallCommand; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>; ModuleNotFoundError: No module named 'distutils.cmd'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/730:306,install,install,306,,https://github.com/google/deepvariant/issues/730,3,"['Install', 'configurat', 'install']","['InstallCommand', 'configuration', 'install']"
Deployability,Update README.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/375:0,Update,Update,0,,https://github.com/google/deepvariant/pull/375,1,['Update'],['Update']
Deployability,Update TensorFlow,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/511:0,Update,Update,0,,https://github.com/google/deepvariant/issues/511,1,['Update'],['Update']
Deployability,Update deepvariant-docker.md,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/424:0,Update,Update,0,,https://github.com/google/deepvariant/pull/424,1,['Update'],['Update']
Deployability,Update deepvariant-pacbio-model-case-study.md,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/473:0,Update,Update,0,,https://github.com/google/deepvariant/pull/473,1,['Update'],['Update']
Deployability,Update documentation.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/397:0,Update,Update,0,,https://github.com/google/deepvariant/pull/397,1,['Update'],['Update']
Deployability,Update issue templates,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/327:0,Update,Update,0,,https://github.com/google/deepvariant/pull/327,1,['Update'],['Update']
Deployability,"Update runtime - when I updated this last time, I used a newer run toâ€¦",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/350:0,Update,Update,0,,https://github.com/google/deepvariant/pull/350,2,"['Update', 'update']","['Update', 'updated']"
Deployability,Update the README.md.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/373:0,Update,Update,0,,https://github.com/google/deepvariant/pull/373,1,['Update'],['Update']
Deployability,Updated the introduction for the ML4H blog post.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/245:0,Update,Updated,0,,https://github.com/google/deepvariant/pull/245,1,['Update'],['Updated']
Deployability,Updates to shuffle_tfrecords_beam script for SparkRunner,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365:0,Update,Updates,0,,https://github.com/google/deepvariant/pull/365,1,['Update'],['Updates']
Deployability,Updates to underlying tools in docker,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/755:0,Update,Updates,0,,https://github.com/google/deepvariant/issues/755,1,['Update'],['Updates']
Deployability,"Upon running dv_make_examples.py -h , dv_call_variants.py -h , or dv_postprocess_variants.py -h , python told me there is a syntax error around a f-string. **Setup**; - Operating system: Centos; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): bioconda. **Steps to reproduce:**; - Command: dv_make_examples.py -h",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/627:227,Install,Installation,227,,https://github.com/google/deepvariant/issues/627,1,['Install'],['Installation']
Deployability,"Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0703 17:18:45.810746 140322304501504 data_providers.py:376] self.input_map_threads=48; W0703 17:18:45.810852 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0703 17:18:46.328745 140322304501504 estimator.py:1147] Calling model_fn.; W0703 17:18:46.330687 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0703 17:18:46.333346 140322304501504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0703 17:18:50.712157 140322304501504 estimator.py:1149] Done calling model_fn.; I0703 17:18:51.788142 140322304501504 monitored_session.py:240] Graph was finalized.; ```; We are on Docker 19.03.11 on Debian 10 and executed deepvariant with ```docker run --gpus all ....``` (proposed way of using nvidia docker for Docker version >19.03 in the nvidia docker docs). Our installed Nvidia driver version is 418.113. Thanks in advance for your help,. Sebastian",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321:6242,install,installed,6242,,https://github.com/google/deepvariant/issues/321,1,['install'],['installed']
Deployability,"WARE, EVEN IF ADVISED OF THE; # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for; # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This; # will skip the installation of TensorFlow.; export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0; export TF_ENABLE_XLA=1; export TF_NEED_CUDA=1; export TF_NEED_GCP=1; export TF_NEED_GDR=0; export TF_NEED_HDFS=0; export TF_NEED_JEMALLOC=0; export TF_NEED_MKL=1; export TF_NEED_MPI=0; export TF_NEED_OPENCL=0; export TF_NEED_OPENCL_SYCL=0; export TF_NEED_S3=1; export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1; export TF_CUDA_VERSION=""10.0""; export CUDA_TOOLKIT_PATH=""/usr/local/cuda""; export TF_CUDNN_VERSION=""7""; export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant.; DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that; # `bazel` will find the latest version of bazel installed in the user's home; # directory. This is set in setting.sh as all DeepVariant scripts source; # settings.sh and assume that `bazel` will find the right version.; export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts.; export DEEPVARIANT_BUCKET=""gs://deepvariant""; export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages""; export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a; # named release version. Set it to an already existing value in the environment; # (allowing command line control of the build), defaulting to 0 (release build).; # Note that setting this to 1 implies that the C++ code in DeepVariant will be; # build using the master branch and not the pinned version to avoid; # incompatibilities between TensorFlow C++ used to build DeepVariant and the; # tf-nightly wheel.; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:3532,install,installed,3532,,https://github.com/google/deepvariant/issues/145,1,['install'],['installed']
Deployability,"We are not taking pull requests at this time.; (Note: this PR is on the gh-pages branch, meant for an update for the goo.gl/deepvariant blog. It's submitted by @pichuan , a DeepVariant team member.)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/216:102,update,update,102,,https://github.com/google/deepvariant/pull/216,1,['update'],['update']
Deployability,We are not taking pull requests at this time.; This update is pushed by the team member (pichuan@) to the gh-pages for https://goo.gl/deepvariant.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/654:52,update,update,52,,https://github.com/google/deepvariant/pull/654,1,['update'],['update']
Deployability,"When I run build_and_test.sh, I get the following isssues; ```. Extracting Bazel installation...; .............................; (12:58:42) INFO: Current date is 2018-03-20; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:03) ERROR: Analysis of target '//deepvariant:binaries' failed; build aborted: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream; (13:01:03) INFO: Elapsed time: 146.946s; (13:01:03) FAILED: Build did NOT complete successfully (60 packages loaded); Fetching https://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz; 26,415b 43s; Fetching https://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz; 32,588b 42s; Fetching https://mirror.bazel.build/www.kurims.kyoto-u.ac.jp/~ooura/fft.tgz; 20,092b 40s; (13:01:03) ERROR: Couldn't start the build. Unable to run tests. ```. Please Help",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/59:81,install,installation,81,,https://github.com/google/deepvariant/issues/59,1,['install'],['installation']
Deployability,When I run the pipeline on females I get lots of PASSED variants on chrY. Why is that?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/196:15,pipeline,pipeline,15,,https://github.com/google/deepvariant/issues/196,1,['pipeline'],['pipeline']
Deployability,"When I tried to create a VM with 8 GPUs using this command line:; export IMAGE_FAMILY=""tf-latest-gpu""; export ZONE=""us-west1-a""; export INSTANCE_NAME=""deep""; export INSTANCE_TYPE=""n1-standard-8""; gcloud compute instances create $INSTANCE_NAME \; --zone=$ZONE \; --image-family=$IMAGE_FAMILY \; --image-project=deeplearning-platform-release \; --maintenance-policy=TERMINATE \; --accelerator=""type=nvidia-tesla-p100,count=8"" \; --machine-type=$INSTANCE_TYPE \; --boot-disk-size=200GB \; --metadata=""install-nvidia-driver=True"". I got this error:; ERROR: (gcloud.compute.instances.create) Could not fetch resource:; - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:; ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/136:332,release,release,332,,https://github.com/google/deepvariant/issues/136,2,"['install', 'release']","['install-nvidia-driver', 'release']"
Deployability,"When I used version 1.6.1 for source code compilation, an error related to the numpy library occurred. I suspect this is due to incompatibility with TensorFlow. I tried using other versions of the numpy library, but the issue persisted. ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Misc setup' starting; ========== [Fri 02 Aug 2024 02:20:04 PM CST] Stage 'Update package list' starting; ========== [Fri 02 Aug 2024 02:20:06 PM CST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Fri 02 Aug 2024 02:20:10 PM CST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:403,Install,Install,403,,https://github.com/google/deepvariant/issues/859,4,"['Install', 'Update']","['Install', 'Update']"
Deployability,"When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:; `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**; - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa); - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run google/deepvariant:1.4.0; - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; CPU information from /proc/cpuinfo; product: Common KVM processor; vendor: Intel Corp.; physical id: 2; bus info: cpu@1; width: 64 bits; capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552:428,Install,Installation,428,,https://github.com/google/deepvariant/issues/552,1,['Install'],['Installation']
Deployability,"With the release of version 1.0.0, it is stated that DeepVariant will not support somatic variant calling because the only genotypes supported are hom-alt, het, and hom-ref. . Is there the potential for a deep variant somatic variant caller in the future? ; or ; Could individuals produce somatic variants using a matched-normal approach? ie by calling variants on the germline and tumor(s) and extracting variants found in the tumor only?; Thank you for your time",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/351:9,release,release,9,,https://github.com/google/deepvariant/issues/351,1,['release'],['release']
Deployability,YTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:3903,Install,Install,3903,,https://github.com/google/deepvariant/issues/89,1,['Install'],['Install']
Deployability,_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Pac,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:4153,update,updates,4153,,https://github.com/google/deepvariant/issues/89,1,['update'],['updates']
Deployability,"_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it possible to identify the problem/typo?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:1636,pipeline,pipelines,1636,,https://github.com/google/deepvariant/issues/214,1,['pipeline'],['pipelines']
Deployability,"_postproc.py"", line 154, in _raw_next; not_done = self._cc_iterable.PythonNext(record); RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0; ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```; singularity exec DeepVariant_1.6.1.sif bash; pip install --user google-nucleus; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; 	--num_shards=12; ```. Error:. ```; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>; from tensorflow.python.ea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:4524,install,installing,4524,,https://github.com/google/deepvariant/issues/812,1,['install'],['installing']
Deployability,"_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}; ##bcftools_viewVersion=1.10.2+htslib-1.10.2; ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022; ```. ### Variant line; ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband; X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:..; ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:; Pipeline; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; export SINGULARITY_CACHEDIR=$PWD; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; Case_ID=Case1; FAMILY_ID=$Case_I",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:6826,pipeline,pipeline,6826,,https://github.com/google/deepvariant/issues/518,2,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']"
Deployability,"`javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 293, in _read_values_to_bundles; read_res",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:1361,pipeline,pipeline,1361,,https://github.com/google/deepvariant/issues/91,1,['pipeline'],['pipeline']
Deployability,"a.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-po",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:3077,install,install,3077,,https://github.com/google/deepvariant/issues/252,1,['install'],['install']
Deployability,a/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:4504,update,updates,4504,,https://github.com/google/deepvariant/issues/489,1,['update'],['updates']
Deployability,"ach memory. It is out of memory of my machine. My machine have 376G memory. Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:1236,pipeline,pipeline,1236,,https://github.com/google/deepvariant/issues/91,1,['pipeline'],['pipeline']
Deployability,"ages (1.11.0); Requirement already satisfied: sklearn in /usr/local/lib/python2.7/dist-packages (0.0); Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:17071,upgrade,upgrade,17071,,https://github.com/google/deepvariant/issues/89,1,['upgrade'],['upgrade']
Deployability,"ain(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. Here is the supported CPU instructions of host:. ```sh; # cat /proc/cpuinfo; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 30; model name : Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz; stepping : 5; microcode : 0xa; cpu MHz : 1197.018; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 11; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs; bogomips : 5851.92; clflush size : 64; cache_alignment : 64; address sizes : 36 bits physical, 48 bits virtual; power management:; ```. I also tried create a env & install on host by `conda install -c bioconda deepvariant`, but it pop-up the same error.; And Deepvariant v0.10.0 also have the same error. Please kindly give me some advice about this thank you. Best,; Jerry",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345:4454,install,install,4454,,https://github.com/google/deepvariant/issues/345,2,['install'],['install']
Deployability,"alled; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:2110,install,installed,2110,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability,"also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; ```. Then I try to downgrade intervaltree from 3.0.2 to 2.1.0. . ```; chungtsai_su@seqslab:~/src/deepvariant$ pip show intervaltree; Name: intervaltree; Version: 3.0.2; Summary: Editable interval tree data structure for Python 2 and 3; Home-page: https://github.com/chaimleib/intervaltree; Author: Chaim Leib Halbert, Konstantin Tretyakov; Author-email: chaim.leib.halbert@gmail.com; License: Apache License, Version 2.0; Location: /home/chungtsai_su/.local/lib/python2.7/site-packages; Requires: sortedcontainers; Required-by:; chungtsai_su@seqslab:~/quickstart-output$ pip uninstall intervaltree; Uninstalling intervaltree-3.0.2:; Would remove:; /home/chungtsai_su/.local/lib/python2.7/site-packages/intervaltree-3.0.2.dist-info/*; /home/chungtsai_su/.local/lib/python2.7/site-packages/intervaltree/*; Proceed (y/n)? Y; Successfully uninstalled intervaltree-3.0.2; chungtsai_su@seqslab:~/src/deepvariant$ pip install --user 'intervaltree==2.1.0'; Collecting intervaltree==2.1.0; Requirement already satisfied: sortedcontainers in /home/chungtsai_su/.local/lib/python2.7/site-packages (from intervaltree==2.1.0) (2.1.0); Installing collected packages: intervaltree; Successfully installed intervaltree-2.1.0; ```; Then the problem is solved. ; ```; chungtsai_su@seqslab:~/src/deepvariant$ ./bazel-bin/deepvariant/make_examples --mode calling --ref ""${REF}"" --reads ""${BAM}"" --regions ""chr20:10,000,000-10,010,000"" --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; 2018-12-20 07:17:31.678190: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.678396 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.681643 140029649073920 make_examples.py:1080] Preparing inputs; 2018-12-20 07:17:31.682071: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized S",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:3083,install,install,3083,,https://github.com/google/deepvariant/issues/131,1,['install'],['install']
Deployability,"arallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/refgenome/GCF_003073045.1_ASM307304v1_genomic.fna"" --reads ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --channels ""insert_size"" --gvcf ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/gvcf.tfrecord@2.gz"" --regions ""NC_037590.1:200,000-950,000"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2024-01-05 15:53:39.096475: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.096611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:39.226747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:4480,install,installed,4480,,https://github.com/google/deepvariant/issues/761,1,['install'],['installed']
Deployability,"ate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. ; **Steps to reproduce:**; - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :; ; ```; E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'; 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX; PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M; 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666:2881,pipeline,pipeline,2881,,https://github.com/google/deepvariant/issues/666,1,['pipeline'],['pipeline']
Deployability,"ate_variables(); ; self._bitgen.state = <void *>&self.rng_state; self._bitgen.next_uint64 = &philox_uint64; ^; ------------------------------------------------------------; ; _philox.pyx:195:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_common.pyx; Processing numpy/random/_philox.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/public/home/zhanghl3/miniconda3/envs/deepvariant/lib/python3.10/subprocess.py"", line 369, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/public/home/zhanghl3/miniconda3/envs/deepvariant/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_philox.c', '_philox.pyx']' returned non-zero exit status 1.; Cythonizing sources; Traceback (most recent call last):; File ""/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>; main(); File ""/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main; json_out['return_val'] = ho",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:9246,install,install-,9246,,https://github.com/google/deepvariant/issues/859,1,['install'],['install-']
Deployability,"b/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:03:04 --:--:-- 0; curl: (56)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:18007,upgrade,upgrade,18007,,https://github.com/google/deepvariant/issues/89,1,['upgrade'],['upgrade']
Deployability,binary releases on github,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/8:7,release,releases,7,,https://github.com/google/deepvariant/issues/8,1,['release'],['releases']
Deployability,bioconda installation v0.7.2,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137:9,install,installation,9,,https://github.com/google/deepvariant/issues/137,1,['install'],['installation']
Deployability,"ble; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http:/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1517,install,installed,1517,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability,"buntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; # 12; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; # 13; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; Focal (20.04) LTS - Last update : Sun, 10 Oct 2021 23:59:52 UTC / Revision: 20211010053033+67964fc4b241; # i386 not available; deb http://apt.llvm.org/focal/ llvm-toolchain-focal main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal main; # 12; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; # 13; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main. ```; `llvm-toolchain-bionic-11` was changed today.; ![image](https://user-images.githubusercontent.com/41360525/136817825-71faa887-08bb-49e7-9126-036e6412d90d.png). Any help?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:9680,update,update,9680,,https://github.com/google/deepvariant/issues/489,1,['update'],['update']
Deployability,"buntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1373,install,installed,1373,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability,"cal/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/loc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:17226,upgrade,upgrade,17226,,https://github.com/google/deepvariant/issues/89,1,['upgrade'],['upgrade']
Deployability,"ch4/path.to.mydir/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref c_elegans.PRJNA13758.WS245.genomic.fa \; --reads aln13448198.pbmm2.bam \; --output_vcf aln13448198.pbmm2.dv.vcf.gz \; --num_shards 64; ```. Here's a long snippet of slurm output:; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****; time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]; # this part is likely unimportant. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader; I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs; 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwan",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:1658,install,installed,1658,,https://github.com/google/deepvariant/issues/614,1,['install'],['installed']
Deployability,"ckstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:1136,update,update,1136,,https://github.com/google/deepvariant/issues/223,2,"['install', 'update']","['install', 'update']"
Deployability,"client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:03:04 --:--:-- 0; curl: (56) GnuTLS recv error (-54): Error in the pull function.; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:18177,upgrade,upgrade,18177,,https://github.com/google/deepvariant/issues/89,1,['upgrade'],['upgrade']
Deployability,"client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:3790,patch,patch,3790,,https://github.com/google/deepvariant/issues/469,1,['patch'],['patch']
Deployability,command line pipeline support?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/288:13,pipeline,pipeline,13,,https://github.com/google/deepvariant/issues/288,1,['pipeline'],['pipeline']
Deployability,conda install dv without model PACBIO,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/395:6,install,install,6,,https://github.com/google/deepvariant/issues/395,1,['install'],['install']
Deployability,conda installation problem,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177:6,install,installation,6,,https://github.com/google/deepvariant/issues/177,1,['install'],['installation']
Deployability,conda installed deepvariant libm.so.6: version `GLIBC_2.23' not found,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/391:6,install,installed,6,,https://github.com/google/deepvariant/issues/391,1,['install'],['installed']
Deployability,"d Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; Ã— Preparing metadata (pyproject.toml) did not run successfully.; â”‚ exit code: 1; â•°â”€> [78 lines of output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); performance hint: _common.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition a",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:1943,update,update,1943,,https://github.com/google/deepvariant/issues/859,3,"['install', 'update', 'upgrade']","['install', 'update', 'upgrade']"
Deployability,"d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 888, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 875, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 472, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 464, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 350, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23423423423423443"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 4. Changing to 0.7.2rc gives following error: ; [12/12/2018 13:12:23 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] Job failed with error {...........cutout...; 13:33:48 Stopped running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512"": exit status 1: turn self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:4276,pipeline,pipeline,4276,,https://github.com/google/deepvariant/issues/129,1,['pipeline'],['pipeline']
Deployability,"dd channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_listener= //deepvariant:make_examples //deepvariant:call_variants //deepvariant:postprocess_variants || { \; echo ""Bazel build failed""; \; exit 1; }. # Final image; FROM base AS final. # Set environment variables; ENV VERSION=1.6.0; ENV PYTHON_VERSION=3.8; ENV PATH=""/opt/miniconda/bin:${PATH}"". # Install Python packages; RUN pip install --upgrade pip setuptools wheel --timeout=120 && \; pip install jaxlib jax --timeout=120 --extra-index-url https://storage.googleapis.com/jax-releases/jax_releases.html. # Copy DeepVariant binaries from the builder stage; COPY --from=builder /opt/deepvariant /opt/deepvariant; WORKDIR /opt/deepvariant. # Ensure executable scripts are correctly set up; RUN BASH_HEADER='#!/bin/bash' && \; for script in make_examples call_variants call_variants_slim postprocess_variants vcf_stats_report show_examples runtime_by_region_vis multisample_make_examples labeled_examples_to_vcf make_examples_somatic train run_deepvariant run_deepsomatic; do \; printf ""%s\n%s\n"" ""${BASH_HEADER}"" ""python3 /opt/deepvariant/bin/${script}.zip \""$@\"""" > /opt/deepvariant/bin/${script} && \; chmod +x /opt/deepvariant/bin/${script}; \; done. # Copy licenses and other necessary files; # Ensure these paths and URLs are correct and accessible; # Replace with valid URLs or remove if not needed; ADD https://storage.googleapis.com/deepvariant/models/DeepVariant/1.6.0/savedmodels/deepvariant.hybrid.savedmodel/saved_model.pb /models/; WORKDIR /opt/deepvariant/bin/; COPY --fro",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:2572,release,releases,2572,,https://github.com/google/deepvariant/issues/871,1,['release'],['releases']
Deployability,deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/598:21,install,installed,21,,https://github.com/google/deepvariant/issues/598,1,['install'],['installed']
Deployability,"deepvariant-1; OUTPUT_BUCKET=gs://mbh-deepvariant-ouput-vcf; STAGING_FOLDER_NAME=staging_folder1; OUTPUT_FILE_NAME=HR090610illuminagr38DeepVariant.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Con",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/225:1447,pipeline,pipeline,1447,,https://github.com/google/deepvariant/issues/225,1,['pipeline'],['pipeline']
Deployability,"ds=1 ; ```; I faced the error:; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. 2019-09-11 14:44:44.030589: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m2.456s; user	0m1.443s; sys	0m1.926s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. I googled for solution also asked you before and found may be compiling from source code would works for me. I FAILED TO DO THIS.; Is still any chance to use install and work with DeepVariant?. Please consider that a number of potential users -like me- are biologist with limited knowledge of informatics. ; Thanks in advance.; Hamid",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/217:2561,install,install,2561,,https://github.com/google/deepvariant/issues/217,1,['install'],['install']
Deployability,dv_* file syntax error from bioconda installed 1.4.0 version DeepVariant,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/627:37,install,installed,37,,https://github.com/google/deepvariant/issues/627,1,['install'],['installed']
Deployability,"e are some problems while running the ./build-prereq.sh:; ```; + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0; Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0); Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0); Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objects: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:1049,install,installed,1049,,https://github.com/google/deepvariant/issues/739,1,['install'],['installed']
Deployability,"e creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled""; ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):; ```; docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant; See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md.; ```; I do have the `gcloud alpha genomics pipelines` example working, so this isnâ€™t absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:7854,pipeline,pipelines,7854,,https://github.com/google/deepvariant/issues/171,1,['pipeline'],['pipelines']
Deployability,"e following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong?; My cmd:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref ${fasta} \; --reads ${Input.bam} \; --output_vcf output/output.vcf.gz \; --output_gvcf output/output.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir output/intermediate_results_dir \; --regions chr20 \; --customized_model model/weights-51-0.995354.ckpt; ```. Error message:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco; rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started.; I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]; I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False; /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725:1072,release,release,1072,,https://github.com/google/deepvariant/issues/725,1,['release'],['release']
Deployability,"e term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant?. **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples?. Thank you very much for your help!. Sincerely,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165:1332,pipeline,pipelines,1332,,https://github.com/google/deepvariant/issues/165,1,['pipeline'],['pipelines']
Deployability,"e/ref/GIAB/HG005/hs37d5/ \; --contain \; /paedyl01/disk1/louisshe/tools/DeepVariant/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Ker. For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0619 14:57:56.059498 47403021002560 call_variants.py:563] Total 1 writing processes started.; I0619 14:57:56.063244 47403021002560 dv_utils.py:370] From /tmp/make_examples.tfrecord-00000-of-00010.gz.example_info; I0619 14:57:56.063441 47403021002560 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0619 14:57:56.063909 47403021002560 call_variants.py:592] Use saved model: True; 2024-06-19 14:57:57.916727: F tensorflow/tsl/platform/env.cc:391] Check failed: -1 != path_length (-1 vs. -1); Fatal Python error: Aborted. Current thread 0x00002b1ce03a6740 (most recent call first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/pytho",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:2099,release,release,2099,,https://github.com/google/deepvariant/issues/833,1,['release'],['release']
Deployability,ease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archi,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:4594,update,updates,4594,,https://github.com/google/deepvariant/issues/489,1,['update'],['updates']
Deployability,"ec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-too",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1793,install,installed,1793,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability,"eepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0; ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```; singularity exec DeepVariant_1.6.1.sif bash; pip install --user google-nucleus; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; 	--num_shards=12; ```. Error:. ```; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:4653,install,install,4653,,https://github.com/google/deepvariant/issues/812,1,['install'],['install']
Deployability,"epVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; Traceback (most recent call last):; File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call; with Popen(*popenargs, **kwargs) as p:; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__; restore_signals, start_new_session); File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'; ```. I noticed inside the bazel .zip files the python binary is hard-coded:; /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```; micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml; ``` ; With environment file:; ```; name: dv; channels:; - conda-forge; - bioconda; - defaults; dependencies:; - python=3.6; - deepvariant=1.5.0; ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664:1962,install,installed,1962,,https://github.com/google/deepvariant/issues/664,2,['install'],"['install', 'installed']"
Deployability,"epends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::56",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1983,install,installed,1983,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability,"er version**: Docker version 27.1.1, build 6312585; **Bazel Version**: 7.3.1; **MacBook Model**: M1 chip (ARM64 architecture). **Error**: ; ![IMG_3267](https://github.com/user-attachments/assets/11e28824-b941-42cc-9d33-7e9155a03543); ![IMG_3268](https://github.com/user-attachments/assets/4e923de6-99d5-43ee-80c6-29b32504527d). **My Dockerfilee code**:. ```; # Base image suitable for ARM64 architecture; FROM arm64v8/ubuntu:latest AS base. # Prevent interactive prompts; ENV DEBIAN_FRONTEND=noninteractive. # Install necessary packages; RUN apt-get update && \; apt-get install -y \; git \; curl \; unzip \; wget \; openjdk-17-jdk \; build-essential \; bzip2 \; python3-pip \; parallel && \; apt-get clean && \; rm -rf /var/lib/apt/lists/*. # Install Bazel (adjust version as needed); RUN curl -LO ""https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_listener= //deepvariant:make_examples //deepvariant:call_variants //deepvariant:postprocess_variants || { \; echo ""Bazel build failed""; \; exit 1; }. # Final image; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:1237,Install,Install,1237,,https://github.com/google/deepvariant/issues/871,1,['Install'],['Install']
Deployability,erse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; D,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:5978,upgrade,upgraded,5978,,https://github.com/google/deepvariant/issues/489,1,['upgrade'],['upgraded']
Deployability,"ervice,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Sun Jul 26 15:27:06 UTC 2020; conda 4.9.2; Python 3.7.6. i really hope you can help me.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/391:1246,install,install,1246,,https://github.com/google/deepvariant/issues/391,2,"['install', 'update']","['install', 'update']"
Deployability,"es can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6958,install,installed,6958,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability,"etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/l",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:12387,Install,Install,12387,,https://github.com/google/deepvariant/issues/89,1,['Install'],['Install']
Deployability,"examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512""; 13:33:48 Started running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Stopped running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Execution failed: action 4: unexpected exit status 1 was not ignored; 13:33:51 Worker released; ""run"": operation ""projects/ms-deepvariant/operations/234234234234"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; [12/12/2018 13:33:54 ERROR gcp_de",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:7086,pipeline,pipelines,7086,,https://github.com/google/deepvariant/issues/129,1,['pipeline'],['pipelines']
Deployability,"f my machine. My machine have 376G memory. Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apac",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:1277,pipeline,pipeline,1277,,https://github.com/google/deepvariant/issues/91,1,['pipeline'],['pipeline']
Deployability,"f you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archiv",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:7218,install,installed,7218,,https://github.com/google/deepvariant/issues/489,2,['install'],['installed']
Deployability,get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; De,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1113,install,installed,1113,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability,"gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T18:35:24-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: sta",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:9055,release,releases,9055,,https://github.com/google/deepvariant/issues/292,1,['release'],['releases']
Deployability,"h the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ; Haley . Here is the error traceback: ; `Traceback (most recent call last):; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:1152,pipeline,pipeline,1152,,https://github.com/google/deepvariant/issues/793,1,['pipeline'],['pipeline']
Deployability,"hain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6822,install,installed,6822,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability,"he command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/refgenome/GCF_003073045.1_ASM307304v1_genomic.fna"" --infile ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/call_variants_output.tfrecord.gz"" --outfile ""./outputgpu/output.vcf.gz"" --cpus ""2"" --gvcf_outfile ""./outputgpu/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/gvcf.tfrecord@2.gz"". 2024-01-05 16:00:59.661436: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 16:00:59.661893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 16:01:06.236791: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 16:01:06.304423 140416700553024 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:06.676597 140416700553024 postprocess_variants.py:1313] CVO sorting took 0.006136405467987061 minutes; I0105 16:01:06.677379 140416700553024 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0105 16:01:06.677495 140416700553024 postprocess_variants.py:1318] Using 2 CPUs for parallelization of variant transformation.; I0105 16:01:06.808352 140416700553024 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:08.209710 140416700553024 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:16136,install,installed,16136,,https://github.com/google/deepvariant/issues/761,1,['install'],['installed']
Deployability,"hg38bundle/Homo_sapiens_assembly38.fasta --reads /scratch/XXXX/ONT_WGS/HH/FL9-1/FL9-1.chr10.bam --output_vcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.vcf.gz --output_gvcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.g.vcf.gz --num_shards 64 --logging_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/ --intermediate_results_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results. - Error trace: (if applicable); ; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2p_bcqtz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_2p_bcqtz/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 43, in <module>; import numpy as np; File ""/usr/local/lib/python3.8/dist-packages/numpy/__init__.py"", line 152, in <module>; from . import random; File ""/usr/local/lib/python3.8/dist-packages/numpy/random/__init__.py"", line 180, in <module>; from . import _pickle; File ""/usr/local/lib/python3.8/dist-packages/numpy/random/_pickle.py"", line 1, in <module>; from .mtrand import RandomState; File ""mtrand.pyx"", line 1, in init numpy.random.mtrand; ImportError: /usr/local/lib/python3.8/dist-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so: failed to map segment from shared object; Tra",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/854:1767,install,installed,1767,,https://github.com/google/deepvariant/issues/854,1,['install'],['installed']
Deployability,"how to debug this would be much appreciated. ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116:1169,pipeline,pipeline,1169,,https://github.com/google/deepvariant/issues/116,1,['pipeline'],['pipeline']
Deployability,"http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6616,install,installed,6616,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability,"iants --outfile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started.; I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False; Model: ""inceptionv3""; _______________________",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:14038,release,release,14038,,https://github.com/google/deepvariant/issues/774,1,['release'],['release']
Deployability,"ibgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18--",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1860,install,installable,1860,,https://github.com/google/deepvariant/issues/489,1,['install'],['installable']
Deployability,"id filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: num",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:13289,Install,Install,13289,,https://github.com/google/deepvariant/issues/89,1,['Install'],['Install']
Deployability,"in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory â€˜/mnt/cdw-genomeâ€™: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:5023,upgrade,upgraded,5023,,https://github.com/google/deepvariant/issues/171,3,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability,"int; saver.restore(sess, checkpoint_filename_with_path); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard//model.ckpt; 13:33:48 Unexpected exit status 1 while running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512""; 13:33:48 Started running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Stopped running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Execution failed: action 4: unexpected exit status 1 was not ignored; 13:33:51 Worker released; ""run"": operation ""projects/ms-deepvariant/operations/234234234234"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:6867,release,released,6867,,https://github.com/google/deepvariant/issues/129,2,"['pipeline', 'release']","['pipeline', 'released']"
Deployability,"ioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:3103,install,install,3103,,https://github.com/google/deepvariant/issues/252,1,['install'],['install']
Deployability,"iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:4692,patch,patch,4692,,https://github.com/google/deepvariant/issues/469,9,"['install', 'patch', 'upgrade']","['install', 'patch', 'upgrade']"
Deployability,"ipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116:1202,pipeline,pipelines,1202,,https://github.com/google/deepvariant/issues/116,1,['pipeline'],['pipelines']
Deployability,"irf/.tmp/tmp3vf8mpw9/call_variants_output.tfrecord.gz"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs"". 2024-01-05 15:55:31.140705: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:55:31.140953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-01-05 15:55:38.664328: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:55:38.709242 140372734228288 call_variants.py:471] Total 1 writing processes started.; I0105 15:55:38.765925 140372734228288 dv_utils.py:365] From /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0105 15:55:38.766286 140372734228288 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0105 15:55:38.768594 140372734228288 call_variants.py:510] Use saved model: Tr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:13367,release,release,13367,,https://github.com/google/deepvariant/issues/761,1,['release'],['release']
Deployability,"ist.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Buildin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:11474,Install,Install,11474,,https://github.com/google/deepvariant/issues/89,1,['Install'],['Install']
Deployability,"ittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Here is the complete error msg:; #############################################; **Any additional context:**; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_exampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:2821,install,installed,2821,,https://github.com/google/deepvariant/issues/678,1,['install'],['installed']
Deployability,"k start document:. merge_overlaps() got an unexpected keyword argument 'strict'. Any advice as to how I can resolve the issue is greatly appreciated. ```; This is the context of the error.; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/data/hs37d5.fa.gz"" --reads ""/input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" `--gvcf` ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""20"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I1220 08:40:22.262234 46912496321664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1220 08:40:22.268675 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.272100 46912496321664 make_examples.py:1324] Preparing inputs; I1220 08:40:22.280786 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.292714 46912496321664 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_UJ59Z1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/255:1072,install,installed,1072,,https://github.com/google/deepvariant/issues/255,1,['install'],['installed']
Deployability,"k.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:42:05 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Tra",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:8256,upgrade,upgraded,8256,,https://github.com/google/deepvariant/issues/89,3,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability,"kages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the HPC platform. The non-GPU version works just fine.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243:1971,install,installed,1971,,https://github.com/google/deepvariant/issues/243,3,['install'],['installed']
Deployability,"ke_examples.py"", line 1074, in make_examples_runner; resource_monitor = resources.ResourceMonitor().start(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 59, in __init__; self.metrics_pb = self._initial_metrics_protobuf(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 72, in _initial_metrics_protobuf; cpu_frequency_mhz=_get_cpu_frequency(),; File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 158, in _get_cpu_frequency; freq = psutil.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/__init__.py"", line 1853, in cpu_freq; ret = _psplatform.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); NotImplementedError: can't find current frequency file; parallel: This job failed:; /opt/conda/envs/nf-core-deepvariant-1.0/bin/python /opt/conda/envs/nf-core-deepvariant-1.0/share/deepvariant-0.7.0-0/binaries/DeepVariant/0.7.0/DeepVariant-0.7.0+cl-208818123/make_examples.zip --mode calling --ref PlasmoDB-41_Pfalciparum3D7_Genome.fasta.gz --reads ISO_349.bam --regions PlasmoDB-41_Pfalciparum3D7_Genome_ISO_349.per-base.bed.gz --examples ISO_349_shardedExamples/ISO_349.bam.tfrecord@16.gz --task 10; ```. This is using the Nextflow wrapper script [nf-core/deepvariant](https://github.com/nf-core/deepvariant). The pipeline works with other input data. So it seems unlikely that it is a problem with installation or parameters. DeepVariant was installed using Conda & is using v0.7.0. . Any ideas what the problem is? . Could it be a problem to do with the input data? Perhaps the organism being used? What is the frequency file which it is referring to?. Any help would be much appreciated. Many thanks in advance,; Phil",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/191:4164,pipeline,pipeline,4164,,https://github.com/google/deepvariant/issues/191,3,"['install', 'pipeline']","['installation', 'installed', 'pipeline']"
Deployability,"l	13m21.231s; user	118m36.634s; sys	25m56.983s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"". 2024-02-18 00:47:52.195457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:47:52.196245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-18 00:48:10.043945: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:48:10.133844 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001; I0218 00:48:12.163552 139719065552704 postprocess_variants.py:1313] CVO sorting took 0.03374857902526855 minutes; I0218 00:48:12.163919 139719065552704 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0218 00:48:12.163960 139719065552704 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation.; I0218 00:48:12.684920 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001; I0218 00:48:18.996037 139719065552704 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.06664579312006633 minutes; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:18456,install,installed,18456,,https://github.com/google/deepvariant/issues/774,1,['install'],['installed']
Deployability,"l/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objects: 100% (700/700), done.; remote: Compressing objects: 100% (111/111), done.; remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146; Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done.; Resolving deltas: 100% (4683/4683), done.; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:1394,install,installed,1394,,https://github.com/google/deepvariant/issues/739,1,['install'],['installed']
Deployability,"la/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 8308a7bb3067; 2024-07-03 18:27:31.862563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 8308a7bb3067; 2024-07-03 18:27:31.862607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.6; 2024-07-03 18:27:31.862621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 555.42.6; 2024-07-03 18:27:31.862626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 555.42.6; (then repeated with every call). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; L40S card is CUDA CC = 8.9; supported since CUDA >= 11.8 currently 12.4 and 12.5; https://developer.nvidia.com/cuda-11-8-0-download-archive - installed, still doesn't work with containerized 11.3.1. tensorRT is used; https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html ; currently 10.1 ; version 8 in CUDA 11.8; python >= 3.8. lspci | grep -i nvidia; 0a:00.0 3D controller: NVIDIA Corporation AD102GL [L40S] (rev a1); ae:00.0 3D controller: NVIDIA Corporation AD102GL [L40S] (rev a1). nvidia-container-cli is installed (supersedes nvidia-docker - https://github.com/NVIDIA/nvidia-docker )",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:4675,install,installed,4675,,https://github.com/google/deepvariant/issues/844,2,['install'],['installed']
Deployability,"laptop (M3 macbook). #### PackagesNotFoundError error. The first error I get is -. ```; 1.247 Platform: linux-aarch64; 1.247 Collecting package metadata (repodata.json): ...working... done; 6.190 Solving environment: ...working... failed; 6.260; 6.260 PackagesNotFoundError: The following packages are not available from current channels:; 6.260; 6.260 - bioconda::samtools==1.15; 6.260 - bioconda::bcftools==1.15; 6.260; ```. I resolved this error by removing the version numbers. i.e., removed the `==1.15` from both the lines. #### Error in the build-prerunreq.sh script. Once, I cross the previous error, I get this error -. ```; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:1425,Install,Install,1425,,https://github.com/google/deepvariant/issues/902,1,['Install'],['Install']
Deployability,le example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.geno,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:8472,release,releases,8472,,https://github.com/google/deepvariant/issues/292,1,['release'],['releases']
Deployability,"lease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; C",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6669,install,installable,6669,,https://github.com/google/deepvariant/issues/489,2,['install'],"['installable', 'installed']"
Deployability,"les_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; I0828 10:40:42.589054 140318776715072 train.py:92] Running with debug=False; I0828 10:40:42.589343 140318776715072 train.py:100] Use TPU at local; I0828 10:40:42.589422 140318776715072 train.py:103] experiment_dir: /home/training_outs/epoch1/; WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; W0828 10:40:42.596594 140318776715072 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; IN",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2814,release,release,2814,,https://github.com/google/deepvariant/issues/876,1,['release'],['release']
Deployability,"lf.rng_state.key[i] = val[i]; self.rng_state.pos = i; ; self._bitgen.state = &self.rng_state; self._bitgen.next_uint64 = &mt19937_uint64; ^; ------------------------------------------------------------; ; _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_mt19937.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1.; Cythonizing sources; Traceback (most recent call last):; File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>; main(); File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main; json_out['return_val'] = hook(**hook_input['kwargs']); File ""/root/.local/lib/python3.9/site",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727:1876,install,install-,1876,,https://github.com/google/deepvariant/issues/727,1,['install'],['install-']
Deployability,"libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.096611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:39.226747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.226871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:49.941043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:53:49.987410 140173517489984 genomics_reader.py:222] Reading /public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam with NativeSamReader; W0105 15:53:49.988560 140173517489984 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I0105 15:53:50.021419 140173517489984 make_examples_core.py:301] Task 0/2: Preparing inputs; I0105 15:53:50.036767 140173517489984 genomics_reader.py:222] Reading /public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam with NativeSamReader; I0105 15:53:50.054040 140173517489984 make_examples_core.py:301] Task 0/2: Common contigs are ['NC_037",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:5749,install,installed,5749,,https://github.com/google/deepvariant/issues/761,1,['install'],['installed']
Deployability,"libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.108506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.006781: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.007601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.110201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; ...; 2024-02-17 23:33:25.887517: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:33:25.933275 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:25.939588 140533724936000 make_examples_core.py:301] Task 15/16: Preparing inputs; I0217 23:33:25.967685 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:26.024591 140533724936000 make_example",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:7167,install,installed,7167,,https://github.com/google/deepvariant/issues/774,1,['install'],['installed']
Deployability,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:4139,install,installed,4139,,https://github.com/google/deepvariant/issues/252,3,['install'],"['install', 'installed']"
Deployability,"ltiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:11935,install,installed,11935,,https://github.com/google/deepvariant/issues/89,1,['install'],['installed']
Deployability,"markDup.bam --examples /home/paul/exome-case-study/output/HG002.examples.tfrecord@1.gz --regions /home/paul/exome-case-study/input/data/refseq.coding_exons.b37.extended50.bed --task 0; Illegal instruction (core dumped); $; ```; After digging a bit deeper, I noticed that loading the `pileup_image_native` module was causing this issue. I was curious and looked at the assembly instructions:. ```; $ gdb -ex r --args python -c ""from deepvariant.python import pileup_image_native""; Program received signal SIGILL, Illegal instruction.; 0x00007ffff5d308b4 in google::protobuf::DescriptorPool::Tables::Tables() (); from /home/paul/make-examples/runfiles/genomics/deepvariant/python/../../_solib_k8/libexternal_Sprotobuf_Uarchive_Slibprotobuf.so; (gdb) disassemble $pc,$pc+32; Dump of assembler code from 0x7ffff5d308b4 to 0x7ffff5d308d4:; => 0x00007ffff5d308b4 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+676>: vpxor %xmm0,%xmm0,%xmm0; 0x00007ffff5d308b8 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+680>: lea 0x1b0(%rbx),%rax; 0x00007ffff5d308bf <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+687>: movl $0x0,0x1b0(%rbx); 0x00007ffff5d308c9 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+697>: movq $0x0,0x1b8(%rbx); End of assembler dump.; (gdb); ```. I noticed the `vpxor` instruction, which made me wonder if my CPU is enabled for AVX, so I proceeded as follows:. ```; $ grep flags /proc/cpuinfo; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 x2apic popcnt aes hypervisor lahf_lm; $; ```. This confirmed for me that I don't have AVX support. So it would be great for the examples that will drive usage and be used by many users to learn from, if the provided libraries are compiled with the bare-minimum of CPU qualities. I think it'll make it a bit easier for many users to adopt this nice pipeline. Thanks,; Paul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21:3775,pipeline,pipeline,3775,,https://github.com/google/deepvariant/issues/21,1,['pipeline'],['pipeline']
Deployability,"mator.py"", line 469, in evaluate; name=name); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval; hooks.extend(self._convert_eval_steps_to_hooks(steps)); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks; raise ValueError('Must specify steps > 0, given: {}'.format(steps)); ValueError: Must specify steps > 0, given: 0. real	0m58.116s; user	0m1.590s; sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant; ADD ./bin /home/bin; ADD ./models /home/models; ADD ./oss_clif.ubuntu-16.latest.tgz /; WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh; RUN apt-get -y install python2.7; RUN apt-get -y install python-dev python-pip; RUN apt-get -y install parallel; RUN python2 -m pip install --upgrade --force-reinstall pip; RUN python2 -m pip install apache-beam; RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0; I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None; W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:5860,install,install,5860,,https://github.com/google/deepvariant/issues/172,6,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"mbly=b37>; ##contig=<ID=chr8,length=146364022,assembly=b37>; ##contig=<ID=chr9,length=141213431,assembly=b37>; ##contig=<ID=chr10,length=135534747,assembly=b37>; ##contig=<ID=chr11,length=135006516,assembly=b37>; ##contig=<ID=chr12,length=133851895,assembly=b37>; ##contig=<ID=chr13,length=115169878,assembly=b37>; ##contig=<ID=chr14,length=107349540,assembly=b37>; ##contig=<ID=chr15,length=102531392,assembly=b37>; ##contig=<ID=chr16,length=90354753,assembly=b37>; ##contig=<ID=chr17,length=81195210,assembly=b37>; ##contig=<ID=chr18,length=78077248,assembly=b37>; ##contig=<ID=chr19,length=59128983,assembly=b37>; ##contig=<ID=chr20,length=63025520,assembly=b37>; ##contig=<ID=chr21,length=48129895,assembly=b37>; ##contig=<ID=chr22,length=51304566,assembly=b37>; ##contig=<ID=chrX,length=155270560,assembly=b37>; ##contig=<ID=chrY,length=59373566,assembly=b37>; ##contig=<ID=chrM,length=16569,assembly=b37>; ##fileDate=20160329; ##reference=human_g1k_v37.fasta; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	INTEGRATION; chr20	10000117	.	C	T	50	PASS	platforms=3;platformnames=Illumina,CG,Solid;datasets=3;datasetnames=HiSeqPE300x,CGnormal,SolidSE75bp;callsets=4;callsetnames=HiSeqPE300xfreebayes,HiSeqPE300xGATK,CGnormal,SolidSE75GATKHC;datasetsmissingcall=IonExome,SolidPE50x50bp;lowcov=CS_IonExomeTVC_lowcov,CS_SolidPE50x50GATKHC_lowcov,CS_SolidSE75GATKHC_lowcov;filt=CS_HiSeqPE300xGATK_filt	GT:PS:DP:GQ	0/1:.:706:878; chr20	10000211	.	C	T	50	PASS	platforms=2;platformnames=Illumina,CG;datasets=2;datasetnames=HiSeqPE300x,CGnormal;callsets=3;callsetnames=HiSeqPE300xfreebayes,HiSeqPE300xGATK,CGnormal;datasetsmissingcall=IonExome,SolidPE50x50bp,SolidSE75bp;lowcov=CS_IonExomeTVC_lowcov,CS_SolidPE50x50GATKHC_lowcov,CS_SolidSE75GATKHC_lowcov;filt=CS_SolidPE50x50GATKHC_filt	GT:PS:DP:GQ	0/1:.:695:984; chr20	10000439	.	T	G	50	PASS	platforms=3;platformnames=Illumina,CG,Solid;datasets=4;datasetnames=HiSeqPE300x,CGnormal,SolidPE50x50bp,SolidSE75bp;callsets=5;callsetnames=HiSeqPE300xfreebayes,HiSe",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/239:28848,INTEGRAT,INTEGRATION,28848,,https://github.com/google/deepvariant/issues/239,1,['INTEGRAT'],['INTEGRATION']
Deployability,"med 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>; import apache_beam as beam; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>; from apache_beam import coders; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>; from apache_beam.coders.coders import *; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>; from apache_beam.coders import coder_impl; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>; import numpy as np; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>; from . import core; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>; raise ImportError(msg); ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the numpy C-extensions failed. This error can happen for; many reasons, often due to issues with your setup or how NumPy was; installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3""; * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect.; Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:4088,install,installed,4088,,https://github.com/google/deepvariant/issues/793,1,['install'],['installed']
Deployability,"mmand on Google Cloud). **1b)** I realize that it would take some time (and Iâ€™m not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)?. Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that?. **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove an",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:4102,install,installation,4102,,https://github.com/google/deepvariant/issues/171,1,['install'],['installation']
Deployability,"n to raise exceptions.; performance hint: _common.pyx:1002:32: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; ; Error compiling Cython file:; ------------------------------------------------------------; ...; self.rng_state.ctr.v[i] = counter[i]; ; self._reset_state_variables(); ; self._bitgen.state = <void *>&self.rng_state; self._bitgen.next_uint64 = &philox_uint64; ^; ------------------------------------------------------------; ; _philox.pyx:195:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_common.pyx; Processing numpy/random/_philox.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/public/home/zhanghl3/miniconda3/envs/deepvariant/lib/python3.10/subprocess.py"", line 369, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/public/home/zhanghl3/miniconda3/envs/deepvariant/bin/python3', '-m', 'cython', '-3', '--fa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:8801,install,install-,8801,,https://github.com/google/deepvariant/issues/859,1,['install'],['install-']
Deployability,"n3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; Ã— Preparing metadata (pyproject.toml) did not run successfully.; â”‚ exit code: 1; â•°â”€> [78 lines of output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); performance hint: _common.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Dec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:1888,release,release,1888,,https://github.com/google/deepvariant/issues/859,1,['release'],['release']
Deployability,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432); ```; File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants; ie_estimator = OpenVINOEstimator(; File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__; freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb); File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run; ```; python -c 'from tensorflow.python.tools import optimize_for_inference_lib'; ```. The real issue is openvino is not installed ; ```; python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; ModuleNotFoundError: No module named 'openvino'; ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/541:1282,install,installed,1282,,https://github.com/google/deepvariant/issues/541,2,"['install', 'update']","['installed', 'updates']"
Deployability,"ndencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - &",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1718,install,installable,1718,,https://github.com/google/deepvariant/issues/489,1,['install'],['installable']
Deployability,"nds: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[====================================================================",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:2238,install,installed,2238,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability,"ne 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:3734,patch,patching,3734,,https://github.com/google/deepvariant/issues/469,1,['patch'],['patching']
Deployability,"nfo to /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00013-of-00016.gz.example_info.json; I0203 17:23:09.199875 136895166957376 make_examples_core.py:2958] example_shape = None; I0203 17:23:09.200180 136895166957376 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 7, 9, 10]; I0203 17:23:09.201941 136895166957376 make_examples_core.py:301] Task 13/16: Found 0 candidate variants; I0203 17:23:09.202048 136895166957376 make_examples_core.py:301] Task 13/16: Created 0 examples. real 112m20.375s; user 1760m59.767s; sys 11m47.541s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/cromwell_root/tmp.cd83af44/tmpuzrx3yrs/call_variants_output.tfrecord.gz"" --examples ""/cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0203 17:23:14.218397 132068663560000 call_variants.py:471] Total 1 writing processes started.; W0203 17:23:14.224790 132068663560000 call_variants.py:482] Unable to read any records from /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord@16.gz. Output will contain zero records.; I0203 17:23:14.225926 132068663560000 call_variants.py:623] Complete: call_variants.; ```. And then the program hangs there for 10+ hours (UTC time when I'm reporting is Feb. 04, 04:05, and the program still appears running). . We've observed this for both ONT and HiFi data on multiple samples, further suggesting this isn't a data issue. Thanks!; Steve",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/769:3390,release,release,3390,,https://github.com/google/deepvariant/issues/769,1,['release'],['release']
Deployability,"ng instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```; (base) âœ” /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|â€¦5] ; 20:46 $ samtools flagstat GFX.bam ; ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads); 881297 + 0 primary; 0 + 0 secondary; 59254 + 0 supplementary; 0 + 0 duplicates; 0 + 0 primary duplicates; 940551 + 0 mapped (100.00% : N/A); 881297 + 0 primary mapped (100.00% : N/A); 0 + 0 paired in sequencing; 0 + 0 read1; 0 + 0 read2; 0 + 0 properly paired (N/A : N/A); 0 + 0 with itself and mate mapped; 0 + 0 singletons (N/A : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. ; **Steps to reproduce:**; - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :; ; ```; E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'; 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; 2023-06-20 22:48:40.27288",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666:2189,install,install,2189,,https://github.com/google/deepvariant/issues/666,1,['install'],['install']
Deployability,"ng objects: 100% (111/111), done.; remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146; Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done.; Resolving deltas: 100% (4683/4683), done.; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:3042,INSTALL,INSTALL,3042,,https://github.com/google/deepvariant/issues/739,1,['INSTALL'],['INSTALL']
Deployability,"ng variants (and writing to temporary file) took 0.06664579312006633 minutes; I0218 00:48:39.012242 139719065552704 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.33359973033269247 minutes. real	0m59.941s; user	0m58.218s; sys	0m5.086s. ***** Running the command:*****; time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --outfile_base ""output_apptainer_gpu/HG001.apptainer.gpu.output"". 2024-02-18 00:48:50.006549: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:48:50.008250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-18 00:48:57.417490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:48:57.421117 139673283618624 genomics_reader.py:222] Reading output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz with NativeVcfReader. real	0m23.982s; user	0m12.056s; sys	0m2.006s. ```. ----------------------------------------------------------------------------------; ----------------------------------------------------------------------------------. My system is Ubuntu 22.04. I have two GPUs. . **nvidia-smi** ; ``` ; Sat Feb 17 23:40:49 2024 ; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 525.147.05 Driver Version: 525.147.05 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf P",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:20378,install,installed,20378,,https://github.com/google/deepvariant/issues/774,1,['install'],['installed']
Deployability,"nit: proxychains-ng 4.16; ; Error compiling Cython file:; ------------------------------------------------------------; ...; for i in range(1, RK_STATE_LEN):; self.rng_state.key[i] = val[i]; self.rng_state.pos = i; ; self._bitgen.state = &self.rng_state; self._bitgen.next_uint64 = &mt19937_uint64; ^; ------------------------------------------------------------; ; _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_mt19937.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1.; Cythonizing sources; Traceback (most recent call last):; File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>; main(); File ""/root/.local/lib/python3.9/site-packages/pip/_ve",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727:1695,install,install-,1695,,https://github.com/google/deepvariant/issues/727,1,['install'],['install-']
Deployability,"nment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_listener= //deepvariant:make_examples //deepvariant:call_variants //deepvariant:postprocess_variants || { \; echo ""Bazel build failed""; \; exit 1; }. # Final image; FROM base AS final. # Set environment variables; ENV VERSION=1.6.0; ENV PYTHON_VERSION=3.8; ENV PATH=""/opt/miniconda/bin:${PATH}"". # Install Python packages; RUN pip install --upgrade pip setuptools wheel --timeout=120 && \; pip install jaxlib jax --timeout=120 --extra-index-url https://storage.googleapis.com/jax-releases/jax_releases.html. # Copy DeepVariant binaries from the builder stage; COPY --from=builder /opt/deepvariant /opt/deepvariant; WORKDIR /opt/deepvariant. # Ensure executable scripts are correctly set up; RUN BASH_HEADER='#!/bin/bash' && \; for script in make_examples call_variants call_variants_slim postprocess_variants vcf_stats_report show_examples runtime_by_region_vis multisample_make_examples labeled_examples_to_vcf make_examples_somatic train run_deepvariant run_deepsomatic; do \; printf ""%s\n%s\n"" ""${BASH_HEADER}"" ""python3 /opt/deepvariant/bin/${script}.zip \""$@\"""" > /opt/deepvariant/bin/${script} && \; chmod +x /opt/deepvariant/bin/${script}; \; done. # Copy licenses and other necessary files; # Ensure these paths and URLs are correct and accessible; # Replace with valid URLs or remove if not needed; ADD https://storage.googleapis.com/deepvariant/models/DeepVariant/1.6.0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:2390,Install,Install,2390,,https://github.com/google/deepvariant/issues/871,4,"['Install', 'install', 'upgrade']","['Install', 'install', 'upgrade']"
Deployability,"nt/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.). A variant with VAF value 1 is called as heterozygous. The IGV visualisation of the .bam file shows that it should clearly be a homozygous variant. ![igv_panel_chr2_24146804](https://user-images.githubusercontent.com/84016709/204764192-9bd69aa6-7f23-4490-9391-7af95e909e3f.png). Here the line from .vcf file:; ```; chr2	24146804	.	C	T	29.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:3:162:0,162:1:26,0,0; ```; .gvcf file:; ```; chr2	24146804	.	C	T,<*>	29.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:3:162:0,162,0:1,0:26,0,0,990,990,990; ```. We also asked our collaborators to run the same sample and in their results, a homozygous variant is called:; ```; chr2	24146804	.	C	T	30.8	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:5:161:0,161:1:28,2,0; ```; ```; chr2	24146804	.	C	T,<*>	30.8	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:5:161:0,161,0:1,0:28,2,0,990,990,990; ```. What could cause this discrepancy, if the DeepVariant versions and commands are the same?. **Setup**; - Operating system: Ubuntu16.04; - DeepVariant version: 1.2.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); NovaSeq 6000 using Twist Comprehensive Exome with mtDNA add-in, GRCh38. **Steps to reproduce:**; - Command:; ```; docker run \; -v ${MOUNT_DIR}:${MOUNT_DIR} \; google/deepvariant:1.2.0-rc0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""${REFERENCE}"" \; --reads=""${INPUT}"" \; --regions=""${CAPTURE_KIT}"" \; --output_vcf=${OUTPUT_VCF} \; --output_gvcf=${OUTPUT_GVCF} \; --num_shards=64 \; --postprocess_variants_extra_args=""only_keep_pass=true""; ```; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; No.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/592:1165,Install,Installation,1165,,https://github.com/google/deepvariant/issues/592,1,['Install'],['Installation']
Deployability,"nt/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 806 in realign_reads; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1881 in realign_reads; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1908 in <listcomp>; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1907 in realign_reads_per_sample_multisample; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1709 in process; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref dnaref/genome.fa --reads SAMN02990337.star.bam --examples /tmp/tmp6yy2ufd4/make_examples.tfrecord@16.gz --channels insert_size --regions enough.merge.bed --task 4. **Setup**; - Operating system: Google Docker container; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: RNASeq; ; **Steps to reproduce:**; - Command: /opt/deepvariant/bin/make_examples --mode calling --ref dnaref/genome.fa --reads SAMN02990337.star.bam --examples /tmp/tmp6yy2ufd4/make_examples.tfrecord@16.gz --channels insert_size --regions enough.merge.bed --task 4; - Error trace: (if applicable)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/752:2221,Install,Installation,2221,,https://github.com/google/deepvariant/issues/752,1,['Install'],['Installation']
Deployability,"ntime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing; # value in the environment (allowing command line control of the build),; # defaulting to 0 (CPU only build).; export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file; # compiled with MKL support for corei7 or better chipsets, which; # significantly speeds up execution when running on modern CPUs. The default; # TensorFlow wheel files don't contain these instructions (and thereby run on a; # broader set of CPUs). Using this optimized wheel reduces the runtime of; # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud; # Platform) optimized wheel because all GCP instances have at least Sandy Bridge; # or better chipsets, so this wheel should run anywhere on GCP.; export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl""; export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow""; export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries.; # If you already have CUDA installed, such as on a properly provisioned; # Docker image, it shouldn't be necessary.; export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python); export USE_DEFAULT_PYTHON_LIB_PATH=1; export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; }; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:6337,install,install,6337,,https://github.com/google/deepvariant/issues/145,2,['install'],"['install', 'installed']"
Deployability,"ny.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; Ã— Preparing metadata (pyproject.toml) did not run successfully.; â”‚ exit code: 1; â•°â”€> [78 lines of output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); performance hint: _common.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:322:50: Exception check after calling 'random_func' will always require the GIL to be acquired",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:2124,Install,Install,2124,,https://github.com/google/deepvariant/issues/859,1,['Install'],['Install']
Deployability,"ogle Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory â€˜/mnt/cdw-genomeâ€™: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:5233,upgrade,upgraded,5233,,https://github.com/google/deepvariant/issues/171,3,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability,"om numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); [proxychains] DLL init: proxychains-ng 4.16; [proxychains] DLL init: proxychains-ng 4.16; ; Error compiling Cython file:; ------------------------------------------------------------; ...; for i in range(1, RK_STATE_LEN):; self.rng_state.key[i] = val[i]; self.rng_state.pos = i; ; self._bitgen.state = &self.rng_state; self._bitgen.next_uint64 = &mt19937_uint64; ^; ------------------------------------------------------------; ; _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_mt19937.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727:1431,install,install-,1431,,https://github.com/google/deepvariant/issues/727,1,['install'],['install-']
Deployability,"om pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:17693,upgrade,upgrade,17693,,https://github.com/google/deepvariant/issues/89,1,['upgrade'],['upgrade']
Deployability,"om-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] For more information, consult the worker log at gs://ms_bam/deep_output/stage/logs/call_variants/0; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 908, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 895, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 491, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 483, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 369, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23049213423"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 5. The script is run from a GCP VM instance.; 6. I have a standard setup with increased CPUs to 1025. . It is not clear to me what the issues are and what to make of the errors? ; I am sorry If this is not the correct forum to post in - please let me know where to seek help if not! ; Cheers, C",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:9224,pipeline,pipeline,9224,,https://github.com/google/deepvariant/issues/129,1,['pipeline'],['pipeline']
Deployability,"on (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontaine",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:12751,install,installed,12751,,https://github.com/google/deepvariant/issues/89,1,['install'],['installed']
Deployability,"onnecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; # 12; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; # 13; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; Focal (20.04) LTS - Last update : Sun, 10 Oct 2021 23:59:52 UTC / Re",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:8618,update,update,8618,,https://github.com/google/deepvariant/issues/489,2,"['install', 'update']","['install', 'update']"
Deployability,"onnection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker; put((job, i, (False, wrapped))); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe; Process ForkPoolWorker-42:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes; self._send(buf); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe; ```; There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens?. **Setup**; - Operating system: Ubuntu 22.04; - DeepVariant version: v1.6.1; - Installation method : singularity; - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804:4683,Install,Installation,4683,,https://github.com/google/deepvariant/issues/804,1,['Install'],['Installation']
Deployability,"oogle_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam --examples /tmp/tmpwgmyf_jr/make_examples.tfrecord@4.gz --gvcf /tmp/tmpwgmyf_jr/gvcf.tfrecord@4.gz --task 3. real	0m2.438s; user	0m2.289s; sys	0m3.833s; I0824 08:09:22.720529 140047104423680 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 3 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam"" --examples ""/tmp/tmpwgmyf_jr/make_examples.tfrecord@4.gz"" --gvcf ""/tmp/tmpwgmyf_jr/gvcf.tfrecord@4.gz"" --task {}' returned non-zero exit status 1.; Done...; `; this is my error when I run deepvariant script. - Operating system: CentOS; - DeepVariant version: 0.10.0; - Installation method (Docker, built from source, etc.): Docker. how can I solve this problem",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/337:9868,Install,Installation,9868,,https://github.com/google/deepvariant/issues/337,1,['Install'],['Installation']
Deployability,"ools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader; W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs; I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader; I0614 20:20:46.057183 47288495204160 make_examples_core.py:239] Common contigs are ['NC_000001.11', 'NT_187361.1', 'NT_187362.1', 'NT_187363.1', 'NT_187364.1', 'NT_187365.1', 'NT_187366.1', 'NT_187367.1', 'N",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/542:3315,install,installed,3315,,https://github.com/google/deepvariant/issues/542,1,['install'],['installed']
Deployability,"ope-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 14:37:23 ERROR gcp_deepvariant_runner.py] Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/5187520767668161022"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.1', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; Traceback (most recent call last):;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:2288,pipeline,pipelines,2288,,https://github.com/google/deepvariant/issues/129,1,['pipeline'],['pipelines']
Deployability,opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:4438,Release,Release,4438,,https://github.com/google/deepvariant/issues/89,1,['Release'],['Release']
Deployability,"ord-00008-of-00020.gz.example_info.json; I0822 07:52:09.283617 133276175411008 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0822 07:52:09.283712 133276175411008 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0822 07:52:09.283882 133276175411008 make_examples_core.py:301] Task 8/20: Found 17371 candidate variants; I0822 07:52:09.283904 133276175411008 make_examples_core.py:301] Task 8/20: Created 18820 examples. real 34m15.728s; user 624m43.553s; sys 2m24.932s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@20.gz"" --checkpoint ""/output/checkpoints/ckpt-679"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0822 07:52:10.812179 127086447671104 call_variants.py:563] Total 1 writing processes started.; I0822 07:52:10.813103 127086447671104 dv_utils.py:370] From /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0822 07:52:10.813141 127086447671104 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0822 07:52:10.813338 127086447671104 call_variants.py:592] Use saved model: True; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:4882,release,release,4882,,https://github.com/google/deepvariant/issues/869,1,['release'],['release']
Deployability,"ory ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[====================================================================================================================================================================================================>] 3.07K --.-KB/s in 0s. 2021-10-11 18:34:23 (51.5 MB/s) - written to stdout [3145/3145]. OK; Get:2 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/u",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:3753,update,updates,3753,,https://github.com/google/deepvariant/issues/489,1,['update'],['updates']
Deployability,"oud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:42:05 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/goog",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:8038,install,installed,8038,,https://github.com/google/deepvariant/issues/89,1,['install'],['installed']
Deployability,package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1247,install,installed,1247,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability,"post, but the devil is probably in the details!!). Script:; #!/bin/bash; #set -euo pipefail; # Set common settings.; PROJECT_ID=ms-deepvariant; OUTPUT_BUCKET=gs://ms_bam/deep_output; STAGING_FOLDER_NAME=stage; OUTPUT_FILE_NAME=deeptest_FB4_chr20.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/; IMAGE_VERSION=0.7.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://public_bed/CHR20.bed \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --ref_fai gs://ms_bam/Homo_sapiens_assembly38.fasta.fai \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones europe-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 14:37:23 ERROR gcp_deepvariant_runner.py] Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/5187520767668161022"" failed: executing pipeline: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:1184,pipeline,pipeline,1184,,https://github.com/google/deepvariant/issues/129,1,['pipeline'],['pipeline']
Deployability,problems with bazel installation d.v. v0.7.0 on Ubuntu 16,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/92:20,install,installation,20,,https://github.com/google/deepvariant/issues/92,1,['install'],['installation']
Deployability,"pvariant/blob/r1.5/docs/FAQ.md**:; Yes. ; **Describe the issue:**; I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . ; This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :; ; ``` ; (base) âœ” /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|â€¦5] ; 22:08 $ ls; GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp; ```; I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/).; ; **Setup**; - Operating system: Linux Mint 21.1 x86_64 ; - Kernel: 5.15.0-69-generic ; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): Docker image run through Singularity; - Singularity Verion : singularity-ce version 3.11.3; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```; (base) âœ” /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|â€¦5] ; 20:46 $ samtools flagstat GFX.bam ; ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads); 881297 + 0 primary; 0 + 0 secondary; 59254 + 0 supplementary; 0 + 0 duplicates; 0 + 0 primary duplicates; 940551 + 0 mapped (100.00% : N/A); 881297 + 0 primary mapped (100.00% : N/A); 0 + 0 paired in sequencing; 0 + 0 read1; 0 + 0 read2; 0 + 0 properly paired (N/A : N/A); 0 + 0 with itself and mate mapped; 0 + 0 singletons (N/A : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. ; **Steps to reproduce:**; - Com",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666:1034,Install,Installation,1034,,https://github.com/google/deepvariant/issues/666,1,['Install'],['Installation']
Deployability,"pvariant:1.6.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${6} \; --ref=./human_g1k_v37_decoy.fasta \; --reads=./${2}_md.recal.cram \; --output_vcf=./${2}_hg37.dv.vcf.gz \; --output_gvcf=./${2}_hg37.dv.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=32; ```; Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started.; W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records.; I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants.; ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards; Erin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/776:1307,release,release,1307,,https://github.com/google/deepvariant/issues/776,1,['release'],['release']
Deployability,"py"", line 235, in eval_loop; name=eval_name); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate; name=name); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval; hooks.extend(self._convert_eval_steps_to_hooks(steps)); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks; raise ValueError('Must specify steps > 0, given: {}'.format(steps)); ValueError: Must specify steps > 0, given: 0. real	0m58.116s; user	0m1.590s; sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant; ADD ./bin /home/bin; ADD ./models /home/models; ADD ./oss_clif.ubuntu-16.latest.tgz /; WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh; RUN apt-get -y install python2.7; RUN apt-get -y install python-dev python-pip; RUN apt-get -y install parallel; RUN python2 -m pip install --upgrade --force-reinstall pip; RUN python2 -m pip install apache-beam; RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0; I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None; W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:5826,install,install,5826,,https://github.com/google/deepvariant/issues/172,1,['install'],['install']
Deployability,"r exception occurred:. Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>; import numpy as np; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>; from . import core; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the numpy C-extensions failed. This error can happen for; many reasons, often due to issues with your setup or how NumPy was; installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3""; * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect.; Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**; As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` int",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610:2212,install,installed,2212,,https://github.com/google/deepvariant/issues/610,1,['install'],['installed']
Deployability,"rces and expanding templates; run_build = parse_setuppy_commands(); [proxychains] DLL init: proxychains-ng 4.16; [proxychains] DLL init: proxychains-ng 4.16; ; Error compiling Cython file:; ------------------------------------------------------------; ...; for i in range(1, RK_STATE_LEN):; self.rng_state.key[i] = val[i]; self.rng_state.pos = i; ; self._bitgen.state = &self.rng_state; self._bitgen.next_uint64 = &mt19937_uint64; ^; ------------------------------------------------------------; ; _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_mt19937.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1.; Cythonizing sources; Traceback (most recent call last):; File ""/root/.local/lib/python3.9/site-packages/pip/_vendo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727:1554,install,install-,1554,,https://github.com/google/deepvariant/issues/727,1,['install'],['install-']
Deployability,"ream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1; 2020-07-03 17:18:45.680312: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; 2020-07-03 17:18:45.680339: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 7c1895dbad7c; 2020-07-03 17:18:45.680346: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 7c1895dbad7c; 2020-07-03 17:18:45.680397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.129.0; 2020-07-03 17:18:45.680416: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.113.0; 2020-07-03 17:18:45.680422: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 418.113.0 does not match DSO version 410.129.0 -- cannot find working devices in this configuration; I0703 17:18:45.713418 140322304501504 modeling.py:563] Initializing model with random parameters; W0703 17:18:45.713950 140322304501504 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp7amyb_ws; I0703 17:18:45.714213 140322304501504 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp7amyb_ws', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9f0f102630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321:2305,configurat,configuration,2305,,https://github.com/google/deepvariant/issues/321,1,['configurat'],['configuration']
Deployability,release v0.4.1 failing to compile on ubuntu 16.04,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:0,release,release,0,,https://github.com/google/deepvariant/issues/19,1,['release'],['release']
Deployability,"ritical operations: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-07-03 17:21:57.644332: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-07-03 17:21:58.247052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-07-03 17:21:58.247080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; Runs all 3 steps to go from input DNA reads to output VCF/gVCF files.; (... then -- the list of all options follows). If run on a proper BAM file with all options provided, all TF-TRT warning messages are periodically repeated as well as ; CUDA Version 11.3.1; 2024-07-02 22:47:07.493311: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-02 22:47:12.386498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:2632,install,installed,2632,,https://github.com/google/deepvariant/issues/844,1,['install'],['installed']
Deployability,"rning: Please check that your locale settings:; 	LANGUAGE = ""en_US:en"",; 	LC_ALL = (unset),; 	LC_ADDRESS = ""en_US.UTF-8"",; 	LC_NAME = ""en_US.UTF-8"",; 	LC_MONETARY = ""en_US.UTF-8"",; 	LC_PAPER = ""en_US.UTF-8"",; 	LC_IDENTIFICATION = ""en_US.UTF-8"",; 	LC_TELEPHONE = ""en_US.UTF-8"",; 	LC_MEASUREMENT = ""en_US.UTF-8"",; 	LC_CTYPE = ""C.UTF-8"",; 	LC_TIME = ""en_US.UTF-8"",; 	LC_NUMERIC = ""en_US.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = ""en_US:en"",; 	LC_ALL = (unset),; 	LC_TIME = ""en_US.UTF-8"",; 	LC_MONETARY = ""en_US.UTF-8"",; 	LC_CTYPE = ""C.UTF-8"",; 	LC_ADDRESS = ""en_US.UTF-8"",; 	LC_TELEPHONE = ""en_US.UTF-8"",; 	LC_NAME = ""en_US.UTF-8"",; 	LC_MEASUREMENT = ""en_US.UTF-8"",; 	LC_IDENTIFICATION = ""en_US.UTF-8"",; 	LC_NUMERIC = ""en_US.UTF-8"",; 	LC_PAPER = ""en_US.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2024-02-17 23:32:31.107126: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.108506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.006781: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:5898,install,installed,5898,,https://github.com/google/deepvariant/issues/774,1,['install'],['installed']
Deployability,"running call variants on ONT data using the command . `run_pepper_margin_deepvariant call_variant -b ./Sample/alignments/GRCh38/41195.minimap2.bam -f /data/Homo_sapiens_assembly38.fasta -o Sample -t 4 -s Sample --phased_output --ont_r9_guppy5_sup; `. no errors but it's stuck at Starting Candidate Finding and the CPU is at 0% for hours. Can I stop it and resume? why there is no error? will it ever finish?. ....; [12-24-2023 05:47:47] INFO: SUMMARY PROCESSED 7280/7280.; [12-24-2023 05:47:47] INFO: THREAD 0 FINISHED SUCCESSFULLY.; [12-24-2023 05:54:33] INFO: FINISHED PREDICTION; [12-24-2023 05:54:33] INFO: ELAPSED TIME: 867 Min 41 Sec; [12-24-2023 05:54:33] INFO: PREDICTION FINISHED SUCCESSFULLY.; [12-24-2023 05:54:33] INFO: TOTAL ELAPSED TIME FOR INFERENCE: 867 Min 44 Sec; [12-24-2023 05:54:33] INFO: STEP 3/3 FINDING CANDIDATES; [12-24-2023 05:54:33] INFO: OUTPUT: 41195/pepper/; [12-24-2023 05:55:25] INFO: STARTING CANDIDATE FINDING. - Operating system: Linux; - DeepVariant version: kishwars/pepper_deepvariant r0.8; - Installation method (Docker, built from source, etc.): Docker; ONT long reads",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/758:1032,Install,Installation,1032,,https://github.com/google/deepvariant/issues/758,1,['Install'],['Installation']
Deployability,"s-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-pa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:12969,upgrade,upgraded,12969,,https://github.com/google/deepvariant/issues/89,3,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability,"s/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""CM000663.2"" is 248956422 bp and IS MISSING, ""KI270706.1"" is 175055 bp and IS MISSING, ""KI270707.1"" is 32032 bp and IS MISSING, ""KI270708.1"" is 127682 bp and IS MISSING, ""KI270709.1"" is 66860 bp and IS MISSING, """,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/225:1480,pipeline,pipelines,1480,,https://github.com/google/deepvariant/issues/225,1,['pipeline'],['pipelines']
Deployability,"se examples. I get the error that the code uses too mach memory. It is out of memory of my machine. My machine have 376G memory. Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:1211,release,release,1211,,https://github.com/google/deepvariant/issues/91,1,['release'],['release']
Deployability,"se. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-02-17 23:31:25.687399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-02-17 23:31:39.809521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:31:39.810043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:31:59.620996: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:31:59.623967 140288433825600 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpd74of138; I0217 23:31:59.629002 140288433825600 run_deepvariant.py:551] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load input/weights-51-0.995354.ckpt* instead. ***** Intermediate results will be written to /tmp/tmpd74of138 in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmpd74of138/gv",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:3866,install,installed,3866,,https://github.com/google/deepvariant/issues/774,1,['install'],['installed']
Deployability,"se_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease' is not signed.; ------; Dockerfile:50; --------------------; 49 |; 50 | >>> RUN ./build-prereq.sh \; 51 | >>> && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel; 52 |; --------------------; ERROR: failed to solve: process ""/bin/sh -c ./build-prereq.sh && PATH=\""${HOME}/bin:${PATH}\"" ./build_release_binaries.sh # PATH for bazel"" did not complete successfully: exit cod",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:2208,update,updates,2208,,https://github.com/google/deepvariant/issues/902,1,['update'],['updates']
Deployability,"ses/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_listener= //deepvariant:make_examples //deepvariant:call_variants //deepvariant:postprocess_variants || { \; echo ""Bazel build failed""; \; exit 1; }. # Final image; FROM base AS final. # Set environment variables; ENV VERSION=1.6.0; ENV PYTHON_VERSION=3.8; ENV PATH=""/opt/miniconda/bin:${PATH}"". # Install Python packages; RUN pip install --upgrade pip setuptools wheel --timeout=120 && \; pip install jaxlib jax --timeout=120 --extra-index-url https://storage.googleapis.com/jax-releases/jax_releases.html. # Copy DeepVariant binaries from the builder stage; COPY --from=builder /opt/deepvariant /opt/deepvariant; WORKDIR /opt/deepvariant. # Ensure executable scripts are correctly set up; RUN BASH_HEADER='#!/bin/bash' && \; for script in make_examples call_variants call_variants_slim postprocess_variants vcf_stats_report show_examples runtime_by_region_vis multisample_make_examples labeled_examples_to_vcf make_examples_somatic train run_deepvariant run_deepsomatic; do \; printf ""%s\n%s\n"" ""${BASH_H",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:2015,configurat,configurations,2015,,https://github.com/google/deepvariant/issues/871,1,['configurat'],['configurations']
Deployability,"sing Docker on a Mac M1 and am encountering issues with the Dockerfile during the Bazel build process. I want to ensure compatibility with ARM64 architecture. **Docker version**: Docker version 27.1.1, build 6312585; **Bazel Version**: 7.3.1; **MacBook Model**: M1 chip (ARM64 architecture). **Error**: ; ![IMG_3267](https://github.com/user-attachments/assets/11e28824-b941-42cc-9d33-7e9155a03543); ![IMG_3268](https://github.com/user-attachments/assets/4e923de6-99d5-43ee-80c6-29b32504527d). **My Dockerfilee code**:. ```; # Base image suitable for ARM64 architecture; FROM arm64v8/ubuntu:latest AS base. # Prevent interactive prompts; ENV DEBIAN_FRONTEND=noninteractive. # Install necessary packages; RUN apt-get update && \; apt-get install -y \; git \; curl \; unzip \; wget \; openjdk-17-jdk \; build-essential \; bzip2 \; python3-pip \; parallel && \; apt-get clean && \; rm -rf /var/lib/apt/lists/*. # Install Bazel (adjust version as needed); RUN curl -LO ""https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:1092,release,releases,1092,,https://github.com/google/deepvariant/issues/871,1,['release'],['releases']
Deployability,singularity install errors,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/668:12,install,install,12,,https://github.com/google/deepvariant/issues/668,1,['install'],['install']
Deployability,"structions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-01-05 15:52:57.864310: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-01-05 15:53:10.688853: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:10.692890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:26.990784: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:53:27.004992 140619855705920 run_deepvariant.py:519] Re-using the directory for intermediate results in /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9. ***** Intermediate results will be written to /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9 in docker. ****; ***** Running the command:*****; time seq 0 1 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/refgenome/GCF_003073045.1_ASM307304v1_genomic.fna"" --reads ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --channels ""insert_size"" --gvcf ""/public3/group_crf/home/cuirf/.tmp/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:2962,install,installed,2962,,https://github.com/google/deepvariant/issues/761,1,['install'],['installed']
Deployability,"t of slurm output:; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****; time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]; # this part is likely unimportant. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader; I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs; 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader; I0217 20:13:17.376646 23456243894080 make_examples_core.py:243] Task 18/64: Common contigs are ['I', 'II', 'III', 'IV', 'V', 'X', 'MtDNA']; I0217 20:13:17.383403 23456243894080 make_examples_core.py:243] Task 18/64: Starting from",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:1946,install,installed,1946,,https://github.com/google/deepvariant/issues/614,1,['install'],['installed']
Deployability,"t. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm:ðŸ†‘:GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/380:1901,Install,Installation,1901,,https://github.com/google/deepvariant/issues/380,1,['Install'],['Installation']
Deployability,"t:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-dat",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:12153,upgrade,upgraded,12153,,https://github.com/google/deepvariant/issues/89,3,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability,"tfrecordio.py"", line 175, in read_records; record = _TFRecordUtil.read_record(file_handle); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/tfrecordio.py"", line 131, in read_record; buf = file_handle.read(buf_length_expected); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/filesystem.py"", line 240, in read; self._fetch_to_internal_buffer(num_bytes); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/filesystem.py"", line 199, in _fetch_to_internal_buffer; self._read_buffer.write(decompressed); MemoryError: out of memory. ERROR:root:Giving up after 4 attempts.; WARNING:root:A task failed with exception: out of memory; Traceback (most recent call last):; File ""./shuffle_tfrecords_beam.py"", line 229, in <module>; main(); File ""./shuffle_tfrecords_beam.py"", line 224, in main; known_args.output_dataset_config_pbtxt); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/pipeline.py"", line 410, in __exit__; self.run().wait_until_finish(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py"", line 421, in wait_until_finish; self._executor.await_completion(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 398, in await_completion; self._executor.await_completion(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 444, in await_completion; six.reraise(t, v, tb); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:9021,pipeline,pipeline,9021,,https://github.com/google/deepvariant/issues/91,1,['pipeline'],['pipeline']
Deployability,"the deep variant wrapper dv_call_variants.py crushing when installed using conda. **Setup**; - Ubuntu 20.04:; - DeepVariant version - 1.4.0:; - Installation method - Conda; - Type of data - sequencing, illumina. **Steps to reproduce:**; - Command:; dv_call_variants.py --outfile OUTFILE --examples EXAMPLES --sample SAMPLE. - Error trace: (if applicable) ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_zviaa5zy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 39, in <module>; import numpy as np; ModuleNotFoundError: No module named 'numpy'. numpy installed in enviroment",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/573:59,install,installed,59,,https://github.com/google/deepvariant/issues/573,3,"['Install', 'install']","['Installation', 'installed']"
Deployability,"tions.; ; Error compiling Cython file:; ------------------------------------------------------------; ...; self.rng_state.ctr.v[i] = counter[i]; ; self._reset_state_variables(); ; self._bitgen.state = <void *>&self.rng_state; self._bitgen.next_uint64 = &philox_uint64; ^; ------------------------------------------------------------; ; _philox.pyx:195:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_common.pyx; Processing numpy/random/_philox.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/public/home/zhanghl3/miniconda3/envs/deepvariant/lib/python3.10/subprocess.py"", line 369, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/public/home/zhanghl3/miniconda3/envs/deepvariant/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_philox.c', '_philox.pyx']' returned non-zero exit status 1.; Cythonizing sources; Traceback (most recent call last):; File ""/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <modul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:9065,install,install-,9065,,https://github.com/google/deepvariant/issues/859,1,['install'],['install-']
Deployability,"tools like DeepVariant. Iâ€™m trying to build DeepVariant using Docker on a Mac M1 and am encountering issues with the Dockerfile during the Bazel build process. I want to ensure compatibility with ARM64 architecture. **Docker version**: Docker version 27.1.1, build 6312585; **Bazel Version**: 7.3.1; **MacBook Model**: M1 chip (ARM64 architecture). **Error**: ; ![IMG_3267](https://github.com/user-attachments/assets/11e28824-b941-42cc-9d33-7e9155a03543); ![IMG_3268](https://github.com/user-attachments/assets/4e923de6-99d5-43ee-80c6-29b32504527d). **My Dockerfilee code**:. ```; # Base image suitable for ARM64 architecture; FROM arm64v8/ubuntu:latest AS base. # Prevent interactive prompts; ENV DEBIAN_FRONTEND=noninteractive. # Install necessary packages; RUN apt-get update && \; apt-get install -y \; git \; curl \; unzip \; wget \; openjdk-17-jdk \; build-essential \; bzip2 \; python3-pip \; parallel && \; apt-get clean && \; rm -rf /var/lib/apt/lists/*. # Install Bazel (adjust version as needed); RUN curl -LO ""https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:1000,Install,Install,1000,,https://github.com/google/deepvariant/issues/871,1,['Install'],['Install']
Deployability,"tput VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepTrio? Is there a reason why the HG001/NA12891/NA12892 trios were used as training for the child model but not the parent model?. <br>. Many thanks,; Macabe. <br>. ![image](https://user-images.githubusercontent.com/37773554/128098808-740a1ab0-a6af-452f-8bed-d1f4ba0ceb80.png); Current DeepTrio training info (likely typo for Ashkenazim trio, cf. HG002/HG00**3**/HG004). [1] https://academic.oup.com/bioinformatics/article/36/24/5582/6064144 ; [2] https://github.com/google/deepvariant/tree/r1.2/deeptrio/testdata/input",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475:1855,release,released,1855,,https://github.com/google/deepvariant/issues/475,1,['release'],['released']
Deployability,"ts.; (default: 'false'); --[no]add_hp_channel: If true, add another channel to represent HP tags per read.; (default: 'false'); --channels: Comma-delimited list of optional channels to add. Available Channels: read_mapping_percent,avg_base_quality,identity,gap_compressed_identity,gc_content,is_homopolymer,homopolymer_weighted,blank,insert_size; ```. Are there `model-ckpt` files for these channel options available somewhere to provide `call_variants` via:; ```; --checkpoint: Required. Path to the TensorFlow model checkpoint to use to evaluate candidate variant calls.; ```. If so, do they include one additional channel or permutations of multiple channels?. If not, is there an alternative way to have `run_deepvariant` use different channels than what the default checkpoint contains during `call_variants`? For example, I am currently unable to include both `insert_size` and `allele_frequency` with v1.4. **Setup**; - Operating system:; - DeepVariant version: v1.4; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WGS. **Steps to reproduce:**; - Command: ; ```; time singularity run -B '/usr/lib/locale/:/usr/lib/locale/,/path/to/region_files/:/region_dir/,/path/to/container/deep-variant/:/run_dir/,/path/to/output/:/path/to/reference_genome/:/ref_dir/,/path/to/bam_files/:/bam_dir/,/path/to/population_vcf/:/popVCF_dir/' . deepvariant_1.4.0.sif ; /opt/deepvariant/bin/run_deepvariant ; --model_type=WGS; --ref='/ref_dir/reference.fa' ; --reads='/bam_dir/id.bam' ; --output_vcf='/out_dir/test1.vcf.gz' ; --intermediate_results_dir='/out_dir/tmp/test1/' ; --num_shards='39' ; --make_examples_extra_args=""use_allele_frequency=true,population_vcfs=/popVCF_dir/UMAG1.POP.FREQ.vcf.gz"" ; --regions=/region_dir/regions_to_test.bed ; ```; - Error trace: (if applicable); ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/out_dir/tmp/test1/call_variants_output.tfrecord.gz"" --examples ""/out_dir/tmp/test1/make_examples.tfr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568:1709,Install,Installation,1709,,https://github.com/google/deepvariant/issues/568,1,['Install'],['Installation']
Deployability,"ts.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:06.676597 140416700553024 postprocess_variants.py:1313] CVO sorting took 0.006136405467987061 minutes; I0105 16:01:06.677379 140416700553024 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0105 16:01:06.677495 140416700553024 postprocess_variants.py:1318] Using 2 CPUs for parallelization of variant transformation.; I0105 16:01:06.808352 140416700553024 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:08.209710 140416700553024 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.01743464469909668 minutes; I0105 16:01:10.258949 140416700553024 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.03414338032404582 minutes. real 0m21.740s; user 0m13.473s; sys 0m2.305s. ***** Running the command:*****; time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""./outputgpu/output.vcf.gz"" --outfile_base ""./outputgpu/output"". 2024-01-05 16:01:21.188421: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 16:01:21.188700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 16:01:28.513759: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 16:01:28.547411 140591583876928 genomics_reader.py:222] Reading ./outputgpu/output.vcf.gz with NativeVcfReader. real 0m18.513s; user 0m11.281s; sys 0m1.577s. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:18027,install,installed,18027,,https://github.com/google/deepvariant/issues/761,1,['install'],['installed']
Deployability,"tten permission.; #; # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""; # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE; # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE; # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE; # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR; # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF; # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS; # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN; # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE); # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE; # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for; # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This; # will skip the installation of TensorFlow.; export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0; export TF_ENABLE_XLA=1; export TF_NEED_CUDA=1; export TF_NEED_GCP=1; export TF_NEED_GDR=0; export TF_NEED_HDFS=0; export TF_NEED_JEMALLOC=0; export TF_NEED_MKL=1; export TF_NEED_MPI=0; export TF_NEED_OPENCL=0; export TF_NEED_OPENCL_SYCL=0; export TF_NEED_S3=1; export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1; export TF_CUDA_VERSION=""10.0""; export CUDA_TOOLKIT_PATH=""/usr/local/cuda""; export TF_CUDNN_VERSION=""7""; export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant.; DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that; # `bazel` will find the latest version of bazel installed in the user's home; # directory. This is set in setting.sh as all DeepVariant scripts source; # settings.sh and assume that `bazel` will find the right version.; export PATH=""$HOME/bin:$PATH"". # P",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:2736,install,installation,2736,,https://github.com/google/deepvariant/issues/145,1,['install'],['installation']
Deployability,ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:5644,update,updates,5644,,https://github.com/google/deepvariant/issues/489,1,['update'],['updates']
Deployability,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__; self._reader = sam_reader.SamReader.from_file(; ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s; user	0m3.036s; sys	0m0.866s; ```. **Setup**; - Operating system: CentOS Linux release 8.2.2004 (Core); - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity/Docker; - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**; The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/533:5327,release,release,5327,,https://github.com/google/deepvariant/issues/533,2,"['Install', 'release']","['Installation', 'release']"
Deployability,"unlike the case studies?) bam files, ONT. **Steps to reproduce:**; - Command: ; apptainer exec --bind /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint /work/XXXX/ls6/deepvariant/deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant --model_type ONT_R104 --ref /work/XXXX/data/common/human/hg38bundle/Homo_sapiens_assembly38.fasta --reads /scratch/XXXX/ONT_WGS/HH/FL9-1/FL9-1.chr10.bam --output_vcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.vcf.gz --output_gvcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.g.vcf.gz --num_shards 64 --logging_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/ --intermediate_results_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results. - Error trace: (if applicable); ; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2p_bcqtz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_2p_bcqtz/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 43, in <module>; import numpy as np; File ""/usr/local/lib/python3.8/dist-packages/numpy/__init__.py"", line 152, in <module>; from . import random; File ""/usr/local/lib/python3.8/dist-packages/numpy/random/__init__.py"", line 180, in <module>; from . import _pickle; File ""/usr/local/lib/python3.8/dist-packages/numpy/ra",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/854:1479,install,installed,1479,,https://github.com/google/deepvariant/issues/854,1,['install'],['installed']
Deployability,"untu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Misc setup' starting; ========== [Fri 02 Aug 2024 02:20:04 PM CST] Stage 'Update package list' starting; ========== [Fri 02 Aug 2024 02:20:06 PM CST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Fri 02 Aug 2024 02:20:10 PM CST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; Ã— Preparing metadata (pyproject.toml) did not run successfully.; â”‚ exit code: 1; â•°â”€> [78 lines of output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_se",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:1490,install,installed,1490,,https://github.com/google/deepvariant/issues/859,1,['install'],['installed']
Deployability,"untu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; # 12; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; # 13; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; Focal (20.04) LTS - Last update : Sun, 10 Oct 2021 23:59:52 UTC / Revision: 20211010053033+67964fc4b241; # i386 not available; deb http://apt.llvm.org/focal/ llvm-toolchain-focal main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal main; # 12; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; # 13; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main. ```; `llvm-toolchain-bionic-11` was changed ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:9157,update,update,9157,,https://github.com/google/deepvariant/issues/489,1,['update'],['update']
Deployability,"vm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:8232,update,updates,8232,,https://github.com/google/deepvariant/issues/489,1,['update'],['updates']
Deployability,"wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:1356,pipeline,pipelines,1356,,https://github.com/google/deepvariant/issues/214,1,['pipeline'],['pipelines']
Deployability,while installing im getting an bazel error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:6,install,installing,6,,https://github.com/google/deepvariant/issues/231,1,['install'],['installing']
Deployability,"with --ref; I0812 17:25:09.205637 140700470552384 genomics_reader.py:222] Reading ../mapped/SRR18493715.RNA-Seq.Camellia_sp._multipetala.leaf/Aligned.sortedByCoord.out.bam with NativeSamReader; I0812 17:25:10.119529 140700470552384 genomics_reader.py:222] Reading ../mapped/SRR18493715.RNA-Seq.Camellia_sp._multipetala.leaf/Aligned.sortedByCoord.out.bam with NativeSamReader; I0812 17:25:10.163989 140700470552384 make_examples_core.py:301] Task 15/32: Writing gvcf records to /public4/courses/ec3121/shareddata/Camellia_Sect_Chrysantha/star_hapbetter/deepvariant/tmp/tmp0n4wz07d/gvcf.tfrecord-00015-of-00032.gz; I0812 17:25:10.169354 140700470552384 make_examples_core.py:301] Task 15/32: Writing examples to /public4/courses/ec3121/shareddata/Camellia_Sect_Chrysantha/star_hapbetter/deepvariant/tmp/tmp0n4wz07d/make_examples.tfrecord-00015-of-00032.gz; I0812 17:25:10.170929 140700470552384 make_examples_core.py:301] Task 15/32: Overhead for preparing inputs: 281 seconds; I0812 17:25:10.353756 140700470552384 make_examples_core.py:301] Task 15/32: 0 candidates (0 examples) [0.18s elapsed]; '. While RNAseq data is expected to be sparsely mapped onto the genome, wouldn't deepvariant focus on the regions specified by the bed file? I'm a bit surprised that 0 examples were made. Is it possible that the reads do not map with high enough MAPQ in these regions? But I'm still not sure why make_samples get stuck though. **Setup**; - Operating system: Ubuntu server 22.04; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): singularity (docker image); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); RNAseq data NCBI SRA SRR18493715, ref genome (produced by ourself, but can probably be replaced by another version https://www.ncbi.nlm.nih.gov/datasets/genome/GCA_025200525.1/ ); **Steps to reproduce:**; - Map reads with STAR, use samtools index to index the BAM; - run the above deepvariant command",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/867:5767,Install,Installation,5767,,https://github.com/google/deepvariant/issues/867,1,['Install'],['Installation']
Deployability,"work on cluster service,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Sun Jul 26 15:27:06 UTC 2020; conda 4.9.2; Python 3.7.6. i really hope ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/391:1121,update,update,1121,,https://github.com/google/deepvariant/issues/391,1,['update'],['update']
Deployability,"x/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Un",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:1423,Update,Update,1423,,https://github.com/google/deepvariant/issues/231,1,['Update'],['Update']
Deployability,"y:301] Task 1/4: Found 0 candidate variants; I0729 14:44:37.899752 140710547908416 make_examples_core.py:301] Task 1/4: Created 0 examples; I0729 14:44:37.893192 139779121772352 make_examples_core.py:301] Task 2/4: Writing example info to /tmp/tmpkcjcf0p_/make_examples.tfrecord-00002-of-00004.gz.example_info.json; I0729 14:44:37.893293 139779121772352 make_examples_core.py:2958] example_shape = None; I0729 14:44:37.893665 139779121772352 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0729 14:44:37.894033 139779121772352 make_examples_core.py:301] Task 2/4: Found 0 candidate variants; I0729 14:44:37.894105 139779121772352 make_examples_core.py:301] Task 2/4: Created 0 examples. real	0m4.791s; user	0m11.503s; sys	0m2.085s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpkcjcf0p_/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpkcjcf0p_/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wes"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0729 14:44:41.088234 139722246891328 call_variants.py:471] Total 1 writing processes started.; W0729 14:44:41.090612 139722246891328 call_variants.py:482] Unable to read any records from /tmp/tmpkcjcf0p_/make_examples.tfrecord@4.gz. Output will contain zero records.; I0729 14:44:41.091079 139722246891328 call_variants.py:623] Complete: call_variants. **Does the quick start test work on your system?**; yes. **Any additional context:**; Some samples work fine, some very similar samples keep running",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/855:11822,release,release,11822,,https://github.com/google/deepvariant/issues/855,1,['release'],['release']
Deployability,"| 1.823839956 | 1.581195853 | 1.50423718 |; | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |; | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |; | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising resultï¼šUsing the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard setï¼Œbut I don't understand the reason for this difference. **Setup**; - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0; - DeepVariant version:deeptrio-1.4.0; - Installation method (Docker, built from source, etc.):Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); HiFi data,those data download links follows:; * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/; * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb; * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/; **Steps to reproduce:**; - Command:; - Error trace: (if applicable",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:6648,Install,Installation,6648,,https://github.com/google/deepvariant/issues/689,1,['Install'],['Installation']
Deployability,"â€¦ed information. Note that previously there was a typo in WGS case study -- instead of 209m 53s, it was 309m 53s.; (2) Remove BIN_VERSION in the *_binaries.sh script because it was unused.; (3) Update a few instructions in training case study. PiperOrigin-RevId: 220115965. We are not taking pull requests at this time.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/234:194,Update,Update,194,,https://github.com/google/deepvariant/pull/234,1,['Update'],['Update']
Energy Efficiency," encountered this error previously and I cannot figure out what is causing the issue. Looks like something to do with the reference file?. user@node1784:~/MyData$ /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=./hg19.fa --reads=./NA12878_S1.bam --output_vcf=./NA12878_DeepVariant_output.vcf.gz --num_shards=1 ; I1027 14:35:49.384760 139774463268608 run_deepvariant.py:273] Re-using the directory for intermediate results in /tmp/tmps6oyff7s. ***** Intermediate results will be written to /tmp/tmps6oyff7s in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./hg19.fa"" --reads ""./NA12878_S1.bam"" --examples ""/tmp/tmps6oyff7s/make_examples.tfrecord@1.gz"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I1027 14:35:51.541710 140702132172544 genomics_reader.py:223] Reading ./NA12878_S1.bam with NativeSamReader; I1027 14:35:51.552782 140702132172544 make_examples.py:587] Preparing inputs; I1027 14:35:51.576705 140702132172544 genomics_reader.py:223] Reading ./NA12878_S1.bam with NativeSamReader; I1027 14:35:51.590540 140702132172544 make_examples.py:587] Common contigs are ['chrM', 'chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY']; I1027 14:35:56.576697 140702132172544 make_examples.py:587] Writing examples to /tmp/tmps6oyff7s/make_examples.tfrecord-00000-of-00001.gz; I1027 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/372:1003,Power,Power,1003,,https://github.com/google/deepvariant/issues/372,1,['Power'],['Power']
Energy Efficiency,"${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo docker run \; -v ""${BASE}"":""${BASE}"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) 2>&1 | tee ""${LOG_DIR}/call_variants.log""; echo ""Done.""; echo; ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:4841,reduce,reduce,4841,,https://github.com/google/deepvariant/issues/151,1,['reduce'],['reduce']
Energy Efficiency,"**Describe the issue:**; Hello, I want to know what is an efficient way to build and run locally. My intent: make a change in call_variant.py and observe the effect. ; Do I have to always build the docker? ; OR which shell scripts can I use to achieve my purpose?. **Setup**; - Operating system: Ubuntu 18.04 LTS; - DeepVariant version: 0.8.0; - Installation method: build from source; - Type of data: NA. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/340:58,efficient,efficient,58,,https://github.com/google/deepvariant/issues/340,1,['efficient'],['efficient']
Energy Efficiency,"**Describe the issue:**; I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : ; 1. non-parallel + bam âœ…; 2. non-parallel + cram âœ… ; 3. parallel + bam âœ… ; 4. non-parallel + cram ðŸ”´ . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**; - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64; - DeepVariant version: v1.6.0; - Installation method (Docker, built from source, etc.): HPC, sorry I don't know; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Not special, I used common toy data. **Steps to reproduce:**; - Command: ; ```; seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \; --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --channels insert_size \; --task {} \; || exit 1; ```; - Error trace: (if applicable); ```; META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/786:240,reduce,reduce,240,,https://github.com/google/deepvariant/issues/786,1,['reduce'],['reduce']
Energy Efficiency,",010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sy",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:1661,monitor,monitoring,1661,,https://github.com/google/deepvariant/issues/471,1,['monitor'],['monitoring']
Energy Efficiency,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]; I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]; I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]; I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]; I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]; I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]; I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]; I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]; I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]; I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]; I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]; # the program died here; ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks!; Steve",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:35861,allocate,allocated,35861,,https://github.com/google/deepvariant/issues/491,1,['allocate'],['allocated']
Energy Efficiency,"6737.31s elapsed]; I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526.; I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]; I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234.; I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]; I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s; user	632m52.969s; sys	6m20.594s; INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:; ```; Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.; ```; This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:12893,efficient,efficiently,12893,,https://github.com/google/deepvariant/issues/614,1,['efficient'],['efficiently']
Energy Efficiency,9:34:11.184109 47167827691328 make_examples_core.py:257] 7332195 candidates (8325720 examples) [13.01s elapsed]; I0720 09:34:37.427601 47167827691328 make_examples_core.py:257] 7334598 candidates (8328193 examples) [26.24s elapsed]; I0720 09:34:53.436255 47167827691328 make_examples_core.py:257] 7336041 candidates (8329660 examples) [16.01s elapsed]; I0720 09:35:18.419953 47167827691328 make_examples_core.py:257] 7338676 candidates (8332321 examples) [24.98s elapsed]; I0720 09:35:32.331245 47167827691328 make_examples_core.py:257] 7340143 candidates (8333833 examples) [13.91s elapsed]; I0720 09:35:48.913797 47167827691328 make_examples_core.py:257] 7342460 candidates (8336192 examples) [16.58s elapsed]; I0720 09:36:05.716144 47167827691328 make_examples_core.py:257] 7344261 candidates (8338017 examples) [16.80s elapsed]; I0720 09:36:24.168858 47167827691328 make_examples_core.py:257] 7347004 candidates (8340828 examples) [18.45s elapsed]; I0720 09:36:42.921647 47167827691328 make_examples_core.py:257] 7349633 candidates (8343603 examples) [18.75s elapsed]; I0720 09:36:47.287455 47167827691328 make_examples_core.py:257] 7350149 candidates (8344123 examples) [4.37s elapsed]; I0720 09:37:06.694804 47167827691328 make_examples_core.py:257] 7352217 candidates (8346237 examples) [19.41s elapsed]; I0720 09:37:27.305462 47167827691328 make_examples_core.py:257] 7354095 candidates (8348169 examples) [20.61s elapsed]; I0720 09:37:48.644136 47167827691328 make_examples_core.py:257] 7356001 candidates (8350103 examples) [21.34s elapsed]. My question is: ; Is the long running time due to the complexity of my samples? Do I need to allocate more threads to speed up this process? As I recall make_examples is a single-thread program. . **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Yes; **Any additional context:**,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/683:5124,allocate,allocate,5124,,https://github.com/google/deepvariant/issues/683,1,['allocate'],['allocate']
Energy Efficiency,Adaptation to polyploid organisms.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/562:0,Adapt,Adaptation,0,,https://github.com/google/deepvariant/issues/562,1,['Adapt'],['Adaptation']
Energy Efficiency,Building DeepVariant on Power 8 with nvidia-docker,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/122:24,Power,Power,24,,https://github.com/google/deepvariant/issues/122,1,['Power'],['Power']
Energy Efficiency,"Dear,. I tried the combination of the DeepVariant and GLnexus and they are working perfectly fine. I would like to know is there a way to incrementally add to the output of the pipeline? of course, if all the previous gvcfs are available in case of new variants. I mean something like the genomicsDB which is now being used by GATK. This will be a great help in case of a large cohort because it can save a lot of time and computation power to calculate frequencies. Kind regards; Amin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/405:435,power,power,435,,https://github.com/google/deepvariant/issues/405,1,['power'],['power']
Energy Efficiency,"Hello guys,. following an email discussion ... I am unable to use a different model. The help seems to suggest one can set --model_type=WGS and then use --customized_model=""PATH_to_model.cpk"". ```; --model_type: <WGS|WES|PACBIO>: Required. Type of model to use for variant ; calling. Each model_type has an associated default model, which can be ; overridden by the --customized_model flag.; ```. But then the run produces. `""I0206 11:58:24.997612 140003716306688 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt""; `; which I interpret as deepvariant falling back on its default model. ; What I notice is that even with the customized-model flag, it doesn't run if I don't set up --model-type. It seems like when both flags are set (and there is no way to do otherwise) it gives priority to its default model, which is the opposite of the intended behaviour right? . Or is there something I am missing? . Thanks a lot for any help or suggestion (and sorry for the many messages, I just really want to try that model because it seems very promising and have it fit in my schedule)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/268:1089,schedul,schedule,1089,,https://github.com/google/deepvariant/issues/268,1,['schedul'],['schedule']
Energy Efficiency,"Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error?. My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/139:311,efficient,efficient,311,,https://github.com/google/deepvariant/issues/139,1,['efficient'],['efficient']
Energy Efficiency,"Hello! I've found a performance issue in deepvariant/data_providers.py: `batch()` should be called before `map()`, which could make your program more efficient. Here is [the tensorflow document](https://tensorflow.google.cn/guide/data_performance?hl=zh_cn#vectorized_mapping) to support it. Detailed description is listed below:. - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size, drop_remainder=True)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L316) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L314).; - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L364) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L362). Besides, you need to check the function called in `map()`(e.g., `self.parse_tfexample` called in `dataset.map()`) whether to be affected or not to make the changed code work properly. For example, if `self.parse_tfexample` needs data with shape (x, y, z) as its input before fix, it would require data with shape (batch_size, x, y, z). Looking forward to your reply. Btw, I am very glad to create a PR to fix it if you are too busy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/479:150,efficient,efficient,150,,https://github.com/google/deepvariant/issues/479,1,['efficient'],['efficient']
Energy Efficiency,"Hello, ; I think the answer is ""yes I could"". To be quick ... I have 25 datasets of high-coverage illumina data. But mapping them on the rotifer genomes is a nightmare, it seems there are many mismappings. But ... we finally start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant.; I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right?. E",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/716:421,power,powerful,421,,https://github.com/google/deepvariant/issues/716,1,['power'],['powerful']
Energy Efficiency,"Hello, I am trying to install DeepVariant on an IBM Power 8 machine within a docker container. I get the following error during ./build_and_test.sh, which I understand is tied to Intel SSE2 instruction set. `external/libssw/src/ssw.c:38:23: fatal error: emmintrin.h: No such file or directory`. I did `export DV_USE_GCP_OPTIMIZED_TF_WHL=0` from the command line before running the compile. I also changed `DV_COPT_FLAGS` to `--copt=-Wno-sign-compare --copt=-Wno-write-strings` within settings.sh (removing the corei7 option). I am using bazel version '0.15.0-' (settings.sh is changed to reflect this). I am using scikit-learn=0.20 (run-prereq.sh changed to reflect this). pyclif was compiled from source. Is there a way to circumvent this error? The complete error message is as follows. ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/libssw/BUILD.bazel:11:1: C++ compilation of rule '@libssw//:ssw' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction; -sections -fdata-sections -MD -MF bazel-out/ppc-opt/bin/external/libssw/_objs/ssw/external/libssw/src/ssw.pic.d -fPIC -iquote external/libssw -iquote bazel-out/ppc-opt/genfiles/external/libssw -iquote ext; ernal/bazel_tools -iquote bazel-out/ppc-opt/genfiles/external/bazel_tools -Wno-maybe-uninitial",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123:52,Power,Power,52,,https://github.com/google/deepvariant/issues/123,1,['Power'],['Power']
Energy Efficiency,"Hello, I am trying to run DeepVariant but ...; here is my command; `time seq 0 $((N_SHARDS-1)) |parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python bin/make_examples.zip --mode calling --ref ""${REF}"" --reads ""${BAM}"" --sample_name FalconSet --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. And here is the output. ```; When using programs that use GNU Parallel to process data for publication please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; and it won't cost you a cent.; Or you can get GNU Parallel without this requirement by paying 10000 EUR. To silence this citation notice run 'parallel --bibtex' once or use '--no-notice'. Computers / CPU cores / Max jobs to run; 1:local / 48 / 40. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 40 AVG: 0.00s local:40/0/100%/0.0s WARNING: Logging before flag parsing goes to stderr.; I0601 15:22:01.182291 140355759671040 make_examples.py:1024] Preparing inputs; 2018-06-01 15:22:01.188982: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0601 15:22:01.189755 140355759671040 genomics_reader.py:174] Reading ../Falcon_Unzip/out.bam with NativeSamReader; I0601 15:22:01.543628 140355759671040 make_examples.py:946] Common contigs are [u'000000F', u'000001F', u'000002F', u'000003F', u'000004F', u'000005F', u'000006F', u'000007F', u'000009F', u'000010F', u'000011F', u'000012F', u'000013F', u'000014F', u'000015F', u'000016F', u'000017F', u'000018F', u'000019F', u'000020F', u'000021F', u'000022F', u'000023F', u'000024F', u'000025F', u'000026F', u'000027F', u'000028F', u'000029F', u'000030F', u'000031F', u'000032F', u'000033F', u'000034F', u'000035F', u'000036F', u'000037F', u'000038F', u'000039F', u'000040F', u'000041F', u'000042F', u'000043F', u'000045F', u'000046F', u'000047F', u'000048F', u'000049F'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/77:501,Power,Power,501,,https://github.com/google/deepvariant/issues/77,1,['Power'],['Power']
Energy Efficiency,"Hello,. I've been using DeepTrio for de novo variant analysis, and it's been performing excellently. However, I noticed from the logs that DeepTrio uses the CPU to prepare data, which is quite time-consuming. In my case, it took 12 hours for one trio analysis, with an additional 4 hours on the GPU. Given that renting GPU servers( usually with less CPU) is more expensive than CPU servers and access to privately owned GPU servers is limited, it seems inefficient to run lengthy CPU processes on a GPU server. It feels like a bit of a waste, and sometimes I half-jokingly feel there might be someone out there with murderous intent because of it!. Would it be possible in future updates to partition the DeepTrio analysis into separate steps? This way, CPU-intensive tasks could be completed on a CPU server, and then the job could be transferred to a GPU server for the remaining tasks. Alternatively, could the data preparation (CPU) and analysis (GPU) be run at the same time? This would help optimize resource usage and reduce costs. Thank you for considering these suggestions.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/873:1025,reduce,reduce,1025,,https://github.com/google/deepvariant/issues/873,1,['reduce'],['reduce']
Energy Efficiency,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! (I realized posting in a new issue made more sense than adding to a closed issue). ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```. _Originally posted by @ksw9 in https://github.com/google/deepvariant/issues/243#issuecomment-579406829_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/265:1108,Power,Power,1108,,https://github.com/google/deepvariant/issues/265,1,['Power'],['Power']
Energy Efficiency,"Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:; ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```; rule deepvariant:; input:; bam=rules.apply_bqsr.output.bam,; ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'; output:; vcf=""results/deepvariant/{sample}.vcf.gz""; params:; model=""WES""; threads: ; 64; resources:; mem_mb=163840; log:; ""logs/deepvariant/{sample}/stdout.log""; singularity:; ""singularity/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:544,schedul,scheduler,544,,https://github.com/google/deepvariant/issues/602,1,['schedul'],['scheduler']
Energy Efficiency,"Hi Mark (@depristo),. Sorry, I meant to put this together a while ago - regarding https://github.com/google/deepvariant/issues/27#issuecomment-364683474 - but got a bit swamped with a research deadline. In any case, this is purely for intellectual curiosity and discussion. Regarding the first point, where differences in allocated CPUs might be the cause for the timing, that could be remedied by specifying a minimal CPU requirement, as noted here:. https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform. So to control for the variability in the test, the two options are either: a) to set the `--min-cpu-platform` setting to the maximum available (`""Intel Sandy Bridge""`), or b) to keep requesting and canceling instances until the desired one is allocated on which all tests should be performed, thus satisfying consistency. Just as a quick inspection, by looking at the CPU cycles utilization, I just ran a performance analysis of 0.4 and 0.5.1 on `make_examples` - since it displayed the initial discrepancy - and there seem to be some slight increases in `0.5.1`, which might cumulatively affect things. In any case, below is the top of the call-graph of percent utilization by method (per version):. #### DV 0.4. ```; # Samples: 186K of event 'cpu-clock'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--lea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:322,allocate,allocated,322,,https://github.com/google/deepvariant/issues/50,2,['allocate'],['allocated']
Energy Efficiency,"Hi all,; I have read the discussion about deep sequencing on issue #62. I have tried to modify three options, downsample_fraction, pileup_image_height, and vsc_min_fraction_snps for our deep sequencing data but it didn't work and output many false-positive calls. Here I want to train a new model for deep sequencing data with rare somatic mutation(MAF~1%) and there is some confusion.; 1) There is a maximum threshold for pileup_height of 362, can I modify it?; 2) There is only one training tutorial with Google cloud platform, is there any guideline for training with the Linux system?; 3) Can Deepvariant be adapted to a somatic mutation caller?; Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/308:613,adapt,adapted,613,,https://github.com/google/deepvariant/issues/308,1,['adapt'],['adapted']
Energy Efficiency,"Hi there,. I've been trying to figure out how to actually run deepvariant in a cluster environment but thus far, the instructions seems a little cryptic to me. ; Is there perhaps a step-by-step guide to running deepvariant on a cluster with a PBS scheduler for instance?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/474:247,schedul,scheduler,247,,https://github.com/google/deepvariant/issues/474,1,['schedul'],['scheduler']
Energy Efficiency,"Hi, ; I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". ; I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. ; FranÃ§ois",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/779:109,power,powerful,109,,https://github.com/google/deepvariant/issues/779,3,"['power', 'reduce']","['powerful', 'reduced']"
Energy Efficiency,"Hi,. I am running DV on CPU on a fragmented reference 3GB genome with pseudochromosomes + a very large number of small contigs. having these seems to eat up my RAM and I rapidly reach 250 of the 252 GB in use and my system freezes with oom. After adding a line to include only reference contigs larger than 500bps I got the command to work and it now uses only 100GB ram. ``--regions=/inref/${ref}_ge500.bed \``. I therefore edited this ticket (originally trying to allocate a max memory argument) and it this post can be closed and marked solved. best regards",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/674:466,allocate,allocate,466,,https://github.com/google/deepvariant/issues/674,1,['allocate'],['allocate']
Energy Efficiency,"Hi,. I am trying to run DeepVariant 1.2.0 on a few human samples PacBio HiFi data (about 30x coverage per sample). I first ran my samples through the [PEPPER-Margin pipeline r0.4](https://github.com/kishwarshafin/pepper) to get a haplotagged BAM file. Then I ran DeepVariant as follows:; ```; singularity exec -B ${SOME_PATHS} deepvariant_1.2.0.sif bash /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref ${PATH_TO_REF} --reads MARGIN_PHASED.PEPPER_SNP_MARGIN.happlotagged.bam --output_vcf sample.vcf.gz --output_gvcf sample.g.vcf.gz --num_shards 24 --make_examples_extra_args=""realign_reads=false,min_mapping_quality=5"" --sample_name MYSAMPLE --use-hp-information;; ```. I have two problems:; 1. Right from the beginning (`CALL VARIANT MODULE SELECTED`), for each interval processed. I get thousands of `READ TAG: n_elements is zero` messages in the console. What does it mean and is it a problem or just a warning?; 2. I allocate 200GB of RAM for per job and they all seem to systematically fail on memory. I do not recall DeepVariant using that much memory in the past but I might be wrong. Is 200GB too light for a human genome PacBio Hifi 30x coverage dataset?. Thank you for your help,; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/490:935,allocate,allocate,935,,https://github.com/google/deepvariant/issues/490,1,['allocate'],['allocate']
Energy Efficiency,"Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity.; Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/531:360,reduce,reduce,360,,https://github.com/google/deepvariant/issues/531,1,['reduce'],['reduce']
Energy Efficiency,"Hi,; I want to call variations on the Pacbio bam data to get gvcf files.; But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? ; Please help!; (No root permission;Centos7 x86;Only the user directory has read and write permission); ```; dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF; ```; ```; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemente",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/598:673,Power,Power,673,,https://github.com/google/deepvariant/issues/598,1,['Power'],['Power']
Energy Efficiency,"I am trying to install DeepVariant on an IBM Power 8 system within a docker container. The docker container has the following Bazel version installed: 0.15.0- (https://github.com/bazelbuild/bazel/releases/tag/0.15.0) . I installed tensorflow r1.11 from source inside the docker container for CPU-only execution. This same source-code is placed so that it is seen by the build-prereq.sh script. I set the `export DV_USE_PREINSTALLED_TF=1`. In settings.sh, I changed DV_BAZEL_VERSION to DV_BAZEL_VERSION=""0.15.0-"" (to match the bazel version above). I also removed the corei7 option in DV_COPT_FLAGS. . In build-prereq.sh, I hard-coded the following in: `DV_PLATFORM=""ubuntu-16""`, since `lsb_release` didn't match the case statement conditions there. The following is the result `lsb_release`.; root@1f07cee05809:~/deepvariant# lsb_release; LSB Version: core-9.20160110ubuntu0.2-noarch:core-9.20160110ubuntu0.2-ppc64el:security-9.20160110ubuntu0.2-noarch:security-9.20160110ubuntu0.2-ppc64el. After these changes, build-prereq.sh runs fine. However, build_and_test.sh fails with the following error:; (03:21:40) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:424:1: ClifProtoLibraryGeneration third_party/nucleus/protos/reads_pyclif.h failed (Exit 2): proto failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.cc -h bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.h '--strip_dir=bazel; -out/ppc-opt/genfiles' '--source_dir='\''.'\''' third_party/nucleus/protos/reads.proto); bazel-out/host/bin/external/clif/proto: 3: bazel-out/host/bin/external/clif/proto: __requires__: not found; bazel-out/host/bin/external/clif/proto: 4: bazel-out/host/bin/external/clif/proto: import: not found; bazel-out/host/bin/external/clif/proto: 5: bazel-out/host/bin/external/clif/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/122:45,Power,Power,45,,https://github.com/google/deepvariant/issues/122,1,['Power'],['Power']
Energy Efficiency,I am using deepvariant on our cluster using singularity.; I could setup singularity and get it running. My problem now is that the process is pretty slow and it takes more than a day for just one sample. However I allocate more cpu core and memory does not help.; Increasing the number of shard does not resolve it and even make it worse.; Could someone please give me some clue on how to find it solution for this issue?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463:214,allocate,allocate,214,,https://github.com/google/deepvariant/issues/463,1,['allocate'],['allocate']
Energy Efficiency,"I'm attempting to write a bam file of the realigned reads, as I'm seeing ADs in the vcf that do not line up with what is present in the input bam file. I'm mainly concerned with two specific locations in the genome. The full genome is approx 11 Gbp, the input bam file is about 190 GB, and the drive I'm attempting to output to has more than 4 TB available. When using `-emit_realigned_reads` and using `-realigner_diagnostics` to provide an output directory, the log file tells me that deepvariant attempts to write more than seven million bam files, making it impossible to access the directory before crashing due to running out of disk space. Is there some way of getting around this? I'm thinking either a more storage efficient way of getting the entire bam file, or a way of getting the bam file of the specific regions I'm interested in. I'm using deepvariant 1.0 from docker on Ubuntu 18.04. The data is short read Illumina data. . The command I'm using to run this:. ```; seq 0 $((60-1)) |\; parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \; --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \; --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord@60.gz \; --sample_name sample --task {} 2> results/sample/deepvariant/tmp/make_examples.log ; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/370:724,efficient,efficient,724,,https://github.com/google/deepvariant/issues/370,1,['efficient'],['efficient']
Energy Efficiency,"I'm trying to fine tune the original DeepVariant model with some extra data.; However, during the fine tuning process, the model suddenly losses all its predictive power in the first 10000 or 20000 steps. The call-variants output of these models are like all sites have homo-alt variants with a same qual value, 6.8 for example.; The sudden change in the model happens in the first step of fine tuning, as the saved model.ckpt-0 in the begining already gives the above output.; I find this result quite confusing, as the loss should increase dramatically. Is that a normal output of the fine tuning process?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185:164,power,power,164,,https://github.com/google/deepvariant/issues/185,1,['power'],['power']
Energy Efficiency,Model losses all predictive power in the first steps during fine tuning,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185:28,power,power,28,,https://github.com/google/deepvariant/issues/185,1,['power'],['power']
Energy Efficiency,"To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159:284,allocate,allocate,284,,https://github.com/google/deepvariant/pull/159,2,['allocate'],['allocate']
Energy Efficiency,"ain(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. Here is the supported CPU instructions of host:. ```sh; # cat /proc/cpuinfo; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 30; model name : Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz; stepping : 5; microcode : 0xa; cpu MHz : 1197.018; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 11; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs; bogomips : 5851.92; clflush size : 64; cache_alignment : 64; address sizes : 36 bits physical, 48 bits virtual; power management:; ```. I also tried create a env & install on host by `conda install -c bioconda deepvariant`, but it pop-up the same error.; And Deepvariant v0.10.0 also have the same error. Please kindly give me some advice about this thank you. Best,; Jerry",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345:4047,monitor,monitor,4047,,https://github.com/google/deepvariant/issues/345,2,"['monitor', 'power']","['monitor', 'power']"
Energy Efficiency,"arser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in fina",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:14751,Monitor,MonitoredSession,14751,,https://github.com/google/deepvariant/issues/537,2,['Monitor'],['MonitoredSession']
Energy Efficiency,"hub.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory â€˜/mnt/cdw-genomeâ€™: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:5706,Power,Power,5706,,https://github.com/google/deepvariant/issues/171,1,['Power'],['Power']
Energy Efficiency,"ib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:14608,Monitor,MonitoredSession,14608,,https://github.com/google/deepvariant/issues/537,2,['Monitor'],['MonitoredSession']
Energy Efficiency,"le deepvariant:; input:; bam=rules.apply_bqsr.output.bam,; ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'; output:; vcf=""results/deepvariant/{sample}.vcf.gz""; params:; model=""WES""; threads: ; 64; resources:; mem_mb=163840; log:; ""logs/deepvariant/{sample}/stdout.log""; singularity:; ""singularity/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided cores: 64; Rules claiming more threads will be scaled down.; Select jobs to execute... > [Wed Jan 4 18:30:51 2023]; > rule deepvariant:; > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta; > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz; > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log; > jobid: 0; > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2; > threads: 64; > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323; > ; > Activating singularity image singularity/deepvariant_1.4.0.sif; > INFO: Convert SIF",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:2294,schedul,scheduler,2294,,https://github.com/google/deepvariant/issues/602,1,['schedul'],['scheduler']
Energy Efficiency,"markDup.bam --examples /home/paul/exome-case-study/output/HG002.examples.tfrecord@1.gz --regions /home/paul/exome-case-study/input/data/refseq.coding_exons.b37.extended50.bed --task 0; Illegal instruction (core dumped); $; ```; After digging a bit deeper, I noticed that loading the `pileup_image_native` module was causing this issue. I was curious and looked at the assembly instructions:. ```; $ gdb -ex r --args python -c ""from deepvariant.python import pileup_image_native""; Program received signal SIGILL, Illegal instruction.; 0x00007ffff5d308b4 in google::protobuf::DescriptorPool::Tables::Tables() (); from /home/paul/make-examples/runfiles/genomics/deepvariant/python/../../_solib_k8/libexternal_Sprotobuf_Uarchive_Slibprotobuf.so; (gdb) disassemble $pc,$pc+32; Dump of assembler code from 0x7ffff5d308b4 to 0x7ffff5d308d4:; => 0x00007ffff5d308b4 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+676>: vpxor %xmm0,%xmm0,%xmm0; 0x00007ffff5d308b8 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+680>: lea 0x1b0(%rbx),%rax; 0x00007ffff5d308bf <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+687>: movl $0x0,0x1b0(%rbx); 0x00007ffff5d308c9 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+697>: movq $0x0,0x1b8(%rbx); End of assembler dump.; (gdb); ```. I noticed the `vpxor` instruction, which made me wonder if my CPU is enabled for AVX, so I proceeded as follows:. ```; $ grep flags /proc/cpuinfo; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 x2apic popcnt aes hypervisor lahf_lm; $; ```. This confirmed for me that I don't have AVX support. So it would be great for the examples that will drive usage and be used by many users to learn from, if the provided libraries are compiled with the bare-minimum of CPU qualities. I think it'll make it a bit easier for many users to adopt this nice pipeline. Thanks,; Paul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21:3394,monitor,monitor,3394,,https://github.com/google/deepvariant/issues/21,1,['monitor'],['monitor']
Energy Efficiency,"ng deepvariant. However, it's been three days and the program is still at 'make_examples' stage.; **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Operating system:; Redhat enterprise v7.9, x86_64; **Steps to reproduce:**; - Command:; BIN_VERSION=""1.5.0""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/HG38.fa \; --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=4 \; --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \; --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2""; I allocated 4 cores and 70GBs to run that program.; I added the VAF thresholds for SNPs and Indels because I read the reported issues:; https://github.com/google/deepvariant/issues/578; - Error trace: (if applicable); And here are some most recent results I got from stdout:; I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]; I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]; I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]; I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 7306537 candidates (8299251 examples) [113.76s elapsed]; I0720 09:30:47.913182 47167827691328 make_examples_core.py:257] 7309312 candidates (8302162 examples) [38.17s elapsed]; I0720 09:30:54.306647 47167827691328 make_examples_core.py:257] 7310398 candidates (8303278 examples) [6.39s elapsed]; I0720 09:31:09.601874 47167",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/683:1279,allocate,allocated,1279,,https://github.com/google/deepvariant/issues/683,1,['allocate'],['allocated']
Energy Efficiency,"nt/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session; return self._get_s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:19496,Monitor,MonitoredSession,19496,,https://github.com/google/deepvariant/issues/537,1,['Monitor'],['MonitoredSession']
Energy Efficiency,"om_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session; return self._get_session_manager().prepare_session(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:19640,Monitor,MonitoredSession,19640,,https://github.com/google/deepvariant/issues/537,1,['Monitor'],['MonitoredSession']
Energy Efficiency,"risk having version incompatibilities between our C++ code and the Python; # code we use at runtime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing; # value in the environment (allowing command line control of the build),; # defaulting to 0 (CPU only build).; export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file; # compiled with MKL support for corei7 or better chipsets, which; # significantly speeds up execution when running on modern CPUs. The default; # TensorFlow wheel files don't contain these instructions (and thereby run on a; # broader set of CPUs). Using this optimized wheel reduces the runtime of; # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud; # Platform) optimized wheel because all GCP instances have at least Sandy Bridge; # or better chipsets, so this wheel should run anywhere on GCP.; export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl""; export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow""; export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries.; # If you already have CUDA installed, such as on a properly provisioned; # Docker image, it shouldn't be necessary.; export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python); export USE_DEFAULT_PYTHON_LIB_PATH=1; export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-st",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:5692,reduce,reduces,5692,,https://github.com/google/deepvariant/issues/145,1,['reduce'],['reduces']
Energy Efficiency,"tart to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant.; I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right?. EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/716:1871,adapt,adapted,1871,,https://github.com/google/deepvariant/issues/716,1,['adapt'],['adapted']
Integrability," 2020-09-24 03:47:45.654628: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call; return fn(*args); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn; target_list, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:12711,message,message,12711,,https://github.com/google/deepvariant/issues/358,1,['message'],['message']
Integrability," \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Ker. For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0619 14:57:56.059498 47403021002560 call_variants.py:563] Total 1 writing processes started.; I0619 14:57:56.063244 47403021002560 dv_utils.py:370] From /tmp/make_examples.tfrecord-00000-of-00010.gz.example_info; I0619 14:57:56.063441 47403021002560 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0619 14:57:56.063909 47403021002560 call_variants.py:592] Use saved model: True; 2024-06-19 14:57:57.916727: F tensorflow/tsl/platform/env.cc:391] Check failed: -1 != path_length (-1 vs. -1); Fatal Python error: Aborted. Current thread 0x00002b1ce03a6740 (most recent call first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 414 in import_graph_def; File ""/usr/loc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:2197,depend,dependencies,2197,,https://github.com/google/deepvariant/issues/833,1,['depend'],['dependencies']
Integrability," absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0; Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0); Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0); Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objects: 100% (700/700), done.; remote: Compressing objects: 100% (111/111), done.; remote: Total",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:1106,depend,dependency,1106,,https://github.com/google/deepvariant/issues/739,1,['depend'],['dependency']
Integrability," help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1676,Depend,Depends,1676,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability," hooks.extend(self._convert_eval_steps_to_hooks(steps)); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks; raise ValueError('Must specify steps > 0, given: {}'.format(steps)); ValueError: Must specify steps > 0, given: 0. real	0m58.116s; user	0m1.590s; sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant; ADD ./bin /home/bin; ADD ./models /home/models; ADD ./oss_clif.ubuntu-16.latest.tgz /; WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh; RUN apt-get -y install python2.7; RUN apt-get -y install python-dev python-pip; RUN apt-get -y install parallel; RUN python2 -m pip install --upgrade --force-reinstall pip; RUN python2 -m pip install apache-beam; RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0; I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None; W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False; I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1; I0415 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:6124,message,message,6124,,https://github.com/google/deepvariant/issues/172,1,['message'],['message']
Integrability,"# OUTPUT_DIR=""${PWD}/quickstart-output"". # ls quickstart-testdata; NA12878_S1.chr20.10_10p1mb.bam ; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ; ucsc.hg19.chr20.unittest.fasta.gz.fai; NA12878_S1.chr20.10_10p1mb.bam.bai ; ucsc.hg19.chr20.unittest.fasta ; ucsc.hg19.chr20.unittest.fasta.gz.gzi; test_nist.b37_chr20_100kbp_at_10mb.bed ; ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ; ucsc.hg19.chr20.unittest.fasta.gz. # /home/d008/data/covid19/deepvarient/test# sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; ```. And here is the error message from docker. ```sh; ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0907 09:04:08.296450 140053878712064 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345:1205,message,message,1205,,https://github.com/google/deepvariant/issues/345,1,['message'],['message']
Integrability,"# this list of conditions and the following disclaimer.; #; # 2. Redistributions in binary form must reproduce the above copyright; # notice, this list of conditions and the following disclaimer in the; # documentation and/or other materials provided with the distribution.; #; # 3. Neither the name of the copyright holder nor the names of its; # contributors may be used to endorse or promote products derived from this; # software without specific prior written permission.; #; # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""; # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE; # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE; # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE; # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR; # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF; # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS; # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN; # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE); # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE; # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for; # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This; # will skip the installation of TensorFlow.; export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0; export TF_ENABLE_XLA=1; export TF_NEED_CUDA=1; export TF_NEED_GCP=1; export TF_NEED_GDR=0; export TF_NEED_HDFS=0; export TF_NEED_JEMALLOC=0; export TF_NEED_MKL=1; export TF_NEED_MPI=0; export TF_NEED_OPENCL=0; export TF_NEED_OPENCL_SYCL=0; export TF_NEED_S3=1; export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1; export TF_CUDA_VERSION=""10.0""; export CUDA_TOOLKIT_PATH=""/usr/local/cuda""; export TF_CUDNN_VERSION=""7""; export CUDNN",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:2366,CONTRACT,CONTRACT,2366,,https://github.com/google/deepvariant/issues/145,1,['CONTRACT'],['CONTRACT']
Integrability,"### Issue; When running the build-prereq shell script, I'm getting an error when the Tensorflow install begins. ### Error message; ```; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl...; - [1 files][ 41.1 MiB/ 41.1 MiB] 1.0 MiB/s ; Operation completed over 1 objects/41.1 MiB. ; tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.; ```. ### Debugging efforts; After browsing around a bit, I discovered that this issue was solved for some through installing the .whl separately. So, I download the whl from the [GCloud bucket](https://console.cloud.google.com/storage/browser/deepvariant/packages/tensorflow/) and executed `sudo python2.7 pip install <name of .whl file>` through the terminal. It ran, just to tell me â€œ.dist-info directory not foundâ€. I think this might be due to some inconsistency in the packages installed through the build-prereq.sh script, because I can see that all the packages that it installed (e.g. numpy) are for Python 3.5, but the Tensorflow version it's trying to get is for cp27 (Python 2.7). Not sure about where to go from here, would love some assistance :). ### System details; OS: Ubuntu 16.04 LTS; Python interpreters: Default with Ubuntu (2.7 and 3.5.2); Deep Variant version: Installed it today from the main repo, so probably r0.4.1. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/30:122,message,message,122,,https://github.com/google/deepvariant/issues/30,1,['message'],['message']
Integrability,"'/bin/bash', '-x', '/PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh']' returned non-zero exit status 1.; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 327, in _execute_actions; run_script(target_prefix, Dist(pkg_data), 'post-unlink' if is_unlink else 'post-link'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 513, in run_script; raise LinkError(message); conda.exceptions.LinkError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 281, in execute; pkg_data, actions); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 344, in _execute_actions; reverse_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:1641,message,messages,1641,,https://github.com/google/deepvariant/issues/252,1,['message'],['messages']
Integrability,"(versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Iâ€™m not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnâ€™t match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Iâ€™m not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)?. Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that?. **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:3548,depend,depending,3548,,https://github.com/google/deepvariant/issues/171,1,['depend'],['depending']
Integrability,"**Describe the issue:**. - I setup version 0.8. ( I have to stick to TF1.x ); - Modified the run-prereq.sh : to install some dependencies before installing tensor2tensor; - Modified the setings.sh : ""DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.14"" (the 1.13.1 was giving errors); - Followed [instruction ](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) to setup the INPUT_DIR & OUTPUT_DIR. ; - Then built the docker using ""docker run ."" ; - When inside docker, I run : . /opt/deepvariant/bin/make_examples --mode calling --ref ""/INPUT/ucsc.hg19.chr20.unittest.fasta"" --reads ""/INPUT/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/OUTPUT/tmp0cv1ybnt/make_examples.tfrecord@8.gz"" --gvcf ""/OUTPUT/tmp0cv1ybnt/gvcf.tfrecord@8.gz"" --regions ""chr20:10,000,000-10,010,000"". But, I get error shown in stack trace section (below). I can manually import tensorflow and print version on the terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/342:125,depend,dependencies,125,,https://github.com/google/deepvariant/issues/342,1,['depend'],['dependencies']
Integrability,"**Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55; `; Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588; `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0; BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**; - Operating system:; - DeepVariant version: 1.3.0, latest ; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**; - Command: Call variants with run_deepvariant wrapper script. ; - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/520:1408,wrap,wrapper,1408,,https://github.com/google/deepvariant/issues/520,1,['wrap'],['wrapper']
Integrability,"**Describe the issue:**. I am trying to build deepvariant on my machine that has Centos 7 and it seems there are no instructions to do that. All the instructions are based on Ubuntu operation system. I install all the dependencies but it seems there is no version of CLIF for Centos 7. So I tried building CLIF on my machine using clang/llvm 11.0.0. I get the following error during installation of that. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm:ðŸ†‘:GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequen",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/380:218,depend,dependencies,218,,https://github.com/google/deepvariant/issues/380,2,['depend'],['dependencies']
Integrability,"**Describe the issue:**; Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**; - Operating system: Amazon Linux 2023; - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda; - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**; - Command: `mamba install deepvariant -c bioconda`; - Error trace: ; ```; Pinned packages:; - python 3.10.*. Could not solve for environment specs; The following packages are incompatible; â””â”€ deepvariant is installable with the potential options; â”œâ”€ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require; â”‚ â””â”€ tensorflow 1.12.* , which does not exist (perhaps a missing channel);; â”œâ”€ deepvariant [0.10.0|1.0.0] would require; â”‚ â””â”€ tensorflow 2.0.* , which does not exist (perhaps a missing channel);; â”œâ”€ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require; â”‚ â””â”€ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;; â”œâ”€ deepvariant [0.7.1|0.7.2] would require; â”‚ â””â”€ tensorflow 1.11.* , which does not exist (perhaps a missing channel);; â””â”€ deepvariant [1.0.0|1.1.0|...|1.5.0] would require; â””â”€ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel).; ```. **Does the quick start test work on your system?**; N/A. **Any additional context:**; My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/736:145,depend,dependencies,145,,https://github.com/google/deepvariant/issues/736,1,['depend'],['dependencies']
Integrability,"**Describe the issue:**; Cannot install latest DeepVariant via Conda in my new environment. Error prompts, similar to that of #736. If having everything in default, I can have python=2.7 and deepvariant=0.7.0; but cannot update to 1.15 or latest. **Setup**; - Operating system: CentOS Linux release 7.4.1708 (ssh to university, docker unavailable); - DeepVariant version: 0.7.0 installable, but cannot get 1.15 or latest; - Installation method (Docker, built from source, etc.): conda; - Type of data: N/A. **Steps to reproduce:**; - Command:; $ create -n deepvariant python=3.8 (current version 3.8.19); $ conda install deepvariant. - Error trace: (if applicable). > #warning libmamba Problem type not implemented SOLVER_RULE_STRICT_REPO_PRIORITY _- many times_; > warning libmamba Added empty dependency for problem type SOLVER_RULE_UPDATE; > warning libmamba Problem type not implemented SOLVER_RULE_STRICT_REPO_PRIORITY _- many times_; > failed; > ; > LibMambaUnsatisfiableError: Encountered problems while solving:; > - package deepvariant-0.4.1-np113py27_0 requires python 2.7*, but none of the providers can be installed; > ; > Could not solve for environment specs; > The following packages are incompatible; > â”œâ”€ deepvariant is installable with the potential options; > â”‚ â”œâ”€ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require; > â”‚ â”‚ â””â”€ tensorflow 1.12.* , which conflicts with any installable versions previously reported;; > â”‚ â”œâ”€ deepvariant [0.10.0|1.0.0] would require; > â”‚ â”‚ â””â”€ tensorflow 2.0.* , which conflicts with any installable versions previously reported;; > â”‚ â”œâ”€ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require; > â”‚ â”‚ â””â”€ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;; > â”‚ â”œâ”€ deepvariant [0.7.1|0.7.2] would require; > â”‚ â”‚ â””â”€ tensorflow 1.11.* , which conflicts with any installable versions previously reported;; > â”‚ â””â”€ deepvariant [1.0.0|1.1.0|...|1.5.0] would require; > â”‚ â””â”€ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/835:795,depend,dependency,795,,https://github.com/google/deepvariant/issues/835,1,['depend'],['dependency']
Integrability,"**Describe the issue:**; Hello, I am trying to run the deepvariant RNA model on HG005 data locally (not restricted to chr20 like the Github example) but the model keeps generating this error while running the call_variants.py step. WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0626 13:39:06.145823 140632388314944 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:876,message,message,876,,https://github.com/google/deepvariant/issues/845,1,['message'],['message']
Integrability,"**Describe the issue:**; Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**; - Operating system: CentOS Linux 7 (Core); - Singularity version: 3.5-8.el7; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: WGS. **Steps to reproduce:**; - Command:; > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=${reference_genome} \; > --reads=${bam} \; > --regions=""chr1"" \; > --output_vcf=${vcf_dir}/${sample}.vcf.gz \; > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \; > --intermediate_results_dir ${tmp_dir} \; > --num_shards=${ncpu}. - Error trace: (if applicable); > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**; I also tried with `--no-home` flag which did not work at all. ; I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/601:307,message,message,307,,https://github.com/google/deepvariant/issues/601,1,['message'],['message']
Integrability,"**Describe the issue:**; I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : ; 1. non-parallel + bam âœ…; 2. non-parallel + cram âœ… ; 3. parallel + bam âœ… ; 4. non-parallel + cram ðŸ”´ . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**; - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64; - DeepVariant version: v1.6.0; - Installation method (Docker, built from source, etc.): HPC, sorry I don't know; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Not special, I used common toy data. **Steps to reproduce:**; - Command: ; ```; seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \; --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --channels insert_size \; --task {} \; || exit 1; ```; - Error trace: (if applicable); ```; META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/786:361,message,message,361,,https://github.com/google/deepvariant/issues/786,1,['message'],['message']
Integrability,"**Describe the issue:**; In ```make_examples```: The middle base of reference sequence in the window doesn't match first character of variant.reference_bases. **Setup**; - Operating system: CentOS Linux v7; - DeepVariant version: 1.1.0; - Installation method: Docker; - Type of data: WGS (Illumina 150nt pairs from GIAB HG002). **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes, it does.; Is there any way to reproduce the issue by using the quick start? No. **Any additional context:**; The goal is to call SNPs and indels in GIAB HG002 WGS data, and to compare the results with a truthset. High-confidence intervals and the truthset are at https://github.com/genome-in-a-bottle/giab_latest_release. Please see the attached bash script (command line) and output files. Two questions:; - Is ```make_examples``` parameterized correctly (see attached script and output files)?; - Can someone please explain what this error message means and suggest an appropriate approach to troubleshooting and fixing it?. [vcall.log](https://github.com/google/deepvariant/files/5858295/vcall.log); [vcall.sh.txt](https://github.com/google/deepvariant/files/5858303/vcall.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/411:996,message,message,996,,https://github.com/google/deepvariant/issues/411,1,['message'],['message']
Integrability,"**Describe the issue:**; Thank you so much for the great tool. . I'm working on a heterozygous mouse long-read RNA-seq dataset from PacBio and would like to perform variant call + phasing at read-level. I'm wondering whether you have some recommendations regarding the points below:; - I'm currently using `--model_type=PACBIO` with the bam files processed with `gatk SplitNCigarReads`. Does this model consider RNA editing? Or should I use `--model_type=WES`? I saw some discussions mentioning WES model considers RNA-editing in https://github.com/google/deepvariant/issues/775; - Is there anyway that I could integrate the known variants from genomic data into the variant calling? Or should it be integrated after `DeepVariant` variant call at vcf-level?. **Setup**; - Operating system: Ubuntu 2.20; - DeepVariant version: v1.6.1; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HiFi, mm10, long-read RNA-seq data. **Steps to reproduce:**; - Command:; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.1 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=GRCm38.primary_assembly.genome.fa \; --reads=SNCR.bam \; --output_vcf=output.vcf.gz \; --num_shards 16; ```. Thank you so much for your kind help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/890:611,integrat,integrate,611,,https://github.com/google/deepvariant/issues/890,2,['integrat'],"['integrate', 'integrated']"
Integrability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I am building the deep variant Dockerfile (v1.2) off the Databricks Runtime base image (Ubuntu 18.04).; Run into issues at Stage 'Install CLIF binary', I get the error,. `ModuleNotFoundError: No module named 'apt_pkg'`. I see in the build-prereq.sh script this comment,. `Build clif binary from scratch. Might not be ideal because it installs a; bunch of dependencies, but this works fine when we used this in a Dockerfile; because we don't do build-prereq.sh in the final image.`. Please advise how to get around this when building your own Docker Image. Cheers,. William. **Setup**; - Operating system: Ununtu 18.04; - DeepVariant version: 1.2; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476:476,depend,dependencies,476,,https://github.com/google/deepvariant/issues/476,1,['depend'],['dependencies']
Integrability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**; Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**; - Operating system: Centos7; - DeepVariant version: 1.2.0, 1.3.0, latest; - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**; - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16; - ; - Error trace: (if applicable). Command output:; sys.exit(main(argv)); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main; proto_utils.uses_fast_cpp_protos_or_die(); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die; raise ValueError('Expected to be using C++ protobuf implementation '; ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python; Traceback (most recent call last):; File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>; app.run(main); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499:154,message,message,154,,https://github.com/google/deepvariant/issues/499,1,['message'],['message']
Integrability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes. **Describe the issue:**; I was following the quick start guide for running singularity on a gpu node. Initially, I encounter the dynamic cast failed error similar to #559 . After installing the google-nucleus package, I encountered this new error about protobuf package. I tried protobuf version 3.20.3 and 4.21.9, but the error message is the same. In order to run DeepVariant successfully, what additional packages (version) should I install besides cloning the singularity image?. **Setup**; - Operating system: ; - DeepVariant version: 1.4.0; - Installation method: singularity; - Type of data: quick start test; ; **Steps to reproduce:**; ; ```; SINGULARITY_TMPDIR=/scratch/midway3/weilu1/tmp SINGULARITY_CACHEDIR=/scratch/midway3/weilu1/cache singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_1.4.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1. INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580:426,message,message,426,,https://github.com/google/deepvariant/issues/580,1,['message'],['message']
Integrability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; The postprocess_variants step fails with following error message:; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Setup**; - Operating system: CentOS Linux 7 (Core); - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio Sequencing. **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main; sample_name = get_sample_name(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name; _, record = get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record; raise ValueError(; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; ???. **Any additional context:**; Yes. I can change the parameter ""--infile"" of the postprocess_variants.py call from ""./call_variants_output.tfrecord.gz"" to ""./call_variants_output@1.tfrecord.gz"" and it works. Anyway, the cal",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818:176,message,message,176,,https://github.com/google/deepvariant/issues/818,1,['message'],['message']
Integrability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; I can't pull the container using singularity. **Setup**; - Operating system: Ubuntu; - Installation method (Docker, built from source, etc.): tried with singularity; My system has singularity installed, I tried getting the container but it failed:. I used this code; ```; BIN_VERSION=""1.6.1""; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; ```. and It got several warning messages:. WARNING: pull for Docker Hub is not guaranteed to produce the; WARNING: same image on repeated pull. Use Singularity Registry; WARNING: (shub://) to pull exactly equivalent images.; /usr/bin/env: â€˜pythonâ€™: No such file or directory; Cleaning up...; ERROR: pulling container failed!. I was thinking of installing DeepVariant with conda, but the version in conda is 1.5.0, and I was told to always run the most up-to-date one.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/886:512,message,messages,512,,https://github.com/google/deepvariant/issues/886,1,['message'],['messages']
Integrability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes; **Describe the issue:**; This isn't a code problem, but rather a documentation issue. I've run DeepVariant via your docker with success. To integrate it with our project I would like to install it via conda. I was able to do that but it isn't clear how to run deep variant. Do you have documentation/examples of what commands to send? . When using docker, we invoke the google/deepvariant:1.6.1 image and send it the command ""/opt/deepvariant/bin/run_deepvariant"" with appropriate arguments. What do we run when using conda? . Note the docs/deepvariant-quick-start.md has examples for docker (very useful and they work with our data) but nothing for conda. **Setup**; - Operating system: linux; - DeepVariant version: 1.5.0 (latest from conda); - Installation method (Docker, built from source, etc.): conda; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. Do you have plans to update conda with the latest deepvariant version? It is still at 1.5.0. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/865:239,integrat,integrate,239,,https://github.com/google/deepvariant/issues/865,1,['integrat'],['integrate']
Integrability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; I am struggling to get DeepTrio to run to completion on a small dataset. It completes at the end of call_variants.py but my system just collapses when at postprocess_variants.; Through using --dry_run=true, I'm able to keep going only after being sufficiently confident the last step has completed without error.; So in short, is it possible to re-run the wrapper command and have the analysis pipeline pick up where it left off? . **Setup**; - Operating system: Rocky Linux 8; - DeepVariant version: 1.6; - Installation method (Docker, built from source, etc.): through Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) illumina, 151bp, same reference as case studies; - RAM 64 GB; - CPUs 32 (c6i.8xlarge). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes they do. they complete because they are small. ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; Unfortunately, i cant run it on g4dn.8xlarge available to me since that EC2 running Amazon Linux 2, and GPU DeepVariant seems to need Ubuntu.; In short, a ""step_x_completed"" sentinel file at end of each step would be great IMO. . Thanks,; -Daniel",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/749:473,wrap,wrapper,473,,https://github.com/google/deepvariant/issues/749,1,['wrap'],['wrapper']
Integrability,"**Hello, I ran deep variant for mutation calling of single cell iso seq bam file (Pacbio long read rna-seq), and I got this error messages:**; `9-03 12:26:09.055921: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/44871400: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.055938: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/22677071: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.055954: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/31298741: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.055967: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/31321506: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.055980: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/31927218: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.055993: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/25969828: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056006: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/6236354: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056021: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/32560810: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056034: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/12686626: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056046: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/12686630: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056060: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/50928829: NOT_FOUN",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/877:130,message,messages,130,,https://github.com/google/deepvariant/issues/877,1,['message'],['messages']
Integrability,"- Command:. ```; udocker run \; -v ${INPUT_DIR}:""/input"" \; -v ${OUTPUT_DIR}:""/output"" \; DeepVariant \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \; --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=16; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants; output_queue = multiprocessing.Queue(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:1844,depend,dependencies,1844,,https://github.com/google/deepvariant/issues/733,1,['depend'],['dependencies']
Integrability,". I tried running ""_run_deepvariant_keras.py_"" script from the latest release of deepvaraint and faced some issues while running the keras-based call variant module. . I used the following command to run:. `; python bazel-out/k8-opt/bin/deepvariant/run_deepvariant_keras.py --model_type=WGS --ref=ref.fa --reads=reads.bam --regions ""chr19"" --output_vcf=${OUTPUT_DIR}/output.vcf.gz --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz --intermediate_results_dir ${OUTPUT_DIR}/intermediate_results_dir --num_shards=10; `. The above command successfully ran for _make_example_ module and failed at _call_varaint_keras.py_ with the following error message.. -------------. _Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 399, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 312, in run_; _run_main(main, args); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 387, in main; call_variants(; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 344, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/636:1038,message,message,1038,,https://github.com/google/deepvariant/issues/636,1,['message'],['message']
Integrability,"/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 257, in get_master; return self.master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 241, in master; cluster_spec = self.cluster_spec(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 311, in cluster_spec; network_endpoints = self._cloud_tpu_client.network_endpoints(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 320, in network_endpoints; response = self._fetch_cloud_tpu_metadata(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 234, in _fetch_cloud_tpu_metadata; service = self._tpu_service(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < impor",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:2819,depend,dependency,2819,,https://github.com/google/deepvariant/issues/469,1,['depend'],['dependency']
Integrability,"/input/wes2_38_3col.sorted.bed --task 2. I have ran the following command with a successful docker installation:; 	BIN_VERSION=""1.2.0"". 	sudo docker run \; 	-v ""${PWD}/input"":""/input"" \; 	-v ""${PWD}/output"":""/output"" \; 	-v ""${PWD}/reference"":""/reference"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type WES \; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta \; 	--reads /input/wes_deepvarfast_38.sorted.bam \; 	--regions /input/wes2_38_3col.sorted.bed \; 	--output_vcf /output/output_38.vcf.gz \; 	--output_gvcf /output/output_38.g.vcf.gz \; 	--num_shards=8 \; 	--intermediate_results_dir /output/intermediate_results_dir; with bam and bed files I've created of my own sample (paired end sequencing result of a human genome). The alignment of the bam file was successful (used bwa and samtools) and created the bed file out of the bam file by bedtools. . I've further checked FAQ and tried to run the following command, to better understand what is the error or where it fails:; 	BIN_VERSION=""1.2.0"". 	sudo docker run; 	-v ""${PWD}/input"":""/input""; 	-v ""${PWD}/output"":""/output""; 	-v ""${PWD}/reference"":""/reference""; 	google/deepvariant:""${BIN_VERSION}""; 	/opt/deepvariant/bin/make_examples; 	--mode calling; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta; 	--reads /input/wes_deepvarfast_38.sorted.bam; 	--examples ""/output/make_examples.tfrecord@1.gz""; 	--gvcf ""/output/gvcf.tfrecord@1.gz""; 	--regions ""/input/wes2_38_3col.sorted.bed"" \. However I get no error message, some lines of this kind are printed: ""Adding interval chr1:1523790-1523940 to intervaltree"" and than it finishes without creating any files. Any Idea of what happens and how can I make deepvariant work on my sample and create a vcf file?. (**Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes it works)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483:2033,message,message,2033,,https://github.com/google/deepvariant/issues/483,1,['message'],['message']
Integrability,"/opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref $REFERENCE \; --reads $BAM_FILE \; --regions 6:32509320-32669663 \; --output_vcf $VCF_FILE \; --num_shards 12; done; ``` . - Error trace: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference_HLA/chr6_hg19.fa"" --reads ""./MDC05_1463_3.final.bam"" --examples ""/tmp/7361351.1.gpu.q/tmpzsp9g_vq/make_examples.tfrecord@12.gz"" --channels ""insert_size"" --regions ""chr6:32509320-32669663"" --task {}. [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when serializing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; Traceback (most recent call last):; File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>; app.run(main); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 220, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 157, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/870:2140,protocol,protocol,2140,,https://github.com/google/deepvariant/issues/870,1,['protocol'],['protocol']
Integrability,"/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz; 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:; 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op.; I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op.; I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt.; 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory.; 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory.; 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory.; 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory.; 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory.; I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1; I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt.; I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for final step: 0.0037548377. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. real	3m6.726s; user	2m58.710s; sys	0m25.780s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:124714,depend,depend,124714,,https://github.com/google/deepvariant/issues/172,1,['depend'],['depend']
Integrability,"0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format.; WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 b",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:1060,depend,dependencies,1060,,https://github.com/google/deepvariant/issues/722,1,['depend'],['dependencies']
Integrability,"14.875045 139621316941568 tf_logging.py:82] Recording summary at step 0.; INFO:tensorflow:global step 1: loss = 0.2608 (4.963 sec/step); I0502 10:59:15.326091 139632719935232 tf_logging.py:82] global step 1: loss = 0.2608 (4.963 sec/step); 2018-05-02 10:59:15.584978: E tensorflow/core/kernels/check_numerics_op.cc:157] abnormal_detected_host @0x104ef6dce00 = {1, 0} LossTensor is inf or nan; 2018-05-02 10:59:15.615399: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: LossTensor is inf or nan : Tensor had NaN values; [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](control_dependency_4)]]; INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, LossTensor is inf or nan : Tensor had NaN values; [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](control_dependency_4)]]; [[Node: train_op/control_dependency/_5647 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_14228_train_op/control_dependency"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]; Caused by op u'train_op/CheckNumerics', defined at:; File ""/tmp/Bazel.runfiles_ecWAzH/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 364, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_ecWAzH/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 352, in main; parse_and_run(); File ""/tmp/Bazel.runfiles_ecWAzH/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 314, in parse_and_run",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69:3862,message,message,3862,,https://github.com/google/deepvariant/issues/69,1,['message'],['message']
Integrability,"28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:2742,rout,routines,2742,,https://github.com/google/deepvariant/issues/231,1,['rout'],['routines']
Integrability,"3vf8mpw9/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs"". 2024-01-05 15:55:31.140705: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:55:31.140953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-01-05 15:55:38.664328: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:55:38.709242 140372734228288 call_variants.py:471] Total 1 writing processes started.; I0105 15:55:38.765925 140372734228288 dv_utils.py:365] From /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0105 15:55:38.766286 140372734228288 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0105 15:55:38.768594 140372734228288 call_variants.py:510] Use saved model: True; I0105 15:56:02.220975 140372734228288 dv_utils.py:365] From /opt/models/wgs/example_info.json: Shape ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:13465,depend,dependencies,13465,,https://github.com/google/deepvariant/issues/761,1,['depend'],['dependencies']
Integrability,"49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 281, in execute; pkg_data, actions); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 344, in _execute_actions; reverse_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:2359,message,messages,2359,,https://github.com/google/deepvariant/issues/252,1,['message'],['messages']
Integrability,"6%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.16%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.12%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.58%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.62%--deepvariant_core_python_sam__reader_clifwrap::pySamIterable::wrapNext; | | ; | --0.57%--google::protobuf::python::cmessage::DeepCopy; | | ; | --0.56%--google::protobuf::python::cmessage::MergeFrom; | | ; | --0.56%--google::protobuf::Message::MergeFrom; | | ; | --0.52%--google::protobuf::internal::ReflectionOps::Merge; | ; |--1.92%--0x903b40; | | ; | --1.78%--PyEval_EvalFrameEx; | ; |--1.09%--0x905d60; | | ; | --1.09%--PyEval_EvalFrameEx; | ; |--0.78%--0x8fecc0; | PyEval_EvalFrameEx; | ; |--0.62%--0x905200; | | ; | --0.62%--PyEval_EvalFrameEx; | ; --0.54%--0x9060a0; | ; --0.54%--PyEval_EvalFrameEx. 33.23% , 0.00% ,python ,[unknown] ,[.] 0x00000000009060a0; |; ---0x9060a0; | ; --32.46%--PyEval_EvalFrameEx; | ; --31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | ; --30.63%--StripedSmithWaterman::Aligner::Align; | ; |--28.27%--ssw_align; | | ; | |--14.88%--sw_sse2_word; | | ; | |--8.45%--sw_sse2_byte; | | ; | |--2.89%--banded_sw; | | ; | --1.19%--__memcpy_sse2_unaligned; | ; --1.38%--ssw_init; | ; --0.92%--qP_byte. 31.13% , 0.08% ,python ,libssw_cclib.so ,[.] deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | ; --31.05%--deepvariant_realigner_python_ssw_clifwrap::p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:8923,Message,Message,8923,,https://github.com/google/deepvariant/issues/50,1,['Message'],['Message']
Integrability,"6737.31s elapsed]; I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526.; I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]; I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234.; I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]; I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s; user	632m52.969s; sys	6m20.594s; INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:; ```; Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.; ```; This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:12769,message,message,12769,,https://github.com/google/deepvariant/issues/614,2,['message'],"['message', 'messages']"
Integrability,"9ec44bd Replace C++ `#import <...>` with `#include <...>`; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:4715,message,message,4715,,https://github.com/google/deepvariant/issues/739,1,['message'],['message']
Integrability,":59:10.099534 139621333726976 tf_logging.py:82] Saving checkpoint to path /tmp/deepvariant/model.ckpt; INFO:tensorflow:Starting Queues.; I0502 10:59:10.102293 139632719935232 tf_logging.py:82] Starting Queues.; INFO:tensorflow:global_step/sec: 0; I0502 10:59:13.668776 139621325334272 tf_logging.py:121] global_step/sec: 0; INFO:tensorflow:Recording summary at step 0.; I0502 10:59:14.875045 139621316941568 tf_logging.py:82] Recording summary at step 0.; INFO:tensorflow:global step 1: loss = 0.2608 (4.963 sec/step); I0502 10:59:15.326091 139632719935232 tf_logging.py:82] global step 1: loss = 0.2608 (4.963 sec/step); 2018-05-02 10:59:15.584978: E tensorflow/core/kernels/check_numerics_op.cc:157] abnormal_detected_host @0x104ef6dce00 = {1, 0} LossTensor is inf or nan; 2018-05-02 10:59:15.615399: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: LossTensor is inf or nan : Tensor had NaN values; [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](control_dependency_4)]]; INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, LossTensor is inf or nan : Tensor had NaN values; [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](control_dependency_4)]]; [[Node: train_op/control_dependency/_5647 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_14228_train_op/control_dependency"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]; Caused by op u'train_op/CheckNumerics', defined at:; File ""/tmp/Bazel.runfiles_ecWAzH/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 364, in <module>; tf.app.run(); File ""/usr/local/li",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69:3517,message,message,3517,,https://github.com/google/deepvariant/issues/69,1,['message'],['message']
Integrability,Cannot build on Centos 7 due to CLIF dependency,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/380:37,depend,dependency,37,,https://github.com/google/deepvariant/issues/380,1,['depend'],['dependency']
Integrability,"Context: issue #116 . Htslib integration with GCS doesn't load app default credential from worker, and thus is only able to read from public bucket. The workaround is to localize BED file into VM worker prior running make_examples.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/119:29,integrat,integration,29,,https://github.com/google/deepvariant/issues/119,1,['integrat'],['integration']
Integrability,"DV} \; --model_type=WES \; --customized_model=/autofs/bal34/xyu/run_software/dv_illu/model/model.ckpt \; --ref ${REF_FILE_PATH} \; --reads {1} \; --output_vcf ${BASE_DIR}/{2}/output.vcf.gz \; --num_shards 30 \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir ${BASE_DIR}/{2}/intermediate_results_dir; ```; - Error trace: (if applicable); ```; WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; a",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:1663,message,message,1663,,https://github.com/google/deepvariant/issues/857,1,['message'],['message']
Integrability,"Dear Deepvariant team,. I was attempting to run Deepvariant on GCP by following the sample scripts from the tutorials, but it failed. I have checked the configuration regarding the Compute Engine quota and it should meet the requirements (i.e. CPU, Persistent Disk and In-use IP addresses). The error message from the log is like:; ""RuntimeError: Job failed with error ""run"": operation ""projects/deepvariant-phh/operations/7761698599878123803"" failed: executing pipeline: Execution failed: action 2: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION)"". I have read some of the related discussed issues but still can't solve my problem. The log files and my script file are attached. Your help is appreciated. . [staging_temp%2Frunner_logs_20181118_014355.log](https://github.com/google/deepvariant/files/2592663/staging_temp.2Frunner_logs_20181118_014355.log); [log.txt](https://github.com/google/deepvariant/files/2592666/log.txt). [script.txt](https://github.com/google/deepvariant/files/2592665/script.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/120:301,message,message,301,,https://github.com/google/deepvariant/issues/120,1,['message'],['message']
Integrability,"Dear Developers,. Thank you very much for the amazing tool! I recently encountered an issue while running DeepVariant on a GIAB sample. I managed to run DeepVariant open another sample successfully so I believe there may be something wrong in the current sample input. It would be most helpful and appreciated if you could kindly take a look at the error messages. **Setup**; - Operating system: CentOS 7 x86_64; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): Singularity (v3.10.0); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Input BAM was downsampled 10-fold to 30X. **Steps to reproduce:**; - Command:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/ \; -B /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/novoalign_bam/:/input_reads \; -B /paedyl01/disk1/louisshe/out/GIAB/HG005/heterozygous_deletions/heterozygous_sites/:/output \; -B /tmp:/tmp \; -B /paedyl01/disk1/louisshe/ref/hs37d5:/ref/hs37d5 \; -B /paedyl01/disk1/louisshe/ref/hg19:/ref/hg19 \; --home /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/ \; --contain \; /paedyl01/disk1/louisshe/tools/DeepVariant/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) h",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:355,message,messages,355,,https://github.com/google/deepvariant/issues/833,1,['message'],['messages']
Integrability,"Dear all. I am new to deepvariant. We are trying to use deepvariant on a HPC cluster with singularity.; We installed nvidia and cuda drivers through conda, and tested it with other python programs that used gpu with success.; I also managed to run the CPU version with deepvariant with singularity with success. ; However when running deepvariant on a gpu node with the following command, deepvariant complained that certain libraries are not found which prevented it from using the GPU:. `apptainer run --nv -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3 \; /public/software/deepvariants/1.6.0/gpuver/deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$REF \; --reads=""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" \; --regions ""NC_037590.1:200,000-950,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --num_shards=2`. Error messages:; `==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enabl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:969,message,messages,969,,https://github.com/google/deepvariant/issues/761,1,['message'],['messages']
Integrability,"Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format.; WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_n",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:95,message,messages,95,,https://github.com/google/deepvariant/issues/722,1,['message'],['messages']
Integrability,"Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:7103,Depend,Depends,7103,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability,Hello DeepVariant team. We're trying to use your great tool and I'm creating our own Docker for this.; Yesterday I finally fixed all issues with v0.8.0 version - all tests from build_and_test.sh passed and saw v0.9.0 release happened several days before - I tried to switch to in and get error at build_and_test.sh stage:; ```; deepvariant/variant_calling.cc:36:20: fatal error: optional: No such file or directory; #include <optional>; ^; compilation terminated.; ```; Looks like you added new include lines in `deepvariant/variant_calling.cc` between releases - [this](https://github.com/google/deepvariant/blob/r0.9/deepvariant/variant_calling.cc#L36) line causing my error.; Can you help me please? Do I need to install some external dependencies or what?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/236:738,depend,dependencies,738,,https://github.com/google/deepvariant/issues/236,1,['depend'],['dependencies']
Integrability,"Hello guys,. following an email discussion ... I am unable to use a different model. The help seems to suggest one can set --model_type=WGS and then use --customized_model=""PATH_to_model.cpk"". ```; --model_type: <WGS|WES|PACBIO>: Required. Type of model to use for variant ; calling. Each model_type has an associated default model, which can be ; overridden by the --customized_model flag.; ```. But then the run produces. `""I0206 11:58:24.997612 140003716306688 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt""; `; which I interpret as deepvariant falling back on its default model. ; What I notice is that even with the customized-model flag, it doesn't run if I don't set up --model-type. It seems like when both flags are set (and there is no way to do otherwise) it gives priority to its default model, which is the opposite of the intended behaviour right? . Or is there something I am missing? . Thanks a lot for any help or suggestion (and sorry for the many messages, I just really want to try that model because it seems very promising and have it fit in my schedule)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/268:988,message,messages,988,,https://github.com/google/deepvariant/issues/268,1,['message'],['messages']
Integrability,"Hello, ; I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this?. `docker pull google/deepvariant:deeptrio-1.5.0`; ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply.; Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/718:167,message,message,167,,https://github.com/google/deepvariant/issues/718,1,['message'],['message']
Integrability,"Hello, I am trying to install DeepVariant on an IBM Power 8 machine within a docker container. I get the following error during ./build_and_test.sh, which I understand is tied to Intel SSE2 instruction set. `external/libssw/src/ssw.c:38:23: fatal error: emmintrin.h: No such file or directory`. I did `export DV_USE_GCP_OPTIMIZED_TF_WHL=0` from the command line before running the compile. I also changed `DV_COPT_FLAGS` to `--copt=-Wno-sign-compare --copt=-Wno-write-strings` within settings.sh (removing the corei7 option). I am using bazel version '0.15.0-' (settings.sh is changed to reflect this). I am using scikit-learn=0.20 (run-prereq.sh changed to reflect this). pyclif was compiled from source. Is there a way to circumvent this error? The complete error message is as follows. ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/libssw/BUILD.bazel:11:1: C++ compilation of rule '@libssw//:ssw' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction; -sections -fdata-sections -MD -MF bazel-out/ppc-opt/bin/external/libssw/_objs/ssw/external/libssw/src/ssw.pic.d -fPIC -iquote external/libssw -iquote bazel-out/ppc-opt/genfiles/external/libssw -iquote ext; ernal/bazel_tools -iquote bazel-out/ppc-opt/genfiles/external/bazel_tools -Wno-maybe-uninitial",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123:766,message,message,766,,https://github.com/google/deepvariant/issues/123,1,['message'],['message']
Integrability,"Hello,. I am facing the below error when running built_and_test.sh. I have protocol buf built and installed in my home directory. (19:11:12) ERROR: /uufs/chpc.utah.edu/common/home/u1142888/deepvariant/bazel_tmp/_bazel_u1142888/bc41070ad1d30708841b968fbd6bc540/external/com_google_protobuf/BUILD:104:1: C++ compilation of rule '@com_google_protobuf//:protobuf' failed (Exit 1): gcc failed: error executing command . I have built the latest protobuf from source following the instructions [here](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md) and have installed it under $HOME/protobuf and updated my PATH and LD_LIBRARY_PATH accordingly. Looking for some pointers on how to resolve this issue by tweaking the build procedure. Thank you,; Ram",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94:75,protocol,protocol,75,,https://github.com/google/deepvariant/issues/94,2,['protocol'],"['protocol', 'protocolbuffers']"
Integrability,"Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. ; Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,; Saurabh",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/607:167,depend,dependencies,167,,https://github.com/google/deepvariant/issues/607,1,['depend'],['dependencies']
Integrability,"Hello,. I am trying to build deepavariant on a HPC node on which all the required dependency is met except pyclif. I do not have root privileges to install it under /usr/local/clif. Hence I downloaded pyclif source code and ran the INSTALL.sh to get it successfully built and installed under $HOME and activated the pyclif virtualenv. . (clif) [test-node]$ which pyclif; ~/opt/clif/bin/pyclif. However build_and_test.sh fails with the below error. Any changes required to deepvariant build setup to pick up the pyclif installation in my home directory?; (18:02:14) ERROR: missing input file '@clif//:clif/bin/pyclif'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/93:82,depend,dependency,82,,https://github.com/google/deepvariant/issues/93,1,['depend'],['dependency']
Integrability,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/443:1041,message,message,1041,,https://github.com/google/deepvariant/issues/443,2,['message'],['message']
Integrability,"Hello,. I tried running ""_run_deepvariant_keras.py_"" script from the latest release of deepvaraint and faced some issues while running the keras-based call variant module. . I used the following command to run:. `; python bazel-out/k8-opt/bin/deepvariant/run_deepvariant_keras.py --model_type=WGS --ref=ref.fa --reads=reads.bam --regions ""chr19"" --output_vcf=${OUTPUT_DIR}/output.vcf.gz --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz --intermediate_results_dir ${OUTPUT_DIR}/intermediate_results_dir --num_shards=10; `. The above command successfully ran for _make_example_ module and failed at _call_varaint_keras.py_ with the following error message.. -------------. _Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 399, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 312, in run_; _run_main(main, args); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 387, in main; call_variants(; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 344, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/636:641,message,message,641,,https://github.com/google/deepvariant/issues/636,1,['message'],['message']
Integrability,"Hello,. I'm trying to run DeepVariant using ultima data (cram file).; I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page.; But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/711:213,message,message,213,,https://github.com/google/deepvariant/issues/711,1,['message'],['message']
Integrability,"Hello,. I've been trying to set up the **google/deepvariant:1.6.1-gpu** or **google/deepvariant:latest-gpu** image on a GPU instance, but I've encountered the error message mentioned below when running the **run_deepvariant** or **train** scripts, and despite generating the flags (screenshot) as expected, I believe those incompatible/missing TensorRT libraries are preventing these scripts from using the GPU. **Command used:** ; ` sudo docker run --runtime=nvidia --gpus 1 google/deepvariant:1.6.1-gpu train --help; `. **Error message:**; ```; 2024-05-08 15:11:26.358196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-05-08 15:11:26.358229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; ```. ![image](https://github.com/google/deepvariant/assets/169280348/fd17bf4e-0b6c-46b7-b5e8-74a3525d07a5)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819:165,message,message,165,,https://github.com/google/deepvariant/issues/819,2,['message'],['message']
Integrability,"Hello,; I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong?; My cmd:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref ${fasta} \; --reads ${Input.bam} \; --output_vcf output/output.vcf.gz \; --output_gvcf output/output.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir output/intermediate_results_dir \; --regions chr20 \; --customized_model model/weights-51-0.995354.ckpt; ```. Error message:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco; rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started.; I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]; I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False; /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725:94,message,message,94,,https://github.com/google/deepvariant/issues/725,2,['message'],['message']
Integrability,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:280,depend,dependency,280,,https://github.com/google/deepvariant/issues/489,6,"['Depend', 'depend']","['Depends', 'dependencies', 'dependency']"
Integrability,"Helloï¼ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5.; First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant.; I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp.; Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py.; The final outcome is very good, but I find a detail didn't make sense: as the bed lengthenedï¼Œthe SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below.; ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png); Can you give me a detailed explanation of this detailï¼Ÿ Thank you very muchï¼ ; Finally, thank you very much for developing such a great toolï¼",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/616:225,protocol,protocol,225,,https://github.com/google/deepvariant/issues/616,1,['protocol'],['protocol']
Integrability,"Hey guys,. I got deepvariant installed with conda fine, but my run failed by needing glibc, when I installed glibc all processes get a segmentation fault, if I remove glibc it works until failing needing the dependency. Any advice? We can't install docker images on our HPC, I haven't tried converting docker to singularity, as I have no experience with that but I'm comfortable compiling from source, but couldn't find the right files/instructions. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/179:208,depend,dependency,208,,https://github.com/google/deepvariant/issues/179,1,['depend'],['dependency']
Integrability,"Hi . I am a Newbie/Student with some ""best practices"" questions. I am using the docker version 0.9.0 and following the script in the quick start example. I have two data sets of Illumina reads one was from a control group, the other from a treatment group. I think the number of reads is comparable. One batch has been generating examples for over 3.5 days. The other batch is running on 32 vCPU machine. It is still generating images after 1.5 days. I checked the OS level stats, the machines are not swapping. all cpu's are at 100%. Still alot of unused memory and disk. 1) Any idea how I might estimate the expected run time? . 2) Any idea of how I do a better job sizing compute resources?. 3) How do I know if docker is making progress or not?; I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. 4) I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. thanks. Andy. p.s. I am running in AWS . not sure if that makes a difference or not. p.p.s. Is there a better place to ask questions like this?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/260:911,message,message,911,,https://github.com/google/deepvariant/issues/260,1,['message'],['message']
Integrability,"Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. Iâ€™m looking forward to using this tool for my research.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/235:295,message,message,295,,https://github.com/google/deepvariant/issues/235,3,['message'],['message']
Integrability,"Hi Mark (@depristo),. Sorry, I meant to put this together a while ago - regarding https://github.com/google/deepvariant/issues/27#issuecomment-364683474 - but got a bit swamped with a research deadline. In any case, this is purely for intellectual curiosity and discussion. Regarding the first point, where differences in allocated CPUs might be the cause for the timing, that could be remedied by specifying a minimal CPU requirement, as noted here:. https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform. So to control for the variability in the test, the two options are either: a) to set the `--min-cpu-platform` setting to the maximum available (`""Intel Sandy Bridge""`), or b) to keep requesting and canceling instances until the desired one is allocated on which all tests should be performed, thus satisfying consistency. Just as a quick inspection, by looking at the CPU cycles utilization, I just ran a performance analysis of 0.4 and 0.5.1 on `make_examples` - since it displayed the initial discrepancy - and there seem to be some slight increases in `0.5.1`, which might cumulatively affect things. In any case, below is the top of the call-graph of percent utilization by method (per version):. #### DV 0.4. ```; # Samples: 186K of event 'cpu-clock'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--lea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:684,Bridg,Bridge,684,,https://github.com/google/deepvariant/issues/50,1,['Bridg'],['Bridge']
Integrability,"Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error?. The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python; def test_make_labeler_ref(self, candidates, truths, expected_start,; expected_end, bufsize):; expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader; labeler = _make_labeler(); labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(; ranges.make_range('20', expected_start, expected_end)); self.assertEqual(labeler_ref.start, expected_start); self.assertEqual(labeler_ref.end, expected_end); self.assertEqual(; labeler_ref.bases(expected_start, expected_end), expected_bases); ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python; def make_labeler_ref(self, candidates, true_variants, bufsize=20):; all_variants = candidates + true_variants; contig = all_variants[0].reference_name; start = min(x.start for x in all_variants); end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object; ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]; ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail; contig_nbp = self._ref_reader.con",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154:100,depend,dependencies,100,,https://github.com/google/deepvariant/issues/154,1,['depend'],['dependencies']
Integrability,"Hi all,. I have encountered the following error when running deepvaraint on a wgs file. â€˜[E::vcf_parse_format] Incorrect number of FORMAT fields at 1:19355\n.â€™ I have googled it but I am not sure what is causing this issue and how to resolve it. I will appreciate your help. ; I have attached the full error log, bash and yaml files. . error:; code: 10; message: |-; 11: Docker run failed: ASS""\ncalls {\n info {\n key: ""AD""\n value {\n values {\n int_value: 8\n }\n values {\n int_value: 7\n }\n }\n }\n info {\n key: ""DP""\n value {\n values {\n int_value: 15\n }\n }\n }\n info {\n key: ""GQ""\n value {\n values {\n int_value: 12\n }\n }\n }\nâ€¦â€¦.; â€¦â€¦â€¦.; â€¦â€¦â€¦..Reading /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf with NativeVcfReader\nI0806 01:37:07.984452 140434439055104 postprocess_variants.py:593] Writing output to VCF file: /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf\nI0806 01:37:08.052251 140434439055104 genomics_writer.py:118] Writing /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf with NativeVcfWriter\n**[E::vcf_parse_format] Incorrect number of FORMAT fields at 1:19355\n.** See logs at gs://gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/deepvariant_staging_folder/logs/']]. Operation ID: ENqznd7QLBi1jMjtufCo3ckBIK2c48-5AyoPcHJvZHVjdGlvblF1ZXVl. Best,; Shruti. Shruti Marwaha, PhD.; Research Engineer,; Stanford Center for Undiagnosed Diseases; Stanford University. [UDN631726_gvcf_error_08052018.log](https://github.com/google/deepvariant/files/2263671/UDN631726_gvcf_error_08052018.log); Since GIT does not allow me to attach .sh or .yaml files, I am saving them as text files and attaching.; [deepvariant_v0.6.1_UDN631726.yaml.txt](https://github.com/google/deepvariant/files/2263680/deepvariant_v0.6.1_UDN631726.yaml.txt); [deepvariant_v0.6.1_UDN631726.sh.txt](https://github.com/google/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/86:354,message,message,354,,https://github.com/google/deepvariant/issues/86,1,['message'],['message']
Integrability,"Hi all,. This is not strictly a DeepVariant issue, but it is an issue I ran into while following the Giraffe case study using exome FASTQs from [here](https://www.internationalgenome.org/data-portal/sample/NA12878) (specifically, `SRR1518158_*.fastq.gz`). I'm running this on a DNAnexus cloud workstation (`mem1_ssd1_v2_x8`) and using the same version of KMC used in the case study. I am trying to run KMC using the following command:. ```; TMPDIR=$(mktemp -d); time ./kmc -k29 -okff -t8 sra.fq.paths ./sra.fq $TMPDIR; ```. where `sra.fq.paths` is the result of `ls SRR1518158_*.fastq.gz > sra.fq.paths`. The error message is simply `Error: unknown exception`. Have you seen this before? I thought I'd ask here before filing an issue with the KMC repo. Thanks,; Samantha",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/767:615,message,message,615,,https://github.com/google/deepvariant/issues/767,1,['message'],['message']
Integrability,"Hi all;; Thanks for all the help getting an initial conda package in place for DeepVariant (#9) through bioconda. I wanted to follow up with some suggestions that would help make the pre-built binaries more portable as part of this process, in order of helpfulness for portability:. - Currently the binaries need a recent kernel with GLIBC > 2.23 due to pre-built htslib and other libraries. Would it be possible to build the DeepVariant libraries on an older machine to allow a wider range of system support? We build on CentOS 6 in conda to provide wider compatibility.; - main.py in the zip files hardcodes python to use `/usr/bin/python`. Would it be possible to generalize this by using the python that the zip file gets called with (`sys.executable`)? I currently patch this in the conda build: https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/recipes/deepvariant/build.sh#L22; - This is currently built against numpy 1.13 and ideally we'd want to sync with CONDA_NPY (1.12: https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9). I believe building against 1.12 would make it forward compatible. An alternative to points 1 and 3 is making it easier to build DeepVariant as part of the conda build process. The major blocker here is the `clif` dependency which is difficult to build and the pre-built binaries require unpacking into `/usr`. If we could make this relocatable and easier to install globally we could build with portable binaries and adjustable numpy as part of the bioconda preparation process. Thanks again for all the help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29:1355,depend,dependency,1355,,https://github.com/google/deepvariant/issues/29,1,['depend'],['dependency']
Integrability,"Hi, I got an error when running ; `( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21 chr22'"" \; ) >""${LOG_DIR}/validation_set.with_label.make_examples.log"" 2>&1`. the log message in validation_set.with_label.make_examples.log is as below:. `[E::hts_open_format] Failed to open file /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184:611,message,message,611,,https://github.com/google/deepvariant/issues/184,1,['message'],['message']
Integrability,"Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue?. **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command?. This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \; -v /mnt/efs-genome:/mnt/efs-genome \; gcr.io/deepvariant-docker/deepvariant \; /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. Thank you very much for your assistance!. Sincerely,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167:246,message,message,246,,https://github.com/google/deepvariant/issues/167,1,['message'],['message']
Integrability,"Hi,. I am trying to run DeepVariant 1.2.0 on a few human samples PacBio HiFi data (about 30x coverage per sample). I first ran my samples through the [PEPPER-Margin pipeline r0.4](https://github.com/kishwarshafin/pepper) to get a haplotagged BAM file. Then I ran DeepVariant as follows:; ```; singularity exec -B ${SOME_PATHS} deepvariant_1.2.0.sif bash /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref ${PATH_TO_REF} --reads MARGIN_PHASED.PEPPER_SNP_MARGIN.happlotagged.bam --output_vcf sample.vcf.gz --output_gvcf sample.g.vcf.gz --num_shards 24 --make_examples_extra_args=""realign_reads=false,min_mapping_quality=5"" --sample_name MYSAMPLE --use-hp-information;; ```. I have two problems:; 1. Right from the beginning (`CALL VARIANT MODULE SELECTED`), for each interval processed. I get thousands of `READ TAG: n_elements is zero` messages in the console. What does it mean and is it a problem or just a warning?; 2. I allocate 200GB of RAM for per job and they all seem to systematically fail on memory. I do not recall DeepVariant using that much memory in the past but I might be wrong. Is 200GB too light for a human genome PacBio Hifi 30x coverage dataset?. Thank you for your help,; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/490:847,message,messages,847,,https://github.com/google/deepvariant/issues/490,1,['message'],['messages']
Integrability,"Hi,. I don't know if this is the place to report issues with running the docker pipeline on the google cloud, I have been following instructions at [https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1) and it mostly works, but the second command within the big docker call dies with some python error. . That page indicates I should email google directly at google-genomics-contact@google.com, but this address bounces, which is why I came here. Anyway I have all the commands and error messages etc, so let me know if this is the right place for that and I will post. Thanks,; Ariel",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399:588,message,messages,588,,https://github.com/google/deepvariant/issues/399,1,['message'],['messages']
Integrability,"Hi,; I have a question about the information used by DeepVariant (v0.9.0) with the model `PACBIO`: does this model rely on/benefit from the additional quality information contained in ""PacBio-native"" BAM files? In other words, are the variant calls identical for a dataset that is processed (i) using the PacBio-native BAM as input, requiring the alignment to be done with pbmm2 to keep said information intact; and (ii), using FASTQ as input (w/o the additional quality information), and the alignment is performed with minimap2 (since pbmm2 is essentially just a wrapper around minimap2, let's assume the resulting alignments are identical)? Thanks for the clarification.; Best,; Peter",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/275:565,wrap,wrapper,565,,https://github.com/google/deepvariant/issues/275,1,['wrap'],['wrapper']
Integrability,"Hi. I'm running a pipeline but get an error on ""make_examples"" stage. Could you help me to debug possible cause, please?. Here is a log:; ```; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:51.456603: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0814 12:36:53.581777 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:36:54.201131 140158089049856 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:58.286794: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0814 12:36:59.914532 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:45:36.568115 140158089049856 make_examples.py:946] Common contigs are [u'LKUA01000001.1', u'LKUA01000002.1', ...<ANOTHER 300k NAMES>..., u'LKUA01311038.1', u'LKUA01311039.1']; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/cannabis-3k-vcf/staging/SRS1107973_LKUA01/staging/examples/0/examples_output.tfrecord@512.gz --reads /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam --ref /mnt/google/.google/input/cannabis-3k/reference/LKUA01/LKUA01.fa --task 8; ```. For me it looks that the error message is `parallel: This job failed:` and failure doesn't relate to the warnings at the beginning of the file?. Regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/207:1843,message,message,1843,,https://github.com/google/deepvariant/issues/207,1,['message'],['message']
Integrability,"Hi;; I would like to build DeepVariant on CentOS 7. I have installed dependencies of the Centos version corresponding to run-prereq.sh. But I also cann't run copying binaries on my local machines which have installed CentOS 7. The wrror message indicate it cann't find some dependent package in environment.; the error message:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 41, in <module>; from deepvariant import pileup_image; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 42, in <module>; from third_party.nucleus.util import ranges; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 42, in <module>; from third_party.nucleus.io import bed; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 79, in <module>; from third_party.nucleus.io.python import bed_reader; ImportError: libbz2.so.1.0: cannot open shared object file: No such file or directory; ```; I know we can use docker to run DeepVariant on CentOS 7. But There are some reason why I cann't use docker to run DeepVariant. Did you try to build DeepVariant for CentOS 7. Or, you know who have build DeepVariant with some way on CentOS 7. If you know, can give me some adviceï¼Ÿ; Thanks a lot,; Simon.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/95:69,depend,dependencies,69,,https://github.com/google/deepvariant/issues/95,4,"['depend', 'message']","['dependencies', 'dependent', 'message']"
Integrability,"Hi~; I tried to run Deepvariant v1.0.0 by docker image.; But it returned error message when I run test dataset. Here is the code:. ```sh; # BIN_VERSION=""1.0.0""; # INPUT_DIR=""${PWD}/quickstart-testdata""; # OUTPUT_DIR=""${PWD}/quickstart-output"". # ls quickstart-testdata; NA12878_S1.chr20.10_10p1mb.bam ; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ; ucsc.hg19.chr20.unittest.fasta.gz.fai; NA12878_S1.chr20.10_10p1mb.bam.bai ; ucsc.hg19.chr20.unittest.fasta ; ucsc.hg19.chr20.unittest.fasta.gz.gzi; test_nist.b37_chr20_100kbp_at_10mb.bed ; ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ; ucsc.hg19.chr20.unittest.fasta.gz. # /home/d008/data/covid19/deepvarient/test# sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; ```. And here is the error message from docker. ```sh; ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0907 09:04:08.296450 140053878712064 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unitt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345:79,message,message,79,,https://github.com/google/deepvariant/issues/345,1,['message'],['message']
Integrability,"I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10.; I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed.; On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ?. Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not; unavailable for this platform. Please install CLIF at; https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**.; A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/160:568,message,message,568,,https://github.com/google/deepvariant/issues/160,2,['message'],['message']
Integrability,"I am trying to install deepvariant within a singularity container using recipe build. I have successfully complied google-sdk within the container, when it comes to the point where its runs ""./build-prereq.sh"" script, it terminates with this error: . **Setting up unzip (6.0-9ubuntu1) ...; Setting up zip (3.0-8) ...; Setting up zlib1g-dev:amd64 (1:1.2.8.dfsg-1ubuntu1) ...; Processing triggers for libc-bin (2.19-0ubuntu6) ...; Processing triggers for sgml-base (1.26+nmu4ubuntu1) ...; ========== [Tue Apr 17 00:09:23 UTC 2018] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; E: Unable to locate package python-wheel; ABORT: Aborting with RETVAL=255; Cleaning up...**. I am using singularity within in vagrant on my mac and here is my recipe file:. **Bootstrap: shub; From: singularityhub/ubuntu. %runscript; exec echo ""The runscript is the containers default runtime command!"". %files; # /home/vanessa/Desktop/hello-kitty.txt # copied to root of container; # /home/vanessa/Desktop/party_dinosaur.gif /opt/the-party-dino.gif #. %environment. export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)"". %labels; AUTHOR mnoon@email.arizona.edu. %post; apt-get update && apt-get -y install python2.7 git wget curl ; mkdir /data; echo ""The post section is where you can install, and configure your container."". echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" > /etc/apt/sources.list.d/google-cloud-sdk.list; curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -. apt-get update && apt-get install -y google-cloud-sdk; ##download deepVariant scripts and run them; git clone https://github.com/google/deepvariant.git; cd deepvariant; ; ./build-prereq.sh. ./build_and_test.sh. ./run-prereq.sh**. I have no idea whats causing this error. any help will be appreciated. -M",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/66:627,depend,dependency,627,,https://github.com/google/deepvariant/issues/66,1,['depend'],['dependency']
Integrability,I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137:542,message,messages,542,,https://github.com/google/deepvariant/issues/137,1,['message'],['messages']
Integrability,"I am using Ubuntu 16. I got binaries from file:; https://github.com/google/deepvariant/releases/download/v0.7.0/deepvariant.zip. **I run run-prereq.sh first and warning message appears:** ; Cloning into 'tensorflow'...; Switched to a new branch 'r1.9'; Extracting Bazel installation...; WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"". **When I run ./build_and_test.sh command, an error appears:**; ./build_and_test.sh: line 54: bazel: command not found. **When I run ./run-prereq.sh command, it stops at ""unable to re-open stdin:""** ; debconf: unable to initialize frontend: Dialog; debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.); debconf: falling back to frontend: Readline; debconf: unable to initialize frontend: Readline; debconf: (This frontend requires a controlling tty.); debconf: falling back to frontend: Teletype; dpkg-preconfigure: unable to re-open stdin: ; debconf: unable to initialize frontend: Dialog; debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.); debconf: falling back to frontend: Readline; debconf: unable to initialize frontend: Readline; debconf: (This frontend requires a controlling tty.); debconf: falling back to frontend: Teletype; dpkg-preconfigure: unable to re-open stdin: . As I can see the problem is bazel installation and already some ways of resolving the problem were suggested - one of suggestion was to change .txt.sh file, another one to manually install bazel package (which seems to me regarding instructions on bazel site not a straightforward approach). I am running DeepVariant on a cluster, therefore would be very grateful for any more straightforward; suggestion. Thank you very much.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/92:169,message,message,169,,https://github.com/google/deepvariant/issues/92,1,['message'],['message']
Integrability,"I have used the following commandline for running deepvariant. sudo docker run \; -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \; -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \; google/deepvariant:1.2.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/G1.asm.hic.hap2.p_ctg.fa \; --reads=/input/G1_hap2.bam \; --output_vcf=/output/output.vcf \; --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******; I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_; [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'; I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader; W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'; I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader; W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'; I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader; W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; [E::idx_find_and_load] Co",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/492:556,Message,Message,556,,https://github.com/google/deepvariant/issues/492,1,['Message'],['Message']
Integrability,"I ran DeepVariant twice based on ""https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md"". ; deepvariant1- whatshap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488:409,message,message,409,,https://github.com/google/deepvariant/issues/488,1,['message'],['message']
Integrability,"I ran deeptrio on a trio WGS data. I got the gvcf and vcf for parent 1 and 2 but I didn't get output from child. There were no error messages that I could find as to why. The output seems complete. **Setup**; - Operating system: Windows 10; - DeepVariant version: DeepTrio version 1.1.0; - Installation method: Docker; - Type of data: Illumina, GRCh38, trio WGS. **Steps to reproduce:**; - Command:; `/opt/deepvariant/bin/deeptrio/run_deeptrio . - --model_type=WGS ; - --ref=GRCh38_full_analysis_set_plus_decoy_hla.fa ; - --reads_child=20A0012672_P_GRCh38.bam ; - --reads_parent1=20A0012673_M_GRCh38.bam ; - --reads_parent2=NBVY8432_GRCh38.bam; - --output_vcf_child 20A0012672_P_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent1 20A0012673_M_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent2 NBVY8432_GRCh38_deeptrio.vcf.gz ; - --sample_name_child '20A0012672_P' ; - --sample_name_parent1 '20A0012673_M' ; - --sample_name_parent2 'NBVY8432' ; - --num_shards $(nproc) ; - --intermediate_results_dir ../home/tmp ; - --output_gvcf_child 20A0012672_P_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent1 20A0012673_M_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent2 NBVY8432_GRCh38_deeptrio.gvcf.gz`. - Error trace: NA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/431:133,message,messages,133,,https://github.com/google/deepvariant/issues/431,1,['message'],['messages']
Integrability,"I tried to build deepvariant on a local ubuntu server.; With GCP support turned off, so far I am stuck with an error after build_and_test.sh . `(13:58:12) ERROR: /root/deepvariant/deepvariant/core/python/BUILD:174:1: CLIF wrapping deepvariant/core/python/hts_verbose.clif failed (Exit 4): pyclif failed: error execut; ing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/genomics && \; exec env - \; bazel-out/host/bin/external/clif/pyclif --modname deepvariant.core.python.hts_verbose -c bazel-out/k8-opt/genfiles/deepvariant/core/python/hts_verbose.cc -g bazel-out/k8-; opt/genfiles/deepvariant/core/python/hts_verbose.h -i bazel-out/k8-opt/genfiles/deepvariant/core/python/hts_verbose_init.cc --prepend /root/opt/clif/python/types.h -Iextern; al/protobuf_archive -Ibazel-out/k8-opt/genfiles -Ibazel-out/k8-opt/genfiles/external/local_config_python -Iexternal/htslib -Ibazel-out/k8-opt/genfiles/external/htslib -I. -; Iexternal/bazel_tools -Ibazel-out/k8-opt/genfiles/external/bazel_tools -Iexternal/htslib/htslib/htslib_1_6 -Ibazel-out/k8-opt/genfiles/external/htslib/htslib/htslib_1_6 -Ie; xternal/bazel_tools/tools/cpp/gcc3 -Iexternal/clif -Ibazel-out/k8-opt/genfiles/external/clif -Iexternal/local_config_python -Ibazel-out/k8-opt/genfiles/external/protobuf_ar; chive -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/genfiles/external/local_config_python/python_include -Iexternal/protobuf_archive/src -Ibazel-out/k8-o; pt/genfiles/external/protobuf_archive/src '-f-Iexternal/protobuf_archive -Ibazel-out/k8-opt/genfiles -Ibazel-out/k8-opt/genfiles/external/local_config_python -Iexternal/hts; lib -Ibazel-out/k8-opt/genfiles/external/htslib -I. -Iexternal/bazel_tools -Ibazel-out/k8-opt/genfiles/external/bazel_tools -Iexternal/htslib/htslib/htslib_1_6 -Ibazel-out/; k8-opt/genfiles/external/htslib/htslib/htslib_1_6 -Iexternal/bazel_tools/tools/cpp/gcc3 -Iexternal/clif -Ibazel-out/k8-opt/genfiles/external/clif -Iexternal/local_config_py;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12:222,wrap,wrapping,222,,https://github.com/google/deepvariant/issues/12,1,['wrap'],['wrapping']
Integrability,"I would like to training a new model with two different dataset. There are about 8,500,00 training examples. But I used the python script shuffle_tfrecords_beam.py to shuffling these examples. I get the error that the code uses too mach memory. It is out of memory of my machine. My machine have 376G memory. Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:893,message,message,893,,https://github.com/google/deepvariant/issues/91,1,['message'],['message']
Integrability,"I would like to try to make a custom singularity version of deepvariant in which is integrated with some custom scripts, do you know where I could find the singularity recepie of deepvariant? It would be a good starting point for me. Thank you in advance for any suggestion,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/862:84,integrat,integrated,84,,https://github.com/google/deepvariant/issues/862,1,['integrat'],['integrated']
Integrability,"I would rather not install the DeepVariant dependencies into my global python environment. When the python dependencies are installed into a virtual environment, make_examples.zip cannot find tensorflow. The steps below show the error. Any suggestions?. ```; $ mkvirtualenv -p /usr/bin/python2.7 DeepVariant.2.7; (DeepVariant.2.7) $ cd bin; bash run-prereq.sh; cd -. (DeepVariant.2.7) $ python bin/make_examples.zip --mode calling --ref ""${REF}"" --reads ""${BAM}"" --regions ""chr20:10,000,000-10,010,000"" --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_r1oZvM/runfiles/genomics/deepvariant/make_examples.py"", line 38, in <module>; import tensorflow as tf; ImportError: No module named tensorflow; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/25:43,depend,dependencies,43,,https://github.com/google/deepvariant/issues/25,2,['depend'],['dependencies']
Integrability,"I'm using google/deepvariant-0.10.0 through docker on AVX-512 instruction capable Intel skylake processor. But the docker image ""google/deepvariant-0.10.0"" binaries are not built for AVX-512. Here are the warnings for the same:. ` I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA `. Where can I get deepvariant or tensorflow binaries with AVX-512 optimzation? I tried to build deepvariant from source, but couldn't due to lot of dependencies. . Please let me know if there is any way to get AVX-512 optimized binaries for deepvariant/tensorflow.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/301:534,depend,dependencies,534,,https://github.com/google/deepvariant/issues/301,1,['depend'],['dependencies']
Integrability,"Is there a way to redirect the DeepVariant output to another program? For example, to any annotation tool. I tried this command:; ```; sudo -S docker run -v ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/seq1/bowtie2/"":""/ref"" -v ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/Ð ÐµÐ·/Ð½Ð¾Ð²Ð°Ñ_Ð¿Ð°Ð¿ÐºÐ°/SRR062634.filt/"":""/trg"" \; > google/deepvariant /opt/deepvariant/bin/run_deepvariant \; > --num_shards=4 --model_type=WGS \; > --ref=/ref/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz \; > --reads=/trg/SRR062634.filt_srtd.bam |; > sudo docker run -a stdin -v $HOME/vep_data:/opt/vep/.vep -v ""$HOME/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/Ð ÐµÐ·/Ð½Ð¾Ð²Ð°Ñ_Ð¿Ð°Ð¿ÐºÐ°/SRR062634.filt/"":""/SRR062634_filt"" \; > ensemblorg/ensembl-vep ./vep \; > --tab --quiet --no_stats --offline --cache --dir_cache /opt/vep/.vep/ \; > -o /SRR062634_filt/SRR062634.filt_ann.tsv; ```. Then an error message appears:; `FATAL Flags parsing error: flag --output_vcf=None: Flag --output_vcf must have a value other than None.`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/253:809,message,message,809,,https://github.com/google/deepvariant/issues/253,1,['message'],['message']
Integrability,"My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do?. ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737:1343,message,message,1343,,https://github.com/google/deepvariant/issues/737,1,['message'],['message']
Integrability,NumPy dependency error despite matching versions,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610:6,depend,dependency,6,,https://github.com/google/deepvariant/issues/610,1,['depend'],['dependency']
Integrability,OpenVINO integration,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363:9,integrat,integration,9,,https://github.com/google/deepvariant/pull/363,1,['integrat'],['integration']
Integrability,Questions about GLnexus integration and DeepTrio training data for config evaluation,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475:24,integrat,integration,24,,https://github.com/google/deepvariant/issues/475,1,['integrat'],['integration']
Integrability,"R_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, since apt-utils is not installed; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 663 100 663 0 0 5012 0 --:--:-- --:--:-- --:--:-- 5022; debconf: delaying package configuration, since apt-utils is not installed; WARNING: Logging before flag parsing goes to stderr.; ```. But I then see many messages about candidate variants it's found. . The directory `staging-folder/examples/0/` also includes 8 `.gz` files like `examples_output.tfrecord-00007-of-00008.gz`. . Can you help me figure out what I'm doing wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:3910,message,messages,3910,,https://github.com/google/deepvariant/issues/60,1,['message'],['messages']
Integrability,The deep variant wrapper dv_call_variants.py crushing when installed using conda.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/573:17,wrap,wrapper,17,,https://github.com/google/deepvariant/issues/573,1,['wrap'],['wrapper']
Integrability,"The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is; https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/789:294,depend,depending,294,,https://github.com/google/deepvariant/issues/789,1,['depend'],['depending']
Integrability,"This is from DeepVariant's documentation. ""For somatic data or any other samples where the genotypes go beyond two copies of DNA, DeepVariant will not work out of the box because the only genotypes supported are hom-alt, het, and hom-ref."". Does the same apply to the RNA-seq model of DeepVariant? If a mutation has a low VAF is it predicted as hom-ref?. Because in RNA-seq, low VAF doesn't necessarily mean the variant is hom-ref in the genome. . ### In RNA-seq:; - **Expression Levels**: RNA-seq measures gene expression, so the VAF of a mutation depends on the expression of the mutant and wild-type alleles. If a gene is highly expressed in some cells and not others, or if only one allele is expressed (allele-specific expression), the VAF may be skewed.; - **Variable Expression**: The VAF in RNA-seq data can be influenced by tissue-specific expression, transcriptional noise, or RNA degradation, making it less consistent compared to DNA-seq.; - **Allelic Imbalance**: In RNA-seq, you might observe allelic imbalance due to factors like imprinting or preferential expression of one allele, further complicating the interpretation of VAF.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/864:549,depend,depends,549,,https://github.com/google/deepvariant/issues/864,1,['depend'],['depends']
Integrability,Version Build Channel; _libgcc_mutex 0.1 main ; _openmp_mutex 5.1 1_gnu ; absl-py 2.1.0 pypi_0 pypi; argparse 1.4.0 pypi_0 pypi; blas 1.0 mkl ; bzip2 1.0.8 h5eee18b_6 ; ca-certificates 2024.7.2 h06a4308_0 ; chex 0.1.86 pypi_0 pypi; clu 0.0.9 pypi_0 pypi; contextlib2 21.6.0 pypi_0 pypi; cython 3.0.10 pypi_0 pypi; enum34 1.1.8 pypi_0 pypi; etils 1.7.0 pypi_0 pypi; flax 0.8.5 pypi_0 pypi; fsspec 2024.6.1 pypi_0 pypi; importlib-resources 6.4.0 pypi_0 pypi; intel-openmp 2023.1.0 hdb19cb5_46306 ; intervaltree 3.0.2 pypi_0 pypi; jax 0.4.31 pypi_0 pypi; jaxlib 0.4.31 pypi_0 pypi; ld_impl_linux-64 2.38 h1181459_1 ; libffi 3.4.4 h6a678d5_1 ; libgcc-ng 11.2.0 h1234567_1 ; libgomp 11.2.0 h1234567_1 ; libstdcxx-ng 11.2.0 h1234567_1 ; libuuid 1.41.5 h5eee18b_0 ; markdown-it-py 3.0.0 pypi_0 pypi; mdurl 0.1.2 pypi_0 pypi; mkl 2023.1.0 h213fc3f_46344 ; mkl-service 2.4.0 py310h5eee18b_1 ; mkl_fft 1.3.8 py310h5eee18b_0 ; mkl_random 1.2.4 py310hdb19cb5_0 ; ml-collections 0.1.1 pypi_0 pypi; ml-dtypes 0.4.0 pypi_0 pypi; mock 5.1.0 pypi_0 pypi; msgpack 1.0.8 pypi_0 pypi; ncurses 6.4 h6a678d5_0 ; nest-asyncio 1.6.0 pypi_0 pypi; numpy 1.24.3 py310h5f9d8c6_1 ; numpy-base 1.24.3 py310hb5e798b_1 ; openssl 3.0.14 h5eee18b_0 ; opt-einsum 3.3.0 pypi_0 pypi; optax 0.2.3 pypi_0 pypi; orbax-checkpoint 0.5.23 pypi_0 pypi; packaging 24.1 pypi_0 pypi; pip 24.0 py310h06a4308_0 ; protobuf 3.13.0 pypi_0 pypi; pygments 2.18.0 pypi_0 pypi; python 3.10.14 h955ad1f_1 ; pyyaml 6.0.1 pypi_0 pypi; readline 8.2 h5eee18b_0 ; rich 13.7.1 pypi_0 pypi; scipy 1.14.0 pypi_0 pypi; setuptools 69.5.1 py310h06a4308_0 ; six 1.16.0 pypi_0 pypi; sortedcontainers 2.1.0 pypi_0 pypi; sqlite 3.45.3 h5eee18b_0 ; tbb 2021.8.0 hdb19cb5_0 ; tensorstore 0.1.64 pypi_0 pypi; tf-slim 1.1.0 pypi_0 pypi; tk 8.6.14 h39e8969_0 ; toolz 0.12.1 pypi_0 pypi; typing-extensions 4.12.2 pypi_0 pypi; tzdata 2024a h04d1e81_0 ; wheel 0.43.0 py310h06a4308_0 ; wrapt 1.16.0 pypi_0 pypi; xz 5.4.6 h5eee18b_1 ; zipp 3.19.2 pypi_0 pypi; zlib 1.2.13 h5eee18b_1,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:13617,wrap,wrapt,13617,,https://github.com/google/deepvariant/issues/859,1,['wrap'],['wrapt']
Integrability,"When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:; `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**; - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa); - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run google/deepvariant:1.4.0; - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; CPU information from /proc/cpuinfo; product: Common KVM processor; vendor: Intel Corp.; physical id: 2; bus info: cpu@1; width: 64 bits; capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552:83,message,message,83,,https://github.com/google/deepvariant/issues/552,1,['message'],['message']
Integrability,"When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. ; Here is part of the command I used:. `; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \; --reads_parent1=markduplicates/S_500061.md.bam \; --reads_parent2=markduplicates/S_500062.md.bam \; --reads_child=markduplicates/S_500063.md.bam \; --output_vcf_parent1 output/S_500061.output.vcf.gz \; --output_vcf_parent2 output/S_500062.output.vcf.gz \; --output_vcf_child output/S_500063.output.vcf.gz \; --sample_name_parent1 'S_500061' \; --sample_name_parent2 'S_500062' \; --sample_name_child 'S_500063' \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir \; --output_gvcf_parent1 output/S_500061.g.vcf.gz \; --output_gvcf_parent2 output/S_500062.g.vcf.gz \; --output_gvcf_child output/S_500063.g.vcf.gz \; --output_gvcf_merged output/FAM1.g.vcf.gz\; --dry_run=true \; --vcf_stats_report=true; `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/687:55,message,message,55,,https://github.com/google/deepvariant/issues/687,1,['message'],['message']
Integrability,"] example_shape = [100, 221, 7]; I0822 07:52:09.283712 133276175411008 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0822 07:52:09.283882 133276175411008 make_examples_core.py:301] Task 8/20: Found 17371 candidate variants; I0822 07:52:09.283904 133276175411008 make_examples_core.py:301] Task 8/20: Created 18820 examples. real 34m15.728s; user 624m43.553s; sys 2m24.932s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@20.gz"" --checkpoint ""/output/checkpoints/ckpt-679"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0822 07:52:10.812179 127086447671104 call_variants.py:563] Total 1 writing processes started.; I0822 07:52:10.813103 127086447671104 dv_utils.py:370] From /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0822 07:52:10.813141 127086447671104 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0822 07:52:10.813338 127086447671104 call_variants.py:592] Use saved model: True; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tm",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:4980,depend,dependencies,4980,,https://github.com/google/deepvariant/issues/869,1,['depend'],['dependencies']
Integrability,"] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-07-03 17:21:58.247052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-07-03 17:21:58.247080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; Runs all 3 steps to go from input DNA reads to output VCF/gVCF files.; (... then -- the list of all options follows). If run on a proper BAM file with all options provided, all TF-TRT warning messages are periodically repeated as well as ; CUDA Version 11.3.1; 2024-07-02 22:47:07.493311: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-02 22:47:12.386498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 8308a7bb3067; 2024-07-03 18:27:31.862563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 8308a7bb3067; 2024-07-0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:2845,message,messages,2845,,https://github.com/google/deepvariant/issues/844,1,['message'],['messages']
Integrability,"_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants; model = modeling.inceptionv3(; File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3; backbone = add_l2_regularizers(; File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers; model.save_weights(tmp_weights_path); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__; fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr); File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid; fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create; OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'); Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main; for line in proc.stdout:; KeyboardInterrupt; ```; Looking forward to your reply.; Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725:3735,wrap,wrapper,3735,,https://github.com/google/deepvariant/issues/725,3,"['message', 'wrap']","['message', 'wrapper']"
Integrability,"_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. This script made errors [(please see this file)](https://github.com/google/deepvariant/files/2708579/shuffle.train.0_id_masked.log). However, the codes which used chromosome-divided tfrecords (chr1-10 and chr11-chr19) worked fine as follows:. INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 10`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. and. INPUT_PATTERN_LIST=examples.train/sample_id/11/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 12 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. I cannot figure out the reason why this divided approach succeeded. Could you tell me any suggestions of points to check?; Because I'm using personal data under strict data management, I cannot provide or share our tfrecords. I used apache-beam v2.9.0 in python 2.7.5 (not in docker of DeepVariant v0.7.1).; If some version dependencies exist, I'd like to get some dockerfile like your environment. Best regards,. Masaru",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133:2782,depend,dependencies,2782,,https://github.com/google/deepvariant/issues/133,1,['depend'],['dependencies']
Integrability,"ack (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker; put((job, i, (False, wrapped))); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe; Process ForkPoolWorker-42:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes; self._send(buf); File ""/usr/lib/pytho",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804:3366,wrap,wrapped,3366,,https://github.com/google/deepvariant/issues/804,1,['wrap'],['wrapped']
Integrability,"aise value; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1345, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1418, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1176, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variant",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:15545,message,message,15545,,https://github.com/google/deepvariant/issues/358,1,['message'],['message']
Integrability,"alled; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:2121,Depend,Depends,2121,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability,"ate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. ; **Steps to reproduce:**; - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :; ; ```; E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'; 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX; PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M; 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666:2859,message,message,2859,,https://github.com/google/deepvariant/issues/666,1,['message'],['message']
Integrability,"ble; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http:/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1528,Depend,Depends,1528,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability,"buntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1402,Depend,Depends,1402,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability,"ch of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory â€˜/mnt/cdw-genomeâ€™: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further developme",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:4816,message,message,4816,,https://github.com/google/deepvariant/issues/171,1,['message'],['message']
Integrability,com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/deepvariant/deepvariant/testing/BUILD:19:1: Target '@com_googlesource_code_re2//:re2' contains an error and its package is in error and referenced by '//deepvariant/testing:gunit_extras'; (09:27:18) ERROR: Analysis of target '//deepvariant/testing:gunit_extras_test' failed; build aborted: Loading failed; (09:27:18) INFO: Elapsed time: 14.618s; (09:27:18) FAILED: Build did NOT complete successfully (48 packages loaded); (09:27:18) ERROR: Couldn't start the build. Unable to run tests; ```; Could anyone shed some light on this issue? Interestingly this was working a few days ago but possibly on a different host. Could it be hardware dependent?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:21157,depend,dependent,21157,,https://github.com/google/deepvariant/issues/19,1,['depend'],['dependent']
Integrability,"d_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}; I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/output/trained_model; I0415 07:35:14.785435 140713377441536 evaluation.py:198] Found new checkpoint at /data/output/trained_model/model.ckpt-0; I0415 07:35:14.787266 140713377441536 model_eval.py:225] Starting to evaluate. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop; name=eval_name); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate; name=name); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval; hooks.extend(self._convert_eval_steps_to_hooks(steps)); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimato",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:4248,depend,depend,4248,,https://github.com/google/deepvariant/issues/172,1,['depend'],['depend']
Integrability,"dev/stdout \; --num_shards=4 \; 2> stderr.txt; ```; it could work, but print some debug information to stdout and pollute the result; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --regions ""chr20:10,000,000-10,000,100"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --outfile ""/dev/stdout"". ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; ##FILTER=<ID=LowQual,Description=""Confidence in this variant being real is below calling threshold."">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End position (for use with symbolic alleles)"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read de",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/288:1554,depend,depend,1554,,https://github.com/google/deepvariant/issues/288,1,['depend'],['depend']
Integrability,"e are some problems while running the ./build-prereq.sh:; ```; + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0; Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0); Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0); Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objects: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:966,depend,dependency,966,,https://github.com/google/deepvariant/issues/739,1,['depend'],['dependency']
Integrability,"e creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled""; ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):; ```; docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant; See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md.; ```; I do have the `gcloud alpha genomics pipelines` example working, so this isnâ€™t absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:7315,message,message,7315,,https://github.com/google/deepvariant/issues/171,2,['message'],['message']
Integrability,"earning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.05%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.07%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.63%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.59%--google::protobuf::python::cmessage::DeepCopy; | | | ; | | --0.58%--google::protobuf::python::cmessage::MergeFrom; | | | ; | | --0.57%--google::protobuf::Message::MergeFrom; | | | ; | | --0.54%--google::protobuf::internal::ReflectionOps::Merge; | | ; | --0.59%--deepvariant_core_python_sam__reader_clifwrap::pySamIterable::wrapNext; | ; |--1.84%--0x903b40; | | ; | --1.71%--PyEval_EvalFrameEx; | ; |--1.06%--0x905d60; | | ; | --1.06%--PyEval_EvalFrameEx; | ; |--0.76%--0x8fecc0; | PyEval_EvalFrameEx; | ; |--0.59%--0x905200; | | ; | --0.59%--PyEval_EvalFrameEx; | ; --0.51%--0x9060a0; | ; --0.51%--PyEval_EvalFrameEx. 32.95% , 0.00% ,python ,[unknown] ,[.] 0x00000000009060a0; |; ---0x9060a0; | ; --32.10%--PyEval_EvalFrameEx; | ; --30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | ; --30.34%--StripedSmithWaterman::Aligner::Align; | ; |--27.87%--ssw_align; | | ; | |--14.65%--sw_sse2_word; | | ; | |--8.32%--sw_sse2_byte; | | ; | |--2.91%--banded_sw; | | ; | --1.19%--__memcpy_sse2_unaligned; | ; --1.36%--ssw_init; | ; --0.89%--qP_byte. 30.81% , 0.07% ,python ,libssw_cclib.so ,[.] deepvariant_realigner_python_ssw_clifwrap:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:3317,Message,Message,3317,,https://github.com/google/deepvariant/issues/50,1,['Message'],['Message']
Integrability,"ec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-too",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1818,Depend,Depends,1818,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability,"epVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; Traceback (most recent call last):; File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call; with Popen(*popenargs, **kwargs) as p:; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__; restore_signals, start_new_session); File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'; ```. I noticed inside the bazel .zip files the python binary is hard-coded:; /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```; micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml; ``` ; With environment file:; ```; name: dv; channels:; - conda-forge; - bioconda; - defaults; dependencies:; - python=3.6; - deepvariant=1.5.0; ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664:2176,depend,dependencies,2176,,https://github.com/google/deepvariant/issues/664,1,['depend'],['dependencies']
Integrability,"epends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::56",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1994,Depend,Depends,1994,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability,"equencing instrument, reference genome, anything special that is unlike the case studies?); EXAMPLE DATA PROVIDED. **Steps to reproduce:**; - Command:. INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ \; /fh/fast/furlan_s/grp/sifs/deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz. - Error trace: (if applicable) SEE BELOW. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. YES THIS IS WITH THE QUICK START EXAMPLE. **Any additional context:**. Message:. 2023-05-02 14:40:43.757041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0502 14:40:56.961649 140501830911808 run_deepvariant.py:364] Re-using the directory for intermediate results in /fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmpzz53zv8p. ***** Intermediate results will be written to /fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmpzz53zv8p in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/fh/scratch/delete90/furlan_s/targ_reseq/230117_Sam",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640:1849,Message,Message,1849,,https://github.com/google/deepvariant/issues/640,1,['Message'],['Message']
Integrability,"es can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6969,Depend,Depends,6969,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability,"es_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants; output_queue = multiprocessing.Queue(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s; user 0m6.224s; sys 0m3.683s. ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```; gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz; gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json; gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:3102,synchroniz,synchronize,3102,,https://github.com/google/deepvariant/issues/733,1,['synchroniz'],['synchronize']
Integrability,"e}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \; --mode calling \; --ref {input.reference} \; --reads {input.bam} \; --examples {params.examples} \; --gvcf {params.gvcf} \; --task {params.shard}) > {log} 2>&1; """""". ```; Error:. ```; 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ig",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:1734,message,message,1734,,https://github.com/google/deepvariant/issues/677,1,['message'],['message']
Integrability,"f._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1752, in restore; {self.saver_def.filename_tensor_name: save_path}); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run; run_metadata_ptr); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run; run_metadata); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [3,3,6,32] rhs shape= [3,3,7,32]; 	 [[Node: save_1/Assign_3 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Conv2d_1a_3x3/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Conv2d_1a_3x3/weights, save_1/RestoreV2:3)]]. Caused by op u'save_1/Assign_3', defined at:; File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 399, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 348, in call_variants; p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/117:5236,message,message,5236,,https://github.com/google/deepvariant/issues/117,1,['message'],['message']
Integrability,"flow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.05%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.07%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.63%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.59%--google::protobuf::python::cmessage::DeepCopy; | | | ; | | --0.58%--google::protobuf::python::cmessage::MergeFrom; | | | ; | | --0.57%--google::protobuf::Message::MergeFrom; | | | ; | | --0.54%--google::protobuf::internal::ReflectionOps::Merge; | | ; | --0.59%--deepvariant_core_python_sam__reader_clifwrap::pySamIterable::wrapNext; | ; |--1.84%--0x903b40; | | ; | --1.71%--PyEval_EvalFrameEx; | ; |--1.06%--0x905d60; | | ; | --1.06%--PyEval_EvalFrameEx; | ; |--0.76%--0x8fecc0; | PyEval_EvalFrameEx; | ; |--0.59%--0x905200; | | ; | --0.59%--PyEval_EvalFrameEx; | ; --0.51%--0x9060a0; | ; --0.51%--PyEval_EvalFrameEx. 32.95% , 0.00% ,python ,[unknown] ,[.] 0x00000000009060a0; |; ---0x9060a0; | ; --32.10%--PyEval_EvalFrameEx; | ; --30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | ; --30.34%--StripedSmithWaterman::Aligner::Align; | ; |--27.87%--ssw_align; | | ; | |--14.65%--sw_sse2_word; | | ; | |--8.32%--sw_sse2_byte; | | ; | |--2.91%--banded_sw; | | ; | --1.19%--__memcpy_sse2_unaligned; | ; --1.36%--ssw_init; | ; --0.89%--qP_byte. 30.81% , 0.07% ,python ,libssw_cclib.so ,[.] deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | ; --30.74%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | ; --30.34%--Strip",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:3486,wrap,wrapNext,3486,,https://github.com/google/deepvariant/issues/50,1,['wrap'],['wrapNext']
Integrability,"following: . ```; docker run \; -v ${HOME}:${HOME} \; google/deepvariant:1.6.1 \; train \; --config=${HOME}/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.num_epochs=10 \; --config.learning_rate=0.001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=tpu \; --config.batch_size=1024; ```. However, I am not an expert with TPUs, and this is entirely new to me. Below is the error I encountered. Do you have any suggestions or can you direct me to an updated tutorial to follow?. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0627 21:18:43.707066 139683296487232 train.py:92] Running with debug=False; I0627 21:18:43.707488 139683296487232 train.py:100] Use TPU at local; I0627 21:18:43.707705 139683296487232 train.py:103] experiment_dir: /home/gambardella/training_chk; INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.; I0627 21:18:43.707828 139683296487232 tpu_strategy_util.py:57] Deallocate tpu buffers before initializing tpu system.; INFO:tensorflow:Initializing the TPU system: local; I0627 21:18:43.846984 139683296487232 tpu_strategy_util.py:81] Initializing the TPU system: local; 2024-06-27 21:18:43.848149: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-criti",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:1253,depend,dependencies,1253,,https://github.com/google/deepvariant/issues/841,1,['depend'],['dependencies']
Integrability,get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; De,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1124,Depend,Depends,1124,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability,"h::AddEdgesForRead; | | | ; | | --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.16%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.12%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.58%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.62%--deepvariant_core_python_sam__reader_clifwrap::pySamIterable::wrapNext; | | ; | --0.57%--google::protobuf::python::cmessage::DeepCopy; | | ; | --0.56%--google::protobuf::python::cmessage::MergeFrom; | | ; | --0.56%--google::protobuf::Message::MergeFrom; | | ; | --0.52%--google::protobuf::internal::ReflectionOps::Merge; | ; |--1.92%--0x903b40; | | ; | --1.78%--PyEval_EvalFrameEx; | ; |--1.09%--0x905d60; | | ; | --1.09%--PyEval_EvalFrameEx; | ; |--0.78%--0x8fecc0; | PyEval_EvalFrameEx; | ; |--0.62%--0x905200; | | ; | --0.62%--PyEval_EvalFrameEx; | ; --0.54%--0x9060a0; | ; --0.54%--PyEval_EvalFrameEx. 33.23% , 0.00% ,python ,[unknown] ,[.] 0x00000000009060a0; |; ---0x9060a0; | ; --32.46%--PyEval_EvalFrameEx; | ; --31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | ; --30.63%--StripedSmithWaterman::Aligner::Align; | ; |--28.27%--ssw_align; | | ; | |--14.88%--sw_sse2_word; | | ; | |--8.45%--sw_sse2_byte; | | ; | |--2.89%--banded_sw; | | ; | --1.19%--__memcpy_sse2_unaligned; | ; --1.38%--ssw_init; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:8751,wrap,wrapNext,8751,,https://github.com/google/deepvariant/issues/50,1,['wrap'],['wrapNext']
Integrability,"hain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6833,Depend,Depends,6833,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability,"hap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1; whatshap 1.0; DeepTrio 1.2.0. Does the warning message affect the results or can be ignored?; Should I use a higher version of DeepVariant(1.2.0)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488:2050,message,message,2050,,https://github.com/google/deepvariant/issues/488,1,['message'],['message']
Integrability,"hello, ; I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:; /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s; user 112m32.542s; sys 0m37.407s; I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG00",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/720:92,message,message,92,,https://github.com/google/deepvariant/issues/720,1,['message'],['message']
Integrability,"his mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0626 13:39:06.145823 140632388314944 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 558, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tens",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:1623,message,message,1623,,https://github.com/google/deepvariant/issues/845,1,['message'],['message']
Integrability,"his mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in main; call_variants(; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 640, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tens",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:2410,message,message,2410,,https://github.com/google/deepvariant/issues/857,1,['message'],['message']
Integrability,"http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6627,Depend,Depends,6627,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability,"ibgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18--",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1873,Depend,Depends,1873,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability,"is is set in setting.sh as all DeepVariant scripts source; # settings.sh and assume that `bazel` will find the right version.; export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts.; export DEEPVARIANT_BUCKET=""gs://deepvariant""; export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages""; export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a; # named release version. Set it to an already existing value in the environment; # (allowing command line control of the build), defaulting to 0 (release build).; # Note that setting this to 1 implies that the C++ code in DeepVariant will be; # build using the master branch and not the pinned version to avoid; # incompatibilities between TensorFlow C++ used to build DeepVariant and the; # tf-nightly wheel.; export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not; # the same as the python version of TensorFlow we use, but should be similar or; # we risk having version incompatibilities between our C++ code and the Python; # code we use at runtime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing; # value in the environment (allowing command line control of the build),; # defaulting to 0 (CPU only build).; export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file; # compiled with MKL support for corei7 or better chipsets, which; # significantly speeds up execution when running on modern CPUs. The default; # TensorFlow whee",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:4590,depend,dependencies,4590,,https://github.com/google/deepvariant/issues/145,1,['depend'],['dependencies']
Integrability,"it_fn); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore; {self.saver_def.filename_tensor_name: save_path}); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run; run_metadata_ptr); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run; feed_dict_tensor, options, run_metadata); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run; run_metadata); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?; [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:; File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>; tf.app.run(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:7332,message,message,7332,,https://github.com/google/deepvariant/issues/166,1,['message'],['message']
Integrability,"ke_examples.py"", line 1074, in make_examples_runner; resource_monitor = resources.ResourceMonitor().start(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 59, in __init__; self.metrics_pb = self._initial_metrics_protobuf(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 72, in _initial_metrics_protobuf; cpu_frequency_mhz=_get_cpu_frequency(),; File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 158, in _get_cpu_frequency; freq = psutil.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/__init__.py"", line 1853, in cpu_freq; ret = _psplatform.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); NotImplementedError: can't find current frequency file; parallel: This job failed:; /opt/conda/envs/nf-core-deepvariant-1.0/bin/python /opt/conda/envs/nf-core-deepvariant-1.0/share/deepvariant-0.7.0-0/binaries/DeepVariant/0.7.0/DeepVariant-0.7.0+cl-208818123/make_examples.zip --mode calling --ref PlasmoDB-41_Pfalciparum3D7_Genome.fasta.gz --reads ISO_349.bam --regions PlasmoDB-41_Pfalciparum3D7_Genome_ISO_349.per-base.bed.gz --examples ISO_349_shardedExamples/ISO_349.bam.tfrecord@16.gz --task 10; ```. This is using the Nextflow wrapper script [nf-core/deepvariant](https://github.com/nf-core/deepvariant). The pipeline works with other input data. So it seems unlikely that it is a problem with installation or parameters. DeepVariant was installed using Conda & is using v0.7.0. . Any ideas what the problem is? . Could it be a problem to do with the input data? Perhaps the organism being used? What is the frequency file which it is referring to?. Any help would be much appreciated. Many thanks in advance,; Phil",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/191:4082,wrap,wrapper,4082,,https://github.com/google/deepvariant/issues/191,1,['wrap'],['wrapper']
Integrability,"kpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'); [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore; sess.run(self.saver_def.restore_op_name,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run; result = self._run(None, fetches, feed_dict, options_ptr,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run; results = self._do_run(handle, final_targets, final_fetches,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run; return self._do_call(_run_fn, feeds, fetches, targets, options,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'); [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:12976,message,message,12976,,https://github.com/google/deepvariant/issues/537,1,['message'],['message']
Integrability,"lease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; C",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6682,Depend,Depends,6682,,https://github.com/google/deepvariant/issues/489,2,['Depend'],['Depends']
Integrability,"lim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1089, in convolution; outputs = layer.apply(inputs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548, in __call__; outputs = super(Layer, self).__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__; outputs = call_fn(cast_inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 234, in wrapper; return converted_call(f, options, args, kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 439, in converted_call; return _call_unconverted(f, args, kwargs, options); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 330, in _call_unconverted; return f(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 197, in call; outputs = self._convolution_op(inputs, self.kernel); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 1134, in __call__; return self.conv_op(inp, filter); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 639, in __call__; return self.call(inp, filter); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 238, in __call__; name=self.name); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:19729,wrap,wrapper,19729,,https://github.com/google/deepvariant/issues/358,1,['wrap'],['wrapper']
Integrability,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:4091,message,messages,4091,,https://github.com/google/deepvariant/issues/252,2,"['depend', 'message']","['dependencies', 'messages']"
Integrability,"ll_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; I0828 10:40:42.589054 140318776715072 train.py:92] Running with debug=False; I0828 10:40:42.589343 140318776715072 train.py:100] Use TPU at local; I0828 10:40:42.589422 140318776715072 train.py:103] experiment_dir: /home/training_outs/epoch1/; WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; W0828 10:40:42.596594 140318776715072 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',); I082",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2912,depend,dependencies,2912,,https://github.com/google/deepvariant/issues/876,1,['depend'],['dependencies']
Integrability,"ltiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest ve",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:11554,depend,dependency,11554,,https://github.com/google/deepvariant/issues/89,1,['depend'],['dependency']
Integrability,"mbly=b37>; ##contig=<ID=chr8,length=146364022,assembly=b37>; ##contig=<ID=chr9,length=141213431,assembly=b37>; ##contig=<ID=chr10,length=135534747,assembly=b37>; ##contig=<ID=chr11,length=135006516,assembly=b37>; ##contig=<ID=chr12,length=133851895,assembly=b37>; ##contig=<ID=chr13,length=115169878,assembly=b37>; ##contig=<ID=chr14,length=107349540,assembly=b37>; ##contig=<ID=chr15,length=102531392,assembly=b37>; ##contig=<ID=chr16,length=90354753,assembly=b37>; ##contig=<ID=chr17,length=81195210,assembly=b37>; ##contig=<ID=chr18,length=78077248,assembly=b37>; ##contig=<ID=chr19,length=59128983,assembly=b37>; ##contig=<ID=chr20,length=63025520,assembly=b37>; ##contig=<ID=chr21,length=48129895,assembly=b37>; ##contig=<ID=chr22,length=51304566,assembly=b37>; ##contig=<ID=chrX,length=155270560,assembly=b37>; ##contig=<ID=chrY,length=59373566,assembly=b37>; ##contig=<ID=chrM,length=16569,assembly=b37>; ##fileDate=20160329; ##reference=human_g1k_v37.fasta; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	INTEGRATION; chr20	10000117	.	C	T	50	PASS	platforms=3;platformnames=Illumina,CG,Solid;datasets=3;datasetnames=HiSeqPE300x,CGnormal,SolidSE75bp;callsets=4;callsetnames=HiSeqPE300xfreebayes,HiSeqPE300xGATK,CGnormal,SolidSE75GATKHC;datasetsmissingcall=IonExome,SolidPE50x50bp;lowcov=CS_IonExomeTVC_lowcov,CS_SolidPE50x50GATKHC_lowcov,CS_SolidSE75GATKHC_lowcov;filt=CS_HiSeqPE300xGATK_filt	GT:PS:DP:GQ	0/1:.:706:878; chr20	10000211	.	C	T	50	PASS	platforms=2;platformnames=Illumina,CG;datasets=2;datasetnames=HiSeqPE300x,CGnormal;callsets=3;callsetnames=HiSeqPE300xfreebayes,HiSeqPE300xGATK,CGnormal;datasetsmissingcall=IonExome,SolidPE50x50bp,SolidSE75bp;lowcov=CS_IonExomeTVC_lowcov,CS_SolidPE50x50GATKHC_lowcov,CS_SolidSE75GATKHC_lowcov;filt=CS_SolidPE50x50GATKHC_filt	GT:PS:DP:GQ	0/1:.:695:984; chr20	10000439	.	T	G	50	PASS	platforms=3;platformnames=Illumina,CG,Solid;datasets=4;datasetnames=HiSeqPE300x,CGnormal,SolidPE50x50bp,SolidSE75bp;callsets=5;callsetnames=HiSeqPE300xfreebayes,HiSe",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/239:28848,INTEGRAT,INTEGRATION,28848,,https://github.com/google/deepvariant/issues/239,1,['INTEGRAT'],['INTEGRATION']
Integrability,"model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantMod",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:1540,message,message,1540,,https://github.com/google/deepvariant/issues/172,1,['message'],['message']
Integrability,model_train.py has flag message format error (crashes on --help),MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/78:24,message,message,24,,https://github.com/google/deepvariant/issues/78,1,['message'],['message']
Integrability,"mples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started.; I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False; Model: ""inceptionv3""; __________________________________________________________________________________________________; Layer (type) Output Shape Pa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:14136,depend,dependencies,14136,,https://github.com/google/deepvariant/issues/774,1,['depend'],['dependencies']
Integrability,"n Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory â€˜/mnt/cdw-genomeâ€™: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:5125,depend,dependency,5125,,https://github.com/google/deepvariant/issues/171,1,['depend'],['dependency']
Integrability,"nd estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)?. Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that?. **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:4328,message,messages,4328,,https://github.com/google/deepvariant/issues/171,1,['message'],['messages']
Integrability,"ndencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - &",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1731,Depend,Depends,1731,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability,"nfo to /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00013-of-00016.gz.example_info.json; I0203 17:23:09.199875 136895166957376 make_examples_core.py:2958] example_shape = None; I0203 17:23:09.200180 136895166957376 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 7, 9, 10]; I0203 17:23:09.201941 136895166957376 make_examples_core.py:301] Task 13/16: Found 0 candidate variants; I0203 17:23:09.202048 136895166957376 make_examples_core.py:301] Task 13/16: Created 0 examples. real 112m20.375s; user 1760m59.767s; sys 11m47.541s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/cromwell_root/tmp.cd83af44/tmpuzrx3yrs/call_variants_output.tfrecord.gz"" --examples ""/cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0203 17:23:14.218397 132068663560000 call_variants.py:471] Total 1 writing processes started.; W0203 17:23:14.224790 132068663560000 call_variants.py:482] Unable to read any records from /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord@16.gz. Output will contain zero records.; I0203 17:23:14.225926 132068663560000 call_variants.py:623] Complete: call_variants.; ```. And then the program hangs there for 10+ hours (UTC time when I'm reporting is Feb. 04, 04:05, and the program still appears running). . We've observed this for both ONT and HiFi data on multiple samples, further suggesting this isn't a data issue. Thanks!; Steve",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/769:3488,depend,dependencies,3488,,https://github.com/google/deepvariant/issues/769,1,['depend'],['dependencies']
Integrability,"ntime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing; # value in the environment (allowing command line control of the build),; # defaulting to 0 (CPU only build).; export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file; # compiled with MKL support for corei7 or better chipsets, which; # significantly speeds up execution when running on modern CPUs. The default; # TensorFlow wheel files don't contain these instructions (and thereby run on a; # broader set of CPUs). Using this optimized wheel reduces the runtime of; # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud; # Platform) optimized wheel because all GCP instances have at least Sandy Bridge; # or better chipsets, so this wheel should run anywhere on GCP.; export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl""; export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow""; export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries.; # If you already have CUDA installed, such as on a properly provisioned; # Docker image, it shouldn't be necessary.; export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python); export USE_DEFAULT_PYTHON_LIB_PATH=1; export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; }; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:5871,Bridg,Bridge,5871,,https://github.com/google/deepvariant/issues/145,1,['Bridg'],['Bridge']
Integrability,"ocker image and ran the following test on Centos OS 7. Everything worked fine. . INPUT_DIR=""/test/DeepVariant/quickstart-testdata""; OUTPUT_DIR=""/test/DeepVariant/quickstart-output""; BIN_VERSION=""0.8.0"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222:1050,message,message,1050,,https://github.com/google/deepvariant/issues/222,1,['message'],['message']
Integrability,"on.py"", line 1397, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run; outputs = _WrappedSession.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run; result = self._run(None, fetches, feed_dict, options_ptr,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run; results = self._do_run(handle, final_targets, final_fetches,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run; return self._do_call(_run_fn, feeds, fetches, targets, options,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call; raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter; tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; features, input_hooks = se",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:8056,message,message,8056,,https://github.com/google/deepvariant/issues/679,1,['message'],['message']
Integrability,"on.py"", line 1405, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1473, in run; outputs = _WrappedSession.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 970, in run; result = self._run(None, fetches, feed_dict, options_ptr,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1193, in _run; results = self._do_run(handle, final_targets, final_fetches,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1373, in _do_run; return self._do_call(_run_fn, feeds, fetches, targets, options,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1399, in _do_call; raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter; tensorflow.python.framework.errors_impl.DataLossError: truncated record at 19179998357' failed with EOF reached; 	 [[node IteratorGetNext; (defined at /usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py:60); ]]. Errors may have originated from an input operation.; Input Source operations connected to node IteratorGetNext:; In[0] IteratorV2 (defined at /usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py:58). Operation defined at: (most recent call last); >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; >>> tf.compat.v1.app.run(); >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 300, in run; >>> _run_main(main, args); >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 251, in _run_main; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:12782,message,message,12782,,https://github.com/google/deepvariant/issues/564,1,['message'],['message']
Integrability,"or/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7; 2020-09-24 03:47:45.652085: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] **Could not create cudnn handle:** CUDNN_STATUS_INTERNAL_ERROR; 2020-09-24 03:47:45.654628: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call; return fn(*args); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn; target_list, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:12480,message,message,12480,,https://github.com/google/deepvariant/issues/358,1,['message'],['message']
Integrability,"p_project).get_master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 257, in get_master; return self.master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 241, in master; cluster_spec = self.cluster_spec(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 311, in cluster_spec; network_endpoints = self._cloud_tpu_client.network_endpoints(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 320, in network_endpoints; response = self._fetch_cloud_tpu_metadata(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 234, in _fetch_cloud_tpu_metadata; service = self._tpu_service(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:2749,depend,dependency,2749,,https://github.com/google/deepvariant/issues/469,1,['depend'],['dependency']
Integrability,package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1258,Depend,Depends,1258,,https://github.com/google/deepvariant/issues/489,1,['Depend'],['Depends']
Integrability,"pt/bin/deepvariant/vcf_stats_report.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/show_examples.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/runtime_by_region_vis.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/multisample_make_examples.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_somatic.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/train.zip .; COPY --from=builder /opt/deepvariant/scripts/run_deepvariant.py .; COPY --from=builder /opt/deepvariant/scripts/run_deepsomatic.py . RUN ./run-prereq.sh. RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 0 && \; 	update-alternatives --install /usr/bin/python python /usr/bin/python${PYTHON_VERSION} 0. # Create shell wrappers for python zip files for easier use.; RUN \; BASH_HEADER='#!/bin/bash' && \; printf ""%s\n%s\n"" \; 	""${BASH_HEADER}"" \; 	'python3 /opt/deepvariant/bin/make_examples.zip ""$@""' > \; 	/opt/deepvariant/bin/make_examples && \; printf ""%s\n%s\n"" \; 	""${BASH_HEADER}"" \; 	'python3 /opt/deepvariant/bin/call_variants.zip ""$@""' > \; 	/opt/deepvariant/bin/call_variants && \; printf ""%s\n%s\n"" \; 	""${BASH_HEADER}"" \; 	'python3 /opt/deepvariant/bin/call_variants_slim.zip ""$@""' > \; 	/opt/deepvariant/bin/call_variants_slim && \; printf ""%s\n%s\n"" \; 	""${BASH_HEADER}"" \; 	'python3 /opt/deepvariant/bin/postprocess_variants.zip ""$@""' > \; 	/opt/deepvariant/bin/postprocess_variants && \; printf ""%s\n%s\n"" \; 	""${BASH_HEADER}"" \; 	'python3 /opt/deepvariant/bin/vcf_stats_report.zip ""$@""' > \; 	/opt/deepvariant/bin/vcf_stats_report && \; printf ""%s\n%s\n"" \; 	""${BASH_HEADER}"" \; 	'python3 /opt/deepvariant/bin/show_examples.zip ""$@""' > \; 	/opt/deepvariant/bin/show_examples && \; printf ""%s\n%s\n"" \; 	""${BASH_HEADE",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:5131,wrap,wrappers,5131,,https://github.com/google/deepvariant/issues/871,1,['wrap'],['wrappers']
Integrability,"pvariant:1.6.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${6} \; --ref=./human_g1k_v37_decoy.fasta \; --reads=./${2}_md.recal.cram \; --output_vcf=./${2}_hg37.dv.vcf.gz \; --output_gvcf=./${2}_hg37.dv.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=32; ```; Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started.; W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records.; I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants.; ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards; Erin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/776:1405,depend,dependencies,1405,,https://github.com/google/deepvariant/issues/776,1,['depend'],['dependencies']
Integrability,pyclif dependency,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/93:7,depend,dependency,7,,https://github.com/google/deepvariant/issues/93,1,['depend'],['dependency']
Integrability,"python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:16099,message,message,16099,,https://github.com/google/deepvariant/issues/358,1,['message'],['message']
Integrability,"rectory and extension); BASE_NAME=$(basename ""${BAM_FILE}"" .bam). # Define the output VCF file name; VCF_FILE=""${VCF_DIR}/${BASE_NAME}.vcf.gz""; echo $BAM_FILE; echo $VCF_FILE; singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref $REFERENCE \; --reads $BAM_FILE \; --regions 6:32509320-32669663 \; --output_vcf $VCF_FILE \; --num_shards 12; done; ``` . - Error trace: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference_HLA/chr6_hg19.fa"" --reads ""./MDC05_1463_3.final.bam"" --examples ""/tmp/7361351.1.gpu.q/tmpzsp9g_vq/make_examples.tfrecord@12.gz"" --channels ""insert_size"" --regions ""chr6:32509320-32669663"" --task {}. [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when serializing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; Traceback (most recent call last):; File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>; app.run(main); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 220, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/870:1876,protocol,protocol,1876,,https://github.com/google/deepvariant/issues/870,1,['protocol'],['protocol']
Integrability,"rity, but I got the same error. I've tried to find a solution through various online resources, but nothing has helped so far. `Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 532, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 518, in main; train(FLAGS.config); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 121, in train; tune_dataset_config = data_providers.read_dataset_config(; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/data_providers.py"", line 634, in read_dataset_config; dataset_config = text_format.Parse(; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 648, in Parse; return ParseLines(text.split(b'\n' if isinstance(text, bytes) else u'\n'),; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 722, in ParseLines; return parser.ParseLines(lines, message); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 776, in ParseLines; self._ParseOrMerge(lines, message); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 804, in _ParseOrMerge; self._MergeField(tokenizer, message); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 894, in _MergeField; raise tokenizer.ParseErrorPreviousToken(; google.protobuf.text_format.ParseError: 13:1 : Message type ""learning.genomics.deepvariant.DeepVariantDatasetConfig"" has no field named ""s2"".`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/837:1937,message,message,1937,,https://github.com/google/deepvariant/issues/837,4,"['Message', 'message']","['Message', 'message']"
Integrability,"s [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory â€˜/mnt/cdw-genomeâ€™: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence thi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:4918,depend,dependency,4918,,https://github.com/google/deepvariant/issues/171,1,['depend'],['dependency']
Integrability,"s/tensorflow_core/python/training/monitored_session.py"", line 1418, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1176, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolera",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:15775,message,message,15775,,https://github.com/google/deepvariant/issues/358,1,['message'],['message']
Integrability,"s/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model; return self._train_model_default(input_fn, hooks, saving_listeners); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default; return self._train_with_estimator_spec(estimator_spec, worker_hooks,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec; tf.compat.v1.train.warm_start(*self._warm_start_settings); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start; checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint; distribution_strategy_context.get_replica_context().merge_call(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call; return self._merge_call(merge_fn, args, kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call; return merge_fn(self._strategy, *args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper; return func(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>; init_from_checkpoint_fn = lambda _: _init_from_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint; raise ValueError(; ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader.; ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/500:3981,wrap,wrapper,3981,,https://github.com/google/deepvariant/issues/500,1,['wrap'],['wrapper']
Integrability,"se tell me what went wrong?; My cmd:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref ${fasta} \; --reads ${Input.bam} \; --output_vcf output/output.vcf.gz \; --output_gvcf output/output.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir output/intermediate_results_dir \; --regions chr20 \; --customized_model model/weights-51-0.995354.ckpt; ```. Error message:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco; rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started.; I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]; I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False; /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels.; input_shape = imagenet_utils.obtain_input_shape(; Traceback (most recent call last):; F",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725:1170,depend,dependencies,1170,,https://github.com/google/deepvariant/issues/725,1,['depend'],['dependencies']
Integrability,security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~2021101,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:5907,depend,dependency,5907,,https://github.com/google/deepvariant/issues/489,1,['depend'],['dependency']
Integrability,"t packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-pac",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:12478,depend,dependency,12478,,https://github.com/google/deepvariant/issues/89,1,['depend'],['dependency']
Integrability,"t.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:42:05 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:7886,depend,dependency,7886,,https://github.com/google/deepvariant/issues/89,1,['depend'],['dependency']
Integrability,"the deep variant wrapper dv_call_variants.py crushing when installed using conda. **Setup**; - Ubuntu 20.04:; - DeepVariant version - 1.4.0:; - Installation method - Conda; - Type of data - sequencing, illumina. **Steps to reproduce:**; - Command:; dv_call_variants.py --outfile OUTFILE --examples EXAMPLES --sample SAMPLE. - Error trace: (if applicable) ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_zviaa5zy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 39, in <module>; import numpy as np; ModuleNotFoundError: No module named 'numpy'. numpy installed in enviroment",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/573:17,wrap,wrapper,17,,https://github.com/google/deepvariant/issues/573,1,['wrap'],['wrapper']
Integrability,"ting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```; sudo sh run_deepvariant.sh; I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0; 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA; 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters; W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ; I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:1096,message,message,1096,,https://github.com/google/deepvariant/issues/166,1,['message'],['message']
Integrability,"tions/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 497, in run_script; subprocess_call(command_args, env=env, path=dirname(path)); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/gateways/subprocess.py"", line 56, in subprocess_call; output=_format_output(command_str, path, rc, stdout, stderr)); subprocess.CalledProcessError: Command '['/bin/bash', '-x', '/PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh']' returned non-zero exit status 1.; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 327, in _execute_actions; run_script(target_prefix, Dist(pkg_data), 'post-unlink' if is_unlink else 'post-link'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 513, in run_script; raise LinkError(message); conda.exceptions.LinkError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 281, in execute; pkg_data, actions); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 344, in _execute_actions; reverse_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:1331,message,message,1331,,https://github.com/google/deepvariant/issues/252,1,['message'],['message']
Integrability,tu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (=,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6113,depend,dependency,6113,,https://github.com/google/deepvariant/issues/489,1,['depend'],['dependency']
Integrability,"u bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6496,depend,dependencies,6496,,https://github.com/google/deepvariant/issues/489,2,"['Depend', 'depend']","['Depends', 'dependencies']"
Integrability,"unfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants; output_queue = multiprocessing.Queue(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s; user 0m6.224s; sys 0m3.683s. ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```; gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz; gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json; gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz; gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.exa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:3233,synchroniz,synchronize,3233,,https://github.com/google/deepvariant/issues/733,1,['synchroniz'],['synchronize']
Integrability,"y:301] Task 1/4: Found 0 candidate variants; I0729 14:44:37.899752 140710547908416 make_examples_core.py:301] Task 1/4: Created 0 examples; I0729 14:44:37.893192 139779121772352 make_examples_core.py:301] Task 2/4: Writing example info to /tmp/tmpkcjcf0p_/make_examples.tfrecord-00002-of-00004.gz.example_info.json; I0729 14:44:37.893293 139779121772352 make_examples_core.py:2958] example_shape = None; I0729 14:44:37.893665 139779121772352 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0729 14:44:37.894033 139779121772352 make_examples_core.py:301] Task 2/4: Found 0 candidate variants; I0729 14:44:37.894105 139779121772352 make_examples_core.py:301] Task 2/4: Created 0 examples. real	0m4.791s; user	0m11.503s; sys	0m2.085s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpkcjcf0p_/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpkcjcf0p_/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wes"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0729 14:44:41.088234 139722246891328 call_variants.py:471] Total 1 writing processes started.; W0729 14:44:41.090612 139722246891328 call_variants.py:482] Unable to read any records from /tmp/tmpkcjcf0p_/make_examples.tfrecord@4.gz. Output will contain zero records.; I0729 14:44:41.091079 139722246891328 call_variants.py:623] Complete: call_variants. **Does the quick start test work on your system?**; yes. **Any additional context:**; Some samples work fine, some very similar samples keep running",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/855:11920,depend,dependencies,11920,,https://github.com/google/deepvariant/issues/855,1,['depend'],['dependencies']
Integrability,"ys.exit(main(argv)); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 220, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 157, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 109, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 170, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 240, in __init__; self.header = self._reader.header; google.protobuf.message.DecodeError: Error parsing message. ```. I ran the WES example from you with no problem, but I experience issues with my own data (I have the same setup when running singularity). ; I checked the reference and input bam files, they don't seem to be corrupted... but just googling the error did not help much. Cannot think of anything else, maybe you have some suggestions where the problem could be coming from? Otherwise I will try to find the raw data for my .bam files and do remapping to hg38 and use the ref that worked previously. . Thanks again!!; Alisa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/870:4062,message,message,4062,,https://github.com/google/deepvariant/issues/870,2,['message'],['message']
Modifiability," 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Kera",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1999,config,config,1999,,https://github.com/google/deepvariant/issues/876,1,['config'],['config']
Modifiability," 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I1214 06:10:30.995939 140363019278144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; 2022-12-14 06:10:31.060396: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-12-14 06:10:31.101084: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1214 06:10:31.288279 140363019278144 openvino_estimator.py:55] Processing ckpt=/opt/models/wes/model.ckpt, tensor_shape=[100, 221, 7]; /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1083: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:678: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:2441: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:118: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1638: UserWarning: `layer.apply` is deprecated",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597:7950,layers,layers,7950,,https://github.com/google/deepvariant/issues/597,2,['layers'],['layers']
Modifiability, InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:118877,variab,variable,118877,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability, Unchanged; I0415 07:34:37.968360 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.969041 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.969827 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.970808 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.971487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.972088 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.972737 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.973396 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.974112 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.974828 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.975663 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.976491 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:91811,variab,variable,91811,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability, Unchanged; I0415 07:34:37.969827 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.970808 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.971487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.972088 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.972737 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.973396 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.974112 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.974828 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.975663 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.976491 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.977374 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.978363 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixe,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:92154,variab,variable,92154,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability, Unchanged; I0415 07:34:37.994658 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.995343 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.996014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.997136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.997966 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.998590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.999377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.000032 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.000669 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.001136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.001605 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.002159 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixe,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:97792,variab,variable,97792,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability, Unchanged; I0415 07:34:38.006367 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006792 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007198 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.008188 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.008778 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.009377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.010077 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.010946 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.011956 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013191 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013811 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:101197,variab,variable,101197,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability, Unchanged; I0415 07:34:38.016633 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.017230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.017909 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018465 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018997 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.019490 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.020006 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.020472 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.020929 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.021404 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.021804 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.022231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:103766,variab,variable,103766,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability, Unchanged; I0415 07:34:38.017230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.017909 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018465 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018997 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.019490 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.020006 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.020472 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.020929 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.021404 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.021804 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.022231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.022672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:103934,variab,variable,103934,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability, Unchanged; I0415 07:34:38.024280 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.024672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025057 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025403 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.026254 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.026756 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027156 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027498 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.027940 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.028300 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.028651 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:106499,variab,variable,106499,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability, Unchanged; I0415 07:34:38.025403 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.026254 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.026756 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027156 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027498 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.027940 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.028300 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.028651 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.029017 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029366 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029716 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixe,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:107017,variab,variable,107017,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability, Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:110254,variab,variable,110254,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability, Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/B,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:112140,variab,variable,112140,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability, Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:114512,variab,variable,114512,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability, Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/w,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:117062,variab,variable,117062,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability," \; -t deepvariant_gpu .; ```. I am building this docker image on my laptop (M3 macbook). #### PackagesNotFoundError error. The first error I get is -. ```; 1.247 Platform: linux-aarch64; 1.247 Collecting package metadata (repodata.json): ...working... done; 6.190 Solving environment: ...working... failed; 6.260; 6.260 PackagesNotFoundError: The following packages are not available from current channels:; 6.260; 6.260 - bioconda::samtools==1.15; 6.260 - bioconda::bcftools==1.15; 6.260; ```. I resolved this error by removing the version numbers. i.e., removed the `==1.15` from both the lines. #### Error in the build-prerunreq.sh script. Once, I cross the previous error, I get this error -. ```; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-update",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:1352,config,config,1352,,https://github.com/google/deepvariant/issues/902,1,['config'],['config']
Modifiability," and everything works when I run the command on my local machine with my GPU or on our cluster with CPUs. The only difference is that the shuffled examples are stored on a mounted bucket with gcsfuse when I run the command on the Google VM. The bucket is mounted using the following command: `sudo gcsfuse -o allow_other`. I have also tried to use singularity, but I got the same error. I've tried to find a solution through various online resources, but nothing has helped so far. `Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 532, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 518, in main; train(FLAGS.config); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 121, in train; tune_dataset_config = data_providers.read_dataset_config(; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/data_providers.py"", line 634, in read_dataset_config; dataset_config = text_format.Parse(; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 648, in Parse; return ParseLines(text.split(b'\n' if isinstance(text, bytes) else u'\n'),; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 722, in ParseLines; return parser.ParseLines(lines, message); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 776, in ParseLines; self._ParseOrMerge(lines, message); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 804, in _ParseOrMerg",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/837:1228,config,config,1228,,https://github.com/google/deepvariant/issues/837,1,['config'],['config']
Modifiability," hyperparameters to optimise the training. . However . . . . I notice when tracking the model eval stats (specifically f1, precision, recall), that the hom_ref classifications are much less reliable than hom_alt and het classes. My question is whether this is to be expected, or whether there might be something wrong with my training setup, or perhaps the examples. . The test example set I am using to tune the hyperparams looks like this:. ```; # Generated by shuffle_tfrecords_beam.py; # class2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1208,config,config,1208,,https://github.com/google/deepvariant/issues/904,1,['config'],['config']
Modifiability," its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.5.0; - Tensorflow 2.11.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HIFI reads. **Steps to reproduce:**; ```; rule deepvariant_make_examples:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \; --mode callin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:1246,config,config,1246,,https://github.com/google/deepvariant/issues/677,1,['config'],['config']
Modifiability, prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:113819,variab,variable,113819,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability," the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:; ```; #!/bin/bash. #SBATCH --job-name=Deepvariant_debug; #SBATCH --cpus-per-task=2 # change this according to your needs; #SBATCH --mem=8G # change this according to your needs; #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes; #SBATCH --output=myrun.o%j; #SBATCH --error=myrun.e%j. mkdir -p output; mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000; BIN_VERSION=""1.2.0""; # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder; export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output ; export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata ; # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add; # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container; # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif); singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \; /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \; --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \; --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed; --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/515:1073,variab,variables,1073,,https://github.com/google/deepvariant/issues/515,1,['variab'],['variables']
Modifiability," wrong with my training setup, or perhaps the examples. . The test example set I am using to tune the hyperparams looks like this:. ```; # Generated by shuffle_tfrecords_beam.py; # class2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840515, train/f1_homalt=0.964401364326477, train/f1_homref=0.902255654335022, train/f1_macro=0.; 8698380589485168, train/f1_micro=0.91796875, train/f1_weighted=0.9241795539855957, train/false_negatives=34.0, train/false_positives=14.0, train/learning_rate=9.999999747378752e-06, train/loss=0.6384731531143188, trai",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1524,config,config,1524,,https://github.com/google/deepvariant/issues/904,1,['config'],['config']
Modifiability,"![DeepTrio_QUAL](https://user-images.githubusercontent.com/22089494/114759224-e5d49180-9d2b-11eb-9c5e-cb33c9979d2d.png); Hello, . I am running DeepTrio for a dataset with known true-positive SNPs and indels. I followed guidelines for DeepTrio but had to change --config DeepVariantWGS to DeepVariant_unfiltered at the glnexus_cli step as a default QUAL threshold of 10 removed a lot of my TP calls.; I have compared distributions of QUAL score in the TP subset and all calls found by DeepTrio. Please see an attached histogram of all DeepTrio calls vs TP calls. Could you please tell me if it is expected that QUAL of TP calls is between 0 and30, while there are calls with QUAL of up to 100? If not, what I could do wrong?; Thank you!. Best regards,; Maria. **Setup**; - Operating system: Linux; - DeepVariant version: deepvariant_deeptrio-1.1.0.sif; - Installation method (Docker, built from source, etc.): singularity/3.6.4; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) HiSeq X Ten, hg38, WGS",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440:263,config,config,263,,https://github.com/google/deepvariant/issues/440,1,['config'],['config']
Modifiability,"${MODEL}"", \; DOCKER_IMAGE=""${DOCKER_IMAGE}"", \; DOCKER_IMAGE_GPU=""${DOCKER_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, since apt-utils is not installed; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 663 100 663 0 0 5012 0 --:--:-- --:--:-- --:--:-- 5022; debconf: delaying package configuration, since apt-utils is not installed; WARNING: Logging before flag parsing goes to stderr.; ```. But I then see many messages about candidate variants it's found. . The directory `staging-folder/examples/0/` also includes 8 `.gz` files like `examples_output.tfrec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:3055,config,configuration,3055,,https://github.com/google/deepvariant/issues/60,2,['config'],['configuration']
Modifiability,"'_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0826 20:44:28.953605 47737984214848 call_variants.py:446] Writing calls to /paedyl01/disk1/yangyxt/test_tmp/call_variants_output.tfrecord.gz; INFO:tensorflow:Calling model_fn.; I0826 20:44:29.309295 47737984214848 estimator.py:1173] Calling model_fn.; /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1083: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:678: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:2441: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:118: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1638: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); INFO:tensorflow:Done calling model_fn.; I0826 20:44:33.173107 47737984214848 estimator.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:5007,layers,layers,5007,,https://github.com/google/deepvariant/issues/564,2,['layers'],['layers']
Modifiability,"(from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0626 13:39:06.145823 140632388314944 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 558, in call_variants; model.load_weights(checkpoint_path).e",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:1374,variab,variables,1374,,https://github.com/google/deepvariant/issues/845,1,['variab'],['variables']
Modifiability,"(from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in main; call_variants(; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 640, in call_variants; model.load_weights(checkpoint_path).e",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:2161,variab,variables,2161,,https://github.com/google/deepvariant/issues/857,1,['variab'],['variables']
Modifiability,") and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 558, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored: ; <tf.Variable 'conv2d/kernel:0' shape=(3, 3, 7, 32) dtype=float32, numpy=; ; My knowledge in deep learning models is not the best, so if you could please tell me how to overcome this error, as the RNA model seems to have very promising results for RNA variant calling and i want to use it. **Setup**; - Operating system: Ubuntu 20.0; - DeepVariant version: Latest version 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: GIAB benchmark data used in the deepvariant-rnaseq-case-study.md but not restricted to chr20. **Steps to reproduce:**; - Command: ; docker run -v ""$(pwd):$(pwd)"" -w $(pwd) google/deepvariant:latest run_deepvariant --model_type=WES --customized_model=model/model.ckpt --ref=GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta --reads=STAR/Mapping/marked_split.bam --output_vcf=STAR/Mapping/deepvariant.rna.vcf --num_shards=$(nproc)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:2762,Variab,Variable,2762,,https://github.com/google/deepvariant/issues/845,1,['Variab'],['Variable']
Modifiability,"**Describe the issue:**; /opt/deepvariant/bin/run_deepvariant crashes when start the GPU stage of call variants. **Setup**; google/deepvariant:0.10.0; Docker; subset of illumina resequencing data; $nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2017 NVIDIA Corporation; Built on Fri_Nov__3_21:07:56_CDT_2017; Cuda compilation tools, release 9.1, V9.1.85; $ nvidia-smi; NVIDIA-SMI 450.51.06 Driver Version: 450.51.06 CUDA Version: 11.0 ; GeForce RTX 2070 super. **Workaround**; Apparently the gpu module is consuming all my memmory (8gb), possilbe "" config.gpu_options.allow_growth = True"" not present in the script?. **Command line**. `BIN_VERSION=""1.0.0""`; `BASE=""${PWD}/deepvariant-run""`; `INPUT_DIR=""${BASE}/input""`; `REF=""10consensus.fasta""`; `REF2=""reftst.fa""`; `BAM=""268_041_m10.sorted.bam""`; `BAM2=""tst.sorted.bam""`; `OUTPUT_DIR=""${BASE}/output""`; `DATA_DIR=""${INPUT_DIR}/data""`; `OUTPUT_VCF=""M10.output.vcf.gz""`; `OUTPUT_VCF2=""TST.output.vcf.gz""`; `OUTPUT_GVCF=""M10.output.g.vcf.gz""`; `OUTPUT_GVCF2=""TST.output.g.vcf.gz""`; `sudo docker run --gpus 1 -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}-gpu"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""/input/${REF2}"" --reads=""/input/${BAM2}"" --output_vcf=/output/${OUTPUT_VCF} --output_gvcf=/output/${OUTPUT_GVCF} --intermediate_results_dir /output/intermediate_results_dir --num_shards=30`. **Error trace**; ................ 2020-09-24 03:47:35.386802: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores GWNJ-1012:204:GW191209000:1:1101:22544:2049: Not found: Could not read base quality scores; I0924 03:47:35.394492 139826099087104 make_examples.py:587] Task 28/30: Found 88 candidate variants; I0924 03:47:35.394706 139826099087104 make_examples.py:587] Task 28/30: Created 88 examples; I0924 03:47:35.416212 139915800631040 make_examples.py:587] Task 9/30: Found 74 candidate variants; I0924 03:47:35.416471 139915800631040 make_ex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:576,config,config,576,,https://github.com/google/deepvariant/issues/358,1,['config'],['config']
Modifiability,"**Describe the issue:**; Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.5.0; - Tensorflow 2.11.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HIFI reads. **Steps to reproduce:**; ```; rule deepvariant_make_examples:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:870,config,config,870,,https://github.com/google/deepvariant/issues/677,1,['config'],['config']
Modifiability,"**Describe the issue:**; Hello, I am trying to run the deepvariant RNA model on HG005 data locally (not restricted to chr20 like the Github example) but the model keeps generating this error while running the call_variants.py step. WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0626 13:39:06.145823 140632388314944 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:627,variab,variables,627,,https://github.com/google/deepvariant/issues/845,1,['variab'],['variables']
Modifiability,"**Describe the issue:**; In ```make_examples```: The middle base of reference sequence in the window doesn't match first character of variant.reference_bases. **Setup**; - Operating system: CentOS Linux v7; - DeepVariant version: 1.1.0; - Installation method: Docker; - Type of data: WGS (Illumina 150nt pairs from GIAB HG002). **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes, it does.; Is there any way to reproduce the issue by using the quick start? No. **Any additional context:**; The goal is to call SNPs and indels in GIAB HG002 WGS data, and to compare the results with a truthset. High-confidence intervals and the truthset are at https://github.com/genome-in-a-bottle/giab_latest_release. Please see the attached bash script (command line) and output files. Two questions:; - Is ```make_examples``` parameterized correctly (see attached script and output files)?; - Can someone please explain what this error message means and suggest an appropriate approach to troubleshooting and fixing it?. [vcall.log](https://github.com/google/deepvariant/files/5858295/vcall.log); [vcall.sh.txt](https://github.com/google/deepvariant/files/5858303/vcall.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/411:886,parameteriz,parameterized,886,,https://github.com/google/deepvariant/issues/411,1,['parameteriz'],['parameterized']
Modifiability,"**Describe the issue:**; When calling I get a low number of records in a vcf file (20-40k variants). In comparison with GATK there are at last 10 times less variants generated. Also, it is much less than from 1 thousand genomes deepvariant dataset truncated to the same exome region - 1.7 mln records. **Setup**; - Operating system:; CentoOS 7; - DeepVariant version:; 1.3.0; - Installation method (Docker, built from source, etc.):; podman/singularity; - Type of data:; The data are human's whole exome sequences from Illumina. As a reference I use hg38 with alt contigs. I use truseq v1.2 exome bed file. Besides using bwa-mem2, samtools merge and sort there is not much preprocessing. **Steps to reproduce:**; - Command:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=\$PWD/$idxbase \; --reads=\$PWD/${sample_id}.bam \; --regions=\$PWD/$bed_file \; --output_vcf=\$PWD/${sample_id}.vcf.gz \; --output_gvcf=\$PWD/${sample_id}.g.vcf.gz \; --num_shards=${task.cpus}. ```. I also used glnexus; ```; glnexus_cli ; --bed /in/truseq-dna-exome-targeted-regions-manifest-v1-2.bed ; --config DeepVariantWES /in/vcf_deepvariant/*.g.vcf.gz > merged.bcf; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/501:1102,config,config,1102,,https://github.com/google/deepvariant/issues/501,1,['config'],['config']
Modifiability,"**Describe the issue:**; i have use docker pull from google/deepvariant:1.6.0-gpu ,but the python version is 3.11 in this containï¼Œi don't know why. <img width=""1165"" alt=""iShot_2023-12-14_15 23 49"" src=""https://github.com/google/deepvariant/assets/15654389/d61f07f0-b540-4bda-8a9e-ccb633cfe3e7"">. I want to debug with source code, but there will be this error, does this deepvariant.proto need to be compiled manuallyï¼Ÿ. `ironment variable `TF_ENABLE_ONEDNN_OPTS=0`.; Traceback (most recent call last):; File ""/code/deepvariant/deepvariant/call_variants.py"", line 48, in <module>; from deepvariant import dv_utils; File ""/code/deepvariant/deepvariant/dv_utils.py"", line 44, in <module>; from deepvariant.protos import deepvariant_pb2; ImportError: cannot import name 'deepvariant_pb2' from 'deepvariant.protos' (unknown location); `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753:430,variab,variable,430,,https://github.com/google/deepvariant/issues/753,1,['variab'],['variable']
Modifiability,"**Have you checked the FAQ? [](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-wgs-case-study.md). **Describe the issue:**; Merging vcf files error.; **Setup**; - Operating system: working on cluster ; - DeepVariant version:latest; - Installation method (Docker):; - Type of data: (GIAB AshkenazimTrio [HG002,HG003,HG004] analysis.). **Steps to reproduce:**; - Command: ; ```; udocker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | udocker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | udocker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```; - Error trace: (if applicable); ; > Num BCF records read 118736378 query hits 14552613; > [E::bgzf_read_block] Invalid BGZF header at offset 265038798; > [E::bgzf_read] Read block operation failed with error 2 after 0 of 32 bytes; > [E::bgzf_read] Read block operation failed with error 3 after 0 of 32 bytes; > Error: BCF read err. ![Screenshot from 2024-05-06 15-00-29](https://github.com/google/deepvariant/assets/45700858/2c6e5565-78aa-4e55-9488-82b2f5f04514)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/815:503,config,config,503,,https://github.com/google/deepvariant/issues/815,1,['config'],['config']
Modifiability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: **YES**. **Describe the issue:**. Manually selected regions (a single region is formed by a locus extending 500 bp to both sides) were used in my project to make examples, and it was also succeed in calling variants. However, when I running postprocess_variants, something went wrong. I check the log, and I guess it was related to the wrong ""call_variant_outputs"". So I printed one ""call_variant_outputs"" out of the whole tfrecord, and found out there are several repeated variant in one call. Where did I go wrong?. **The log file is attached.**; [postprocess_variants.log](https://github.com/google/deepvariant/files/7149887/postprocess_variants.log). **Setup**; - Operating system: ubuntu **16**; - DeepVariant version: **0.7.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **NO**. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). > W0912 23:51:01.891268 140429229119232 postprocess_variants.py:331] Alt allele indices found from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/485:189,extend,extending,189,,https://github.com/google/deepvariant/issues/485,1,['extend'],['extending']
Modifiability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; yes. **Describe the issue:**; Version 1.2 installed via docker on a linux server (over SSH login), running the quickstart test run:; - Expected behavior: when running without sudo, process uses current user's name privilege.; - What happened: file access denied if folder permission is 744. The run successfully returns if manually setting the relevant folders to permission 777, but output (vcf files and report) files were owned by nobody/nobody. . My understanding is that nobody is a special handle meant for OS housekeeping works. Is this an expected behavior? Is it docker?. **Setup**; - Operating system: CentOS 7 (`cat /etc/os-release`); - DeepVariant version: 1.2; - Installation method: docker; - Type of data: The test data and command described in [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). **Steps to reproduce:**; - Command: identical to those of [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). Environment variable setup lines were directly pasted into the shell, the 'run everything' command was pasted into a file `cmd.sh` which was then was ran with `. cmd.sh`. **Does the quick start test work on your system?**; Yes. Outputs are fine. **Any additional context:**; Except having to add `mkdir` and `chmod` lines to the script, I found the run successful. I can read/write to the files owned by nobody and the ownership will transfer automatically upon writing.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/478:1116,variab,variable,1116,,https://github.com/google/deepvariant/issues/478,1,['variab'],['variable']
Modifiability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; Downstream association analysis has yielded a high number of false positive findings, essentially a product of low quality data. It is crucial that these sites are filtered out. . **Setup**; - Operating system: Linux; - DeepVariant version: Latest; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): Illumina WES. **Steps to reproduce:**; - Command: . DeepVariant:; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref ${ref} \; --reads ${cram_in} \; --regions ${regions} \; --output_gvcf ${sample}.g.vcf.gz \; --output_vcf ${sample}.vcf.gz \; --num_shards 8 \. GLnexus:; glnexus_cli --config DeepVariantWES --bed ${regions} \; 2_gvcf/*.g.vcf.gz > 3_bcf/Exomes.bcf. **Any additional context:**. Hi there!; Apologies for bringing up another similar issue, but I would like some help with the correct filtering of my merged vcf file.; Essentially, I have identified a significant number of false positive sites in a downstream assoc. analysis, where MAF for these variants is widely different than the population average. This strongly suggests that these sites are of low quality and need to be filtered out. Here are some examples from the merged vcf file. For each variant I have only shown a handful of samples (total is over 5000):. False positive / bad site that needs filtering:. `1	1722625	1_1722625_A_T	A	T	48	.	AF=0.222894;AQ=48	GT:DP:AD:GQ:PL:RNC	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	1/1:7:0,7:42:44,47,0:..	0/0:0:0,0:1:0,0,0:..	1/1:6:0,6:36:38,38,0:..	0/1:12:3,9:0:19,2,0:..	./.:3:3,0:0:20,0,50:II	1/1:2:0,2:23:29,25,0:..	0/0:2:2,0:6:0,6,59:..	1/1:2:0,2:22:31,24,0:..	1/1:2:0,2:26:28,29,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	1/1:7:0,7:40:43,42,0:..	./.:3:3,0:0:20,0,50:II	1/1:4:1,3:1:28,3,0:..	0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/645:844,config,config,844,,https://github.com/google/deepvariant/issues/645,1,['config'],['config']
Modifiability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; I do not understand how Deepvariant decides if there is a call at sites with low/zero GQ/DP... below is an example of a merged VCF with GLNexus and the same sites in the individual gVCFs. . **Setup**; - Operating system: Linux; - DeepVariant version: 1.5; - Installation method (Docker, built from source, etc.): Docker; - Type of data: WES Data (Illumina). **Steps to reproduce:**; - Command:; - ; DeepVariant:. ```; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref ${ref} \; --reads ${cram_in} \; --regions ${regions} \; --output_gvcf ${sample}.g.vcf.gz \; --output_vcf ${sample}.vcf.gz \; --num_shards 8 \; ```. GLNexus:. ```; glnexus_cli --config DeepVariantWES --bed /work_beegfs/***/exomes/hg38_exomregions_withoutCHR.bed /work_beegfs/***/exomes/2_gvcf/*.g.vcf.gz > /work_beegfs/***/exomes/3_bcf/Exomes.bcf; bcftools view ../3_bcf/Exomes.bcf | bgzip -@ 4 -c > Exomes.vcf.gz; bcftools index Exomes.vcf.gz; ```. Merged VCF with GLNexus:. `1	69897	1_69897_T_C	T	C	48	.	AF=0.076465;AQ=48	GT:DP:AD:GQ:PL:RNC	0/0:0:0,0:1:0,3,29:..	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,3,29:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,3,29:..	0/0:0:0,0:1:0,0,0:..	1/1:3:0,3:15:31,19,0:..	0/0:0:0,0:1:0,3,29:..	./.:6:2,4:8:0,8,13:II	./.:1:1,0:0:29,3,0:II	1/1:2:0,2:7:13,11,0:..	./.:1:1,0:0:29,3,0:II`. Same site/samples gVCF (each line is one sample in same order as merged file above):; ```. 1	69792	.	T	<*>	0	.	END=70036	GT:GQ:MIN_DP:PL	0/0:1:0:0,3,29; 1	69638	.	C	<*>	0	.	END=70036	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0; 1	69850	.	C	<*>	0	.	END=69929	GT:GQ:MIN_DP:PL	0/0:1:0:0,3,29; 1	69897	.	T	<*>	0	.	END=69897	GT:GQ:MIN_DP:PL	./.:0:1:29,3,0; 1	69683	.	T	<*>	0	.	END=69911	GT:GQ:MIN_DP:PL	0/0:1:0:0,3,29; 1	69037	.	G	<*>	0	.	END=70036	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0; 1	69897	.	T	C,<*>	31.7	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:19:3:0,3,0:1,0:31,19,0,990,990,990; 1	69848	.	G	<*>	0	.	END=69920	GT:GQ:M",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/639:782,config,config,782,,https://github.com/google/deepvariant/issues/639,1,['config'],['config']
Modifiability,"**Issue**; I am using the docker you provided, while working on a remote machine.; Using Pycharm Professional's Services tab, I configured my interpreter to run the code I have on my local clone of the entire git. ; I am trying to run the ""make_examples.py"" file line-by-line, to understand it better. The entire clone is on my remote machine, and it runs with the docker container's interpreter. When I debug the code, there are many unresolved references. ; Some examples are:; `from third_party.nucleus.protos import reads_pb2`; `from deepvariant.protos import deepvariant_pb2`; `from deepvariant.python import pileup_image_native`; `from deepvariant.protos import deepvariant_pb2`; `from deepvariant.python import allelecounter`; `from third_party.nucleus.io.python import hts_verbose`; ...; I looked for these files, and they aren't there.; I understand there is something very basic that I misunderstand, so thanks in advance for your patience!. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: irrelevant. **Does the quick start test work on your system?**; I have succeeded in running the quick start example on the remote machine, through the terminal.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/359:128,config,configured,128,,https://github.com/google/deepvariant/issues/359,1,['config'],['configured']
Modifiability,"*Describe the issue:**; run demo inside Best practices for multi-sample variant calling with DeepVariant failed. **Setup**; - Operating system: centos 7,; - DeepVariant version:1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run -v ""${DIR}"":""/data"" google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/data/hs37d5.fa"" --reads=""/data/HG002.bam"" --regions=""/data/agilent_sureselect_human_all_exon_v5_b37_targets.bed"" --output_vcf=""/data/HG002.vcf.gz"" --output_gvcf=""/data/HG002.gvcf.gz"" --num_shards=25; - Error trace: (if applicable): ; [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1529, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126, in m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/455:992,extend,extend,992,,https://github.com/google/deepvariant/issues/455,1,['extend'],['extend']
Modifiability,", data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2068,config,config,2068,,https://github.com/google/deepvariant/issues/876,1,['config'],['config']
Modifiability,", precision, recall), that the hom_ref classifications are much less reliable than hom_alt and het classes. My question is whether this is to be expected, or whether there might be something wrong with my training setup, or perhaps the examples. . The test example set I am using to tune the hyperparams looks like this:. ```; # Generated by shuffle_tfrecords_beam.py; # class2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840515, train/f1_homalt=0.964401364326477, train/f1_homref=0.902255654335022, train/f1_macro=0.; 8698380589485168, train/f1_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1332,config,config,1332,,https://github.com/google/deepvariant/issues/904,1,['config'],['config']
Modifiability,"-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:42:05 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:801::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:801::200e 80]; W: Some index files failed to download. They have been",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:9117,config,configured,9117,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"-error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:2881,config,config,2881,,https://github.com/google/deepvariant/issues/797,1,['config'],['config']
Modifiability,"-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:801::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:801::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Build",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:10555,config,configured,10555,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:6978,config,configured,6978,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:36,config,config,36,,https://github.com/google/deepvariant/issues/231,2,['config'],['config']
Modifiability,.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/socksipy-branch/socks.pyc'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/rpm/mapping/command_mapping.yaml'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/rpm/mapping/component_mapping.yaml'; specified in the package manifest cannot be found.; ```; (there are many similar CondaVerificationErrors before this); Conda info:; ```; active environment : longshot; active env location : /home/pedge/anaconda3/envs/longshot; shell level : 2; user config file : /home/pedge/.condarc; populated config files : /home/pedge/.condarc; conda version : 4.6.9; conda-build version : 3.17.6; python version : 3.7.1.final.0; base environment : /home/pedge/anaconda3 (writable); channel URLs : https://conda.anaconda.org/conda-forge/linux-64; https://conda.anaconda.org/conda-forge/noarch; https://conda.anaconda.org/bioconda/linux-64; https://conda.anaconda.org/bioconda/noarch; https://repo.anaconda.com/pkgs/main/linux-64; https://repo.anaconda.com/pkgs/main/noarch; https://repo.anaconda.com/pkgs/free/linux-64; https://repo.anaconda.com/pkgs/free/noarch; https://repo.anaconda.com/pkgs/r/linux-64; https://repo.anaconda.com/pkgs/r/noarch; https://conda.anaconda.org/OpenMDAO/linux-64; https://conda.anaconda.org/OpenMDAO/noarch; package cache : /home/pedge/anaconda3/pkgs; /home/pedge/.conda/pkgs; envs directories : /home/pedge/anaconda3/envs; /home/pedge/.conda/envs; platform : linux-64; user-agent : conda/4.6.9 requests/2.21.0 CPython/3.7.1 Linux/2.6.32-696.18.7.el6.x86_64 centos/6.6 glibc/2.12; UI,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177:4491,config,config,4491,,https://github.com/google/deepvariant/issues/177,1,['config'],['config']
Modifiability,".1 wgs model so I cant imagine the model is really that bad at calling variants initially, even if in a fish. . **Setup**; Running on a university computing cluster (https://hpc-unibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarni",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1717,config,config,1717,,https://github.com/google/deepvariant/issues/876,1,['config'],['config']
Modifiability,".contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.experimental.parallel_interleave(...)`.; I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn.; W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use tf.cast instead.; W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.; Instructions for updating:; Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.; I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn.; I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/model.ckpt', vars_to_warm_start='InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:9912,layers,layers,9912,,https://github.com/google/deepvariant/issues/172,1,['layers'],['layers']
Modifiability,".py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantModel(name=inception_v3); I0415 07:34:19.585236 140713377441536 model_eval.py:198] Dataset has 8 samples, doing eval over 0; max_examples is 1000000, num_batches is 0; I0415 07:34:19.585772 140713377441536 modeling.py:357] Initializing model with random parameters; I0415 07:34:19.586724 140713377441536 estimator.py:201] Using config: {'_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa3b29ad50>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}; I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/output/trained_model; I0415 07:35:14.785435 140713377441536 evaluation.py:198] Found new checkpoint at /data/output/trained_model/model.ckpt-0; I0415 07:35:14.787266 140713377441536 model_eval.py:225] Starting to evaluate. WARNING: Th",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:2869,config,config,2869,,https://github.com/google/deepvariant/issues/172,1,['config'],['config']
Modifiability,"/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz; I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner; runtimes) = region_processor.process(region); File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process; reads = self.region_reads(; File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads; raise err; File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query; return self._reader.query(region); File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query; return self._reader.query(region); ValueError: Failed precondition: Cannot query without an index; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1.asm.hic.hap2.p_ctg.fa --reads /input/G1_hap2.bam --examples /tmp/tmp6zw47x4_/make_examples.tfrecord@4.gz --task 3",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/492:18510,extend,extend,18510,,https://github.com/google/deepvariant/issues/492,1,['extend'],['extend']
Modifiability,/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inceptio,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:113144,variab,variable,113144,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,/beta; prev_var_name: Unchanged; I0415 07:34:37.973396 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.974112 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.974828 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.975663 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.976491 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.977374 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.978363 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.979172 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.979801 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.980334 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.980947 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.981682 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:93001,variab,variable,93001,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,/beta; prev_var_name: Unchanged; I0415 07:34:37.998590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.999377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.000032 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.000669 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.001136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.001605 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.002159 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.002743 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.003241 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.003774 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.004208 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.004652 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:98639,variab,variable,98639,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,/beta; prev_var_name: Unchanged; I0415 07:34:38.007590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.008188 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.008778 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.009377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.010077 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.010946 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.011956 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013191 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013811 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.014904 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.015494 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.016149 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:101701,variab,variable,101701,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,/beta; prev_var_name: Unchanged; I0415 07:34:38.027498 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.027940 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.028300 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.028651 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.029017 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029366 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029716 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.030065 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:107864,variab,variable,107864,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0; 2021-05-06 16:56:52.216108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1; 2021-05-06 16:58:21.551842: E tensorflow/core/common_runtime/session.cc:91] Failed to create session: Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid; 2021-05-06 16:58:21.551943: E tensorflow/c/c_api.cc:2184] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 393, in call_variants; with tf.compat.v1.Session(config=config) as sess:; File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1586, in __init__; super(Session, self).__init__(target, graph, config=config); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 701, in __init__; self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts); tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid. real	1m31.942s; user	1m31.967s; sys	0m8.062s; `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452:4468,config,config,4468,,https://github.com/google/deepvariant/issues/452,4,['config'],['config']
Modifiability,"/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1191, in convolution2d; conv_dims=2); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1089, in convolution; outputs = layer.apply(inputs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548, in __call__; outputs = super(Layer, self).__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__; outputs = call_fn(cast_inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 234, in wrapper; return converted_call(f, options, args, kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 439, in converted_call; return _call_unconverted(f, args, kwargs, options); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 330, in _call_unconverted; return f(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 197, in call; outputs = self._convolution_op(inputs, self.kernel); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:19353,layers,layers,19353,,https://github.com/google/deepvariant/issues/358,1,['layers'],['layers']
Modifiability,"/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SING",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:3224,config,config,3224,,https://github.com/google/deepvariant/issues/797,1,['config'],['config']
Modifiability,"05.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objects: 100% (700/700), done.; remote: Compressing objects: 100% (111/111), done.; remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146; Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done.; Resolving deltas: 100% (4683/4683), done.; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:2918,config,config,2918,,https://github.com/google/deepvariant/issues/739,2,"['config', 'variab']","['config', 'variable']"
Modifiability,"0826 20:44:28.898550 47737984214848 call_variants.py:317] From /opt/models/wgs/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; 2022-08-26 20:44:28.903729: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-08-26 20:44:28.905866: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 3. Tune using inter_op_parallelism_threads for best performance.; WARNING:tensorflow:Using temporary folder as model directory: /tmp/pbs.1173981.omics/tmpag6nq5vt; W0826 20:44:28.952679 47737984214848 estimator.py:1864] Using temporary folder as model directory: /tmp/pbs.1173981.omics/tmpag6nq5vt; INFO:tensorflow:Using config: {'_model_dir': '/tmp/pbs.1173981.omics/tmpag6nq5vt', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0826 20:44:28.953302 47737984214848 estimator.py:202] Using config: {'_model_dir': '/tmp/pbs.1173981.omics/tmpag6nq5vt', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_ch",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:2860,config,config,2860,,https://github.com/google/deepvariant/issues/564,1,['config'],['config']
Modifiability,"1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:801::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:801::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:10196,config,configured,10196,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:6619,config,configured,6619,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"1.6.1"" make_examples --mode training --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/27_r_groups.bam"" --examples ""/output/validation_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/validation_set.gz --output_pattern_prefix=""output/validation_shuffled"" --output_dataset_name=""27"" --output_dataset_config_pbtxt=""output/validation.pbtxt"" --job_name=shuffle-tfrecords `; _Model trainning_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" train --config=/input/dv_config.py:base --config.train_dataset_pbtxt=""/output/training.pbtxt"" --config.tune_dataset_pbtxt=""/output/validation.pbtxt"" --config.num_epochs=10 --config.learning_rate=0.0001 --config.num_validation_examples=0 --strategy=mirrored --experiment_dir=""/output/"" --config.batch_size=512`; _Model test_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type WES --customized_model ""/output/checkpoints/ckpt-679"" --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/33_r_groups.bam"" --output_vcf ""/output/33.vcf.gz"" --output_gvcf ""/output/33.g.vcf.gz"" -intermediate_results_dir ""/output/intermediate_results_dir"" --num_shards 10`. - Error trace: ; ` - I0822 07:51:54.272576 127450974123840 make_examples_core.py:301] Task 17/20: Writing example info to /output/intermediate_results_dir/make_examples.tfrecord-00017-of-00020.gz.example_info.json; I0822 07:51:54.272647 12745097412",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:2344,config,config,2344,,https://github.com/google/deepvariant/issues/869,1,['config'],['config']
Modifiability,1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.019490 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.020006 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.020472 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.020929 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.021404 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.021804 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.022231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.022672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.023087 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023875 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.024280 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:104602,variab,variable,104602,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:112801,variab,variable,112801,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,1/weights; prev_var_name: Unchanged; I0415 07:34:38.021404 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.021804 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.022231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.022672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.023087 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023875 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.024280 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.024672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025057 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025403 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025782 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:105288,variab,variable,105288,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"20:44:28.952679 47737984214848 estimator.py:1864] Using temporary folder as model directory: /tmp/pbs.1173981.omics/tmpag6nq5vt; INFO:tensorflow:Using config: {'_model_dir': '/tmp/pbs.1173981.omics/tmpag6nq5vt', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0826 20:44:28.953302 47737984214848 estimator.py:202] Using config: {'_model_dir': '/tmp/pbs.1173981.omics/tmpag6nq5vt', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0826 20:44:28.953605 47737984214848 call_variants.py:446] Writing calls to /paedyl01/disk1/yangyxt/test_tmp/call_variants_output.tfrecord.gz; INFO:tensorflow:Calling model_fn.; I0826 20:44:29.309295 47737984214848 estimator.py:117",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:3700,config,config,3700,,https://github.com/google/deepvariant/issues/564,1,['config'],['config']
Modifiability,"29,3,0:II	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	1/1:5:0,5:9:40,12,0:..	1/1:3:0,3:7:36,10,0:..	0/0:0:0,0:1:0,3,29:.`. This is messing with downstream analysis, and overall just looks like poor QC. Additionally, the annotation/filter field is missing. In the gVCFs there was still a ""PASS"" label. This is also required for downstream analysis. ; So I am wondering where I went wrong, or whether there is a more suitable software to merge gVCFs. Thank you!. **Setup**; - Operating system: linux/cluster; - DeepVariant version: latest (1.5); - Installation method: Docker; - Type of data: ; Illumina WES data (.cram to .gvcf). **Steps to reproduce:**; - Command:; ```; glnexus_cli --config DeepVariant --bed ${regions} \; folder/*.g.vcf.gz > output.bcf; ```. - Error trace: no errors. This is the vcf header:; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##GLnexusVersion=v1.4.1-0-g68e25e5; ##GLnexusConfigName=DeepVariant; ##GLnexusConfigCRC32C=2932316105; ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, snv_prior_calibration: 0.600000024, indel_prior_calibration: 0.449999988, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,T",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/633:1885,config,config,1885,,https://github.com/google/deepvariant/issues/633,1,['config'],['config']
Modifiability,2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:115683,variab,variable,115683,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:115515,variab,variable,115515,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"2d_0b_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Conv2d_3b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Conv2d_4a_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp', var_name_to_vocab_info={}, var_name_to_prev_var_name={}); I0415 07:34:37.687702 140368878327552 warm_starting_util.py:417] Warm-starting from: ('/home/models/model.ckpt',); I0415 07:34:37.964245 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.965248 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966092 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966933 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.967533 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.968360 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.969041 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:90121,variab,variable,90121,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:119045,variab,variable,119045,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:118384,variab,variable,118384,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,3/weights; prev_var_name: Unchanged; I0415 07:34:38.029366 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029716 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.030065 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:108707,variab,variable,108707,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"4: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56a1fd0 executing computations on platform Host. Devices:; 2020-04-24 15:59:50.378283: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-04-24 15:59:50.380979: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0424 15:59:50.447775 139872277903104 modeling.py:563] Initializing model with random parameters; W0424 15:59:50.449538 139872277903104 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp3bl4tsmc; I0424 15:59:50.450443 139872277903104 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp3bl4tsmc', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3659263518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0424 15:59:50.451262 139872277903104 call_variants.py:384] Writing calls to /tmp/tmp9_28zx5u/call_variants_output.tfrecord.gz; W0424 15:59:50.467876 139872277903104 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions for updating:; I",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304:1733,config,config,1733,,https://github.com/google/deepvariant/issues/304,1,['config'],['config']
Modifiability,"72277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0424 15:59:51.794167 139872277903104 estimator.py:1147] Calling model_fn.; W0424 15:59:51.800228 139872277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0424 15:59:51.806498 139872277903104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0424 16:00:02.682547 139872277903104 estimator.py:1149] Done calling model_fn.; I0424 16:00:06.021238 139872277903104 monitored_session.py:240] Graph was finalized.; I0424 16:00:06.037272 139872277903104 saver.py:1284] Restoring parameters from /opt/models/wes/model.ckpt; I0424 16:00:10.817819 139872277903104 session_manager.py:500] Running local_init_op.; I0424 16:00:11.060626 139872277903104 session_manager.py:502] Done running local_init_op.; I0424 16:00:12.403780 139872277903104 modeling.py:413] Reloading EMA...; I0424 16:00:12.405867 139872277903104 saver.py:1284] Restoring parameters from /opt/models/wes/model.ckpt; I0424 16:00:48.634510 139872277903104 call_variants.py:402] Processed 1 examples in 1 batches [5816.472 sec per 100]. real	4m2.970s; user	5m54.674s; sy",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304:4795,layers,layers,4795,,https://github.com/google/deepvariant/issues/304,2,['layers'],['layers']
Modifiability,7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.028300 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.028651 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.029017 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029366 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029716 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.030065 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inceptio,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:108200,variab,variable,108200,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"9ec44bd Replace C++ `#import <...>` with `#include <...>`; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:4545,config,config,4545,,https://github.com/google/deepvariant/issues/739,1,['config'],['config']
Modifiability,":..	0/1:3:0,3:2:27,4,0:..	0/0:0:0,0:1:0,3,29:..	0/1:2:0,2:3:19,3,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	1/1:4:0,4:2:30,8,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,3,29:..	1/1:8:0,8:12:41,18,0:..	0/1:2:0,2:2:22,4,0:..	1/1:6:0,6:6:30,12,0:..	0/1:2:0,2:2:16,4,0:..	0/1:2:0,2:3:15,3,0:..	./.:1:1,0:0:29,3,0:II`. The overall coverage of both these sites is really quite low... compared to a site such as this, which has good coverage and no ./. or gVCF generated 0/0:. `1	6633042	1_6633042_C_T	C	T	57	.	AF=0.025366;AQ=57	GT:DP:AD:GQ:PL:RNC	0/0:62:62,0:50:0,258,2579:..	0/0:50:50,0:50:0,180,1799:..	0/0:60:60,0:50:0,195,1949:..	0/0:57:57,0:50:0,252,2519:..	0/0:57:57,0:50:0,228,2279:..	0/0:39:39,0:50:0,159,1589:..	0/0:46:46,0:50:0,189,1889:..	0/0:23:23,0:48:0,69,689:..	0/0:25:25,0:50:0,75,749:..	0/0:23:23,0:50:0,69,689:..	0/0:40:40,0:50:0,189,1889:..	0/0:63:63,0:50:0,171,1949:..	0/0:56:56,0:50:0,213,2129:..	0/0:53:53,0:50:0,159,1589:..	0/0:60:60,0:50:0,213,2129:..	0/0:24:24,0:50:0,72,719:..	0/0:49:49,0:50:0,147,1469:..	0/0:27:27,0:50:0,51,749:..	0/0:40:40,0:50:0,156,1559:..	0/0:19:19,0:50:0,57,569:..	0/0:43:43,0:50:0,180,1799:..	0/0:21:21,0:50:0,66,659:..	0/0:69:69,0:50:0,207,2069:..	0/0:16:16,0:48:0,48,479:..	0/0:69:69,0:50:0,207,2069:..	0/0:46:46,0:50:0,138,1379:..	0/1:78:36,39:47:48,0,52:..	0/0:49:49,0:50:0,159,1589:..	0/0:25:25,0:50:0,81,809:..	0/0:101:101,0:50:0,300,2999:..	0/1:77:40,36:50:51,0,55:..	0/0:65:65,0:50:0,240,2399:..	0/0:22:22,0:48:0,66,659:..	0/0:55:55,0:50:0,219,2189:..	0/0:38:38,0:50:0,210,2099:..	0/0:56:56,0:50:0,171,1709:..	0/1:78:32,46:46:49,0,48:..`. It would be good to know the most appropriate way to remove the bad variants. As I understand the GQ field is the best place to start. If I am correct, the GLnexus command which I used filters by GQ > 20 ( see: https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration ). Perhaps upping this GQ to > 30 or slightly higher would remove this sites? I am very appreciative of every input, thank you so much!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/645:6660,Config,Configuration,6660,,https://github.com/google/deepvariant/issues/645,1,['Config'],['Configuration']
Modifiability,":1173] Calling model_fn.; /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1083: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:678: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:2441: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:118: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1638: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); INFO:tensorflow:Done calling model_fn.; I0826 20:44:33.173107 47737984214848 estimator.py:1175] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0826 20:44:34.048544 47737984214848 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0826 20:44:34.048974 47737984214848 saver.py:1399] Restoring parameters from /opt/models/wgs/model.ckpt; INFO:tensorflow:Running local_init_op.; I0826 20:44:34.790676 47737984214848 session_manager.py:531] Running local_init_op.; INFO:tensorflow:Done running local_init_op.; I0826 20:44:34.816158 47737984214848 session_manager.py:534] Done running local_init_op.; INFO:tensorflow:Reloading EMA...; I0826 20:44:35.138201 47737984214848 modeling.py:418] Reloading EMA...; INFO:te",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:5711,layers,layers,5711,,https://github.com/google/deepvariant/issues/564,2,['layers'],['layers']
Modifiability,; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists...,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:3834,config,config,3834,,https://github.com/google/deepvariant/issues/89,1,['config'],['config']
Modifiability,"; 2020-07-03 17:18:45.680397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.129.0; 2020-07-03 17:18:45.680416: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.113.0; 2020-07-03 17:18:45.680422: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 418.113.0 does not match DSO version 410.129.0 -- cannot find working devices in this configuration; I0703 17:18:45.713418 140322304501504 modeling.py:563] Initializing model with random parameters; W0703 17:18:45.713950 140322304501504 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp7amyb_ws; I0703 17:18:45.714213 140322304501504 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp7amyb_ws', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9f0f102630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0703 17:18:45.714398 140322304501504 call_variants.py:384] Writing calls to /tmp/tmp7l6e69ft/call_variants_output.tfrecord.gz; W0703 17:18:45.719665 140322304501504 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions for updating:; I",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321:2598,config,config,2598,,https://github.com/google/deepvariant/issues/321,1,['config'],['config']
Modifiability,"AMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:3047,config,config,3047,,https://github.com/google/deepvariant/issues/797,1,['config'],['config']
Modifiability,Adaptation to polyploid organisms.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/562:0,Adapt,Adaptation,0,,https://github.com/google/deepvariant/issues/562,1,['Adapt'],['Adaptation']
Modifiability,"Changed undeclared $bazel_version variable to declared $bazel_ver variable. After implementing this fix, I was able to build all prerequisite packages.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/34:34,variab,variable,34,,https://github.com/google/deepvariant/pull/34,2,['variab'],['variable']
Modifiability,"Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:42:05 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple ti",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:8572,config,configured,8572,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:115851,variab,variable,115851,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"Dear Deepvariant team,. I was attempting to run Deepvariant on GCP by following the sample scripts from the tutorials, but it failed. I have checked the configuration regarding the Compute Engine quota and it should meet the requirements (i.e. CPU, Persistent Disk and In-use IP addresses). The error message from the log is like:; ""RuntimeError: Job failed with error ""run"": operation ""projects/deepvariant-phh/operations/7761698599878123803"" failed: executing pipeline: Execution failed: action 2: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION)"". I have read some of the related discussed issues but still can't solve my problem. The log files and my script file are attached. Your help is appreciated. . [staging_temp%2Frunner_logs_20181118_014355.log](https://github.com/google/deepvariant/files/2592663/staging_temp.2Frunner_logs_20181118_014355.log); [log.txt](https://github.com/google/deepvariant/files/2592666/log.txt). [script.txt](https://github.com/google/deepvariant/files/2592665/script.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/120:153,config,configuration,153,,https://github.com/google/deepvariant/issues/120,1,['config'],['configuration']
Modifiability,"Deepvariant fails without clear reason. . **Setup**; JHU Rockfish HPC; Singularity 3.8.7; singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:; ```; #!/bin/bash; #SBATCH --job-name=deep64_13448198; #SBATCH --time=24:00:00; #SBATCH --nodes=2; #SBATCH --ntasks-per-node=1; #SBATCH --cpus-per-task=32; #SBATCH --mem=0. ml anaconda; conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref c_elegans.PRJNA13758.WS245.genomic.fa \; --reads aln13448198.pbmm2.bam \; --output_vcf aln13448198.pbmm2.dv.vcf.gz \; --num_shards 64; ```. Here's a long snippet of slurm output:; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****; time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]; # this part is likely unimportant. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:1039,sandbox,sandbox,1039,,https://github.com/google/deepvariant/issues/614,1,['sandbox'],['sandbox']
Modifiability,"Describe the issue:**; Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.5.0; - Tensorflow 2.11.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HIFI reads. **Steps to reproduce:**; ```; rule deepvariant_make_examples:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=250",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:1002,config,config,1002,,https://github.com/google/deepvariant/issues/677,1,['config'],['config']
Modifiability,"Describe the issue:; I can't get vcf output after bind mount a root directory. Setup; - Operating system: Windows 11, but mount an Ubuntu VM through multipass; - Type of data: fasta, bam and vcf file. Steps to reproduce:; - Command:; #Configure the DeepVariant environment variables (missing input directory,...); BIN_VERSION=""1.5.0"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; PWD=/mountpoint/fastQ; INPUT_DIR=""${PWD}/testdata_input""; mkdir -p ${INPUT_DIR}; OUTPUT_DIR=""${PWD}/04.deepvariant_out""; mkdir -p ""${OUTPUT_DIR}"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/input/Homo_sapiens_assembly38.fasta \; --reads=/input/$FQ.align.sort.marked.bam \; --output_vcf=/output/$FQ.vcf.gz \; --output_gvcf=/output/$FQ.g.vcf.gz \; --num_shards=2 ; - Error trace: ; ; It displays: Task reading input the .bam file but it ends up with 0 candidates.; I suppose it can read the input files. Does the quick start test work on your system?; This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675:235,Config,Configure,235,,https://github.com/google/deepvariant/issues/675,2,"['Config', 'variab']","['Configure', 'variables']"
Modifiability,Enable configurable GPU memory allocation of call_variants,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159:7,config,configurable,7,,https://github.com/google/deepvariant/pull/159,1,['config'],['configurable']
Modifiability,H_ANYTHING; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/61,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:4651,config,config,4651,,https://github.com/google/deepvariant/issues/608,1,['config'],['config']
Modifiability,"Hello, . I working through the DeepVariant set up workflow under the docker umbrella, but the variable ""ref"" indicating the path of the reference .fasta is not responding properly. . sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \ ; **<cmd-b>FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.</cmd-b>**; Pass --helpshort or --helpfull to see help on flags.; shaba033$ <cmd-b> --ref=/input/ucsc.hg19.chr20.unittest.fasta </cmd-b>\; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=4; -bash: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/181:94,variab,variable,94,,https://github.com/google/deepvariant/issues/181,1,['variab'],['variable']
Modifiability,"Hello, follow-up ... I was not able to make it work, also we noticed, even when it's not taking all the threads, when it's running with the numbers of threads given by the shards argument, other processes (so, not DeepVariant) are on S as shown in htop. Is it that DeepVariant co-opt all the threads even when it doesn't currently need them? . Got it. Thanks for the context! If you end up tweaking the config, let me know whether it works for you or not.; I'll close this issue now. _Originally posted by @pichuan in https://github.com/google/deepvariant/issues/271#issuecomment-586523576_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274:403,config,config,403,,https://github.com/google/deepvariant/issues/274,1,['config'],['config']
Modifiability,"Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:; ```; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/...; [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:; Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2; [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1; ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/511:428,Inherit,Inherited,428,,https://github.com/google/deepvariant/issues/511,3,"['Inherit', 'config']","['Inherited', 'config']"
Modifiability,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/443:233,config,config,233,,https://github.com/google/deepvariant/issues/443,4,['config'],"['config', 'configured']"
Modifiability,"Hello,. I have inherited some haloplex data, and ideally I would like to use deepvariant for its analyses as I have used this for my WES. . Do you think deepvariant would work proficiently for haloplex data?. Thanks!; Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/583:15,inherit,inherited,15,,https://github.com/google/deepvariant/issues/583,1,['inherit'],['inherited']
Modifiability,"Hello,. I tried running ""_run_deepvariant_keras.py_"" script from the latest release of deepvaraint and faced some issues while running the keras-based call variant module. . I used the following command to run:. `; python bazel-out/k8-opt/bin/deepvariant/run_deepvariant_keras.py --model_type=WGS --ref=ref.fa --reads=reads.bam --regions ""chr19"" --output_vcf=${OUTPUT_DIR}/output.vcf.gz --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz --intermediate_results_dir ${OUTPUT_DIR}/intermediate_results_dir --num_shards=10; `. The above command successfully ran for _make_example_ module and failed at _call_varaint_keras.py_ with the following error message.. -------------. _Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 399, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 312, in run_; _run_main(main, args); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 387, in main; call_variants(; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 344, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/636:789,variab,variables,789,,https://github.com/google/deepvariant/issues/636,1,['variab'],['variables']
Modifiability,"Hello,. We have found that a known de novo variant was missed when using DeepTrio and GLnexus in our pipeline (WGS, hg38). I know that a similar issue has already been raised and appreciate the interesting discussion on this (i.e. https://github.com/google/deepvariant/issues/440), but to recap for others this was the result of two contributing factors:. 1. DeepTrio being less confident in the de novo call for the proband than when DeepVariant is run in singleton mode on the proband. In our case, comparing the output VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepT",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475:755,config,configuration,755,,https://github.com/google/deepvariant/issues/475,2,['config'],['configuration']
Modifiability,"Hello,. We noticed that adjacent variants of the same haplotype (i.e. MNVs) are being called as separate variants in the DeepVariant and DeepTrio outputs with VCF and gVCF files. During downstream processing these MNVs are then treated as two individual SNVs at two different loci, leading to faulty assessments. . For example two variants for a site of interest (reference TCG -> Serine) were separated between two lines in the DeepVariant/DeepTrio output VCF and then categorised as containing a nonsynonymous (T**G**G -> Tryptophan) and synonymous mutation (TC**A** -> Serine). Whereas the correct and desired way to handle this, at least for us but I imagine others too, would seem to be to recognise both mutations on a single line in the VCF as a combined substitution, which could then be identified as resulting in a stopgain (T**GA** -> Nonsense mutation). Are there plans to support these MNV calls in the DeepVariant/DeepTrio outputs? Or alternatively are there any current post-processing approaches that you may be using and can recommend to handle these cases? Can understand these may be challenging to manage in some aspects but could be important to flag given some recent literature around this topic. For reference this was using hg38 with WGS. We initially identified this using the original DeepTrio release (docker image deeptrio:1.0.1rc), but then also using the most recent DeepVariant release (via docker, v1.2). We also tested this to see whether the change to the unfiltered GLnexus config could be contributing to this for processing of DeepTrio gVCFs due to the joint genotyping parameter, but reverting back to the WGS config did not result in a merged MNV in this instance. . Many thanks,; Macabe.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/486:1510,config,config,1510,,https://github.com/google/deepvariant/issues/486,2,['config'],['config']
Modifiability,"Helloï¼Œ; Thanks for this fast and useful germline calling tool. When I used DeepVariant 1.6.0 for single sample WES germline calling, I found that some real germline mutations with VAF (variant allele frequency) values less than 0.3 to 0.4 could not be called. IGV view figures of these variants are below. May I ask if DeepVariant considers VAF parameters during runtime or sets threshold filtering for VAF parameters? We look forward to your reply. ; Our Codes(All variables have been defined):; `singularity run \; -B ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"" \; deepvariant_1.6.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/input/testinput/human_g1k_v37_modified.fasta \; --reads=/input/${i}.sorted.markdup.BQSR.bam \; --regions /input/testinput/use_agilent_region_padding_100.bed \; --output_vcf=/output/${i}.vcf.gz \; --output_gvcf=/output/${i}.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir/${i} \; --num_shards=10; `; IGV figures:; ![EP90t_R](https://github.com/google/deepvariant/assets/174405155/6e9263f9-ff9e-4495-b51d-54127dbfc837); ![EP40b](https://github.com/google/deepvariant/assets/174405155/0b8f1fec-b5d9-4bb5-be97-6431c38135b0)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/843:466,variab,variables,466,,https://github.com/google/deepvariant/issues/843,1,['variab'],['variables']
Modifiability,"Hi , when i run call_variant , it arises this warn which means can't use the gpu,but i can make sure that the tensorflow can use the gpu.There are the screen shots of the warn and the existence of the gpu. - the gpu existence; ![image](https://github.com/google/deepvariant/assets/71956115/367b1a98-123c-48fb-b170-3f8e4aae7d30). ```python; tensorflow.test.is_gpu_available(); WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.config.list_physical_devices('GPU')` instead.; 2024-05-12 21:36:00.744470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 15089 MB memory: -> device: 0, name: Vega 20, pci bus id: 0000:26:00.0; True; ```. - the warn ; ![image](https://github.com/google/deepvariant/assets/71956115/246d5cfd-a9b3-4ac5-aea7-bf4c89401c76). ```shell; warnings.warn(; 2024-05-12 21:43:29.067332: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ```. - Operating system: Linux ; - DeepVariant version: 1.6.1-gpu; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820:565,config,config,565,,https://github.com/google/deepvariant/issues/820,1,['config'],['config']
Modifiability,"Hi . I am using the quick start docker image. I think all the examples have been created at this point. When it first started creating examples top showed that all 64 of my cpu's where at 100% utilization, and there was still lots of available memory. I have not seen any new log files in over a day. I have check top several times over the last 2 days. It only shows 2 python processes and each of them is at 800% utilization. In my experience training models takes a long time, however making predictions is quick. Should I kill my job and try and start over again? I ran into a problem like this before on a much smaller machine. After 11 days I killed the jobs. I do not know much about docker. I looked in /var/lib/docker/containers. I did not find anything that looked a like a log file. any debugging tips would be appreciated. Andy. config ; ```; google/deepvariant:0.9.0; --model_type=WES; --regions=/input/agilent_sureselect_human_all_exon_v5_b37_targets.bed; --num_shards=64; ```; Looks like make_example completed; ```; I0208 03:49:00.939260 140440947410688 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00063-of-00064.gz; I0208 03:49:00.940793 140440947410688 make_examples.py:1334] Writing gvcf records to /tmp/deepvariant_tmp_output/gvcf.tfrecord-00063-of-00064.gz; I0208 03:49:01.427521 140440947410688 make_examples.py:905] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2020-02-08 03:49:01.428281: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728; I0208 03:49:01.743115 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:01.755232 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam wit",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/269:841,config,config,841,,https://github.com/google/deepvariant/issues/269,1,['config'],['config']
Modifiability,"Hi Mark (@depristo),. Sorry, I meant to put this together a while ago - regarding https://github.com/google/deepvariant/issues/27#issuecomment-364683474 - but got a bit swamped with a research deadline. In any case, this is purely for intellectual curiosity and discussion. Regarding the first point, where differences in allocated CPUs might be the cause for the timing, that could be remedied by specifying a minimal CPU requirement, as noted here:. https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform. So to control for the variability in the test, the two options are either: a) to set the `--min-cpu-platform` setting to the maximum available (`""Intel Sandy Bridge""`), or b) to keep requesting and canceling instances until the desired one is allocated on which all tests should be performed, thus satisfying consistency. Just as a quick inspection, by looking at the CPU cycles utilization, I just ran a performance analysis of 0.4 and 0.5.1 on `make_examples` - since it displayed the initial discrepancy - and there seem to be some slight increases in `0.5.1`, which might cumulatively affect things. In any case, below is the top of the call-graph of percent utilization by method (per version):. #### DV 0.4. ```; # Samples: 186K of event 'cpu-clock'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--lea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:548,variab,variability,548,,https://github.com/google/deepvariant/issues/50,1,['variab'],['variability']
Modifiability,"Hi all,. The DeepVariant case study scripts for running via binaries on CPU install the `intel-tensorflow` package. We have noticed the below error when installing this package and are looking into how this can be fixed. ```; $ pip install intel-tensorflow; ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info; ```; If you run into this issue, we recommend one of the following options in the meantime:; * Use the Docker scripts instead of the binaries scripts.; * Set the [`DV_USE_GCP_OPTIMIZED_TF_WHL`](https://github.com/google/deepvariant/blob/r0.9/settings.sh#L90) variable to 0 prior to setting up DeepVariant and running the case study scripts for binaries. `intel-tensorflow` is only installed when this variable is set.; * Use the GPU scripts instead of the CPU scripts. Best,; The DeepVariant Team",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/263:664,variab,variable,664,,https://github.com/google/deepvariant/issues/263,2,['variab'],['variable']
Modifiability,"Hi all,; I have read the discussion about deep sequencing on issue #62. I have tried to modify three options, downsample_fraction, pileup_image_height, and vsc_min_fraction_snps for our deep sequencing data but it didn't work and output many false-positive calls. Here I want to train a new model for deep sequencing data with rare somatic mutation(MAF~1%) and there is some confusion.; 1) There is a maximum threshold for pileup_height of 362, can I modify it?; 2) There is only one training tutorial with Google cloud platform, is there any guideline for training with the Linux system?; 3) Can Deepvariant be adapted to a somatic mutation caller?; Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/308:613,adapt,adapted,613,,https://github.com/google/deepvariant/issues/308,1,['adapt'],['adapted']
Modifiability,"Hi all;; Thanks for all the help getting an initial conda package in place for DeepVariant (#9) through bioconda. I wanted to follow up with some suggestions that would help make the pre-built binaries more portable as part of this process, in order of helpfulness for portability:. - Currently the binaries need a recent kernel with GLIBC > 2.23 due to pre-built htslib and other libraries. Would it be possible to build the DeepVariant libraries on an older machine to allow a wider range of system support? We build on CentOS 6 in conda to provide wider compatibility.; - main.py in the zip files hardcodes python to use `/usr/bin/python`. Would it be possible to generalize this by using the python that the zip file gets called with (`sys.executable`)? I currently patch this in the conda build: https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/recipes/deepvariant/build.sh#L22; - This is currently built against numpy 1.13 and ideally we'd want to sync with CONDA_NPY (1.12: https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9). I believe building against 1.12 would make it forward compatible. An alternative to points 1 and 3 is making it easier to build DeepVariant as part of the conda build process. The major blocker here is the `clif` dependency which is difficult to build and the pre-built binaries require unpacking into `/usr`. If we could make this relocatable and easier to install globally we could build with portable binaries and adjustable numpy as part of the bioconda preparation process. Thanks again for all the help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29:207,portab,portable,207,,https://github.com/google/deepvariant/issues/29,3,['portab'],"['portability', 'portable']"
Modifiability,"Hi i have deepvariant 1.5.0 version singularity SIF file,; I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below; #nextflow.config; ```; singularity {; process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'; cacheDir = ""/data/shared/clinical/LongRead/cache/""; singularity.enabled = true; singularity.autoMounts = true; SINGULARITY_BINDPATH = ""/data""; }. conda; {; enabled = true; cacheDir = ""/data/shared/clinical/LongRead/Programs/""; }; params; {; path=""/data/shared/clinical/LongRead/""; ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""; pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi""; at=18; st=6; data_input=""/data/shared/clinical/LongRead/Data/""; }; ```; #deepvariant.nf; ```; process pbc_varicall {; publishDir ""/data/shared/clinical/LongRead/Data/resources/""; container 'docker://google/deepvariant:1.5.0'. input:; path 'fa'; output:; file ""*""; path 'm84011_220902_175841_NF_sif.vcf.gz'. script:; """"""; run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40; """"""; }. workflow {; fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""); pbc_varicall(fa); }; ```; after running for several hours i do not get any output, instead during run , i get msg as; `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `; this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659:174,config,config,174,,https://github.com/google/deepvariant/issues/659,1,['config'],['config']
Modifiability,"Hi there,. We ran deepvariant on test data, and we had the following error:. I0416 16:33:15.202579 46954465520640 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 3 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/tmp/tmpmxakjtgh/make_examples.tfrecord@4.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 2. We converted docker image to singularity sandbox. And our command is like this:; singularity exec -B /data -B /home -B /localhd/ \; ../deepvariant-cpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards ${nproc} \; --regions chr20. I am guessing we may have misconfigured some module. Any idea how to fix it?. Thanks. George",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/444:1273,sandbox,sandbox,1273,,https://github.com/google/deepvariant/issues/444,1,['sandbox'],['sandbox']
Modifiability,"Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/355:372,Config,Configurable,372,,https://github.com/google/deepvariant/issues/355,2,"['Config', 'config']","['Configurable', 'configuration']"
Modifiability,"Hi,. Are there any plans to extend DeepVariant to somatic variant calling? The current model seems to be inherently diploid. What is the training time for the released versions of DeepVariant? The Supplementary information from the Nature paper mentions something about ""80 hours"" but does not specify which kind of hardware was used?. Do you have any numbers on how much the neural network improves the accuracy as compared to the raw (over-sensitive) variant calls output after the haplotype-aware realignment step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/241:28,extend,extend,28,,https://github.com/google/deepvariant/issues/241,1,['extend'],['extend']
Modifiability,"Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:; ```; CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990; ```; We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:; ```; CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV; ```; The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here?. Thank you for your help,; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/521:384,config,config,384,,https://github.com/google/deepvariant/issues/521,1,['config'],['config']
Modifiability,"Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:; ```; time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=s3-mount/deepvariant_training/script/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \; --config.num_epochs=0 \; --config.learning_rate=0.02 \; --config.num_validation_examples=0 \; --experiment_dir=""model_train"" \; --strategy=mirrored \; --config.batch_size=512 \; --debug 'true'; ```. I received an error regarding about the checkpoint: ```No checkpoint found.```; I also attached my log for training step here: ; [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802:395,config,config,395,,https://github.com/google/deepvariant/issues/802,8,['config'],['config']
Modifiability,"Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity.; Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/531:178,inherit,inherited,178,,https://github.com/google/deepvariant/issues/531,1,['inherit'],['inherited']
Modifiability,"Hi,; When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:; 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; FATAL Flags parsing error: Unknown command line flag 'gpus'; Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/799:354,variab,variable,354,,https://github.com/google/deepvariant/issues/799,1,['variab'],['variable']
Modifiability,"Hi. I am trying to apply variant calling on CCS reads of 16 samples. I am getting the following 2 errors:. 1. `[E::hts_open_format] Failed to open file /scratch/041-Melon-Reseq/Pacbio_targeted/alignedBams/ARUM_R--ARUM_R_vsMelonv4.MD.bam`; 2. `/var/log/slurm/log_slurmd//job2563083/slurm_script: lÃ­nea 45: --num_shards=4: command not found; `. Below I present the defined variables and the command. `INPUT_DIR=""/scratch/041-Melon-Reseq/Pacbio_targeted/alignedBams""`; `OUTPUT_DIR=""/scratch/041-Melon-Reseq/Pacbio_targeted/DeepVariant""`; `NCPUS=4`; `BIN_VERSION=""0.10.0""`. singularity run; -B /usr/lib/locale/:/usr/lib/locale/ \; 	-B /home/kalexiou/PacBio/ \; 	docker://google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=PACBIO \; 	--ref=/home/kalexiou/PacBio/Melon_v4.0_PacBio.fasta \; 	--reads=""${INPUT_DIR}""/${base} \; 	--regions ""chr05:24760000-27020000"" \; 	--output_vcf=""${OUTPUT_DIR}""/${name}.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/${name}.g.vcf.gz \ ; 	--num_shards=""${NCPUS}"". Any help will be appreciated!. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/309:371,variab,variables,371,,https://github.com/google/deepvariant/issues/309,1,['variab'],['variables']
Modifiability,"I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/208:896,variab,variables,896,,https://github.com/google/deepvariant/issues/208,1,['variab'],['variables']
Modifiability,"I am following the [Building from sources](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-build-test.md) tutorial. I cloned the repository, then started building and got the following errors:. ```; viniws@woese:~/Code/deepvariant$ ./build-prereq.sh ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Install the runtime packages' starting ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Misc setup' starting ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Update package list' starting ; E: The repository 'http://apt.postgresql.org/pub/repos/apt YOUR_UBUNTU_VERSION_HERE-pgdg Release' does not have a Release file. ; E: The repository 'http://ppa.launchpad.net/gnome-terminator/ppa/ubuntu bionic Release' does not have a Release file ; ```. And after:. ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ DV_BAZEL_VERSION=0.15.0; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98:285,config,config,285,,https://github.com/google/deepvariant/issues/98,2,['config'],['config']
Modifiability,"I am running DeepVariant on google cloud following the wiki (Cost-optimized configuration): https://cloud.google.com/genomics/deepvariant, and it works for one of our sample. . Now I have several bam files for several samples. I can run them one by one. But I wonder is there some options to set a list of bam files? Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/33:76,config,configuration,76,,https://github.com/google/deepvariant/issues/33,1,['config'],['configuration']
Modifiability,"I am trying to build DeepVariant from source, and **trying to use a custom python installation rather than the standard one.** However, ```bazel test ``` fails because it tries to use the standard library python. The requisite python is accessible as ""python"" because it is in the PATH variable, but bazel seems to ignore that and looks for python in the standard location. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/loca",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356:286,variab,variable,286,,https://github.com/google/deepvariant/issues/356,1,['variab'],['variable']
Modifiability,"I am trying to install deepvariant within a singularity container using recipe build. I have successfully complied google-sdk within the container, when it comes to the point where its runs ""./build-prereq.sh"" script, it terminates with this error: . **Setting up unzip (6.0-9ubuntu1) ...; Setting up zip (3.0-8) ...; Setting up zlib1g-dev:amd64 (1:1.2.8.dfsg-1ubuntu1) ...; Processing triggers for libc-bin (2.19-0ubuntu6) ...; Processing triggers for sgml-base (1.26+nmu4ubuntu1) ...; ========== [Tue Apr 17 00:09:23 UTC 2018] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; E: Unable to locate package python-wheel; ABORT: Aborting with RETVAL=255; Cleaning up...**. I am using singularity within in vagrant on my mac and here is my recipe file:. **Bootstrap: shub; From: singularityhub/ubuntu. %runscript; exec echo ""The runscript is the containers default runtime command!"". %files; # /home/vanessa/Desktop/hello-kitty.txt # copied to root of container; # /home/vanessa/Desktop/party_dinosaur.gif /opt/the-party-dino.gif #. %environment. export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)"". %labels; AUTHOR mnoon@email.arizona.edu. %post; apt-get update && apt-get -y install python2.7 git wget curl ; mkdir /data; echo ""The post section is where you can install, and configure your container."". echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" > /etc/apt/sources.list.d/google-cloud-sdk.list; curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -. apt-get update && apt-get install -y google-cloud-sdk; ##download deepVariant scripts and run them; git clone https://github.com/google/deepvariant.git; cd deepvariant; ; ./build-prereq.sh. ./build_and_test.sh. ./run-prereq.sh**. I have no idea whats causing this error. any help will be appreciated. -M",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/66:1379,config,configure,1379,,https://github.com/google/deepvariant/issues/66,1,['config'],['configure']
Modifiability,"I generated Pacbio gvcfs of 30 samples using Deepvariant.; And now I want to merge those gvcfs using GLnexus, what --config should I choose? Is ""DeepVariant"" suitable?. Name CRC32C	Description; gatk 1926883223	Joint-call GATK-style gVCFs; gatk_unfiltered 4039280095	Merge GATK-style gVCFs with no QC filters or genotype revision; xAtlas 1991666133	Joint-call xAtlas gVCFs; xAtlas_unfiltered 221875257	Merge xAtlas gVCFs with no QC filters or genotype revision; weCall 2898360729	Joint-call weCall gVCFs; weCall_unfiltered 4254257210	Merge weCall gVCFs with no filtering or genotype revision; DeepVariant 2932316105	Joint call DeepVariant whole genome sequencing gVCFs; DeepVariantWGS 2932316105	Joint call DeepVariant whole genome sequencing gVCFs; DeepVariantWES 1063427682	Joint call DeepVariant whole exome sequencing gVCFs; DeepVariantWES_MED_DP 2412618877	Joint call DeepVariant whole exome sequencing gVCFs, populating 0/0 DP from MED_DP instead of MIN_DP; DeepVariant_unfiltered 3285998180	Merge DeepVariant gVCFs with no QC filters or genotype revision; Strelka2 395868656	[EXPERIMENTAL] Merge Strelka2 gVCFs with no QC filters or genotype revision",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/796:117,config,config,117,,https://github.com/google/deepvariant/issues/796,1,['config'],['config']
Modifiability,"I managed to start training on a GPU, but it took too much time. Now, I am attempting to train DeepVariant on a TPU v3-8 VM. However, the most recent tutorial I found is for version 0.9, and I am unsure how to proceed with version 1.6.1. The command I used is the following: . ```; docker run \; -v ${HOME}:${HOME} \; google/deepvariant:1.6.1 \; train \; --config=${HOME}/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.num_epochs=10 \; --config.learning_rate=0.001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=tpu \; --config.batch_size=1024; ```. However, I am not an expert with TPUs, and this is entirely new to me. Below is the error I encountered. Do you have any suggestions or can you direct me to an updated tutorial to follow?. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0627 21:18:43.707066 139683296487232 train.py:92] Running with debug=False; I0627 21:18:43.707488 139683296487232 train.py:100] Use TPU at local; I0627 21:18:43.707705 139683296487232 train.py:103] experiment_dir: /home/gambardella/training_chk; INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.; I0627 21:18:43.707828 139683296487232 tpu_strategy_util.py:57] Deallocate tpu buffers before initializing tpu system.; INFO:tensorflow:Initializing the TPU system: local; I0627 21:18:43.846984 139683296487232 tpu_strategy_util.py:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:357,config,config,357,,https://github.com/google/deepvariant/issues/841,7,['config'],['config']
Modifiability,"I rebuilt docker images from instruction on https://github.com/google/deepvariant/issues/99#issuecomment-428366972. ```; gcloud builds submit \; --project ""${PROJECT_ID}"" \; --config cloudbuild.yaml \; --substitutions TAG_NAME=""${VERSION_NUMBER}"" \; --timeout 2h .; ```. I see three images on GCP Container Registry:. 1. **deepvariant**; 1. **deepvariant_gpu**; 1. **deepvariant_runner**. After finishing make_examples, now I am running calll_variant, but seems my rebuilt deepvariant_gpu image doesn't have CUDA installed, seeing such error. ```; ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory; ```. The command I used:; ```; ( time nvidia-docker run -v /home/${USER}:/home/${USER} gcr.io/my_project/deepvariant_gpu:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) | tee ""${LOG_DIR}/call_variants.log"" 2>&1; ```. I confirmed this is NOT an issue with gcr.io/deepvariant-docker/deepvariant_gpu, which means that it's just my rebuilt image missing CUDA driver. How should I modify the build command to build an image with CUDA driver, please?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102:176,config,config,176,,https://github.com/google/deepvariant/issues/102,1,['config'],['config']
Modifiability,"I understand that PRs are not performed on github. So, I just wanted to recommend/discuss some potential changes for the shuffle_tfrecords_beam.py script to enable running it with SparkRunner (PortableRunner). Without these changes the script works only in LOOPBACK mode (which is a testing mode where the actual work is performed on the submitting host).",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365:193,Portab,PortableRunner,193,,https://github.com/google/deepvariant/pull/365,1,['Portab'],['PortableRunner']
Modifiability,"I used deepvariant to call variant on HIFI and ONT sequencing data, and merged the generated gvcf files. When I used gatk to merge, an error occurred. When using glnexus to merge gvcf, its config options are only DeepVariantWGS and DeepVariantWES. Can you provide me with some help and suggestions about merging gvcf files? grateful.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/710:189,config,config,189,,https://github.com/google/deepvariant/issues/710,1,['config'],['config']
Modifiability,"I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04; - DeepVariant version: 1.4.0; - Building docker image locally. **Steps to reproduce:**; - Command: `docker build .` in source directory (no modifications); - Error trace: ; ; ```; #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting; #16 1484.7 Extracting Bazel installation...; #16 1487.8 Starting local Bazel server and connecting to it...; #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc); #16 1489.8 (21:51:01) INFO: Options provided by the client:; #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:887,Inherit,Inherited,887,,https://github.com/google/deepvariant/issues/608,1,['Inherit'],['Inherited']
Modifiability,"I was trying to follow the quick start guide. While running the run-prereq.sh file, I got; `========== Load config settings.`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Misc setup' starting`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Update package list' starting`; `sudo: apt-get: command not found`; Then, I realize it is because I am running it on mac. Is there any quick fix to this problem?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/44:108,config,config,108,,https://github.com/google/deepvariant/issues/44,1,['config'],['config']
Modifiability,"I would like to limit number of vCores used by DeepVariant on my server (4 cpu cores), say 1 cpu cores for call_variants. Based on [https://github.com/google/deepvariant/issues/42](https://github.com/google/deepvariant/issues/42), I try to add those configs (intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) into [https://github.com/google/deepvariant/blob/r0.7/deepvariant/call_variants.py#L306](https://github.com/google/deepvariant/blob/r0.7/deepvariant/call_variants.py#L306). However, I failed to limit the number of vCores. Here is my command and the status of the process.; ; Command:. ~/deepvariant$ time ~/deepvariant/bazel-bin/deepvariant/call_variants \; --outfile=${OUTPUT_DIR}/HG002.cvo.tfrecord.22.gz \; --examples=${OUTPUT_DIR}/HG002.examples.tfrecord.22.gz \; --checkpoint=""${MODEL}"" \; --execution_hardware=""cpu"" \; --num_readers=1. And the process list by `top`:. PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND; 1767 chungts+ 20 0 6622252 2.838g 103516 S 387.0 19.3 3:35.57 /usr/bin/python /tmp/Bazel.runfiles_dnbQVK/runfiles/com_google_deepvariant/+. and the status of the process:. ~/deepvariant$ cat /proc/1767/status; Name:	python; Umask:	0002; State:	S (sleeping); Tgid:	1767; Ngid:	0; Pid:	1767; PPid:	1766; TracerPid:	0; Uid:	1002	1002	1002	1002; Gid:	1003	1003	1003	1003; FDSize:	64; Groups:	4 20 24 25 29 30 44 46 109 110 1000 1001 1003; NStgid:	1767; NSpid:	1767; NSpgid:	1766; NSsid:	2233; VmPeak:	 6967476 kB; VmSize:	 6621996 kB; VmLck:	 0 kB; VmPin:	 0 kB; VmHWM:	 3321704 kB; VmRSS:	 2976936 kB; RssAnon:	 2873420 kB; RssFile:	 103516 kB; RssShmem:	 0 kB; VmData:	 5083652 kB; VmStk:	 148 kB; VmExe:	 2936 kB; VmLib:	 362804 kB; VmPTE:	 6888 kB; VmSwap:	 0 kB; HugetlbPages:	 0 kB; CoreDumping:	0; Threads:	12; Cpus_allowed:	f; Cpus_allowed_list:	0-3; voluntary_ctxt_switches:	96; nonvoluntary_ctxt_switches:	135. Is there anything missing?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90:250,config,configs,250,,https://github.com/google/deepvariant/issues/90,1,['config'],['configs']
Modifiability,"I'm sorry to bother you again, but I have a few more questions for you. . At present, the sequencing data we used are pacbio clr and ont r9.4. Can deepvariant be used to call varinat? If so, what should be selected for the --model_type parameter (HIFI / PACBIO ONT_R104). There is another question mentioned above. According to your reply, my understanding is as follows: When merging gvcf with glnexus, regardless of what deepvatriant's --model_type parameter selects (WGS,PACBIO,HIFI, etc.), glnexus's --config can select DeepVariantWGS. If not, please correct. Thank you again and look forward to your reply",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/713:506,config,config,506,,https://github.com/google/deepvariant/issues/713,1,['config'],['config']
Modifiability,"I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook ; n1-standard-64 - 64v CPUs - 240GB RAM; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): docker deepvariant ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:; export BIN_VERSION=""1.5.0""; export INPUT_DIR=""/home/jupyter/input""; export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""; export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam""; export OUTPUT_DIR=""/home/jupyter/output""; export OUTPUT_VCF=""HG003.deepvariant.vcf.gz""; export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md; ; I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. ; Is there a reason for the differences in runtime? ; ; Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/735:1460,config,configuration,1460,,https://github.com/google/deepvariant/issues/735,1,['config'],['configuration']
Modifiability,"I0826 20:44:28.953605 47737984214848 call_variants.py:446] Writing calls to /paedyl01/disk1/yangyxt/test_tmp/call_variants_output.tfrecord.gz; INFO:tensorflow:Calling model_fn.; I0826 20:44:29.309295 47737984214848 estimator.py:1173] Calling model_fn.; /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1083: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:678: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:2441: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:118: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1638: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); INFO:tensorflow:Done calling model_fn.; I0826 20:44:33.173107 47737984214848 estimator.py:1175] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0826 20:44:34.048544 47737984214848 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0826 20:44:34.048974 47737984214848 saver.py:1399] Restoring parameters from /opt/models/wgs/model.ckpt; INFO:tensorflow:Running local_init_op.; I0826 20:44:34.790676 47737984214848 session_manager.py:531] Running local_init_op.; INFO:tensorflow:Done ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:5484,layers,layers,5484,,https://github.com/google/deepvariant/issues/564,2,['layers'],['layers']
Modifiability,"My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do?. ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737:1173,config,config,1173,,https://github.com/google/deepvariant/issues/737,1,['config'],['config']
Modifiability,Norm/beta; prev_var_name: Unchanged; I0415 07:34:37.979801 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.980334 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.980947 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.981682 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.982532 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.983606 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.984400 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.985276 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.986526 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.987484 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.988141 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.988661 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:94355,variab,variable,94355,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,Norm/beta; prev_var_name: Unchanged; I0415 07:34:37.981682 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.982532 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.983606 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.984400 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.985276 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.986526 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.987484 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.988141 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.988661 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989614 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.990134 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:94873,variab,variable,94873,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,Norm/beta; prev_var_name: Unchanged; I0415 07:34:38.000669 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.001136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.001605 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.002159 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.002743 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.003241 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.003774 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.004208 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.004652 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.005083 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.005480 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.005927 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Bra,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:99143,variab,variable,99143,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,Norm/beta; prev_var_name: Unchanged; I0415 07:34:38.020929 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.021404 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.021804 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.022231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.022672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.023087 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023875 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.024280 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.024672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025057 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025403 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:105120,variab,variable,105120,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,Norm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:110579,variab,variable,110579,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,Norm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:112633,variab,variable,112633,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,Norm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:113476,variab,variable,113476,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,Questions about GLnexus integration and DeepTrio training data for config evaluation,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475:67,config,config,67,,https://github.com/google/deepvariant/issues/475,1,['config'],['config']
Modifiability,"R_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, since apt-utils is not installed; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 663 100 663 0 0 5012 0 --:--:-- --:--:-- --:--:-- 5022; debconf: delaying package configuration, since apt-utils is not installed; WARNING: Logging before flag parsing goes to stderr.; ```. But I then see many messages about candidate variants it's found. . The directory `staging-folder/examples/0/` also includes 8 `.gz` files like `examples_output.tfrecord-00007-of-00008.gz`. . Can you help me figure out what I'm doing wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:3499,config,configuration,3499,,https://github.com/google/deepvariant/issues/60,2,['config'],['configuration']
Modifiability,THON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:; #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:4033,config,config,4033,,https://github.com/google/deepvariant/issues/608,1,['config'],['config']
Modifiability,"There are some problems while running the ./build-prereq.sh:; ```; + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0; Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0); Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0); Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:102,parameteriz,parameterized,102,,https://github.com/google/deepvariant/issues/739,2,['parameteriz'],['parameterized']
Modifiability,"This is from DeepVariant's documentation. ""For somatic data or any other samples where the genotypes go beyond two copies of DNA, DeepVariant will not work out of the box because the only genotypes supported are hom-alt, het, and hom-ref."". Does the same apply to the RNA-seq model of DeepVariant? If a mutation has a low VAF is it predicted as hom-ref?. Because in RNA-seq, low VAF doesn't necessarily mean the variant is hom-ref in the genome. . ### In RNA-seq:; - **Expression Levels**: RNA-seq measures gene expression, so the VAF of a mutation depends on the expression of the mutant and wild-type alleles. If a gene is highly expressed in some cells and not others, or if only one allele is expressed (allele-specific expression), the VAF may be skewed.; - **Variable Expression**: The VAF in RNA-seq data can be influenced by tissue-specific expression, transcriptional noise, or RNA degradation, making it less consistent compared to DNA-seq.; - **Allelic Imbalance**: In RNA-seq, you might observe allelic imbalance due to factors like imprinting or preferential expression of one allele, further complicating the interpretation of VAF.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/864:765,Variab,Variable,765,,https://github.com/google/deepvariant/issues/864,1,['Variab'],['Variable']
Modifiability,"To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159:71,config,configurable,71,,https://github.com/google/deepvariant/pull/159,1,['config'],['configurable']
Modifiability,"Traceback (most recent call last):; File ""get-pip.py"", line 32992, in <module>; main(); File ""get-pip.py"", line 135, in main; bootstrap(tmpdir=tmpdir); File ""get-pip.py"", line 111, in bootstrap; monkeypatch_for_cert(tmpdir); File ""get-pip.py"", line 92, in monkeypatch_for_cert; from pip._internal.commands.install import InstallCommand; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>; ModuleNotFoundError: No module named 'distutils.cmd'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/730:995,config,configuration,995,,https://github.com/google/deepvariant/issues/730,1,['config'],['configuration']
Modifiability,"ULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:3014,config,config,3014,,https://github.com/google/deepvariant/issues/797,1,['config'],['config']
Modifiability,"Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0703 17:18:45.810746 140322304501504 data_providers.py:376] self.input_map_threads=48; W0703 17:18:45.810852 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0703 17:18:46.328745 140322304501504 estimator.py:1147] Calling model_fn.; W0703 17:18:46.330687 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0703 17:18:46.333346 140322304501504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0703 17:18:50.712157 140322304501504 estimator.py:1149] Done calling model_fn.; I0703 17:18:51.788142 140322304501504 monitored_session.py:240] Graph was finalized.; ```; We are on Docker 19.03.11 on Debian 10 and executed deepvariant with ```docker run --gpus all ....``` (proposed way of using nvidia docker for Docker version >19.03 in the nvidia docker docs). Our installed Nvidia driver version is 418.113. Thanks in advance for your help,. Sebastian",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321:5660,layers,layers,5660,,https://github.com/google/deepvariant/issues/321,2,['layers'],['layers']
Modifiability,"VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correctly, and **IT IS NOT MY CASE**!!!!. I tested if the volumes were mounted correctly, according to the script:; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; sudo docker run \; -i \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:0.8.0 \; find /input. And the result was:; /input/NA12878_S1.chr20.10_10p1mb.bam; /input/NA12878_S1.chr20.10_10p1mb.bam.bai; /input/test_nist.b37_chr20_100kbp_at_10mb.bed; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; /input/ucsc.hg19.chr20.unittest.fasta; /input/ucsc.hg19.chr20.unittest.fasta.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz; /input/ucsc.hg19.chr20.unittest.fasta.gz.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz.gzi. It means that all files are in the mounted Docker volume /input . Thanks so much for any help,; RogÃ©rio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:2319,variab,variable,2319,,https://github.com/google/deepvariant/issues/223,1,['variab'],['variable']
Modifiability,"Well, this is not really a problem. It's rather a question. Someone asked about the de novo germline calling last year [#377](https://github.com/google/deepvariant/issues/377). Deeptrio is now available and I want to ask general question in regards with denovo variants in the child. . Background: the biggest issue with calling de novo variants (i.e. variants that are found in proband, generally as heterozygous, negative in parents) is there are ton's of false negative calls (Type II error) (i.e. variants not called in parents but are visually obvious in the alignment). . It seems to me that DeepTrio should address this issue particularly well but I am not sure if that is the motivation behind DeepTrio. How does DeepTrio handle type 2 errors for de novo germline variant calling? can this be added as an enhancement?. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/450:813,enhance,enhancement,813,,https://github.com/google/deepvariant/issues/450,1,['enhance'],['enhancement']
Modifiability,"When I used version 1.6.1 for source code compilation, an error related to the numpy library occurred. I suspect this is due to incompatibility with TensorFlow. I tried using other versions of the numpy library, but the issue persisted. ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Misc setup' starting; ========== [Fri 02 Aug 2024 02:20:04 PM CST] Stage 'Update package list' starting; ========== [Fri 02 Aug 2024 02:20:06 PM CST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Fri 02 Aug 2024 02:20:10 PM CST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:333,config,config,333,,https://github.com/google/deepvariant/issues/859,2,['config'],['config']
Modifiability,"When i run train_model.zip, i get a error:; ![image](https://user-images.githubusercontent.com/15261087/33821907-8e07de38-de90-11e7-9461-dc54523691d4.png); Here is my command:; ```; python /leostore/software/deepvariant/bazel-bin/deepvariant/make_examples.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/10:350,config,config,350,,https://github.com/google/deepvariant/issues/10,1,['config'],['config']
Modifiability,YTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:3959,config,config,3959,,https://github.com/google/deepvariant/issues/89,1,['config'],['config']
Modifiability,"_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Conv2d_3b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Conv2d_4a_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp', var_name_to_vocab_info={}, var_name_to_prev_var_name={}); I0415 07:34:37.687702 140368878327552 warm_starting_util.py:417] Warm-starting from: ('/home/models/model.ckpt',); I0415 07:34:37.964245 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.965248 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966092 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966933 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.967533 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.968360 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.969041 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.969827 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.970808 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:90439,variab,variable,90439,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:116026,variab,variable,116026,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:118216,variab,variable,118216,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/validation_set.gz --output_pattern_prefix=""output/validation_shuffled"" --output_dataset_name=""27"" --output_dataset_config_pbtxt=""output/validation.pbtxt"" --job_name=shuffle-tfrecords `; _Model trainning_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" train --config=/input/dv_config.py:base --config.train_dataset_pbtxt=""/output/training.pbtxt"" --config.tune_dataset_pbtxt=""/output/validation.pbtxt"" --config.num_epochs=10 --config.learning_rate=0.0001 --config.num_validation_examples=0 --strategy=mirrored --experiment_dir=""/output/"" --config.batch_size=512`; _Model test_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type WES --customized_model ""/output/checkpoints/ckpt-679"" --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/33_r_groups.bam"" --output_vcf ""/output/33.vcf.gz"" --output_gvcf ""/output/33.g.vcf.gz"" -intermediate_results_dir ""/output/intermediate_results_dir"" --num_shards 10`. - Error trace: ; ` - I0822 07:51:54.272576 127450974123840 make_examples_core.py:301] Task 17/20: Writing example info to /output/intermediate_results_dir/make_examples.tfrecord-00017-of-00020.gz.example_info.json; I0822 07:51:54.272647 127450974123840 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0822 07:51:54.272740 127450974123840 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0822 07:51:54.2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:2540,config,config,2540,,https://github.com/google/deepvariant/issues/869,1,['config'],['config']
Modifiability,"_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.74.226:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}; I0524 21:18:26.620151 140032543119168 estimator.py:191] Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.74.226:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}; INFO:tensorflow:_TPUContext: eval_on_tpu True; I0524 21:18:26.620373 140032543119168",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:3715,config,config,3715,,https://github.com/google/deepvariant/issues/537,1,['config'],['config']
Modifiability,"_secs); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session; return self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session; init_fn=self._scaffold.init_fn); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore; {self.saver_def.filename_tensor_name: save_path}); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run; run_metadata_ptr); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run; feed_dict_tensor, options, run_metadata); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run; run_metadata); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/D",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:6447,config,config,6447,,https://github.com/google/deepvariant/issues/166,2,['config'],['config']
Modifiability,a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:119388,variab,variable,119388,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"age suitable for ARM64 architecture; FROM arm64v8/ubuntu:latest AS base. # Prevent interactive prompts; ENV DEBIAN_FRONTEND=noninteractive. # Install necessary packages; RUN apt-get update && \; apt-get install -y \; git \; curl \; unzip \; wget \; openjdk-17-jdk \; build-essential \; bzip2 \; python3-pip \; parallel && \; apt-get clean && \; rm -rf /var/lib/apt/lists/*. # Install Bazel (adjust version as needed); RUN curl -LO ""https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_listener= //deepvariant:make_examples //deepvariant:call_variants //deepvariant:postprocess_variants || { \; echo ""Bazel build failed""; \; exit 1; }. # Final image; FROM base AS final. # Set environment variables; ENV VERSION=1.6.0; ENV PYTHON_VERSION=3.8; ENV PATH=""/opt/miniconda/bin:${PATH}"". # Install Python packages; RUN pip install --upgrade pip setuptools wheel --timeout=120 && \; pip install jaxlib jax --timeout=120 --extra-index-url https://storage.googleapis.com/jax-releases/jax_releases.html. # Copy DeepVariant binari",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:1526,config,config,1526,,https://github.com/google/deepvariant/issues/871,3,['config'],['config']
Modifiability,"aining a model (starting from wgs.1.6.1) for use in a fish species. The programs are running well, I have confident regions and truth variants defined, and am currently tuning hyperparameters to optimise the training. . However . . . . I notice when tracking the model eval stats (specifically f1, precision, recall), that the hom_ref classifications are much less reliable than hom_alt and het classes. My question is whether this is to be expected, or whether there might be something wrong with my training setup, or perhaps the examples. . The test example set I am using to tune the hyperparams looks like this:. ```; # Generated by shuffle_tfrecords_beam.py; # class2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.3",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1062,config,config,1062,,https://github.com/google/deepvariant/issues/904,1,['config'],['config']
Modifiability,"ake_examples.py"", line 1108, in main; make_examples_runner(options); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1037, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 764, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 802, in region_reads; _, reads = self.realigner.realign_reads(reads, region); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 460, in realign_reads; candidate_windows = self.call_window_selector(region, reads); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 360, in call_window_selector; region.start),; File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 249, in process_reads; for ref_pos in self.process_read(ref, read, ref_offset):; File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 162, in process_read; self._process_soft_clip(cigar, ref, read, ref_pos, read_pos)); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 89, in _process_soft_clip; if read.aligned_quality[read_pos] >= self.config.min_base_quality:; IndexError: list index (0) out of range. Process finished with exit code 1; ```. What might be the cause of the error?; Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/75:2990,config,config,2990,,https://github.com/google/deepvariant/issues/75,1,['config'],['config']
Modifiability,"all_variants...; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] Job failed with error {...........cutout...; 13:33:48 Stopped running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512"": exit status 1: turn self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session; init_fn=self._scaffold.init_fn); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 288, in prepare_session; config=config); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 202, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard//model.ckpt; 13:33:48 Unexpected exit status 1 while running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512""; 13:33:48 Started running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:5655,config,config,5655,,https://github.com/google/deepvariant/issues/129,2,['config'],['config']
Modifiability,"alling variants initially, even if in a fish. . **Setup**; Running on a university computing cluster (https://hpc-unibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduct",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1752,config,config,1752,,https://github.com/google/deepvariant/issues/876,1,['config'],['config']
Modifiability,ame: Unchanged; I0415 07:34:37.967533 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.968360 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.969041 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.969827 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.970808 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.971487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.972088 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.972737 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.973396 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.974112 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.974828 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.975663 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:91636,variab,variable,91636,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:37.988661 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989614 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.990134 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.990647 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.991297 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.991910 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.992630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:37.993726 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.994658 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.995343 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.996014 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:96238,variab,variable,96238,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:37.989120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989614 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.990134 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.990647 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.991297 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.991910 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.992630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:37.993726 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.994658 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.995343 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.996014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.997136 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:96413,variab,variable,96413,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:37.991297 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.991910 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.992630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:37.993726 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.994658 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.995343 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.996014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.997136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.997966 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.998590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.999377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.000032 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:97099,variab,variable,97099,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:37.991910 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.992630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:37.993726 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.994658 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.995343 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.996014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.997136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.997966 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.998590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.999377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.000032 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.000669 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:97274,variab,variable,97274,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:37.993726 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.994658 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.995343 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.996014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.997136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.997966 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.998590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.999377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.000032 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.000669 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.001136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.001605 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:97617,variab,variable,97617,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.015494 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.016149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.016633 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.017230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.017909 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018465 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018997 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.019490 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.020006 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.020472 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.020929 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.021404 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:103416,variab,variable,103416,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.016149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.016633 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.017230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.017909 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018465 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018997 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.019490 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.020006 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.020472 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.020929 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.021404 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.021804 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Bra,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:103591,variab,variable,103591,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.017909 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018465 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018997 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.019490 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.020006 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.020472 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.020929 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.021404 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.021804 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.022231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.022672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.023087 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:104102,variab,variable,104102,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.023487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023875 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.024280 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.024672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025057 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025403 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.026254 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.026756 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027156 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027498 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.027940 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:106149,variab,variable,106149,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.025782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.026254 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.026756 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027156 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027498 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.027940 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.028300 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.028651 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.029017 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029366 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029716 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.030065 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:107185,variab,variable,107185,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:110079,variab,variable,110079,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:111440,variab,variable,111440,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:114162,variab,variable,114162,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Bran,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:114337,variab,variable,114337,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Co,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:114680,variab,variable,114680,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:116712,variab,variable,116712,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/C,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:116887,variab,variable,116887,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weigh,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:117230,variab,variable,117230,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:120074,variab,variable,120074,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:121278,variab,variable,121278,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook.; I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized.; 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_featu,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:121621,variab,variable,121621,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ame: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook.; I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized.; 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_u,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:121796,variab,variable,121796,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"ants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/validation_set.gz --output_pattern_prefix=""output/validation_shuffled"" --output_dataset_name=""27"" --output_dataset_config_pbtxt=""output/validation.pbtxt"" --job_name=shuffle-tfrecords `; _Model trainning_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" train --config=/input/dv_config.py:base --config.train_dataset_pbtxt=""/output/training.pbtxt"" --config.tune_dataset_pbtxt=""/output/validation.pbtxt"" --config.num_epochs=10 --config.learning_rate=0.0001 --config.num_validation_examples=0 --strategy=mirrored --experiment_dir=""/output/"" --config.batch_size=512`; _Model test_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type WES --customized_model ""/output/checkpoints/ckpt-679"" --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/33_r_groups.bam"" --output_vcf ""/output/33.vcf.gz"" --output_gvcf ""/output/33.g.vcf.gz"" -intermediate_results_dir ""/output/intermediate_results_dir"" --num_shards 10`. - Error trace: ; ` - I0822 07:51:54.272576 127450974123840 make_examples_core.py:301] Task 17/20: Writing example info to /output/intermediate_results_dir/make_examples.tfrecord-00017-of-00020.gz.example_info.json; I0822 07:51:54.272647 127450974123840 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0822 07:51:54.272740 127450974123840 make_examples_core.py:2959] example_channels = [1, 2,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:2510,config,config,2510,,https://github.com/google/deepvariant/issues/869,1,['config'],['config']
Modifiability,"arity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1975,config,config,1975,,https://github.com/google/deepvariant/issues/876,1,['config'],['config']
Modifiability,atchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.003774 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.004208 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.004652 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.005083 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.005480 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.005927 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006367 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006792 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007198 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.008188 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.008778 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:100154,variab,variable,100154,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,atchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:119563,variab,variable,119563,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"ate; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1191, in convolution2d; conv_dims=2); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1089, in convolution; outputs = layer.apply(inputs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548, in __call__; outputs = super(Layer, self).__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__; outputs = call_fn(cast_inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 234, in wrapper; return converted_call(f, options, args, kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 439, in convert",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:18903,layers,layers,18903,,https://github.com/google/deepvariant/issues/358,2,['layers'],['layers']
Modifiability,"ave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0924 03:47:37.814187 140325876573952 deprecation.py:323] From /tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0924 03:47:38.164505 140325876573952 estimator.py:1147] Calling model_fn.; W0924 03:47:38.168455 140325876573952 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0924 03:47:41.667636 140325876573952 estimator.py:1149] Done calling model_fn.; I0924 03:47:42.548214 140325876573952 monitored_session.py:240] Graph was finalized.; 2020-09-24 03:47:42.549039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: ; name: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.77; pciBusID: 0000:21:00.0; 2020-09-24 03:47:42.549107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0; 2020-09-24 03:47:42.549121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0; 2020-09-24 03:47:42.549131: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfull",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:8563,layers,layers,8563,,https://github.com/google/deepvariant/issues/358,2,['layers'],['layers']
Modifiability,"azel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner; runtimes) = region_processor.process(region); File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process; reads = self.region_reads(; File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads; raise err; File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query; return self._reader.query(region); File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query; return self._reader.query(region); ValueError: Failed precondition: Cannot query without an index; [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'; I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader; [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'; I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader; I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz; I1104 14:34:03.205575 139685650237248 make_examples_core.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/492:16266,extend,extend,16266,,https://github.com/google/deepvariant/issues/492,1,['extend'],['extend']
Modifiability,"azel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner; runtimes) = region_processor.process(region); File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process; reads = self.region_reads(; File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads; raise err; File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query; return self._reader.query(region); File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query; return self._reader.query(region); ValueError: Failed precondition: Cannot query without an index; [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'; I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader; [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'; I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader; I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz; I1104 14:34:03.209277 140243238704960 make_examples_core.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/492:14022,extend,extend,14022,,https://github.com/google/deepvariant/issues/492,1,['extend'],['extend']
Modifiability,"azel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner; runtimes) = region_processor.process(region); File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process; reads = self.region_reads(; File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads; raise err; File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query; return self._reader.query(region); File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query; return self._reader.query(region); ValueError: Failed precondition: Cannot query without an index; [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'; I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader; [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'; I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader; I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz; I1104 14:34:03.194392 139750224725824 make_examples_core.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/492:11778,extend,extend,11778,,https://github.com/google/deepvariant/issues/492,1,['extend'],['extend']
Modifiability,b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.020006 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.020472 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.020929 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.021404 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.021804 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.022231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.022672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.023087 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023875 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.024280 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.024672 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:104770,variab,variable,104770,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,beta; prev_var_name: Unchanged; I0415 07:34:38.018997 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.019490 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.020006 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.020472 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.020929 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.021404 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.021804 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.022231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.022672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.023087 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023875 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mix,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:104445,variab,variable,104445,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/B,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:115191,variab,variable,115191,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"bove command successfully ran for _make_example_ module and failed at _call_varaint_keras.py_ with the following error message.. -------------. _Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 399, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 312, in run_; _run_main(main, args); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 387, in main; call_variants(; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 344, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored:; <tf.Variable 'classification/kernel:0' shape=(2048, 3) dtype=float32, numpy=_. ------------; Please note that I used the same checkpoint path as I was using while running _run_deepvariant.py_.; Could you please help me to resolve the issue and suggest if I am missing anything while using _call_varaint_keras.py_?. Thanks,; Saurabh",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/636:2195,Variab,Variable,2195,,https://github.com/google/deepvariant/issues/636,1,['Variab'],['Variable']
Modifiability,chNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:117909,variab,variable,117909,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,changed; I0415 07:34:37.969041 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.969827 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.970808 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.971487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.972088 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.972737 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.973396 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.974112 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.974828 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.975663 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.976491 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.977374 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:91979,variab,variable,91979,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,changed; I0415 07:34:38.005083 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.005480 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.005927 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006367 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006792 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007198 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.008188 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.008778 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.009377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.010077 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.010946 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:100679,variab,variable,100679,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,changed; I0415 07:34:38.005927 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006367 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006792 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007198 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.008188 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.008778 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.009377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.010077 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.010946 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.011956 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013191 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:101022,variab,variable,101022,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,changed; I0415 07:34:38.023875 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.024280 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.024672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025057 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025403 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.026254 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.026756 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027156 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027498 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.027940 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.028300 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:106324,variab,variable,106324,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,changed; I0415 07:34:38.024672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025057 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025403 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.026254 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.026756 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027156 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027498 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.027940 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.028300 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.028651 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.029017 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:106667,variab,variable,106667,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,changed; I0415 07:34:38.025057 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025403 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.026254 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.026756 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027156 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027498 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.027940 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.028300 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.028651 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.029017 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029366 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:106842,variab,variable,106842,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,changed; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:111615,variab,variable,111615,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,changed; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Bra,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:111790,variab,variable,111790,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,changed; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/C,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:111965,variab,variable,111965,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,changed; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook.; I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized.; 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz; 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:121971,variab,variable,121971,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"changed; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook.; I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized.; 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz; 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:; 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; I0415 07:34:52.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:122146,variab,variable,122146,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"changed; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook.; I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized.; 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz; 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:; 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op.; I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op.; I0415 07:35:1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:122321,variab,variable,122321,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"cute(ctx._handle, device_name, op_name,; tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]; Registered devices: [CPU]; Registered kernels:; <no registered kernels>. [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/com_google_deepvariant/deepvariant/train.py"", line 532, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/com_google_deepvariant/deepvariant/train.py"", line 518, in main; train(FLAGS.config); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/com_google_deepvariant/deepvariant/train.py"", line 109, in train; tf.tpu.experimental.initialize_tpu_system(resolver); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py"", line 113, in initialize_tpu_system; raise errors.NotFoundError(; tensorflow.python.framework.errors_impl.NotFoundError: TPUs not found in the cluster. Failed in initialization: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]; Registered devices: [CPU]; Registered kernels:; <no registered kernels>. [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:4518,config,config,4518,,https://github.com/google/deepvariant/issues/841,4,"['Config', 'config']","['ConfigureDistributedTPU', 'config']"
Modifiability,"d for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2183,config,config,2183,,https://github.com/google/deepvariant/issues/876,1,['config'],['config']
Modifiability,"d-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an inva",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:11293,config,configured,11293,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"d-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:42:05 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:7716,config,configured,7716,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"d-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:801::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:801::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:9669,config,configured,9669,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"d-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:6092,config,configured,6092,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"dDataset(path)`; I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False; I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1; I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt; I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting....; I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}; W0415 07:34:19.583559 140368878327552 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.; Instructions for updating:; Colocations handled automatically by ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:7555,config,config,7555,,https://github.com/google/deepvariant/issues/172,1,['config'],['config']
Modifiability,d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.002743 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.003241 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.003774 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.004208 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.004652 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.005083 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.005480 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.005927 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006367 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006792 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007198 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007590 140368878327552 warm_starting_util.py:466] Warm-startin,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:99811,variab,variable,99811,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:112969,variab,variable,112969,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"d_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp', var_name_to_vocab_info={}, var_name_to_prev_var_name={}); I0415 07:34:37.687702 140368878327552 warm_starting_util.py:417] Warm-starting from: ('/home/models/model.ckpt',); I0415 07:34:37.964245 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.965248 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966092 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966933 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.967533 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.968360 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.969041 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.969827 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.970808 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.971487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.972088 140368878327552 warm_starting_util.py:466] Warm-starting varia",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:90775,variab,variable,90775,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.003241 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.003774 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.004208 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.004652 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.005083 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.005480 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.005927 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006367 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006792 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007198 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.008188 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:99979,variab,variable,99979,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.029017 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029366 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029716 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.030065 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:108532,variab,variable,108532,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"ding /data/HG002.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; I0508 07:36:27.306630 140432686110464 genomics_reader.py:223] Reading /data/HG002.bam with NativeSamReader; I0508 07:36:27.465119 140432686110464 make_examples.py:648] Task 8/25: Writing examples to /tmp/tmpd7h_gv7l/make_examples.tfrecord-00008-of-00025.gz; I0508 07:36:27.465305 140432686110464 make_examples.py:648] Task 8/25: Writing gvcf records to /tmp/tmpd7h_gv7l/gvcf.tfrecord-00008-of-00025.gz; I0508 07:36:27.465700 140432686110464 make_examples.py:648] Task 8/25: Overhead for preparing inputs: 74 seconds; I0508 07:36:27.529915 140432686110464 make_examples.py:648] Task 8/25: 0 candidates (0 examples) [0.06s elapsed]; [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/455:7431,extend,extend,7431,,https://github.com/google/deepvariant/issues/455,1,['extend'],['extend']
Modifiability,e ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:5175,config,configured,5175,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"e hom_ref classifications are much less reliable than hom_alt and het classes. My question is whether this is to be expected, or whether there might be something wrong with my training setup, or perhaps the examples. . The test example set I am using to tune the hyperparams looks like this:. ```; # Generated by shuffle_tfrecords_beam.py; # class2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840515, train/f1_homalt=0.964401364326477, train/f1_homref=0.902255654335022, train/f1_macro=0.; 8698380589485168, train/f1_micro=0.91796875, train/f1_w",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1364,config,config,1364,,https://github.com/google/deepvariant/issues/904,1,['config'],['config']
Modifiability,"e list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:801::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:801::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:9484,config,configured,9484,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"e training --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/27_r_groups.bam"" --examples ""/output/validation_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/validation_set.gz --output_pattern_prefix=""output/validation_shuffled"" --output_dataset_name=""27"" --output_dataset_config_pbtxt=""output/validation.pbtxt"" --job_name=shuffle-tfrecords `; _Model trainning_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" train --config=/input/dv_config.py:base --config.train_dataset_pbtxt=""/output/training.pbtxt"" --config.tune_dataset_pbtxt=""/output/validation.pbtxt"" --config.num_epochs=10 --config.learning_rate=0.0001 --config.num_validation_examples=0 --strategy=mirrored --experiment_dir=""/output/"" --config.batch_size=512`; _Model test_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type WES --customized_model ""/output/checkpoints/ckpt-679"" --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/33_r_groups.bam"" --output_vcf ""/output/33.vcf.gz"" --output_gvcf ""/output/33.g.vcf.gz"" -intermediate_results_dir ""/output/intermediate_results_dir"" --num_shards 10`. - Error trace: ; ` - I0822 07:51:54.272576 127450974123840 make_examples_core.py:301] Task 17/20: Writing example info to /output/intermediate_results_dir/make_examples.tfrecord-00017-of-00020.gz.example_info.json; I0822 07:51:54.272647 127450974123840 make_examples_core.py",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:2378,config,config,2378,,https://github.com/google/deepvariant/issues/869,1,['config'],['config']
Modifiability,"e) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:5723,config,configured,5723,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"e_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:; #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:; #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:3567,config,config,3567,,https://github.com/google/deepvariant/issues/608,1,['config'],['config']
Modifiability,"eed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0826 20:44:28.953605 47737984214848 call_variants.py:446] Writing calls to /paedyl01/disk1/yangyxt/test_tmp/call_variants_output.tfrecord.gz; INFO:tensorflow:Calling model_fn.; I0826 20:44:29.309295 47737984214848 estimator.py:1173] Calling model_fn.; /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1083: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:678: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:2441: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:118: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1638: UserWarning: `layer.apply` is deprecated",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:4779,layers,layers,4779,,https://github.com/google/deepvariant/issues/564,2,['layers'],['layers']
Modifiability,"ef/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/validation_set.gz --output_pattern_prefix=""output/validation_shuffled"" --output_dataset_name=""27"" --output_dataset_config_pbtxt=""output/validation.pbtxt"" --job_name=shuffle-tfrecords `; _Model trainning_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" train --config=/input/dv_config.py:base --config.train_dataset_pbtxt=""/output/training.pbtxt"" --config.tune_dataset_pbtxt=""/output/validation.pbtxt"" --config.num_epochs=10 --config.learning_rate=0.0001 --config.num_validation_examples=0 --strategy=mirrored --experiment_dir=""/output/"" --config.batch_size=512`; _Model test_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type WES --customized_model ""/output/checkpoints/ckpt-679"" --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/33_r_groups.bam"" --output_vcf ""/output/33.vcf.gz"" --output_gvcf ""/output/33.g.vcf.gz"" -intermediate_results_dir ""/output/intermediate_results_dir"" --num_shards 10`. - Error trace: ; ` - I0822 07:51:54.272576 127450974123840 make_examples_core.py:301] Task 17/20: Writing example info to /output/intermediate_results_dir/make_examples.tfrecord-00017-of-00020.gz.example_info.json; I0822 07:51:54.272647 127450974123840 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0822 07:51:54.272740 127450974123840 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0822 07:51:54.272911 127450974123840 make_examples_core.py:301] ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:2623,config,config,2623,,https://github.com/google/deepvariant/issues/869,1,['config'],['config']
Modifiability,eights; prev_var_name: Unchanged; I0415 07:34:37.980334 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.980947 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.981682 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.982532 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.983606 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.984400 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.985276 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.986526 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.987484 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.988141 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.988661 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989120 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:94523,variab,variable,94523,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,eights; prev_var_name: Unchanged; I0415 07:34:38.004208 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.004652 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.005083 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.005480 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.005927 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006367 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006792 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007198 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.008188 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.008778 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.009377 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:100329,variab,variable,100329,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,eights; prev_var_name: Unchanged; I0415 07:34:38.020472 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.020929 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.021404 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.021804 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.022231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.022672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.023087 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023875 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.024280 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.024672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025057 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:104945,variab,variable,104945,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,eights; prev_var_name: Unchanged; I0415 07:34:38.029716 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.030065 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:108875,variab,variable,108875,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,eights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:110747,variab,variable,110747,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,eights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:113301,variab,variable,113301,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,eights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:113644,variab,variable,113644,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"epvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/home/nyakovenko/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1225, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1127, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 849, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 889, in region_reads; _, reads = self.realigner.realign_reads(reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 606, in realign_reads; self.config.ws_config, self.ref_reader, reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 232, in select_windows; candidates = _candidates_from_reads(config, ref_reader, reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 84, in _candidates_from_reads; region, expanded_region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 152, in _allele_count_linear_selector; allele_counter, model_conf)); TypeError: allele_count_linear_candidates_from_allele_counter() argument counter is not valid for ::learning::genomics::deepvariant::AlleleCounter (deepvariant.python.allelecounter.AlleleCounter instance given): expecting deepvariant.python.allelecounter.AlleleCounter instance, got deepvariant.python.allelecounter.AlleleCounter instance; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199:3013,config,config,3013,,https://github.com/google/deepvariant/issues/199,2,['config'],['config']
Modifiability,es used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autor,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:11108,config,configured,11108,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"es used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:42:05 CST] Stage 'Update package list' starting; W: Target Packages (main/bina",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:7531,config,configured,7531,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"es:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \; --mode calling \; --ref {input.reference} \; --reads {input.bam} \; --examples {params.examples} \; --gvcf {params.gvcf} \; --task {params.shard}) > {log} 2>&1; """""". ```; Error:. ```; 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:1708,config,config,1708,,https://github.com/google/deepvariant/issues/677,1,['config'],['config']
Modifiability,es; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:116194,variab,variable,116194,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"et.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/validation_set.gz --output_pattern_prefix=""output/validation_shuffled"" --output_dataset_name=""27"" --output_dataset_config_pbtxt=""output/validation.pbtxt"" --job_name=shuffle-tfrecords `; _Model trainning_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" train --config=/input/dv_config.py:base --config.train_dataset_pbtxt=""/output/training.pbtxt"" --config.tune_dataset_pbtxt=""/output/validation.pbtxt"" --config.num_epochs=10 --config.learning_rate=0.0001 --config.num_validation_examples=0 --strategy=mirrored --experiment_dir=""/output/"" --config.batch_size=512`; _Model test_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type WES --customized_model ""/output/checkpoints/ckpt-679"" --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/33_r_groups.bam"" --output_vcf ""/output/33.vcf.gz"" --output_gvcf ""/output/33.g.vcf.gz"" -intermediate_results_dir ""/output/intermediate_results_dir"" --num_shards 10`. - Error trace: ; ` - I0822 07:51:54.272576 127450974123840 make_examples_core.py:301] Task 17/20: Writing example info to /output/intermediate_results_dir/make_examples.tfrecord-00017-of-00020.gz.example_info.json; I0822 07:51:54.272647 127450974123840 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0822 07:51:54.272740 127450974123840 make_examples_core.py:2959] examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:2487,config,config,2487,,https://github.com/google/deepvariant/issues/869,1,['config'],['config']
Modifiability,"etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:42:05 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:8752,config,configured,8752,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"f input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled; WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7; W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7; INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000; I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c; I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz; INFO:tensorflow:Calling model_fn.; I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn.; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn.; 2023-07",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:2726,config,config,2726,,https://github.com/google/deepvariant/issues/679,1,['config'],['config']
Modifiability,"formance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-12-14 06:10:31.101084: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1214 06:10:31.288279 140363019278144 openvino_estimator.py:55] Processing ckpt=/opt/models/wes/model.ckpt, tensor_shape=[100, 221, 7]; /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1083: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:678: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:2441: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:118: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1638: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I1214 06:10:37.470187 140363019278144 saver.py:1399] Restoring parameters from /opt/models/wes/model.ckpt; WARNING:tensorflow:From /tmp/Bazel.runfiles_oh47rpgj/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py:75: convert_variables_to_constants (from ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597:8427,layers,layers,8427,,https://github.com/google/deepvariant/issues/597,2,['layers'],['layers']
Modifiability,"gAverage|InceptionV3/Conv2d_4a_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp', var_name_to_vocab_info={}, var_name_to_prev_var_name={}); I0415 07:34:37.687702 140368878327552 warm_starting_util.py:417] Warm-starting from: ('/home/models/model.ckpt',); I0415 07:34:37.964245 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.965248 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966092 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966933 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.967533 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.968360 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.969041 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.969827 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.970808 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.971487 140368878327552 warm_starting_util.py:466] Warm-starting variable: In",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:90607,variab,variable,90607,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"gAverage|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Conv2d_3b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Conv2d_4a_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp', var_name_to_vocab_info={}, var_name_to_prev_var_name={}); I0415 07:34:37.687702 140368878327552 warm_starting_util.py:417] Warm-starting from: ('/home/models/model.ckpt',); I0415 07:34:37.964245 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.965248 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966092 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966933 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.967533 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.968360 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.969041 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.969827 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Bran",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:90289,variab,variable,90289,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ged; I0415 07:34:38.005480 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.005927 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006367 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006792 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007198 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.008188 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.008778 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.009377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.010077 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.010946 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.011956 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixe,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:100854,variab,variable,100854,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; ```; DV=""singularity run /autofs/bal34/xyu/softwares/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant ""; ${DV} \; --model_type=WES \; --customized_model=/autofs/bal34/xyu/run_software/dv_illu/model/model.ckpt \; --ref ${REF_FILE_PATH} \; --reads {1} \; --output_vcf ${BASE_DIR}/{2}/output.vcf.gz \; --num_shards 30 \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir ${BASE_DIR}/{2}/intermediate_results_dir; ```; - Error trace: (if applicable); ```; WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this mess",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:1414,variab,variables,1414,,https://github.com/google/deepvariant/issues/857,1,['variab'],['variables']
Modifiability,"gularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \; --train_dir=""training"" \; --model_name=""inception_v3"" \; --number_of_steps=900 \; --save_interval_secs=300 \; --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}""; ```. But, when I try the following. ```bash; GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \; --train_dir=""training"" \; --model_name=""inception_v3"" \; --number_of_steps=900 \; --save_interval_secs=300 \; --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}""; ```; I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint?. ```python; Warm-starting variables only in TRAINABLE_VARIABLES.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main; parse_and_run(); File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run; return run(; File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run; estimator.train(;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/500:1362,variab,variables,1362,,https://github.com/google/deepvariant/issues/500,1,['variab'],['variables']
Modifiability,h_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:118066,variab,variable,118066,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"hi, I am using DV 0.9 to find *de novo* variants in trios and so I am enriching for weird stuff. ; I am sure your team is aware of some/all of these, but I'll document at least 1 case here for the record. Nearly all obvious false positive de novo calls follow the same pattern. Mostly there is a haplotype from mom , and a haplotype from dad that are different. The kid inherits both the variable haplotypes, but the combination of DV and glnexus gentoype such that the variant appears as de novo. But, here is an example where there is just an incorrect call from DV that leads to 3 neighboring spurious de novo calls in the kid (top row):; ![bad-dn](https://user-images.githubusercontent.com/1739/74561469-d68a6600-4f25-11ea-8c71-105596810ace.png). note that dad in the 3rd row has a single read with a 1-base deletion (dash) followed by an insertion (purple tick). I can't show all of the reads in this image, but I have scrolled through and verified that is the only read. . here is the content of the dad's VCF for that region (the mom's is actually very similar):; ```; chr8	75144980	.	CT	C	44.7	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:44:34:16,18:0.529412:44,0,53; chr8	75144983	.	T	TG	49.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:49:35:18,17:0.485714:49,0,64; ```; note that that is the single-base del and the insertion that occurs in only 1 read. Since the mom's is the same, maybe there's something akin to realignment going on, but; by contrast, here is the kid's (seemingly more sensible) VCF for that region:; ```; chr8	75144981	.	T	A	71.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16:0.592593:71,0,67; chr8	75144982	.	A	T	63.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:61:27:11,16:0.592593:63,0,63; chr8	75144983	.	T	G	67.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16:0.592593:67,0,71; ```; here is the content of the gvcf for dad:; ```; chr8	75144980	.	CT	C,<*>	44.7	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:44:34:16,18,0:0.529412,0:44,0,53,990,990,990; chr8	75144982	.	A	<*>	0	.	END=75144982	GT:GQ:MIN_DP:PL	0/0:48:16:0,48,479; chr8",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272:370,inherit,inherits,370,,https://github.com/google/deepvariant/issues/272,2,"['inherit', 'variab']","['inherits', 'variable']"
Modifiability,"i have used this command to run deep variant and generate VCF file. sudo docker run -v `pwd`:`pwd` -w `pwd` google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=${base_path}/${ref_file_name}.fasta --reads=${base_path}/base_recalib/$i\_vqsr.bam --output_vcf=deep_variant_results/$i\.vcf.gz --output_gvcf=deep_variant_results/$i\.g.vcf.gz. I want to run post-process variants but cannot get it from the above command. 1) Is there some way to add parameters to above command?. 2) I found two links related to it:-; a) [https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md); GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`. There is no make_examples.zip in the bin directory and what should be supplied to this parameter --examples. Can you please give more details about variables?. b) [https://github.com/google/deepvariant/issues/103](https://github.com/google/deepvariant/issues/103); sudo docker run -v ${HOME}:${HOME} gcr.io/deepvariant-docker/deepvariant:0.7.0 /opt/deepvariant/bin/postprocess_variants \; --ref ${OUTDIR}/data/hg19.fa \; --infile ${OUTDIR}/output/cvo.tfrecord.gz \; --outfile ${OUTDIR}/output/output.vcf.gz \; --nonvariant_site_tfrecord_path ${OUTDIR}/output/gvcf.tfrecord@8.gz \; --gvcf_outfile ${OUTDIR}/output/output.gvcf.gz. This is another way and parameters are also different. how to define the infile here?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/319:1248,variab,variables,1248,,https://github.com/google/deepvariant/issues/319,1,['variab'],['variables']
Modifiability,"idia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-01-05 15:52:57.864310: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-01-05 15:53:10.688853: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:10.692890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:26.990784: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:53:27.004992 140619855705920 run_deepvariant.py:519] Re-using the directory for intermediate results in /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9. ***** Intermediate res",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:2344,variab,variable,2344,,https://github.com/google/deepvariant/issues/761,1,['variab'],['variable']
Modifiability,ights; prev_var_name: Unchanged; I0415 07:34:37.974112 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.974828 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.975663 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.976491 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.977374 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.978363 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.979172 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.979801 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.980334 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.980947 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.981682 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.982532 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:93169,variab,variable,93169,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ights; prev_var_name: Unchanged; I0415 07:34:37.999377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.000032 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.000669 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.001136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.001605 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.002159 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.002743 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.003241 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.003774 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.004208 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.004652 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.005083 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:98807,variab,variable,98807,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ights; prev_var_name: Unchanged; I0415 07:34:38.000032 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.000669 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.001136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.001605 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.002159 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.002743 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.003241 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.003774 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.004208 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.004652 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.005083 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.005480 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:98975,variab,variable,98975,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ights; prev_var_name: Unchanged; I0415 07:34:38.001136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.001605 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.002159 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.002743 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.003241 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.003774 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.004208 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.004652 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.005083 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.005480 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.005927 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006367 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:99318,variab,variable,99318,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ights; prev_var_name: Unchanged; I0415 07:34:38.027940 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.028300 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.028651 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.029017 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029366 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029716 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.030065 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:108032,variab,variable,108032,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ime --incompatible_use_python_toolchains=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:4460,config,config,4460,,https://github.com/google/deepvariant/issues/608,1,['config'],['config']
Modifiability,"ing new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1214 06:10:31.288279 140363019278144 openvino_estimator.py:55] Processing ckpt=/opt/models/wes/model.ckpt, tensor_shape=[100, 221, 7]; /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1083: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:678: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:2441: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:118: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1638: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I1214 06:10:37.470187 140363019278144 saver.py:1399] Restoring parameters from /opt/models/wes/model.ckpt; WARNING:tensorflow:From /tmp/Bazel.runfiles_oh47rpgj/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py:75: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.compat.v1.graph_util.convert_variables_to_constants`; W1214 06:10:41.056998 140363019278144",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597:8655,layers,layers,8655,,https://github.com/google/deepvariant/issues/597,2,['layers'],['layers']
Modifiability,ings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-clo,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:4995,config,configured,4995,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"input/27_r_groups.bam"" --examples ""/output/validation_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/validation_set.gz --output_pattern_prefix=""output/validation_shuffled"" --output_dataset_name=""27"" --output_dataset_config_pbtxt=""output/validation.pbtxt"" --job_name=shuffle-tfrecords `; _Model trainning_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" train --config=/input/dv_config.py:base --config.train_dataset_pbtxt=""/output/training.pbtxt"" --config.tune_dataset_pbtxt=""/output/validation.pbtxt"" --config.num_epochs=10 --config.learning_rate=0.0001 --config.num_validation_examples=0 --strategy=mirrored --experiment_dir=""/output/"" --config.batch_size=512`; _Model test_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type WES --customized_model ""/output/checkpoints/ckpt-679"" --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/33_r_groups.bam"" --output_vcf ""/output/33.vcf.gz"" --output_gvcf ""/output/33.g.vcf.gz"" -intermediate_results_dir ""/output/intermediate_results_dir"" --num_shards 10`. - Error trace: ; ` - I0822 07:51:54.272576 127450974123840 make_examples_core.py:301] Task 17/20: Writing example info to /output/intermediate_results_dir/make_examples.tfrecord-00017-of-00020.gz.example_info.json; I0822 07:51:54.272647 127450974123840 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0822 07:51:54.2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:2432,config,config,2432,,https://github.com/google/deepvariant/issues/869,1,['config'],['config']
Modifiability,"ist-packages/tensorflow/python/platform/app.py"", line 40, in run; > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run; > _run_main(main, args); > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main; > sys.exit(main(argv)); > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; > call_variants(; > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants; > init_op = tf.group(tf.compat.v1.global_variables_initializer(),; > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer; > return variables_initializer(global_variables()); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables; > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection; > return get_default_graph().get_collection(key, scope); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph; > return _default_graph_stack.get_default(); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default; > self._global_default_graph = Graph(); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__; > self._scoped_c_graph = c_api_util.ScopedTFGraph(); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__; > self.graph = c_api.TF_NewGraph(); > RuntimeError: random_device::random_device(const std::string&): device not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:6387,variab,variables,6387,,https://github.com/google/deepvariant/issues/602,1,['variab'],['variables']
Modifiability,ixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Bra,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:118552,variab,variable,118552,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,kages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; W: Some index files failed to download. They have been,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:5540,config,configured,5540,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"lename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:5907,config,configured,5907,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"less reliable than hom_alt and het classes. My question is whether this is to be expected, or whether there might be something wrong with my training setup, or perhaps the examples. . The test example set I am using to tune the hyperparams looks like this:. ```; # Generated by shuffle_tfrecords_beam.py; # class2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840515, train/f1_homalt=0.964401364326477, train/f1_homref=0.902255654335022, train/f1_macro=0.; 8698380589485168, train/f1_micro=0.91796875, train/f1_weighted=0.9241795539855957, train/f",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1402,config,config,1402,,https://github.com/google/deepvariant/issues/904,1,['config'],['config']
Modifiability,"list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:801::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:801::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:10376,config,configured,10376,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:6799,config,configured,6799,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest versio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:11611,config,config,11611,,https://github.com/google/deepvariant/issues/89,1,['config'],['config']
Modifiability,"llready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.5.0; - Tensorflow 2.11.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HIFI reads. **Steps to reproduce:**; ```; rule deepvariant_make_examples:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_rea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:1137,config,config,1137,,https://github.com/google/deepvariant/issues/677,1,['config'],['config']
Modifiability,m/beta; prev_var_name: Unchanged; I0415 07:34:37.980947 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.981682 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.982532 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.983606 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.984400 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.985276 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.986526 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.987484 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.988141 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.988661 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989614 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:94698,variab,variable,94698,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,m/beta; prev_var_name: Unchanged; I0415 07:34:38.004652 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.005083 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.005480 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.005927 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006367 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006792 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007198 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.008188 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.008778 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.009377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.010077 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:100504,variab,variable,100504,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,m/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:110922,variab,variable,110922,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,m/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:111097,variab,variable,111097,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,m/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:112458,variab,variable,112458,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:2752,config,config,2752,,https://github.com/google/deepvariant/issues/797,1,['config'],['config']
Modifiability,"move and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:42:05 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:801::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:801::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:9300,config,configured,9300,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,nV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:118727,variab,variable,118727,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"n_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9f898d3630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0924 03:47:37.677164 140325876573952 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz; W0924 03:47:37.681965 140325876573952 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions for updating:; If using Keras pass *_constraint arguments to layers.; W0924 03:47:37.690693 140325876573952 deprecation.py:323] From /tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0924 03:47:37.814187 140325876573952 deprecation.py:323] From /tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:7317,layers,layers,7317,,https://github.com/google/deepvariant/issues/358,1,['layers'],['layers']
Modifiability,nch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.965248 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966092 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966933 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.967533 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.968360 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.969041 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.969827 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.970808 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.971487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.972088 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.972737 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.973396 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:91111,variab,variable,91111,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432); ```; File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants; ie_estimator = OpenVINOEstimator(; File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__; freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb); File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run; ```; python -c 'from tensorflow.python.tools import optimize_for_inference_lib'; ```. The real issue is openvino is not installed ; ```; python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; ModuleNotFoundError: No module named 'openvino'; ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/541:1687,layers,layers,1687,,https://github.com/google/deepvariant/issues/541,1,['layers'],['layers']
Modifiability,"nda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_listener= //deepvariant:make_examples //deepvariant:call_variants //deepvariant:postprocess_variants || { \; echo ""Bazel build failed""; \; exit 1; }. # Final image; FROM base AS final. # Set environment variables; ENV VERSION=1.6.0; ENV PYTHON_VERSION=3.8; ENV PATH=""/opt/miniconda/bin:${PATH}"". # Install Python packages; RUN pip install --upgrade pip setuptools wheel --timeout=120 && \; pip install jaxlib jax --timeout=120 --extra-index-url https://storage.googleapis.com/jax-releases/jax_releases.html. # Copy DeepVariant binaries from the builder stage; COPY --from=builder /opt/deepvariant /opt/deepvariant; WORKDIR /opt/deepvariant. # Ensure executable scripts are correctly set up; RUN BASH_HEADER='#!/bin/bash' && \; for script in make_examples call_variants call_variants_slim postprocess_variants vcf_stats_report show_examples runtime_by_region_vis multisample_make_examples labeled_examples_to_vcf make_examples_somatic train run_deepvariant run_deepsomatic; do \; printf ""%s\n%s\n"" ""${BASH_HEADER}"" ""python3 /opt/deepvariant/bin/${script}.zip \""$@\"""" > /opt/deepvariant/bin/${script} && \; chmod +x /opt/deepvariant/bin/${script}; \; done. # Copy licenses and other necessary files; # Ensure",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:2295,variab,variables,2295,,https://github.com/google/deepvariant/issues/871,1,['variab'],['variables']
Modifiability,"ne 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:3625,config,configuring,3625,,https://github.com/google/deepvariant/issues/469,1,['config'],['configuring']
Modifiability,"ng package metadata (repodata.json): ...working... done; 6.190 Solving environment: ...working... failed; 6.260; 6.260 PackagesNotFoundError: The following packages are not available from current channels:; 6.260; 6.260 - bioconda::samtools==1.15; 6.260 - bioconda::bcftools==1.15; 6.260; ```. I resolved this error by removing the version numbers. i.e., removed the `==1.15` from both the lines. #### Error in the build-prerunreq.sh script. Once, I cross the previous error, I get this error -. ```; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ub",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:1554,config,config,1554,,https://github.com/google/deepvariant/issues/902,1,['config'],['config']
Modifiability,"nibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 20",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1866,config,config,1866,,https://github.com/google/deepvariant/issues/876,1,['config'],['config']
Modifiability,"ning""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:2624,config,config,2624,,https://github.com/google/deepvariant/issues/797,1,['config'],['config']
Modifiability,"nsor_shape=[100, 221, 7]; /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1083: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:678: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:2441: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:118: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1638: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I1214 06:10:37.470187 140363019278144 saver.py:1399] Restoring parameters from /opt/models/wes/model.ckpt; WARNING:tensorflow:From /tmp/Bazel.runfiles_oh47rpgj/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py:75: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.compat.v1.graph_util.convert_variables_to_constants`; W1214 06:10:41.056998 140363019278144 deprecation.py:341] From /tmp/Bazel.runfiles_oh47rpgj/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py:75: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and wi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597:8882,layers,layers,8882,,https://github.com/google/deepvariant/issues/597,2,['layers'],['layers']
Modifiability,"nt=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:2988,config,config,2988,,https://github.com/google/deepvariant/issues/797,1,['config'],['config']
Modifiability,"ny thanks in advance. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.5.0; - Tensorflow 2.11.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HIFI reads. **Steps to reproduce:**; ```; rule deepvariant_make_examples:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \; --mode calling \; --ref {input.reference} \; --reads {input.bam} \; --examples {params.examples} \; --gvcf {params.gvcf} \; --task {params.shard}) > {log} 2>&1; """""". ```; Error:. ```; 2023",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:1416,config,config,1416,,https://github.com/google/deepvariant/issues/677,1,['config'],['config']
Modifiability,obs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:4307,config,config,4307,,https://github.com/google/deepvariant/issues/608,1,['config'],['config']
Modifiability,"on to packages.cloud.google.com:80 (2404:6800:4008:801::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:801::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages w",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:10924,config,configured,10924,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"on to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'goog",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:7347,config,configured,7347,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,onv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:119213,variab,variable,119213,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,onv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.028651 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.029017 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029366 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029716 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.030065 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:108357,variab,variable,108357,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,or 'build' from /opt/tensorflow/.tf_configure.bazelrc:; #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:; #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:3879,config,config,3879,,https://github.com/google/deepvariant/issues/608,1,['config'],['config']
Modifiability,"orflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548, in __call__; outputs = super(Layer, self).__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__; outputs = call_fn(cast_inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 234, in wrapper; return converted_call(f, options, args, kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 439, in converted_call; return _call_unconverted(f, args, kwargs, options); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 330, in _call_unconverted; return f(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 197, in call; outputs = self._convolution_op(inputs, self.kernel); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 1134, in __call__; return self.conv_op(inp, filter); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 639, in __call__; return self.call(inp, filter); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 238, in __call__; name=self.name); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 2010, in conv2d; name=name); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py"", line 1071, in conv2d; data_format=data_format, dilations=dilations, name=name); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py"", line 793, in _apply_op_helper; op_def=op_def); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:20182,layers,layers,20182,,https://github.com/google/deepvariant/issues/358,1,['layers'],['layers']
Modifiability,"ow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:; #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:; #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:3718,config,config,3718,,https://github.com/google/deepvariant/issues/608,1,['config'],['config']
Modifiability,pd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to pack,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:5354,config,configured,5354,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,prev_var_name: Unchanged; I0415 07:34:37.971487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.972088 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.972737 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.973396 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.974112 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.974828 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.975663 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.976491 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.977374 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.978363 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.979172 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.979801 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:92490,variab,variable,92490,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:37.972737 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.973396 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.974112 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.974828 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.975663 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.976491 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.977374 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.978363 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.979172 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.979801 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.980334 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.980947 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:92833,variab,variable,92833,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:37.975663 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.976491 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.977374 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.978363 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.979172 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.979801 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.980334 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.980947 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.981682 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.982532 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.983606 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.984400 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:93512,variab,variable,93512,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:37.982532 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.983606 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.984400 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.985276 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.986526 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.987484 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.988141 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.988661 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989614 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.990134 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.990647 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:95041,variab,variable,95041,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:37.983606 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.984400 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.985276 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.986526 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.987484 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.988141 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.988661 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989614 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.990134 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.990647 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.991297 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:95209,variab,variable,95209,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:37.990134 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.990647 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.991297 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.991910 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.992630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:37.993726 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.994658 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.995343 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.996014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.997136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.997966 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.998590 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:96756,variab,variable,96756,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:37.997136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.997966 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.998590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.999377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.000032 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.000669 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.001136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.001605 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.002159 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.002743 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.003241 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.003774 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixe,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:98303,variab,variable,98303,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:37.997966 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.998590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.999377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.000032 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.000669 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.001136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.001605 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.002159 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.002743 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.003241 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.003774 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.004208 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixe,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:98471,variab,variable,98471,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:38.007198 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.008188 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.008778 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.009377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.010077 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.010946 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.011956 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013191 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013811 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.014904 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.015494 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:101533,variab,variable,101533,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:38.009377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.010077 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.010946 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.011956 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013191 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013811 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.014904 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.015494 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.016149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.016633 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.017230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.017909 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:102219,variab,variable,102219,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:38.010077 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.010946 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.011956 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013191 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013811 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.014904 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.015494 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.016149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.016633 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.017230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.017909 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018465 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:102387,variab,variable,102387,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:38.013191 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013811 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.014904 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.015494 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.016149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.016633 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.017230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.017909 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018465 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018997 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.019490 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.020006 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:102898,variab,variable,102898,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:38.027156 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027498 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.027940 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.028300 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.028651 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.029017 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029366 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029716 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.030065 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:107696,variab,variable,107696,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_n,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:117741,variab,variable,117741,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:120592,variab,variable,120592,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"ps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3659263518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0424 15:59:50.451262 139872277903104 call_variants.py:384] Writing calls to /tmp/tmp9_28zx5u/call_variants_output.tfrecord.gz; W0424 15:59:50.467876 139872277903104 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions for updating:; If using Keras pass *_constraint arguments to layers.; I0424 15:59:50.501495 139872277903104 data_providers.py:369] self.input_read_threads=8; W0424 15:59:50.501965 139872277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0424 15:59:50.681574 139872277903104 data_providers.py:376] self.input_map_threads=48; W0424 15:59:50.681832 139872277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated a",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304:3042,layers,layers,3042,,https://github.com/google/deepvariant/issues/304,1,['layers'],['layers']
Modifiability,"ps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9f0f102630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0703 17:18:45.714398 140322304501504 call_variants.py:384] Writing calls to /tmp/tmp7l6e69ft/call_variants_output.tfrecord.gz; W0703 17:18:45.719665 140322304501504 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions for updating:; If using Keras pass *_constraint arguments to layers.; I0703 17:18:45.730541 140322304501504 data_providers.py:369] self.input_read_threads=8; W0703 17:18:45.730644 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0703 17:18:45.810746 140322304501504 data_providers.py:376] self.input_map_threads=48; W0703 17:18:45.810852 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated a",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321:3907,layers,layers,3907,,https://github.com/google/deepvariant/issues/321,1,['layers'],['layers']
Modifiability,"put examples: [1, 2, 3, 4, 5, 6, 19].; 2022-12-14 06:10:31.060396: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-12-14 06:10:31.101084: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1214 06:10:31.288279 140363019278144 openvino_estimator.py:55] Processing ckpt=/opt/models/wes/model.ckpt, tensor_shape=[100, 221, 7]; /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1083: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:678: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:2441: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:118: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1638: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I1214 06:10:37.470",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597:8178,layers,layers,8178,,https://github.com/google/deepvariant/issues/597,2,['layers'],['layers']
Modifiability,"py:417] Warm-starting from: ('/home/models/model.ckpt',); I0415 07:34:37.964245 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.965248 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966092 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966933 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.967533 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.968360 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.969041 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.969827 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.970808 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.971487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.972088 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.972737 140368878327552 warm_starting_util.py:466] Warm-starting varia",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:90943,variab,variable,90943,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat; return self._build_call_outputs(self._inference_function.call(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call; outputs = execute.execute(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute; tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,; tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]; Registered devices: [CPU]; Registered kernels:; <no registered kernels>. [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/com_google_deepvariant/deepvariant/train.py"", line 532, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/com_google_deepvariant/deepvariant/train.py"", line 518, in main; train(FLAGS.config); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/com_google_deepvariant/deepvariant/train.py"", line 109, in train; tf.tpu.experimental.initialize_tpu_system(resolver); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py"", line 113, in initialize_tpu_system; raise errors.NotFoundError(; tensorflow.python.framework.errors_impl.NotFoundError: TPUs not found",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:3886,Config,ConfigureDistributedTPU,3886,,https://github.com/google/deepvariant/issues/841,1,['Config'],['ConfigureDistributedTPU']
Modifiability,"r this is to be expected, or whether there might be something wrong with my training setup, or perhaps the examples. . The test example set I am using to tune the hyperparams looks like this:. ```; # Generated by shuffle_tfrecords_beam.py; # class2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840515, train/f1_homalt=0.964401364326477, train/f1_homref=0.902255654335022, train/f1_macro=0.; 8698380589485168, train/f1_micro=0.91796875, train/f1_weighted=0.9241795539855957, train/false_negatives=34.0, train/false_positives=14.0, train/learning_ra",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1495,config,config,1495,,https://github.com/google/deepvariant/issues/904,1,['config'],['config']
Modifiability,r_name: Unchanged; I0415 07:34:37.970808 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.971487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.972088 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.972737 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.973396 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.974112 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.974828 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.975663 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.976491 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.977374 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.978363 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.979172 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixe,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:92322,variab,variable,92322,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:37.987484 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.988141 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.988661 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989614 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.990134 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.990647 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.991297 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.991910 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.992630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:37.993726 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.994658 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:95902,variab,variable,95902,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:37.988141 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.988661 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989614 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.990134 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.990647 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.991297 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.991910 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.992630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:37.993726 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.994658 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.995343 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:96070,variab,variable,96070,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:37.989614 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.990134 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.990647 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.991297 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.991910 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.992630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:37.993726 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.994658 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.995343 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.996014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.997136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.997966 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:96588,variab,variable,96588,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:37.992630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:37.993726 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.994658 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.995343 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.996014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.997136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.997966 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.998590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.999377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.000032 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.000669 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.001136 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:97449,variab,variable,97449,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:37.995343 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.996014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.997136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.997966 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.998590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.999377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.000032 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.000669 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.001136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.001605 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.002159 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.002743 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixe,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:97960,variab,variable,97960,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.006792 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007198 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.008188 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.008778 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.009377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.010077 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.010946 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.011956 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013191 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013811 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.014904 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:101365,variab,variable,101365,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.011956 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013191 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013811 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.014904 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.015494 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.016149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.016633 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.017230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.017909 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018465 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018997 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.019490 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:102730,variab,variable,102730,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.018465 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018997 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.019490 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.020006 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.020472 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.020929 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.021404 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.021804 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.022231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.022672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.023087 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:104277,variab,variable,104277,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.022231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.022672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.023087 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023875 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.024280 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.024672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025057 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025403 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.026254 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.026756 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:105631,variab,variable,105631,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.026254 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.026756 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027156 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027498 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.027940 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.028300 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.028651 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.029017 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029366 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029716 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.030065 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:107360,variab,variable,107360,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.026756 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027156 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027498 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.027940 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.028300 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.028651 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.029017 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029366 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.029716 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.030065 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:107528,variab,variable,107528,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:109218,variab,variable,109218,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:109736,variab,variable,109736,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:114855,variab,variable,114855,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:115023,variab,variable,115023,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:116544,variab,variable,116544,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights;,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:117405,variab,variable,117405,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNor,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:117573,variab,variable,117573,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:119738,variab,variable,119738,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:119906,variab,variable,119906,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:120249,variab,variable,120249,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:120935,variab,variable,120935,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,r_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:121453,variab,variable,121453,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"re governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-03 17:21:57.549571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-07-03 17:21:57.644332: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-07-03 17:21:58.247052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-07-03 17:21:58.247080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; Runs all 3 steps to go from input DNA reads to output VCF/gVCF files.; (... then -- the list of all options follows). If run on a proper BAM file with all options provided, all TF-TRT warning messages are periodically repeated as well as ; CUDA Version 11.3.1; 2024-07-02 22:47:07.493311: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: C",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:2035,variab,variable,2035,,https://github.com/google/deepvariant/issues/844,1,['variab'],['variable']
Modifiability,"ream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1; 2020-07-03 17:18:45.680312: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; 2020-07-03 17:18:45.680339: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 7c1895dbad7c; 2020-07-03 17:18:45.680346: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 7c1895dbad7c; 2020-07-03 17:18:45.680397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.129.0; 2020-07-03 17:18:45.680416: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.113.0; 2020-07-03 17:18:45.680422: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 418.113.0 does not match DSO version 410.129.0 -- cannot find working devices in this configuration; I0703 17:18:45.713418 140322304501504 modeling.py:563] Initializing model with random parameters; W0703 17:18:45.713950 140322304501504 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp7amyb_ws; I0703 17:18:45.714213 140322304501504 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp7amyb_ws', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9f0f102630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321:2305,config,configuration,2305,,https://github.com/google/deepvariant/issues/321,1,['config'],['configuration']
Modifiability,"redict; features, None, ModeKeys.PREDICT, self.config); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1148, in _call_model_fn; model_fn_results = self._model_fn(features=features, **kwargs); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1191, in convolution2d; conv_dims=2); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1089, in convolution; outputs = layer.apply(inputs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:18396,layers,layers,18396,,https://github.com/google/deepvariant/issues/358,1,['layers'],['layers']
Modifiability,ref variable path is not being found in deep variant run,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/181:4,variab,variable,4,,https://github.com/google/deepvariant/issues/181,1,['variab'],['variable']
Modifiability,"rently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```; sudo sh run_deepvariant.sh; I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0; 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA; 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters; W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ; I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}; I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz; I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn.; I0331 18:31:24.905831 1405497648",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:1920,config,config,1920,,https://github.com/google/deepvariant/issues/166,1,['config'],['config']
Modifiability,rev_var_name: Unchanged; I0415 07:34:37.978363 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.979172 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.979801 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.980334 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.980947 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.981682 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.982532 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.983606 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.984400 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.985276 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.986526 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.987484 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mix,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:94030,variab,variable,94030,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,rev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inceptio,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:110422,variab,variable,110422,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"rflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop; name=eval_name); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate; name=name); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval; hooks.extend(self._convert_eval_steps_to_hooks(steps)); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks; raise ValueError('Must specify steps > 0, given: {}'.format(steps)); ValueError: Must specify steps > 0, given: 0. real	0m58.116s; user	0m1.590s; sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant; ADD ./bin /home/bin; ADD ./models /home/models; ADD ./oss_clif.ubuntu-16.latest.tgz /; WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh; RUN apt-get -y install python2.7; RUN apt-get -y install python-dev python-pip; RUN apt-get -y install parallel; RUN python2 -m pip install --upgrade --force-reinstall pip; RUN python2 -m pip install apache-beam; RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:5134,extend,extend,5134,,https://github.com/google/deepvariant/issues/172,1,['extend'],['extend']
Modifiability,"riod_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 549, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1012, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1017, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 706, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 477, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1752, in restore; {self.saver_def.filename_tensor_name: save_path}); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run; run_metadata_ptr); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run; run_metadata); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/117:4363,config,config,4363,,https://github.com/google/deepvariant/issues/117,2,['config'],['config']
Modifiability,"riod_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s; user	0m9.233s; sys	0m4.817s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise Ca",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/273:2089,config,config,2089,,https://github.com/google/deepvariant/issues/273,2,['config'],['config']
Modifiability,"robably mainly seems to originate in GLnexus merging, where uncalled alleles (e.g., for ""GT:DP:AD:GQ:PL:RNC"" ""./.:5:5,0:0:14,0,104:II"" where I = gVCF input site is non-called) has a non-zero likelihood of being heterozygous according the PL tag, which is later imputed to a ""0/1"" genotype. But this also occurs for samples called as reference (0/0) but with non-zero PL tags for 0/1. For example, the third sample gains a heterozygous genotype at site `Y:6899016` after imputation, and has the gvcf block; ```; Y 6898989 . C <*> 0 . END=6899117 GT:GQ:MIN_DP:MED_DP:PL 0/0:1:0:0:0,3,29; ```. leads to the GLnexus merged GT; ```; Y 6899016 Y_6899016_T_C T C 47 . 0/0:0:0,0:1:0,3,29:..; ```; and then the imputed; ```; Y 6899016 Y_6899016_T_C T C 47 . 0/1:1:0,1,0; ```. ### variant calling with DeepVariant. Running with singularity image v1.6.1.; This was done with the ""WGS"" model, using `--channels ""insert_size"" --include_med_dp --gvcf <gvcf>` for _make\_examples_ and `--haploid_contigs ""X,Y"" --par_regions_bed <PAR.bed>` for _postprocess_. ### merging with GLnexus. Running with singularity image v1.4.1-0-g68e25e5.; This uses a modified DeepVariant config, where there is no genotype revision (`revise_genotypes: false`). ```; /usr/local/bin/glnexus_cli \; --dir $TMPDIR/GLnexus.DB \; --config <deepvariant_preset_with_revise_genotypes_false> \; --threads 4 \; --mem-gbytes 20 \; *.g.vcf |\; bcftools view - |\; bgzip -@ 8 -c > Yhap.Unrevised.vcf.gz; ```. ### imputing with beagle4. This is where the ""./."" genotypes with non-zero het GL turn into ""0/1"". ```; java -jar -Xss25m -Xmx50G beagle.27Jan18.7e1.jar ne=100 gl=Yhap.Unrevised.vcf.gz out=Yhap.beagle4; ```. The unrevised (glnexus output, with no hets) and imputed (beagle output, with hets) variants are here, along with the gvcfs. I can provide the alignments, references, etc. if you need as these samples are already public.; [Y_haploid_vcf.tar.gz](https://github.com/google/deepvariant/files/15077490/Y_haploid_vcf.tar.gz). Best,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/811:1401,config,config,1401,,https://github.com/google/deepvariant/issues/811,2,['config'],['config']
Modifiability,"rt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:; #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:; #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_gu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:3266,config,config,3266,,https://github.com/google/deepvariant/issues/608,1,['config'],['config']
Modifiability,"ry creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04; - DeepVariant version: 1.4.0; - Building docker image locally. **Steps to reproduce:**; - Command: `docker build .` in source directory (no modifications); - Error trace: ; ; ```; #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting; #16 1484.7 Extracting Bazel installation...; #16 1487.8 Starting local Bazel server and connecting to it...; #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc); #16 1489.8 (21:51:01) INFO: Options provided by the client:; #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:1051,Inherit,Inherited,1051,,https://github.com/google/deepvariant/issues/608,1,['Inherit'],['Inherited']
Modifiability,"s. The programs are running well, I have confident regions and truth variants defined, and am currently tuning hyperparameters to optimise the training. . However . . . . I notice when tracking the model eval stats (specifically f1, precision, recall), that the hom_ref classifications are much less reliable than hom_alt and het classes. My question is whether this is to be expected, or whether there might be something wrong with my training setup, or perhaps the examples. . The test example set I am using to tune the hyperparams looks like this:. ```; # Generated by shuffle_tfrecords_beam.py; # class2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1098,config,config,1098,,https://github.com/google/deepvariant/issues/904,1,['config'],['config']
Modifiability,"s/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model; return self._train_model_default(input_fn, hooks, saving_listeners); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default; return self._train_with_estimator_spec(estimator_spec, worker_hooks,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec; tf.compat.v1.train.warm_start(*self._warm_start_settings); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start; checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint; distribution_strategy_context.get_replica_context().merge_call(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call; return self._merge_call(merge_fn, args, kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call; return merge_fn(self._strategy, *args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper; return func(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>; init_from_checkpoint_fn = lambda _: _init_from_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint; raise ValueError(; ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader.; ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/500:4367,variab,variable,4367,,https://github.com/google/deepvariant/issues/500,1,['variab'],['variable']
Modifiability,"s_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; > tf.compat.v1.app.run(); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run; > _run_main(main, args); > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main; > sys.exit(main(argv)); > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; > call_variants(; > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants; > init_op = tf.group(tf.compat.v1.global_variables_initializer(),; > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer; > return variables_initializer(global_variables()); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables; > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection; > return get_default_graph().get_collection(key, scope); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph; > return _default_graph_stack.get_default(); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default; > self._global_default_graph = Graph(); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__; > self._scoped_c_graph = c_api_util.ScopedTFGraph(); > File ""/usr/local/lib/python3.8/dist-packages",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:6207,variab,variables,6207,,https://github.com/google/deepvariant/issues/602,1,['variab'],['variables']
Modifiability,"s_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1191, in convolution2d; conv_dims=2); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1089, in convolution; outputs = layer.apply(inputs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548, in __call__; outputs = super(Layer, self).__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__; outputs = call_fn(cast_inputs, *args, **kwargs); File ""usr/local/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:18653,layers,layers,18653,,https://github.com/google/deepvariant/issues/358,2,['layers'],['layers']
Modifiability,"service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0826 20:44:28.953605 47737984214848 call_variants.py:446] Writing calls to /paedyl01/disk1/yangyxt/test_tmp/call_variants_output.tfrecord.gz; INFO:tensorflow:Calling model_fn.; I0826 20:44:29.309295 47737984214848 estimator.py:1173] Calling model_fn.; /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1083: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:678: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:2441: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:118: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1638: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); INFO:tensorflow:Done calling model_fn.; I0826 20:44:33.173107 47737984214848 estimator.py:1175] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0826 20:44:34.048544 47737984214848 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0826 20:44:34.048974 47",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:5256,layers,layers,5256,,https://github.com/google/deepvariant/issues/564,2,['layers'],['layers']
Modifiability,"ses/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_listener= //deepvariant:make_examples //deepvariant:call_variants //deepvariant:postprocess_variants || { \; echo ""Bazel build failed""; \; exit 1; }. # Final image; FROM base AS final. # Set environment variables; ENV VERSION=1.6.0; ENV PYTHON_VERSION=3.8; ENV PATH=""/opt/miniconda/bin:${PATH}"". # Install Python packages; RUN pip install --upgrade pip setuptools wheel --timeout=120 && \; pip install jaxlib jax --timeout=120 --extra-index-url https://storage.googleapis.com/jax-releases/jax_releases.html. # Copy DeepVariant binaries from the builder stage; COPY --from=builder /opt/deepvariant /opt/deepvariant; WORKDIR /opt/deepvariant. # Ensure executable scripts are correctly set up; RUN BASH_HEADER='#!/bin/bash' && \; for script in make_examples call_variants call_variants_slim postprocess_variants vcf_stats_report show_examples runtime_by_region_vis multisample_make_examples labeled_examples_to_vcf make_examples_somatic train run_deepvariant run_deepsomatic; do \; printf ""%s\n%s\n"" ""${BASH_H",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:2015,config,configurations,2015,,https://github.com/google/deepvariant/issues/871,1,['config'],['configurations']
Modifiability,"stats (specifically f1, precision, recall), that the hom_ref classifications are much less reliable than hom_alt and het classes. My question is whether this is to be expected, or whether there might be something wrong with my training setup, or perhaps the examples. . The test example set I am using to tune the hyperparams looks like this:. ```; # Generated by shuffle_tfrecords_beam.py; # class2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840515, train/f1_homalt=0.964401364326477, train/f1_homref=0.902255654335022, train/f1_macro=0.; 86983",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1307,config,config,1307,,https://github.com/google/deepvariant/issues/904,1,['config'],['config']
Modifiability,t cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/socksipy-branch/socks.pyc'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/rpm/mapping/command_mapping.yaml'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/rpm/mapping/component_mapping.yaml'; specified in the package manifest cannot be found.; ```; (there are many similar CondaVerificationErrors before this); Conda info:; ```; active environment : longshot; active env location : /home/pedge/anaconda3/envs/longshot; shell level : 2; user config file : /home/pedge/.condarc; populated config files : /home/pedge/.condarc; conda version : 4.6.9; conda-build version : 3.17.6; python version : 3.7.1.final.0; base environment : /home/pedge/anaconda3 (writable); channel URLs : https://conda.anaconda.org/conda-forge/linux-64; https://conda.anaconda.org/conda-forge/noarch; https://conda.anaconda.org/bioconda/linux-64; https://conda.anaconda.org/bioconda/noarch; https://repo.anaconda.com/pkgs/main/linux-64; https://repo.anaconda.com/pkgs/main/noarch; https://repo.anaconda.com/pkgs/free/linux-64; https://repo.anaconda.com/pkgs/free/noarch; https://repo.anaconda.com/pkgs/r/linux-64; https://repo.anaconda.com/pkgs/r/noarch; https://conda.anaconda.org/OpenMDAO/linux-64; https://conda.anaconda.org/OpenMDAO/noarch; package cache : /home/pedge/anaconda3/pkgs; /home/pedge/.conda/pkgs; envs directories : /home/pedge/anaconda3/envs; /home/pedge/.conda/env,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177:4445,config,config,4445,,https://github.com/google/deepvariant/issues/177,1,['config'],['config']
Modifiability,"t holder nor the names of its; # contributors may be used to endorse or promote products derived from this; # software without specific prior written permission.; #; # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""; # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE; # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE; # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE; # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR; # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF; # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS; # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN; # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE); # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE; # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for; # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This; # will skip the installation of TensorFlow.; export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0; export TF_ENABLE_XLA=1; export TF_NEED_CUDA=1; export TF_NEED_GCP=1; export TF_NEED_GDR=0; export TF_NEED_HDFS=0; export TF_NEED_JEMALLOC=0; export TF_NEED_MKL=1; export TF_NEED_MPI=0; export TF_NEED_OPENCL=0; export TF_NEED_OPENCL_SYCL=0; export TF_NEED_S3=1; export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1; export TF_CUDA_VERSION=""10.0""; export CUDA_TOOLKIT_PATH=""/usr/local/cuda""; export TF_CUDNN_VERSION=""7""; export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant.; DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that; # `bazel` will find the latest version of bazel installed in the user's home; # directory. This is set in set",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:2600,config,config,2600,,https://github.com/google/deepvariant/issues/145,1,['config'],['config']
Modifiability,"t.tfrecord.gz"" --examples ""$PWD/PMC01-01_AB082422_S5_L005_R1_001.tfrecord.gz"" --checkpoint ${PWD}/models/model.ckpt. But for some reason , I am getting below error. I1108 10:29:03.150824 140295000123136 call_variants.py:283] Set KMP_BLOCKTIME to 0; 2018-11-08 10:29:03.161879: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 FMA; 2018-11-08 10:29:03.168258: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1108 10:29:03.176211 140295000123136 modeling.py:318] Initializing model with random parameters; W1108 10:29:03.177896 140295000123136 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpkXijQm; I1108 10:29:03.178158 140295000123136 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_session_config': None, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9889d6e750>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmpkXijQm', '_train_distribute': None, '_save_summary_steps': 100}; I1108 10:29:03.178364 140295000123136 call_variants.py:341] Writing calls to /gpfs/projects/bioinfo/najeeb/playGround/deepVariants/PMC01-01_AB082422_S5_L005_R1_001_output.tfrecord.gz; I1108 10:29:03.210211 140295000123136 tf_logging.py:115] Calling model_fn.; I1108 10:29:05.463484 140295000123136 tf_logging.py:115] Done calling model_fn.; I1108 10:29:06.500680 140295000123136 tf_logging.py:115] Graph was finalized.; I1108 10:29:06.501178 140295000123136 tf_loggin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/117:1286,config,config,1286,,https://github.com/google/deepvariant/issues/117,1,['config'],['config']
Modifiability,ta; prev_var_name: Unchanged; I0415 07:34:37.972088 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.972737 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.973396 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.974112 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.974828 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.975663 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.976491 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.977374 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.978363 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.979172 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.979801 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.980334 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:92658,variab,variable,92658,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ta; prev_var_name: Unchanged; I0415 07:34:37.976491 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.977374 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.978363 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.979172 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.979801 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.980334 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.980947 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.981682 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.982532 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.983606 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.984400 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.985276 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:93680,variab,variable,93680,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ta; prev_var_name: Unchanged; I0415 07:34:38.008778 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.009377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.010077 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.010946 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.011956 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013191 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013811 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.014904 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.015494 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.016149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.016633 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.017230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:102044,variab,variable,102044,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ta; prev_var_name: Unchanged; I0415 07:34:38.013811 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.014904 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.015494 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.016149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.016633 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.017230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.017909 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018465 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018997 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.019490 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.020006 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.020472 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:103066,variab,variable,103066,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:113994,variab,variable,113994,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main; make_examples_core.make_examples_runner(options); File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner; runtimes) = region_processor.process(region); File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process; reads = self.region_reads(; File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads; reads.extend(sam_reader.query(region)); File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7; I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader; I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs; 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/559:11357,extend,extend,11357,,https://github.com/google/deepvariant/issues/559,1,['extend'],['extend']
Modifiability,"tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main; make_examples_core.make_examples_runner(options); File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner; runtimes) = region_processor.process(region); File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process; reads = self.region_reads(; File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads; reads.extend(sam_reader.query(region)); File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7; I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader; 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7; I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/559:8636,extend,extend,8636,,https://github.com/google/deepvariant/issues/559,1,['extend'],['extend']
Modifiability,"tart to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant.; I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right?. EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/716:1871,adapt,adapted,1871,,https://github.com/google/deepvariant/issues/716,1,['adapt'],['adapted']
Modifiability,"them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled; WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7; W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7; INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000; I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c; I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz; INFO:tensorflow:Calling model_fn.; I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn.; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn.; 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; INFO:tensorflow:Graph was finalized.; I0715 14:07:18.197918 47821886322496 monitored_session.py",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:2999,config,config,2999,,https://github.com/google/deepvariant/issues/679,1,['config'],['config']
Modifiability,"tion... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:42:05 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to pack",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:8931,config,configured,8931,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"tion=gpu # standard node(s); #SBATCH --ntasks=48; #SBATCH --job-name=""deepvariant_training""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:2538,config,config,2538,,https://github.com/google/deepvariant/issues/797,1,['config'],['config']
Modifiability,"tions: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0; W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0; INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.74.226:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}; I0524 21:18:26.620151 140032543119168 estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:2356,config,config,2356,,https://github.com/google/deepvariant/issues/537,1,['config'],['config']
Modifiability,"tlas ; #SBATCH --time=5-48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu-a100 # standard node(s); #SBATCH --ntasks=1; #SBATCH --job-name=""deepvariant_modeltraining""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. # LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export APPTAINER_CACHEDIR=$TMPDIR ; export APPTAINER_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/training_set_channelsize_F1F1shuffle.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/validation_set_channelsize_F1F2shuffled.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.02 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/modeltrainout/fullindividualmodel"" \; --strategy=mirrored \; --config.batch_size=32`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/840:2765,config,config,2765,,https://github.com/google/deepvariant/issues/840,8,['config'],['config']
Modifiability,"tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 622, in predict; features, None, ModeKeys.PREDICT, self.config); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1148, in _call_model_fn; model_fn_results = self._model_fn(features=features, **kwargs); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, depth(32), [3, 3], stride=",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:17425,config,config,17425,,https://github.com/google/deepvariant/issues/358,1,['config'],['config']
Modifiability,"tpu_system; output = _tpu_init_fn(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 134, in __call__; return concrete_function._call_flat(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat; return self._build_call_outputs(self._inference_function.call(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call; outputs = execute.execute(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute; tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,; tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]; Registered devices: [CPU]; Registered kernels:; <no registered kernels>. [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/com_google_deepvariant/deepvariant/train.py"", line 532, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/com_google_deepvariant/deepvariant/train.py"", line 518, in main; train(FLAGS.config); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/com_google_deepvariant/deepvariant/train.py"", line 109, in train; tf.tpu.experimental.initialize_tpu_system(reso",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:3546,Config,ConfigureDistributedTPU,3546,,https://github.com/google/deepvariant/issues/841,2,['Config'],['ConfigureDistributedTPU']
Modifiability,"tput_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/modeltrainout/modeltestout/2fullindividualmodeltest/${sample}.vcf.gz"". Here are the contents of the checkpoints folder for this training: . > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jun 29 01:06 ..; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 1 22:49 .; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 ckpt-14902; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-7451.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-7451.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-14902.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-14902.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 266 Aug 6 22:51 checkpoint. and finally, here are the contents of ckpt-14902: . > total 7.6M; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 1 22:49 ..; > drwxr-s--- 2 haley.arnold proj-pbarc 4.0K Jul 1 22:49 variables; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 .; > -rw-r----- 1 haley.arnold proj-pbarc 6.9M Aug 6 22:51 saved_model.pb; > -rw-r----- 1 haley.arnold proj-pbarc 677K Aug 6 22:51 keras_metadata.pb; > -rw-r----- 1 haley.arnold proj-pbarc 55 Aug 6 22:51 fingerprint.pb; > -rw-r----- 1 haley.arnold proj-pbarc 80 Aug 6 22:51 example_info.json. Here is the error log file: . > 2024-08-09 20:05:25.101938: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; > I0809 20:05:40.093672 139993880950592 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmp4wzl_5p3; > Traceback (most recent call last):; > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>; app.run",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/866:1663,variab,variables,1663,,https://github.com/google/deepvariant/issues/866,1,['variab'],['variables']
Modifiability,ts; prev_var_name: Unchanged; I0415 07:34:37.966092 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.966933 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.967533 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.968360 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.969041 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.969827 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.970808 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.971487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.972088 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.972737 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.973396 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.974112 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:91286,variab,variable,91286,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ts; prev_var_name: Unchanged; I0415 07:34:37.974828 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.975663 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.976491 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.977374 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.978363 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.979172 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.979801 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.980334 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.980947 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.981682 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.982532 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.983606 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:93337,variab,variable,93337,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ts; prev_var_name: Unchanged; I0415 07:34:37.979172 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.979801 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.980334 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.980947 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.981682 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.982532 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.983606 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.984400 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.985276 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.986526 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.987484 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.988141 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:94187,variab,variable,94187,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ts; prev_var_name: Unchanged; I0415 07:34:38.001605 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.002159 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.002743 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.003241 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.003774 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.004208 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.004652 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.005083 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.005480 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.005927 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006367 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006792 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:99486,variab,variable,99486,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ts; prev_var_name: Unchanged; I0415 07:34:38.008188 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.008778 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.009377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.010077 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.010946 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.011956 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013191 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013811 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.014904 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.015494 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.016149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.016633 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:101869,variab,variable,101869,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ts; prev_var_name: Unchanged; I0415 07:34:38.030065 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:109050,variab,variable,109050,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,ts; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:111272,variab,variable,111272,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"u_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:; 2020-09-24 03:47:37.577462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165] 0 ; 2020-09-24 03:47:37.577470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0: N ; 2020-09-24 03:47:37.578993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 **with 6199 MB memory**) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:21:00.0, compute capability: 7.5); W0924 03:47:37.676500 140325876573952 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp3gvrq0ei; I0924 03:47:37.676881 140325876573952 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp3gvrq0ei', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9f898d3630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0924 03:47:37.677164 140325876573952 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz; W0924 03:47:37.681965 140325876573952 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:5992,config,config,5992,,https://github.com/google/deepvariant/issues/358,1,['config'],['config']
Modifiability,"ud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:801::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:801::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is alr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:10741,config,configured,10741,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"ud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:7164,config,configured,7164,,https://github.com/google/deepvariant/issues/89,1,['config'],['configured']
Modifiability,"uf package. I tried protobuf version 3.20.3 and 4.21.9, but the error message is the same. In order to run DeepVariant successfully, what additional packages (version) should I install besides cloning the singularity image?. **Setup**; - Operating system: ; - DeepVariant version: 1.4.0; - Installation method: singularity; - Type of data: quick start test; ; **Steps to reproduce:**; ; ```; SINGULARITY_TMPDIR=/scratch/midway3/weilu1/tmp SINGULARITY_CACHEDIR=/scratch/midway3/weilu1/cache singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_1.4.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1. INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 7, in <module>; from google.protobuf import descriptor as _descriptor; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py"", line 40, in <module>; from google.protobuf.internal",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580:1372,sandbox,sandbox,1372,,https://github.com/google/deepvariant/issues/580,1,['sandbox'],['sandbox']
Modifiability,"uild DeepVariant and the; # tf-nightly wheel.; export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not; # the same as the python version of TensorFlow we use, but should be similar or; # we risk having version incompatibilities between our C++ code and the Python; # code we use at runtime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing; # value in the environment (allowing command line control of the build),; # defaulting to 0 (CPU only build).; export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file; # compiled with MKL support for corei7 or better chipsets, which; # significantly speeds up execution when running on modern CPUs. The default; # TensorFlow wheel files don't contain these instructions (and thereby run on a; # broader set of CPUs). Using this optimized wheel reduces the runtime of; # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud; # Platform) optimized wheel because all GCP instances have at least Sandy Bridge; # or better chipsets, so this wheel should run anywhere on GCP.; export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl""; export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow""; export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries.; # If you already have CUDA installed, such as on a properly provisioned; # D",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:5348,variab,variable,5348,,https://github.com/google/deepvariant/issues/145,1,['variab'],['variable']
Modifiability,"used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2158,config,config,2158,,https://github.com/google/deepvariant/issues/876,1,['config'],['config']
Modifiability,v_var_name: Unchanged; I0415 07:34:37.966933 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.967533 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.968360 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.969041 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.969827 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.970808 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.971487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.972088 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.972737 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.973396 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.974112 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.974828 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:91461,variab,variable,91461,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:37.977374 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.978363 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.979172 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.979801 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.980334 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.980947 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.981682 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.982532 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.983606 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.984400 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.985276 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.986526 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:93855,variab,variable,93855,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:37.984400 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.985276 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.986526 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.987484 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.988141 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.988661 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989614 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.990134 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.990647 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.991297 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.991910 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:95377,variab,variable,95377,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:37.985276 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.986526 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.987484 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.988141 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.988661 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989614 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.990134 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.990647 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.991297 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.991910 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.992630 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:95552,variab,variable,95552,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:37.986526 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.987484 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.988141 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:37.988661 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.989614 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.990134 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:37.990647 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.991297 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.991910 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.992630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:37.993726 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:95727,variab,variable,95727,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:37.990647 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:37.991297 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.991910 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.992630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:37.993726 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:37.994658 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.995343 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.996014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.997136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.997966 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.998590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.999377 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:96924,variab,variable,96924,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:37.996014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.997136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.997966 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:37.998590 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:37.999377 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.000032 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.000669 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.001136 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.001605 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.002159 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.002743 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.003241 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:98128,variab,variable,98128,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:38.010946 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.011956 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013191 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.013811 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.014904 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.015494 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.016149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.016633 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.017230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.017909 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018465 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018997 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:102555,variab,variable,102555,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:38.014904 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.015494 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.016149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.016633 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.017230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.017909 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018465 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.018997 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.019490 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.020006 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.020472 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.020929 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:103241,variab,variable,103241,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:38.021804 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.022231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.022672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.023087 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023875 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.024280 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.024672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025057 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025403 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.026254 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:105456,variab,variable,105456,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:38.022672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.023087 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023875 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.024280 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.024672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025057 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025403 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.026254 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.026756 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027156 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:105799,variab,variable,105799,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:38.023087 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023487 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.023875 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.024280 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.024672 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025057 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025403 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.025782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.026254 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.026756 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027156 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.027498 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:105974,variab,variable,105974,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:109386,variab,variable,109386,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:109561,variab,variable,109561,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:109904,variab,variable,109904,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:116369,variab,variable,116369,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:120417,variab,variable,120417,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:120760,variab,variable,120760,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,v_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:121103,variab,variable,121103,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:112308,variab,variable,112308,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"ve_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; parallel: This job failed:; sudo docker run -v /home/root:/home/root gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/make_examples --mode training --ref /home/chenyangwang600/training-case-study/input/data/ucsc_hg19.fa --reads /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam --examples /home/chenyangwang600/training-case-study/output/validation_set.with_label.tfrecord@8.gz --truth_variants /home/chenyangwang600/training-case-study/input/data/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz --confident_regions /home/chenyangwang600/training-case-study/input/data/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed --task 1 --regions 'chr21 chr22'. real 0m4.444s; user 0m0.318s; sys 0m0.216s`. I thought I followed the instructions in the guide(Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data) except that I used a 8vCPUs with ; `gcloud beta compute instances create ""cpu-eight"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-8"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""`. and set variables; N_SHARDS=""8"". I tried to use another VM but also failed. How can I solve this issue?. Thanks,; Yang",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184:3501,variab,variables,3501,,https://github.com/google/deepvariant/issues/184,1,['variab'],['variables']
Modifiability,weights; prev_var_name: Unchanged; I0415 07:34:38.002159 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.002743 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.003241 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.003774 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.004208 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.004652 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.005083 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.005480 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.005927 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006367 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.006792 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.007198 140368878327552 warm_starting_util.py:466] Warm-starting variable: Ince,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:99661,variab,variable,99661,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,x --threads 10 -h -b -S HG003.sort.bam chr20 -O BAM -o HG003.chr20.sort.bam ; samtools view --write-index --threads 10 -h -b -S HG004.sort.bam chr20 -O BAM -o HG004.chr20.sort.bam ; /usr/bin/singularity exec --cleanenv -B /share/:/share/ Singularity/deepvariant.deeptrio-1.4.0.sif /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type PACBIO \; --ref=genome/hg38.fa \; --reads_child HG002.chr20.sort.bam \; --reads_parent1 HG003.chr20.sort.bam \; --reads_parent2 HG004.chr20.sort.bam \; --output_vcf_child HG002.chr20.output.vcf.gz \; --output_vcf_parent1 HG003.chr20.output.vcf.gz \; --output_vcf_parent2 HG004.chr20.output.vcf.gz \; --sample_name_child 'HG002' \; --sample_name_parent1 'HG003' \; --sample_name_parent2 'HG004' \; --num_shards 10 \; --intermediate_results_dir tmp_ramdom_TrioDemo_chr20 \; --output_gvcf_child HG002.chr20.g.vcf.gz \; --output_gvcf_parent1 HG003.chr20.g.vcf.gz \; --output_gvcf_parent2 HG004.chr20.g.vcf.gz ; glnexus_cli_v1.4.1 \; --config DeepVariant_unfiltered \; --squeeze \; --dir chr20_GLnexus.DB \; HG002.chr20.g.vcf.gz HG003.chr20.g.vcf.gz HG004.chr20.g.vcf.gz \; --threads 10 | \; /opt/conda/envs/bio/bin/bcftools view \; --threads 10 -O z \; -o TrioDemo_chr20.trio_merged.vcf.gz - ; ```; The following is the code for single sample mutation detection:; ```; samtools view --threads 10 -h -b -S HG002.sort.bam chr20 > HG002/HG002.chr20.sort.bam ; samtools index -@ 10 HG002/HG002.chr20.sort.bam ; singularity run /share:/share Singularity/google-deepvariant.1.5.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref genome/hg38.fa \; --intermediate_results_dir HG002/tmp_ramdom_HG002_chr20 \; --reads HG002/HG002.chr20.sort.bam \; --output_vcf HG002/HG002.chr20.vcf.gz \; --num_shards 10 ; rm -fr HG002/tmp_ramdom_HG002_chr20; ```. The evaluation results of the trio family sample are as follows:. | HG002 | INDEL | PASS | 525469 | 190669 | 334800 | 361066 | 7821 | 151401 | 4826 | 2317 | 0.362855 | 0.962698 | 0.419317 | 0.527055 ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:2936,config,config,2936,,https://github.com/google/deepvariant/issues/689,1,['config'],['config']
Modifiability,x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:115347,variab,variable,115347,,https://github.com/google/deepvariant/issues/172,1,['variab'],['variable']
Modifiability,"xamples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2031,config,config,2031,,https://github.com/google/deepvariant/issues/876,1,['config'],['config']
Modifiability,"y run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided cores: 64; Rules claiming more threads will be scaled down.; Select jobs to execute... > [Wed Jan 4 18:30:51 2023]; > rule deepvariant:; > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta; > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz; > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log; > jobid: 0; > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2; > threads: 64; > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323; > ; > Activating singularity image singularity/deepvariant_1.4.0.sif; > INFO: Convert SIF file to sandbox...; > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp; > ; > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****; > ; > ; > ***** Running the command:*****; > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *; > *; > *; > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:3259,sandbox,sandbox,3259,,https://github.com/google/deepvariant/issues/602,1,['sandbox'],['sandbox']
Modifiability,"y; - Type of data: PacBio HIFI reads. **Steps to reproduce:**; ```; rule deepvariant_make_examples:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \; --mode calling \; --ref {input.reference} \; --reads {input.bam} \; --examples {params.examples} \; --gvcf {params.gvcf} \; --task {params.shard}) > {log} 2>&1; """""". ```; Error:. ```; 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>; To enable them in other operations, rebuild TensorFlow with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:1612,config,config,1612,,https://github.com/google/deepvariant/issues/677,1,['config'],['config']
Modifiability,"zelrc:; #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:; #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --act",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:1666,config,config,1666,,https://github.com/google/deepvariant/issues/608,2,['config'],['config']
Performance," ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled; WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7; W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7; INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000; I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7',",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:1949,optimiz,optimized,1949,,https://github.com/google/deepvariant/issues/679,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance, #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:4830,Load,Loading,4830,,https://github.com/google/deepvariant/issues/608,1,['Load'],['Loading']
Performance," #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found; #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':; #16 1497.0 Traceback (most recent call last):; #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:5840,cache,cache,5840,,https://github.com/google/deepvariant/issues/608,1,['cache'],['cache']
Performance, 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@co,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:8235,cache,cache,8235,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance," /fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmpzz53zv8p. ***** Intermediate results will be written to /fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmpzz53zv8p in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmpzz53zv8p/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmpzz53zv8p/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2023-05-02 14:41:58.436801: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0502 14:42:09.254817 140040320145216 genomics_reader.py:222] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0502 14:42:09.301696 140040320145216 make_examples_core.py:257] Preparing inputs; I0502 14:42:09.324342 140040320145216 genomics_reader.py:222] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0502 14:42:09.328370 140040320145216 make_examples_core.py:257] Common contigs are ['chr20']; I0502 14:42:09.424478 140040320145216 make_examples_core.py:257] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2023-05-02 14:42:09.427029: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728; I0502 14:42:09.448862 140040320145216 genomics_reader.py:222] Reading /input/N",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640:3218,optimiz,optimized,3218,,https://github.com/google/deepvariant/issues/640,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance," File ""/tmp/Bazel.runfiles_PMLPk5/runfiles/genomics/deepvariant/make_examples.py"", line 966, in main; htslib_gcp_oauth.init(); File ""/tmp/Bazel.runfiles_PMLPk5/runfiles/genomics/deepvariant/core/htslib_gcp_oauth.py"", line 79, in init; token = cloud_utils.oauth2_token(); File ""/tmp/Bazel.runfiles_PMLPk5/runfiles/genomics/deepvariant/core/cloud_utils.py"", line 58, in oauth2_token; credentials = oauth2_client.GoogleCredentials.get_application_default(); File ""/usr/local/lib/python2.7/dist-packages/oauth2client/client.py"", line 1271, in get_application_default; return GoogleCredentials._get_implicit_credentials(); File ""/usr/local/lib/python2.7/dist-packages/oauth2client/client.py"", line 1256, in _get_implicit_credentials; credentials = checker(); File ""/usr/local/lib/python2.7/dist-packages/oauth2client/client.py"", line 1187, in _implicit_credentials_from_gce; if not _in_gce_environment():; File ""/usr/local/lib/python2.7/dist-packages/oauth2client/client.py"", line 1042, in _in_gce_environment; if NO_GCE_CHECK != 'True' and _detect_gce_environment():; File ""/usr/local/lib/python2.7/dist-packages/oauth2client/client.py"", line 999, in _detect_gce_environment; http, _GCE_METADATA_URI, headers=_GCE_HEADERS); File ""/usr/local/lib/python2.7/dist-packages/oauth2client/transport.py"", line 282, in request; connection_type=connection_type); File ""/usr/local/lib/python2.7/dist-packages/httplib2/__init__.py"", line 1659, in request; (response, content) = self._request(conn, authority, uri, request_uri, method, body, headers, redirections, cachekey); File ""/usr/local/lib/python2.7/dist-packages/httplib2/__init__.py"", line 1399, in _request; (response, content) = self._conn_request(conn, request_uri, method, body, headers); File ""/usr/local/lib/python2.7/dist-packages/httplib2/__init__.py"", line 1355, in _conn_request; response = conn.getresponse(); File ""/usr/lib/python2.7/httplib.py"", line 1123, in getresponse; raise ResponseNotReady(); httplib.ResponseNotReady; root@720aed86585e:/#",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/14:2617,cache,cachekey,2617,,https://github.com/google/deepvariant/issues/14,1,['cache'],['cachekey']
Performance," GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1917,tune,tune,1917,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance, Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 280,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:10804,Tune,Tune,10804,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance," \; -t deepvariant_gpu .; ```. I am building this docker image on my laptop (M3 macbook). #### PackagesNotFoundError error. The first error I get is -. ```; 1.247 Platform: linux-aarch64; 1.247 Collecting package metadata (repodata.json): ...working... done; 6.190 Solving environment: ...working... failed; 6.260; 6.260 PackagesNotFoundError: The following packages are not available from current channels:; 6.260; 6.260 - bioconda::samtools==1.15; 6.260 - bioconda::bcftools==1.15; 6.260; ```. I resolved this error by removing the version numbers. i.e., removed the `==1.15` from both the lines. #### Error in the build-prerunreq.sh script. Once, I cross the previous error, I get this error -. ```; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-update",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:1347,Load,Load,1347,,https://github.com/google/deepvariant/issues/902,1,['Load'],['Load']
Performance," `external/libssw/src/ssw.c:38:23: fatal error: emmintrin.h: No such file or directory`. I did `export DV_USE_GCP_OPTIMIZED_TF_WHL=0` from the command line before running the compile. I also changed `DV_COPT_FLAGS` to `--copt=-Wno-sign-compare --copt=-Wno-write-strings` within settings.sh (removing the corei7 option). I am using bazel version '0.15.0-' (settings.sh is changed to reflect this). I am using scikit-learn=0.20 (run-prereq.sh changed to reflect this). pyclif was compiled from source. Is there a way to circumvent this error? The complete error message is as follows. ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/libssw/BUILD.bazel:11:1: C++ compilation of rule '@libssw//:ssw' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction; -sections -fdata-sections -MD -MF bazel-out/ppc-opt/bin/external/libssw/_objs/ssw/external/libssw/src/ssw.pic.d -fPIC -iquote external/libssw -iquote bazel-out/ppc-opt/genfiles/external/libssw -iquote ext; ernal/bazel_tools -iquote bazel-out/ppc-opt/genfiles/external/bazel_tools -Wno-maybe-uninitialized -Wno-unused-function -Wno-sign-compare -Wno-write-strings -fno-inline -fno-canonical-system-headers -Wno-; builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""red",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123:1000,cache,cache,1000,,https://github.com/google/deepvariant/issues/123,1,['cache'],['cache']
Performance," a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, ; Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt); ; **Code to train the model:** ; `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu # standard node(s); #SBATCH --ntasks=48; #SBATCH --job-name=""deepvariant_training""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learni",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:2000,LOAD,LOAD,2000,,https://github.com/google/deepvariant/issues/797,1,['LOAD'],['LOAD']
Performance," are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2023-04-13 03:58:43.152070: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:43.528447 140301397178176 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam with NativeSamReader; 2023-04-13 03:58:42.555017: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.980837: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.565788: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.798464: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.381208: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:12741,optimiz,optimized,12741,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance," dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0627 21:18:43.707066 139683296487232 train.py:92] Running with debug=False; I0627 21:18:43.707488 139683296487232 train.py:100] Use TPU at local; I0627 21:18:43.707705 139683296487232 train.py:103] experiment_dir: /home/gambardella/training_chk; INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.; I0627 21:18:43.707828 139683296487232 tpu_strategy_util.py:57] Deallocate tpu buffers before initializing tpu system.; INFO:tensorflow:Initializing the TPU system: local; I0627 21:18:43.846984 139683296487232 tpu_strategy_util.py:81] Initializing the TPU system: local; 2024-06-27 21:18:43.848149: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-06-27 21:18:43.855816: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: ; Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py"", line 110, in initialize_tpu_system; output = _tpu_init_fn(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 134, in __call__; return concrete_function._call_flat(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat; return self._build_call_outputs(self._inference_function.call(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call; outputs = execute.e",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:2148,optimiz,optimized,2148,,https://github.com/google/deepvariant/issues/841,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance," examples. real	25m4.324s; user	39m40.647s; sys	0m24.239s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpiy9bfzyx/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpiy9bfzyx/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/tmpiy9bfzyx"" --use_openvino; I1214 06:10:30.972710 140363019278144 call_variants.py:317] From /tmp/tmpiy9bfzyx/make_examples.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I1214 06:10:30.995939 140363019278144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; 2022-12-14 06:10:31.060396: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-12-14 06:10:31.101084: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1214 06:10:31.288279 140363019278144 openvino_estimator.py:55] Processing ckpt=/opt/models/wes/model.ckpt, tensor_shape=[100, 221, 7]; /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1083: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:678: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); /usr/local/lib/python3.8/dist-packages/tf_s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597:7319,optimiz,optimized,7319,,https://github.com/google/deepvariant/issues/597,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance, function to raise exceptions.; performance hint: _common.pyx:465:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:509:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:592:36: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:596:36: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:600:36: Exception check after calling 'f2' will always require the GIL to be acquired. Declare 'f2' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:604:36: Exception check after calling 'f3' will always require the GIL to be acquired. Declare 'f3' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:638:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:675:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:712:63: Exception check af,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:4477,perform,performance,4477,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance, function to raise exceptions.; performance hint: _common.pyx:600:36: Exception check after calling 'f2' will always require the GIL to be acquired. Declare 'f2' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:604:36: Exception check after calling 'f3' will always require the GIL to be acquired. Declare 'f3' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:638:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:675:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:712:63: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:754:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:785:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:903:40: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:907:40: Exception check afte,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:5433,perform,performance,5433,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance, function to raise exceptions.; performance hint: _common.pyx:754:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:785:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:903:40: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:907:40: Exception check after calling 'fd' will always require the GIL to be acquired. Declare 'fd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:911:41: Exception check after calling 'fdd' will always require the GIL to be acquired. Declare 'fdd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:916:40: Exception check after calling 'fi' will always require the GIL to be acquired. Declare 'fi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:920:41: Exception check after calling 'fdi' will always require the GIL to be acquired. Declare 'fdi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:924:38: Exception check after calling 'fiii' will always require the GIL to be acquired. Declare 'fiii' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:960:31: Except,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:6627,perform,performance,6627,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance," load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T18:35:24-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such file or directory: Try",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:9117,load,load,9117,,https://github.com/google/deepvariant/issues/292,1,['load'],['load']
Performance, mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:8807,cache,cache,8807,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance," modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants; output_queue = multiprocessing.Queue(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s; user 0m6.224s; sys 0m3.683s. ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```; gvcf.tfrecord-00000-of-00016.gz ma",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:2797,Queue,Queue,2797,,https://github.com/google/deepvariant/issues/733,2,['Queue'],['Queue']
Performance, raise exceptions.; performance hint: _common.pyx:911:41: Exception check after calling 'fdd' will always require the GIL to be acquired. Declare 'fdd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:916:40: Exception check after calling 'fi' will always require the GIL to be acquired. Declare 'fi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:920:41: Exception check after calling 'fdi' will always require the GIL to be acquired. Declare 'fdi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:924:38: Exception check after calling 'fiii' will always require the GIL to be acquired. Declare 'fiii' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:960:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:1002:32: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; ; Error compiling Cython file:; ------------------------------------------------------------; ...; self.rng_state.ctr.v[i] = counter[i]; ; self._reset_state_variables(); ; self._bitgen.state = <void *>&self.rng_state; self._bitgen.next_uint64 = &philox_uint64; ^; ------------------------------------------------------------; ; _philox.pyx:195:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the valu,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:7595,perform,performance,7595,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance," step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.5",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:11219,Tune,Tune,11219,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance, the function to raise exceptions.; performance hint: _common.pyx:638:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:675:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:712:63: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:754:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:785:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:903:40: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:907:40: Exception check after calling 'fd' will always require the GIL to be acquired. Declare 'fd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:911:41: Exception check after calling 'fdd' will always require the GIL to be acquired. Declare 'fdd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:916:40: Exception chec,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:5909,perform,performance,5909,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance, the function to raise exceptions.; performance hint: _common.pyx:675:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:712:63: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:754:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:785:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:903:40: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:907:40: Exception check after calling 'fd' will always require the GIL to be acquired. Declare 'fd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:911:41: Exception check after calling 'fdd' will always require the GIL to be acquired. Declare 'fdd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:916:40: Exception check after calling 'fi' will always require the GIL to be acquired. Declare 'fi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:920:41: Exception ch,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:6147,perform,performance,6147,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance," want to debug source codeï¼Œand execute â€œpython deepvariant/call_variants.pyâ€. **Setup**; i have execute build-prereq.sh and build_and_test.sh. In order to get the compilation result .; i execute ""bazel build ..."",get the file like this :; <img width=""529"" alt=""image"" src=""https://github.com/google/deepvariant/assets/15654389/50fbd52c-afed-4ade-a3fa-f2eaf0859b3d"">; ; The same name comes from different directories.so,the error happy:; â€œfrom third_party.nucleus.io import sharded_file_utils â€ from root workspace; â€œfrom third_party.nucleus.protos import variants_pb2â€ from bazel-bin. <img width=""647"" alt=""image"" src=""https://github.com/google/deepvariant/assets/15654389/76e1373b-dbfb-48b6-95d1-59bd07badbcc"">. `root@7065ad26b62a:/deepvariant# python deepvariant/call_variants.py; 2023-12-19 06:28:51.254398: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Traceback (most recent call last):; File ""deepvariant/call_variants.py"", line 53, in <module>; from deepvariant import dv_utils; File ""/deepvariant/./deepvariant/dv_utils.py"", line 47, in <module>; from deepvariant.protos import deepvariant_pb2; File ""/deepvariant/bazel-bin/deepvariant/protos/deepvariant_pb2.py"", line 17, in <module>; from deepvariant.protos import realigner_pb2 as deepvariant_dot_protos_dot_realigner__pb2; File ""/deepvariant/bazel-bin/deepvariant/protos/realigner_pb2.py"", line 21, in <module>; from third_party.nucleus.protos import range_pb2 as third__party_dot_nucleus_dot_protos_dot_range__pb2; ImportError: cannot import name 'range_pb2' from 'third_party.nucleus.protos' (/deepvariant/./third_party/nucleus/protos/__init__.py); `; How can i debug the source code in the right way?; How to correctly compile proto files and c++",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/756:916,optimiz,optimized,916,,https://github.com/google/deepvariant/issues/756,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"""en_US.UTF-8"",; 	LC_MEASUREMENT = ""en_US.UTF-8"",; 	LC_IDENTIFICATION = ""en_US.UTF-8"",; 	LC_NUMERIC = ""en_US.UTF-8"",; 	LC_PAPER = ""en_US.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2024-02-17 23:32:31.107126: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.108506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.006781: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.007601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.110201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; ...; 2024-02-17 23:33:25.887517: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:33:25.933275 1405337249360",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:6701,load,load,6701,,https://github.com/google/deepvariant/issues/774,1,['load'],['load']
Performance,"### Issue; When running the build-prereq shell script, I'm getting an error when the Tensorflow install begins. ### Error message; ```; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl...; - [1 files][ 41.1 MiB/ 41.1 MiB] 1.0 MiB/s ; Operation completed over 1 objects/41.1 MiB. ; tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.; ```. ### Debugging efforts; After browsing around a bit, I discovered that this issue was solved for some through installing the .whl separately. So, I download the whl from the [GCloud bucket](https://console.cloud.google.com/storage/browser/deepvariant/packages/tensorflow/) and executed `sudo python2.7 pip install <name of .whl file>` through the terminal. It ran, just to tell me â€œ.dist-info directory not foundâ€. I think this might be due to some inconsistency in the packages installed through the build-prereq.sh script, because I can see that all the packages that it installed (e.g. numpy) are for Python 3.5, but the Tensorflow version it's trying to get is for cp27 (Python 2.7). Not sure about where to go from here, would love some assistance :). ### System details; OS: Ubuntu 16.04 LTS; Python interpreters: Default with Ubuntu (2.7 and 3.5.2); Deep Variant version: Installed it today from the main repo, so probably r0.4.1. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/30:169,optimiz,optimized,169,,https://github.com/google/deepvariant/issues/30,1,['optimiz'],['optimized']
Performance,'@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:10233,cache,cache,10233,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,'@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and refer,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:11377,cache,cache,11377,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,(TRAINING) model-ckpt-0 shows low accuracy even when loaded from a previous checkpoint,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/383:53,load,loaded,53,,https://github.com/google/deepvariant/issues/383,1,['load'],['loaded']
Performance,"); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:4859,cache,cache,4859,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"**Describe the issue:**. Hi, I am wondering if there's been a study on the cost-benefit of running DV in the GPU mode.; Back in the days of PEPPER-DeepVariant-Margin, I remember trying to profile (not as a rigorous study) what benefits there'd be if we were to run the pipeline in the GPU mode.; The conclusion back then from my anecdotal runs is that it's not worth it (we can get the CPU version to <$100/sample with little to minimum effort on optimizing cloud resource allocations, but the GPU version is ~$200/sample with P100). Now given that DV has undergone quite a lot of changes since then, I wonder if the conclusion is changed.; So I did a test run on a PacBio Hifi 30X bam with DV 1.5.0, and collected the GPU resource log (using `gpustat -a -i 1 `, log attached below).; Looking at the log file, it doesn't look like GPU is used much still. So I wonder if you have done any study on this subject and if so, can share some insights. Thank you!. Steve. [gpu.usages.log.zip](https://github.com/google/deepvariant/files/11473421/gpu.usages.log.zip)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650:447,optimiz,optimizing,447,,https://github.com/google/deepvariant/issues/650,1,['optimiz'],['optimizing']
Performance,"**Describe the issue:**; I Build the docker image; Inside Docker image: I am reading the checkpoint files to create a frozen graph; When doing ""import_meta_graph"" I get the error. Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339:459,load,loading,459,,https://github.com/google/deepvariant/issues/339,1,['load'],['loading']
Performance,"**Describe the issue:**; I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: ; ```; BIN_VERSION=""1.3.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20; ```. **Error 1**; ```; INFO: Using cached SIF image; I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889.; ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it canno",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/533:909,cache,cached,909,,https://github.com/google/deepvariant/issues/533,1,['cache'],['cached']
Performance,"**Describe the issue:**; Thank you so much for the great tool. . I'm working on a heterozygous mouse long-read RNA-seq dataset from PacBio and would like to perform variant call + phasing at read-level. I'm wondering whether you have some recommendations regarding the points below:; - I'm currently using `--model_type=PACBIO` with the bam files processed with `gatk SplitNCigarReads`. Does this model consider RNA editing? Or should I use `--model_type=WES`? I saw some discussions mentioning WES model considers RNA-editing in https://github.com/google/deepvariant/issues/775; - Is there anyway that I could integrate the known variants from genomic data into the variant calling? Or should it be integrated after `DeepVariant` variant call at vcf-level?. **Setup**; - Operating system: Ubuntu 2.20; - DeepVariant version: v1.6.1; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HiFi, mm10, long-read RNA-seq data. **Steps to reproduce:**; - Command:; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.1 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=GRCm38.primary_assembly.genome.fa \; --reads=SNCR.bam \; --output_vcf=output.vcf.gz \; --num_shards 16; ```. Thank you so much for your kind help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/890:157,perform,perform,157,,https://github.com/google/deepvariant/issues/890,1,['perform'],['perform']
Performance,"**Describe the issue:**; When I try to run DeepVariant using the examples in the quickstart document I receive the following output:. ```; INFO: Using cached SIF image; --ref is required.; Pass --helpshort or --helpfull to see help on flags.run_deepvariant.sh: line 13: --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; ```. I am able to open the FASTA file at that path, so I know that it exists. The full script I am using is:. ```; #!/bin/sh. BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. **Setup**; - Operating system: Linux, cluster; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker, through Singularity; - Type of data: The data from the quickstart . **Steps to reproduce:**; - Command: See above; - Error trace: See above. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. My issue is with the quickstart. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/402:151,cache,cached,151,,https://github.com/google/deepvariant/issues/402,1,['cache'],['cached']
Performance,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.); Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**; - Operating system: Linux HPC; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WES. **Steps to reproduce:**; ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=02:00:00; #SBATCH --time=072:00:00; #SBATCH --mem-per-cpu=32GB. module purge; module load singularity; module load parallel. set -eu. cd /scratch/c.c21087028/; BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \; --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/522:958,load,load,958,,https://github.com/google/deepvariant/issues/522,2,['load'],['load']
Performance,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes. **Describe the issue:**; I was following the quick start guide for running singularity on a gpu node. Initially, I encounter the dynamic cast failed error similar to #559 . After installing the google-nucleus package, I encountered this new error about protobuf package. I tried protobuf version 3.20.3 and 4.21.9, but the error message is the same. In order to run DeepVariant successfully, what additional packages (version) should I install besides cloning the singularity image?. **Setup**; - Operating system: ; - DeepVariant version: 1.4.0; - Installation method: singularity; - Type of data: quick start test; ; **Steps to reproduce:**; ; ```; SINGULARITY_TMPDIR=/scratch/midway3/weilu1/tmp SINGULARITY_CACHEDIR=/scratch/midway3/weilu1/cache singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_1.4.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1. INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580:840,cache,cache,840,,https://github.com/google/deepvariant/issues/580,1,['cache'],['cache']
Performance,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes. Same error msgs were observed. But I was lunching deepvariant with singularity; **Describe the issue:**; (A clear and concise description of what the issue is.); The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity.; **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable); module load singularity; BIN_VERSION=""1.5.0""; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools""; INPUT_DIR=""${LABASE}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${LABASE}/quickstart-output""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:701,load,load,701,,https://github.com/google/deepvariant/issues/678,1,['load'],['load']
Performance,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; yesï¼Œi have checked this FAQ document. ; **Describe the issue:**; I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:; Data comparison to reference genome:; ```; echo HG002.merged.fastq.gz > HG002.fofn ; pbmm2 align \; --preset HIFI \; genome/hg38.fa.mmi \; HG002.fofn \; --sample HG002 \; -j 10 \; HG002.aligned.tmp.bam ; samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam ; samtools index -@ 10 HG002.aligned.tmp.sort.bam ; chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) ; for chromosome in ""${chromosomes[@]}""; \; do \; samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & ; done ; wait ; samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam ; samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam ; samtools index -@ 10 HG002.sort.bam ; ```; Family analysis code:; ```; rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 ; samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o HG002.chr20.sort.bam ; samtools view --write-index --threads 10 -h -b -S HG003.sort.b",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:190,perform,perform,190,,https://github.com/google/deepvariant/issues/689,2,['perform'],['perform']
Performance,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**; Hi developers of DeepVariant,; I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:; ```bash; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes; I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation.; I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF.; I0408 07:29:46.201339 140282942986048",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804:753,load,load,753,,https://github.com/google/deepvariant/issues/804,1,['load'],['load']
Performance,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). Hello All,. I have been testing ONT datasets on the HPC cluster to benchmark and optimize them. While using the mapped ONT BAM files from the HG002 and HG003 datasets from the UCSC studies, I observed that DeepVariant gets stuck at the make_examples stage. Even after 24 hours, it remains in the same stage which is unsual. I would appreciate your input on this issue. **Setup**; - Operating system: Linux, HPC cluster; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) ; -ONT : https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/HG002_GRCh38_ONT-UL_UCSC_20200508.phased.bam; reference -hg38 . **Steps to reproduce:**; - Command: . apptainer exec ; --bind Deepvariant/HG002_HG003_1.5.0 deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant ; --model_type ONT_R104 ; --ref Homo_sapiens_assembly38.fasta ; --reads HG002_GRCh38_ONT-UL_UCSC_20200508.phased.bam ; --output_vcf HG002_chr1.output.vcf.gz ; --output_gvcf HG002_chr1.output.g.vcf.gz ; --regions chr1 --num_shards 56 --logging_dir chr1 ; --intermediate_results_dir chr1/intermediate_results . - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, it did work. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/856:257,optimiz,optimize,257,,https://github.com/google/deepvariant/issues/856,1,['optimiz'],['optimize']
Performance,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; I am running deepvariant 1.6.1 through singularity (apptainer) on both WGS and RNAseq bams. The WGS bam was much larger in file size, but was processed much more quickly than the RNAseq bams produced by STAR. ; Because deepvariant 1.6.1 does not support an rnaseq model, so I just ran the WES model on it, providing a BED file containing all regions with at least 3X read depth. Here is the script I used:. `sID=$1 #sample ID; sBAM=$2 #full path to BAM; REF=$3 #full path to fasta ref; CPU=$4 #number of CPUs to use. module load apptainer/1.2.5; module load clusterbasics; module load samtools; module load bedtools. OUTPUT_DIR=./output/$sID. mkdir -p $OUTPUT_DIR; mkdir -p ./tmp; export TMPDIR=`realpath ./tmp`. if [ ! -f $sBAM.bai ]; then; echo producing bai index for $sBAM; samtools index $sBAM; fi. if [ ! -f ""${OUTPUT_DIR}/dv.log"" ];then; bedtools coverage -g genome.file -sorted -d -a genome.bed -b ""$sBAM"" | awk '{if ($5>=3) print $1""\t""($4-1)""\t""$4""\t""$5}' | bedtools merge -d 1 -c 4 -o mean -i - > ${OUTPUT_DIR}/cov3x.bed; fi. apptainer run -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3,/public4:/public4 \; /public4/software/deepvariant/1.6.1/cpuver/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --make_examples_extra_args=""normalize_reads=true"" \; --model_type=WES \; --ref=$REF \; --reads=""$sBAM"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --regions=""${OUTPUT_DIR}/cov3x.bed"" \; --num_shards=$CPU > ${OUTPUT_DIR}/dv.log 2>&1. `. Inspecting the tail of the log, it appears that the program gets stuck at the make_examples step, with many threads reporting finding 0 examples:; 'I0812 17:25:00.705988 139682501986112 make_examples_core.py:301] Task 14/32: Overhead for preparing inputs: 270 seconds; I0812 17:25:00.763086 139682501986112 make_examples_core.py:301] Task 14/32: 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/867:643,load,load,643,,https://github.com/google/deepvariant/issues/867,4,['load'],['load']
Performance,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; When I check the version of google/deepvariant:1.6.1 it says 1.6.0, and the docker image has `ENV VERSION=1.6.0`; ```; docker run google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --version; 2024-06-13 12:25:30.001574: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; DeepVariant version 1.6.0; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830:429,optimiz,optimized,429,,https://github.com/google/deepvariant/issues/830,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.6; - Installation method (Docker, built from source, etc.): singularity image built form Docker Hub; - Type of data: bacteria whole genome. **Steps to reproduce:**; - Command:; smakemake pipeline; rule run_deepvariant:; output:; vcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.vcf.gz"",; gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz""; input:; reference_fasta = ""/project/databases/bacteroides_genome/reference_genomic.fna"",; reads = rules.sam2bam.output.sorted_bam; params:; inter_dir = ""../../results/deepVariant/{dataset}/{sample}/intermediate"",; log_dir = ""../../results/deepVariant/{dataset}/{sample}/log"",; work_dir = ""/project/"",; deepvariant = ""/project/software/deepVariant.sif""; shell:; """"""; module load singularity/3.7.0; singularity exec -B {params.work_dir} {params.deepvariant} /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref={input.reference_fasta} \; --reads={input.reads} \; --output_vcf={output.vcf} \; --output_gvcf={output.vcf} \; --make_examples_extra_args --channels=insert_size \; --intermediate_results_dir {params.inter_dir} \; --num_shards=6 \; --logging_dir={params.log_dir}; """"""; - Error trace: ; ***** Running the command:*****; time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --outfile_base ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant"". I0626 19:01:30.369722 139699125458752 genomics_reader.py:222] Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz with NativeVcfReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_xq721o6r/runfiles/com_google_deepvariant/deepvariant/vcf_stats_report.py"", line 103, in <module>; tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839:972,load,load,972,,https://github.com/google/deepvariant/issues/839,1,['load'],['load']
Performance,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:YES. **Describe the issue:**; When Running deep variant wes mode, there arised an assetion error when loading the weights of the model. **Setup**; - Operating system:Linux ; - DeepVariant version:1.6.1; - Installation method (Docker, built from source, etc.):Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; ```; DV=""singularity run /autofs/bal34/xyu/softwares/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant ""; ${DV} \; --model_type=WES \; --customized_model=/autofs/bal34/xyu/run_software/dv_illu/model/model.ckpt \; --ref ${REF_FILE_PATH} \; --reads {1} \; --output_vcf ${BASE_DIR}/{2}/output.vcf.gz \; --num_shards 30 \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir ${BASE_DIR}/{2}/intermediate_results_dir; ```; - Error trace: (if applicable); ```; WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:194,load,loading,194,,https://github.com/google/deepvariant/issues/857,1,['load'],['loading']
Performance,"**hello,; I tested DeepVariant 1.5.0 on pacbio public data.; The data link is:; https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_MtSinai_NIST/PacBio_minimap2_bam/HG003_PacBio_GRCh37.bam; But it failed.; The log :** ; 2023-04-13 03:58:10.677743: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0413 03:58:12.505982 140621665130304 run_deepvariant.py:364] Re-using the directory for intermediate results in intermediate_results_dir; ***** Intermediate results will be written to intermediate_results_dir in docker. ****; ***** Running the command:*****; time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/sfs-GCS/ann-BIstorage/DB/data/sentieon/hs37d5/hs37d5.fasta"" --reads ""HG003_PacBio_GRCh37.bam"" --examples ""intermediate_results_dir/make_examples.tfrecord@32.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/gvcf.tfrecord@32.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}; ; 2023-04-13 03:58:35.887616: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.520424: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimiz",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:380,optimiz,optimized,380,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"*Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**; - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - Installation method (Docker, built from source, etc.): Docker; - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```; # This is the command from Pepper, but judged from the log, the command failed during the DV stage.; run_pepper_margin_deepvariant \; call_variant \; -b ~{bam} \; -f ~{ref_fasta} \; -t ""${num_core}"" \; -s ""${SM}"" \; -o ""~{output_root}"" \; -p ""~{prefix}"" \; --gvcf \; --phased_output \; --ont; ```; Relevant part of the log file (which is over 200MB):. ```; run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont; [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED; [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output; ; mkdir -p /cromwell_root/pepper_output/logs; ; mkdir -p /cromwell_root/pepper_output/intermediate_files;; -------; [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:1235,cache,cacheCopy,1235,,https://github.com/google/deepvariant/issues/491,1,['cache'],['cacheCopy']
Performance,"++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; ..................; (09:27:04) INFO: Current date is 2017-12-21; (09:27:04) Loading: ; (09:27:04) Loading: 0 packages loaded; (09:27:05) Loading: 0 packages loaded; (09:27:06) Loading: 7 packages loaded; currently loading: deepvariant/core/genomics ... (6 packages); (09:27:07) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:08) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:09) Analyzing: 242 targets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:3781,load,loaded,3781,,https://github.com/google/deepvariant/issues/19,4,['load'],['loaded']
Performance,"- Can you suggest what would be the best strategy to optimize deepvariant variant calling on large sample sizes (>500 WES/ WGS samples)?; - Is Deepvariant optimized for N+1 Strategy or joint calling (I read there is an option for gVCF output) and if so, what would be the best way or recommended tool to convert multiple deepvariant's gVCFs (500 gVCFs) to one VCF??",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/142:53,optimiz,optimize,53,,https://github.com/google/deepvariant/issues/142,2,['optimiz'],"['optimize', 'optimized']"
Performance,"- Command:#docker; BIN_VERSION=""1.5.0""; reference=/home/data/ref_annotation_Geneset/11.variant_calling/deepvariant/reference; INPUT_DIR=/home/data/ref_annotation_Geneset/11.variant_calling/deepvariant/input; OUTPUT_DIR=/home/data/ref_annotation_Geneset/11.variant_calling/deepvariant/outdir; singularity run --nv -B /home/data/ref_annotation_Geneset/11.variant_calling/deepvariant/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""${reference}""/GRCh38_no_alt_analysis_set.fasta \; --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ; - Error trace: INFO: Using cached SIF image; WARNING: Could not find any nv files on this host!; 2023-04-22 17:10:50.025707: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 51, in <module>; from ._api.v2 import compat; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/__init__.py"", line 37, in <module>; from . import v1; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py"", line 30, in <module>; from . import compat; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py"", line 38, in <module>; from . import v2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/634:867,cache,cached,867,,https://github.com/google/deepvariant/issues/634,1,['cache'],['cached']
Performance,-define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:5162,Load,Loading,5162,,https://github.com/google/deepvariant/issues/608,2,"['Load', 'load']","['Loading', 'loaded']"
Performance,"-regions ""chr20:10,000,000-10,010,000"" --output_vcf=/opt/deepvariant/quickstart-output/output.vcf.gz --output_gvcf=/opt/deepvariant/quickstart-output/output.g.vcf.gz --intermediate_results_dir /opt/deepvariant/quickstart-output/intermediate_results_dir --num_shards=1 --verbosity=2; ```. - Error trace: (if applicable) In the `postprocess_variants` step; ```; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/opt/deepvariant/quickstart-output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/opt/deepvariant/quickstart-output/output.vcf.gz"" --cpus ""1"" --gvcf_outfile ""/opt/deepvariant/quickstart-output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/opt/deepvariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"". 2024-10-31 20:36:34.101345: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-10-31 20:36:34.101375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-10-31 20:36:35.010025: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2027] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.; I1031 20:36:35.011695 132485076334400 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: NA12878; I1031 20:36:35.013445 132485076334400 postprocess_variants.py:1313] CVO sorting took 1.1885166168212891e-05 minutes; I1031 20:36:35.013573 132485076334",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:1856,load,load,1856,,https://github.com/google/deepvariant/issues/901,1,['load'],['load']
Performance,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:31,Load,Load,31,,https://github.com/google/deepvariant/issues/231,2,['Load'],['Load']
Performance,".2 AVX AVX2 AVX512F FMA; 2018-05-02 10:58:57.263635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:; name: Tesla P100-PCIE-12GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285; pciBusID: 0000:3b:00.0; totalMemory: 11.91GiB freeMemory: 11.62GiB; 2018-05-02 10:58:57.263682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:3b:00.0, compute capability: 6.0); INFO:tensorflow:Restoring parameters from /tmp/deepvariant/model.ckpt-0; I0502 10:58:57.455770 139632719935232 tf_logging.py:82] Restoring parameters from /tmp/deepvariant/model.ckpt-0; INFO:tensorflow:Starting Session.; I0502 10:59:09.842276 139632719935232 tf_logging.py:82] Starting Session.; INFO:tensorflow:Saving checkpoint to path /tmp/deepvariant/model.ckpt; I0502 10:59:10.099534 139621333726976 tf_logging.py:82] Saving checkpoint to path /tmp/deepvariant/model.ckpt; INFO:tensorflow:Starting Queues.; I0502 10:59:10.102293 139632719935232 tf_logging.py:82] Starting Queues.; INFO:tensorflow:global_step/sec: 0; I0502 10:59:13.668776 139621325334272 tf_logging.py:121] global_step/sec: 0; INFO:tensorflow:Recording summary at step 0.; I0502 10:59:14.875045 139621316941568 tf_logging.py:82] Recording summary at step 0.; INFO:tensorflow:global step 1: loss = 0.2608 (4.963 sec/step); I0502 10:59:15.326091 139632719935232 tf_logging.py:82] global step 1: loss = 0.2608 (4.963 sec/step); 2018-05-02 10:59:15.584978: E tensorflow/core/kernels/check_numerics_op.cc:157] abnormal_detected_host @0x104ef6dce00 = {1, 0} LossTensor is inf or nan; 2018-05-02 10:59:15.615399: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: LossTensor is inf or nan : Tensor had NaN values; [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](control_dependency_4)]]; INFO:tensorflow:Error ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69:2668,Queue,Queues,2668,,https://github.com/google/deepvariant/issues/69,1,['Queue'],['Queues']
Performance,".3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:03:04 --:--:-- 0; curl: (56) GnuTLS recv error (-54): Error in the pull function.; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build_release_binaries.sh; [sudo] password for solokopi: ; build_release_binaries.sh: line 39: bazel: command not found; build_release_binaries.sh: line 43: bazel: command not found; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:18803,optimiz,optimized,18803,,https://github.com/google/deepvariant/issues/89,1,['optimiz'],['optimized']
Performance,.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 240,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:10472,Tune,Tune,10472,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,".py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.551992654800415, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0, train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; ```. I am ne",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:13029,tune,tune,13029,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,".runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner; regions, calling_regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options; ref_contigs = fasta.IndexedFastaReader(; File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__; self._reader = reference.IndexedFastaReader.from_file(; ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; I0522 08:40:37.063492 139925831092032 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader; I0522 08:40:37.081499 139925831092032 make_examples_core.py:257] Task 6/32: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main; make_ex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:3183,load,load,3183,,https://github.com/google/deepvariant/issues/653,1,['load'],['load']
Performance,".runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner; regions, calling_regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options; ref_contigs = fasta.IndexedFastaReader(; File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__; self._reader = reference.IndexedFastaReader.from_file(; ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Yes, the quick test run as normal.; ```. 3. reference index does; ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:6537,load,load,6537,,https://github.com/google/deepvariant/issues/653,1,['load'],['load']
Performance,".tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840515, train/f1_homalt=0.964401364326477, train/f1_homref=0.902255654335022, train/f1_macro=0.; 8698380589485168, train/f1_micro=0.91796875, train/f1_weighted=0.9241795539855957, train/false_negatives=34.0, train/false_positives=14.0, train/learning_rate=9.999999747378752e-06, train/loss=0.6384731531143188, train/precision=0.9406779408454895, train/precision_het=0.702702701091; 7664, train/precision_homalt=0.978723406791687, train/precision_homref=1.0, train/recall=0.8671875, train/recall_het=1.0, train/recall_homalt=0.8789808750152588, train/recall_homref=0.7945205569267273, train/true_negatives=498.0, train/true_positives=222.0; I1031 11:18:53.873582 14055859",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1867,perform,performance,1867,,https://github.com/google/deepvariant/issues/904,1,['perform'],['performance']
Performance,".tfrecord@1.gz"" --task {}' returned non-zero exit status 1. ```. This is what my input directory looks like:. ```; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module lo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:8143,load,load,8143,,https://github.com/google/deepvariant/issues/292,1,['load'],['load']
Performance,"/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \; --mode calling \; --ref {input.reference} \; --reads {input.bam} \; --examples {params.examples} \; --gvcf {params.gvcf} \; --task {params.shard}) > {log} 2>&1; """""". ```; Error:. ```; 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>; 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>; 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>; I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>; I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs; 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>; 2023-07-12 15:19:59.049123: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>; 2023-07-12 15:19:59.049140: W third_party/nucle",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:2529,optimiz,optimized,2529,,https://github.com/google/deepvariant/issues/677,1,['optimiz'],['optimized']
Performance,/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:5435,cache,cache,5435,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:5627,cache,cache,5627,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:5819,cache,cache,5819,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:6011,cache,cache,6011,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:6203,cache,cache,6203,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:6395,cache,cache,6395,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:6587,cache,cache,6587,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:6779,cache,cache,6779,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:6971,cache,cache,6971,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:7163,cache,cache,7163,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenc,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:7355,cache,cache,7355,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googl,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:7547,cache,cache,7547,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:7739,cache,cache,7739,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl; #16 1497.0 ctx.download_and_extract(; #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:; #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.1 Repository rule _tf_http_archive defined at:; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.2 (21:51:08) Loading: 0 packages loaded; #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/ex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:7750,cache,cache,7750,,https://github.com/google/deepvariant/issues/608,1,['cache'],['cache']
Performance,"/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export DV_TF_NIGHTLY_BUILD=0; ++ DV_TF_NIGHTLY_BUILD=0; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/home/huangl/publib/bin/python; ++ PYTHON_BIN_PATH=/home/huangl/publib/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (08:09:38) INFO: Current date is 2017-12-08; (08:09:38) WARNING: /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/core/BUILD:1806:1: in includes attribute of cc_library rule @org_tensorflow//tensorflow/core:framework_headers_lib: '../../../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'external/org_tensorflow/tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/tensorflow.bzl:1100:30; (08:09:38) INFO: Analysed 241 targets (0 packages loaded).; (08:09:38) INFO: Found 185 targets and 56 test targets...; (08:09:38) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1: //deepvariant/core/protos:core_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvari",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6:2453,cache,cache,2453,,https://github.com/google/deepvariant/issues/6,1,['cache'],['cache']
Performance,"0 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12316,tune,tune,12316,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"0 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c; I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz; INFO:tensorflow:Calling model_fn.; I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn.; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn.; 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; INFO:tensorflow:Graph was finalized.; I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt; I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt; INFO:tensorflow:Running local_init_op.; I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op.; INFO:tensorflow:Done running local_init_op.; I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op.; INFO:tensorflow:Reloading EMA...; I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA...; INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt; I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt; I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:3928,Tune,Tune,3928,,https://github.com/google/deepvariant/issues/679,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:5243,cache,cache,5243,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"0/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; Ã— Preparing metadata (pyproject.toml) did not run successfully.; â”‚ exit code: 1; â•°â”€> [78 lines of output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); performance hint: _common.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:322:50: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:426:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:465:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:509:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:592:36: Ex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:3025,perform,performance,3025,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,"11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']; [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895; [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS; [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]; ...; [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]; [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]; [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY.; [11-03-2021 13:44:25] FINISHED IMAGE GENERATION; [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec; [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE; [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/; [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP.; [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64; [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1; [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX; [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35.; [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35.; [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35.; [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35.; [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35.; [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35.; [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35.; [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY.; [11-03-2021 13:50:44] INFO: FINISHED PREDICTION; [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec; [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. ; [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec; [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES; [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/; [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10; [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOU",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:4047,LOAD,LOADING,4047,,https://github.com/google/deepvariant/issues/491,1,['LOAD'],['LOADING']
Performance,"14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/third_party/nucleus/io/python/_objs/hts_verbose_cclib/hts_verbose.pic.d '-frandom-seed=bazel-out/k8-opt/bin/third_party/nucleus/io/python/_objs/hts_verbose_cclib/hts_verbose.pic.o' -fPIC -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/htslib -iquote bazel-out/k8-opt/genfiles/external/htslib -iquote bazel-out/k8-opt/bin/external/htslib -iquote external/clif -iquote bazel-out/k8-opt/ge",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/189:1470,cache,cache,1470,,https://github.com/google/deepvariant/issues/189,1,['cache'],['cache']
Performance,"16: Overhead for preparing inputs: 8 seconds; I0217 23:33:34.827609 140099871606592 make_examples_core.py:301] Task 0/16: 0 candidates (0 examples) [0.02s elapsed]; ...; I0218 00:34:18.301548 140191938357056 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0218 00:34:18.301738 140191938357056 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0218 00:34:18.302148 140191938357056 make_examples_core.py:301] Task 3/16: Found 9819 candidate variants; I0218 00:34:18.302218 140191938357056 make_examples_core.py:301] Task 3/16: Created 10372 examples. real	62m19.124s; user	928m53.495s; sys	2m16.403s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; 2024-02-",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:13331,load,load,13331,,https://github.com/google/deepvariant/issues/774,1,['load'],['load']
Performance,"174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12261,tune,tune,12261,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"2 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12484,tune,tune,12484,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"22385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12351,tune,tune,12351,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, tr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12514,tune,tune,12514,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"29 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12575,tune,tune,12575,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"2: Writing example info to /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord-00001-of-00002.gz.example_info.json; I0105 15:55:21.255679 140329169033024 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0105 15:55:21.255904 140329169033024 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0105 15:55:21.262568 140329169033024 make_examples_core.py:301] Task 1/2: Found 3672 candidate variants; I0105 15:55:21.263317 140329169033024 make_examples_core.py:301] Task 1/2: Created 3944 examples. real 1m56.796s; user 3m3.813s; sys 0m4.710s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/call_variants_output.tfrecord.gz"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs"". 2024-01-05 15:55:31.140705: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:55:31.140953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-01-05 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:12661,load,load,12661,,https://github.com/google/deepvariant/issues/761,1,['load'],['load']
Performance,"2] Reading HG003_PacBio_GRCh37.bam with NativeSamReader; 2023-04-13 03:58:42.555017: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.980837: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.565788: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.798464: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.381208: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:44.143414 14013848112518",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:13104,optimiz,optimized,13104,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"34488 140301397178176 make_examples_core.py:257] Task 0/32: Common contigs are ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y', 'MT']; I0413 03:58:43.149506 140301397178176 make_examples_core.py:257] Task 0/32: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2023-04-13 03:58:43.152070: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:43.528447 140301397178176 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam with NativeSamReader; 2023-04-13 03:58:42.555017: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.980837: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.565788: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.798464: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:12378,optimiz,optimized,12378,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 250,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:10555,Tune,Tune,10555,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; ..................; (09:27:04) INFO: Current date is 2017-12-21; (09:27:04) Loading: ; (09:27:04) Loading: 0 packages loaded; (09:27:05) Loading: 0 packages loaded; (09:27:06) Loading: 7 packages loaded; currently loading: deepvariant/core/genomics ... (6 packages); (09:27:07) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:08) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:09) Analyzing: 242 targets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:3986,cache,cache,3986,,https://github.com/google/deepvariant/issues/19,2,"['cache', 'load']","['cache', 'load']"
Performance,61] Running tune at step=13993 epoch=0; I0829 07:31:25.152109 140318776715072 train.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 230,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:10389,Tune,Tune,10389,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"62 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12149,tune,tune,12149,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"6447671104 call_variants.py:592] Use saved model: True; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in main; call_variants(; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 598, in call_variants; model_example_shape = dv_utils.get_shape_and_channels_from_json(; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/dv_utils.py"", line 367, in get_shape_and_channels_from_json; example_info = json.load(f); File ""/usr/lib/python3.8/json/__init__.py"", line 293, in load; return loads(fp.read(),; File ""/usr/lib/python3.8/json/__init__.py"", line 357, in loads; return _default_decoder.decode(s); File ""/usr/lib/python3.8/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/usr/lib/python3.8/json/decoder.py"", line 355, in raw_decode; raise JSONDecodeError(""Expecting value"", s, err.value) from None; json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0). Process ForkProcess-1:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 454, in post_processing; item = output_queue.get(timeout=180); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 108, in get; raise Empty; _queue.Empty. real 3m2.335s; user ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:6646,load,load,6646,,https://github.com/google/deepvariant/issues/869,2,['load'],"['load', 'loads']"
Performance,"66404 140318776715072 local.py:41] Setting work unit notes: 0.3 steps/s, 61.4% (13948/22724), ETA: 8h42m; I0829 07:28:44.568708 140305134778112 logging_writer.py:48] [13948] steps_per_sec=0.280075; I0829 07:28:44.568793 140305134778112 logging_writer.py:48] [13948] uptime=74874.6; I0829 07:31:25.151819 140318776715072 train.py:361] Running tune at step=13993 epoch=0; I0829 07:31:25.152109 140318776715072 train.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 /",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:10060,Tune,Tune,10060,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"6715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12602,tune,tune,12602,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"690693 140325876573952 deprecation.py:323] From /tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0924 03:47:37.814187 140325876573952 deprecation.py:323] From /tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0924 03:47:38.164505 140325876573952 estimator.py:1147] Calling model_fn.; W0924 03:47:38.168455 140325876573952 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0924 03:47:41.667636 140325876573952 estimator.py:1149] Done calling model_fn.; I0924 03:47:42.548214 140325876573952 monitored_session.py:240] Graph was finalized.; 2020-09-24 03:47:42.549039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: ; name: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.77; pciBusID: 0000:21:00.0; 2020-09-24 03:47:42.549107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0; 2020-09-24 03:47:42.549121: I ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:8312,optimiz,optimizations,8312,,https://github.com/google/deepvariant/issues/358,1,['optimiz'],['optimizations']
Performance,6] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 260,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:10638,Tune,Tune,10638,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"6] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, trai",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12453,tune,tune,12453,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994]",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12244,tune,tune,12244,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"72 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',); /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels.; input_shape = imagenet_utils.obtain_input_shape(; I0828 10:40:47.952382 140318776715072 keras_modeling.py:325] Number of l2 regularizers: 95.; I0828 10:40:48.007193 140318776715072 keras_modeling.py:362] inceptionv3: load_weights from checkpoint: /home/training_outs/epoch1//checkpoints/ckpt-5997; I0828 10:40:49.193293 140318776715072 train.py:191] Exponential Decay: initial_learning_rate=0.0001; decay_steps=45448; learning_rate_decay_rate=0.947; I0828 10:40:49.193522 140318776715072 train.py:203] Use LinearWarmup:; warmup_steps=10000; warmup_learning_rate=1e-05; I0828 10:40:49.401860 140318776715072 keras_modeling.py:472] Restored checkpoint ckpt-5997 at step=0. tune/f1_weighted=tf.Tensor(0.0, shape=(), dtype=float32); WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.; Instructions for updating:; Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089; W0828 10:40:49.488072 140318776715072 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be rem>; Instructions for updating:; Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/5",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:4946,tune,tune,4946,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"72 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 1403",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12195,tune,tune,12195,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"7403021002560 call_variants.py:592] Use saved model: True; 2024-06-19 14:57:57.916727: F tensorflow/tsl/platform/env.cc:391] Check failed: -1 != path_length (-1 vs. -1); Fatal Python error: Aborted. Current thread 0x00002b1ce03a6740 (most recent call first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 414 in import_graph_def; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/function_def_to_graph.py"", line 87 in func; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/function_deserialization.py"", line 416 i; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 154 in __init__; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 958 in load_partial; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 828 in load; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 596 in call_; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768 in main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789 in <modu. real 0m5.038s; user 0m3.921s; sys 0m1.122s; Process ForkProcess-1:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 454, in post; Fil",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:3722,load,load,3722,,https://github.com/google/deepvariant/issues/833,1,['load'],['load']
Performance,"7403021002560 dv_utils.py:370] From /tmp/make_examples.tfrecord-00000-of-00010.gz.example_info; I0619 14:57:56.063441 47403021002560 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0619 14:57:56.063909 47403021002560 call_variants.py:592] Use saved model: True; 2024-06-19 14:57:57.916727: F tensorflow/tsl/platform/env.cc:391] Check failed: -1 != path_length (-1 vs. -1); Fatal Python error: Aborted. Current thread 0x00002b1ce03a6740 (most recent call first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 414 in import_graph_def; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/function_def_to_graph.py"", line 87 in func; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/function_deserialization.py"", line 416 i; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 154 in __init__; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 958 in load_partial; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 828 in load; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 596 in call_; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768 in main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789 in <modu. real 0m5.038s; user 0m3.921s; sys 0m1.122s; Process ForkProcess-1:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:3504,load,load,3504,,https://github.com/google/deepvariant/issues/833,1,['load'],['load']
Performance,"775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.551992654800415, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0, train/rec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12932,tune,tune,12932,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_pos",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12684,tune,tune,12684,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker; put((job, i, (False, wrapped))); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe; Process ForkPoolWorker-42:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes; self._send(buf); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _sen",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804:3419,queue,queues,3419,,https://github.com/google/deepvariant/issues/804,1,['queue'],['queues']
Performance,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.1 Repository rule _tf_http_archive defined at:; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.2 (21:51:08) Loading: 0 packages loaded; #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s; #16 1497.3 (21:51:09) INFO: 0 processes.; #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded); #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded); #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1; ------; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; ------; executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:8743,cache,cache,8743,,https://github.com/google/deepvariant/issues/608,3,"['cache', 'load']","['cache', 'loaded']"
Performance,"86048 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation.; I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF.; I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz; I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter; I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s; user 45m18.578s; sys 30m4.764s; Process ForkPoolWorker-83:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker; put((job, i, (False, wrapped))); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804:2526,queue,queues,2526,,https://github.com/google/deepvariant/issues/804,1,['queue'],['queues']
Performance,"9 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12747,tune,tune,12747,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:; #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.1 Repository rule _tf_http_archive defined at:; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.2 (21:51:08) Loading: 0 packages loaded; #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s; #16 1497.3 (21:51:09) INFO: 0 processes.; #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded); #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded); #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:8392,Load,Loading,8392,,https://github.com/google/deepvariant/issues/608,2,"['Load', 'load']","['Loading', 'loaded']"
Performance,"948/22724), ETA: 8h42m; I0829 07:28:44.568708 140305134778112 logging_writer.py:48] [13948] steps_per_sec=0.280075; I0829 07:28:44.568793 140305134778112 logging_writer.py:48] [13948] uptime=74874.6; I0829 07:31:25.151819 140318776715072 train.py:361] Running tune at step=13993 epoch=0; I0829 07:31:25.152109 140318776715072 train.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:10142,Tune,Tune,10142,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"9521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:31:39.810043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:31:59.620996: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:31:59.623967 140288433825600 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpd74of138; I0217 23:31:59.629002 140288433825600 run_deepvariant.py:551] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load input/weights-51-0.995354.ckpt* instead. ***** Intermediate results will be written to /tmp/tmpd74of138 in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = ""en_US:en"",; 	LC_ALL = (unset),; 	LC_ADDRESS = ""en_US.UTF-8"",; 	LC_NAME = ""en_US.UTF-8"",; 	LC_MONETARY = ""en_US.UTF-8"",; 	LC_PAPER = ""en_US.UTF-8"",; 	LC_IDENTIFICATION = ""en_US.UTF-8"",; 	LC_TELEPHONE = ""en_US.UTF-8"",; 	LC_MEASUREMENT = ""en_US.UTF-8"",; 	LC_CTYPE = ""C.UTF-8"",; 	LC_TIME = ""en_US.UTF-8"",; 	LC_NUMERIC = ""en_US.UTF-8"",; 	LANG = ""en_US.UTF",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:4331,load,load,4331,,https://github.com/google/deepvariant/issues/774,1,['load'],['load']
Performance,": Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled; WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7; W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7; INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000; I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c; I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz; INFO:tensorflow:Calling model_fn.; I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn.; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:2473,optimiz,optimization,2473,,https://github.com/google/deepvariant/issues/679,1,['optimiz'],['optimization']
Performance,":24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12422,tune,tune,12422,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,":56.063441 47403021002560 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0619 14:57:56.063909 47403021002560 call_variants.py:592] Use saved model: True; 2024-06-19 14:57:57.916727: F tensorflow/tsl/platform/env.cc:391] Check failed: -1 != path_length (-1 vs. -1); Fatal Python error: Aborted. Current thread 0x00002b1ce03a6740 (most recent call first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 414 in import_graph_def; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/function_def_to_graph.py"", line 87 in func; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/function_deserialization.py"", line 416 i; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 154 in __init__; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 958 in load_partial; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 828 in load; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 596 in call_; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768 in main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789 in <modu. real 0m5.038s; user 0m3.921s; sys 0m1.122s; Process ForkProcess-1:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/tm",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:3611,load,load,3611,,https://github.com/google/deepvariant/issues/833,1,['load'],['load']
Performance,"; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \; --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \; --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \; --regions ""Chromosome4"" \; --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(; I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started.; I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:4406,load,load,4406,,https://github.com/google/deepvariant/issues/797,1,['load'],['load']
Performance,; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists...,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:3829,Load,Load,3829,,https://github.com/google/deepvariant/issues/89,1,['Load'],['Load']
Performance,"; 2024-02-17 23:32:31.108506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.006781: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.007601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.110201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; ...; 2024-02-17 23:33:25.887517: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:33:25.933275 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:25.939588 140533724936000 make_examples_core.py:301] Task 15/16: Preparing inputs; I0217 23:33:25.967685 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:26.024591 140533724936000 make_examples_core.py:301] Task 15/16: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'ch",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:7303,load,load,7303,,https://github.com/google/deepvariant/issues/774,1,['load'],['load']
Performance,"; I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12663,tune,tune,12663,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"; app.run(main); File ""/tmp/Bazel.runfiles_2wmov4iu/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2wmov4iu/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2wmov4iu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_2wmov4iu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1746, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_2wmov4iu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1653, in processing_regions_from_options; ref_contigs = fasta.IndexedFastaReader(; File ""/tmp/Bazel.runfiles_2wmov4iu/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 105, in __init__; self._reader = reference.IndexedFastaReader.from_file(; ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /input/chr19_new.fa; [E::idx_find_and_load] Could not retrieve index file for '/input/A_J.chr19.bam'; I1114 07:56:46.139744 140275925796672 genomics_reader.py:222] Reading /input/A_J.chr19.bam with NativeSamReader; I1114 07:56:46.142346 140275925796672 make_examples_core.py:243] Task 0/2: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /input/chr19_new.fa.fai: No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1n42qf4z/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_1n42qf4z/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_1n42qf4z/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1n42qf4z/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main; make_examples_core.make_examples_runner(options); File ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/585:2502,load,load,2502,,https://github.com/google/deepvariant/issues/585,1,['load'],['load']
Performance,"=========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-02-17 23:31:25.687399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-02-17 23:31:39.809521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:31:39.810043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:31:59.620996: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:31:59.623967 140288433825600 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpd74of138; I0217 23:31:59.629002 140288433825600 run_deepvariant.py:551] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load input/weights-51-0.995354.ckpt* instead. ***** Intermediate results will be ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:3400,load,load,3400,,https://github.com/google/deepvariant/issues/774,1,['load'],['load']
Performance,"As bed lengthened, SNP performed better, but indel on the contrary",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/616:23,perform,performed,23,,https://github.com/google/deepvariant/issues/616,1,['perform'],['performed']
Performance,"Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-package",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339:1150,Tune,Tune,1150,,https://github.com/google/deepvariant/issues/339,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"Congratulations on the new release, and glad to see that Intel MKL was utilized for optimized processing. When you have a chance, could you please document the following and provide use-cases for new users:. https://github.com/google/deepvariant/blob/r0.7/cloudbuild.yaml. https://github.com/google/deepvariant/blob/r0.7/cloudbuild_CBI.yaml. Thanks,; ~p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87:84,optimiz,optimized,84,,https://github.com/google/deepvariant/issues/87,1,['optimiz'],['optimized']
Performance,"Context: issue #116 . Htslib integration with GCS doesn't load app default credential from worker, and thus is only able to read from public bucket. The workaround is to localize BED file into VM worker prior running make_examples.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/119:58,load,load,58,,https://github.com/google/deepvariant/issues/119,1,['load'],['load']
Performance,"Dear Deepvariant team,. My collection of data contains samples with non-diploid genomes. As Deepvariant model was built and trained based on diploid data, I am wondering if the team have evaluated how Deepvariant performs on non-diploid dataset (e.g. triploid or tetrapliod)? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/121:213,perform,performs,213,,https://github.com/google/deepvariant/issues/121,1,['perform'],['performs']
Performance,"Dear Devs, . I am currently training a model (starting from wgs.1.6.1) for use in a fish species. The programs are running well, I have confident regions and truth variants defined, and am currently tuning hyperparameters to optimise the training. . However . . . . I notice when tracking the model eval stats (specifically f1, precision, recall), that the hom_ref classifications are much less reliable than hom_alt and het classes. My question is whether this is to be expected, or whether there might be something wrong with my training setup, or perhaps the examples. . The test example set I am using to tune the hyperparams looks like this:. ```; # Generated by shuffle_tfrecords_beam.py; # class2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such traini",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:609,tune,tune,609,,https://github.com/google/deepvariant/issues/904,1,['tune'],['tune']
Performance,"Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format.; WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_n",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:255,load,load,255,,https://github.com/google/deepvariant/issues/722,1,['load'],['load']
Performance,"Dear developers; . Thanks for the great tool for variant calling. I use deepvariant 1.6.1 for small variant calling from picbio HIFI data (Revio system). For HIFI data, there are two ways to get the input bam for deepvariant. The first, like we do in NGS data analysis, extracting fastq reads from raw HIFI bam and mapping the fastq reads to reference genome, eg. T2TCHM13v2, then get the input bam ( fastq-mapping bam ) ```sample.pbmm2.bam```. ```bash. bam2fastq -o sample.fastq raw.bam; pbmm2 align CHM13.fa sample.fastq sample.pbmm2.bam --preset HIFI. ##; run deepvarint with sample.pbmm2.bam . ```. The second, mapping from bam data with methylation signal to reference genome directly (bam-to-bam mapping) , ```sample.pbmm2.jasmine.bam```. ```bash. jasmine raw.bam raw.jasmine.bam # get 5mC methylation informations; pbmm2 align CHM13.fa raw.jasmine.bam sample.pbmm2.jasmine.bam --preset HIFI # bam-to-bam mapping, keep methylation tags in bam file. ##; run deepvarint with sample.pbmm2.jasmine.bam. ```. The first way bam for small variant calling makes no mistakes. But considering the following methylation analysis also needs to map reads to reference genome, so if it is suitable for using mapped bam with methylation tags to call small variant, we only need to map once, this can save time and computational resources. So my question is, is that OK for us to use bam with methylation tags as the input of deepvariant? Is the performance difference between using ```fastq-mapping bam``` and ```bam-to-bam mapping bam```?. best, . Wilson",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/888:1436,perform,performance,1436,,https://github.com/google/deepvariant/issues/888,1,['perform'],['performance']
Performance,"Deepvariant fails without clear reason. . **Setup**; JHU Rockfish HPC; Singularity 3.8.7; singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:; ```; #!/bin/bash; #SBATCH --job-name=deep64_13448198; #SBATCH --time=24:00:00; #SBATCH --nodes=2; #SBATCH --ntasks-per-node=1; #SBATCH --cpus-per-task=32; #SBATCH --mem=0. ml anaconda; conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref c_elegans.PRJNA13758.WS245.genomic.fa \; --reads aln13448198.pbmm2.bam \; --output_vcf aln13448198.pbmm2.dv.vcf.gz \; --num_shards 64; ```. Here's a long snippet of slurm output:; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****; time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]; # this part is likely unimportant. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:982,cache,cached,982,,https://github.com/google/deepvariant/issues/614,1,['cache'],['cached']
Performance,"FLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (08:09:38) INFO: Current date is 2017-12-08; (08:09:38) WARNING: /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/core/BUILD:1806:1: in includes attribute of cc_library rule @org_tensorflow//tensorflow/core:framework_headers_lib: '../../../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'external/org_tensorflow/tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/tensorflow.bzl:1100:30; (08:09:38) INFO: Analysed 241 targets (0 packages loaded).; (08:09:38) INFO: Found 185 targets and 56 test targets...; (08:09:38) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1: //deepvariant/core/protos:core_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1 1 input file(s) do not exist; (08:09:38) INFO: Elapsed time: 0.334s, Critical Path: 0.00s; (08:09:38) FAILED: Build did NOT complete successfully; //deepvariant:allelecounter_test NO STATUS; //deepvariant:call_variants_test NO STATUS; //deepvariant:data_providers_test NO STATUS; //deepvariant:make_examples_test NO STATUS; //deepvariant:model_eval_test NO STATUS; //deepvariant:model_train_test NO STATUS; //deepvariant:modeling_test NO STATUS; //deepvariant:pileup_image_test NO STATUS; //deepvariant:postprocess_variants_lib_test NO STATUS; //deepvariant:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6:3171,load,loaded,3171,,https://github.com/google/deepvariant/issues/6,1,['load'],['loaded']
Performance,Generalized performance analysis between the versions,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:12,perform,performance,12,,https://github.com/google/deepvariant/issues/50,1,['perform'],['performance']
Performance,"Hello! I'm experiencing an issue when trying to run make_examples. Instead of Docker we're using Singularity, and deepvariant has run before with just calling the run_deepvariant.py. . For example, this is what has worked for us in the past in our environment: . > module load singularity; > source activate $condapath/DeepVariant. > singularity exec $softwarepath/Singularity_files/deepvariant_1.5.0.sif python3 $softwarepath/deepvariant/run_deepvariant.py [...]. When trying to run make_examples, this code:. > singularity exec $softwarepath/Singularity_files/deepvariant_1.5.0.sif python3 $softwarepath/deepvariant/deepvariant/make_examples.py [...] . is now throwing this error code: . > Traceback (most recent call last):; > File ""/$softwarepath/deepvariant/deepvariant/make_examples.py"", line 35, in <module>; > from deepvariant import dv_constants; > ModuleNotFoundError: No module named 'deepvariant'. Does this mean there is a problem with our install? Any ideas or suggestions? . Thank you very much for any light you can shed on this issue!. Best, ; Haley",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/771:272,load,load,272,,https://github.com/google/deepvariant/issues/771,1,['load'],['load']
Performance,"Hello! I've found a performance issue in deepvariant/data_providers.py: `batch()` should be called before `map()`, which could make your program more efficient. Here is [the tensorflow document](https://tensorflow.google.cn/guide/data_performance?hl=zh_cn#vectorized_mapping) to support it. Detailed description is listed below:. - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size, drop_remainder=True)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L316) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L314).; - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L364) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L362). Besides, you need to check the function called in `map()`(e.g., `self.parse_tfexample` called in `dataset.map()`) whether to be affected or not to make the changed code work properly. For example, if `self.parse_tfexample` needs data with shape (x, y, z) as its input before fix, it would require data with shape (batch_size, x, y, z). Looking forward to your reply. Btw, I am very glad to create a PR to fix it if you are too busy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/479:20,perform,performance,20,,https://github.com/google/deepvariant/issues/479,1,['perform'],['performance']
Performance,"Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed; . /etc/profile.d/modules.sh; module load xxxxx/singularity/3.5.3. # inputs; reference=$2; bam=$1.final.bam; sampleid=$1; outdir=deepvar. # Create output directories; if [ ! -e deepvar ]; then mkdir deepvar; fi; if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches; if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi; export SINGULARITY_TMPDIR=$PWD/.singularity; export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image; if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant; singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${reference} \; --reads=${bam} \; --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \; --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \; --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/543:436,load,loaded,436,,https://github.com/google/deepvariant/issues/543,4,"['Load', 'cache', 'load']","['Load', 'caches', 'load', 'loaded']"
Performance,"Hello, ; I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this?. `docker pull google/deepvariant:deeptrio-1.5.0`; ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply.; Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/718:80,perform,performed,80,,https://github.com/google/deepvariant/issues/718,1,['perform'],['performed']
Performance,"Hello, ; I used DeepVariant with the mosquito trained model, on 11 samples, then performed joint calling with GLnexus. In the following track, in the first sample it calls 2 SNP G/A and T/C (corresponding to the 2 last blue box) in the D2A1 sample. However, in the D5B3 sample the sites are called as 0/0. There is no ""RefCall"" either so I think it means it did not even generate candidates. ; Point to note: I have no idea if there are, or not, SNPs at those 2 loci. But from the alignment, it is not at clear to me why in one case it thought there were SNPs, and in the other case it thought not. . For the same sites, GATK joint caller decided the site was 0/0 for all samples (again, I don't know which one is the true genotype there). The bam I am showing here are the diagnostic bams emitted by DeepVariant, so my understanding is that I see what it saw. ![igv_snapshot](https://user-images.githubusercontent.com/23341393/80101296-7e995c80-8571-11ea-8e3d-37e306442888.png). As you might notice, the coverage depth across my samples is not always identical. So I wonder if it's not simply a question of coverage (though my lowest average coverage value is 30, which is ok I think). . Would you have any clue of what might be happening? . Thank a lot. EDIT: I can get rid of those regions by filtering on QUAL on the GLnexus pVCF, however. But I am still curious, as I might want to keep them and filter them otherwise (not easy to reach a spot where you get read of FP without removing all the TP).",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/303:81,perform,performed,81,,https://github.com/google/deepvariant/issues/303,1,['perform'],['performed']
Performance,"Hello, ; Operatin system: Linux HPC ; Version: 1.3.0 ; Installation: Singularity ; Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**; **Command**; ```; `#!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p compute; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-23; #SBATCH --time=12:00:00; #SBATCH --mem-per-cpu=128GB. module purge; module load parallel; module load singularity; EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27; HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna; PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam; BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed; OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz; OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz; INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$BED_REGIONS \; --output_vcf=$OUTPUT_VCF \; --output_gvcf=$OUTPUT_GVCF \; --intermediate_results_dir=$INTERMEDIATE_RESULTS""; ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/542:640,load,load,640,,https://github.com/google/deepvariant/issues/542,2,['load'],['load']
Performance,"Hello, I am trying to install DeepVariant on an IBM Power 8 machine within a docker container. I get the following error during ./build_and_test.sh, which I understand is tied to Intel SSE2 instruction set. `external/libssw/src/ssw.c:38:23: fatal error: emmintrin.h: No such file or directory`. I did `export DV_USE_GCP_OPTIMIZED_TF_WHL=0` from the command line before running the compile. I also changed `DV_COPT_FLAGS` to `--copt=-Wno-sign-compare --copt=-Wno-write-strings` within settings.sh (removing the corei7 option). I am using bazel version '0.15.0-' (settings.sh is changed to reflect this). I am using scikit-learn=0.20 (run-prereq.sh changed to reflect this). pyclif was compiled from source. Is there a way to circumvent this error? The complete error message is as follows. ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/libssw/BUILD.bazel:11:1: C++ compilation of rule '@libssw//:ssw' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction; -sections -fdata-sections -MD -MF bazel-out/ppc-opt/bin/external/libssw/_objs/ssw/external/libssw/src/ssw.pic.d -fPIC -iquote external/libssw -iquote bazel-out/ppc-opt/genfiles/external/libssw -iquote ext; ernal/bazel_tools -iquote bazel-out/ppc-opt/genfiles/external/bazel_tools -Wno-maybe-uninitial",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123:803,cache,cache,803,,https://github.com/google/deepvariant/issues/123,1,['cache'],['cache']
Performance,"Hello, I trained a customized model, and am now trying to test it. However, when I try to run it, it says that the model files in the checkpoint do not exist. . Here is the command I tried to run: . > module load apptainer; > ; > apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \; > --model_type WGS \; > --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/modeltrainout/2fullindividualmodel/checkpoints/ckpt-14902"" \; > --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/Bactrocera_dorsalis_rearing_male_mt_chr_unpl.fasta"" \; > --reads ""${filesdir}_mapped/${sample}.bam"" \; > --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/modeltrainout/modeltestout/2fullindividualmodeltest/${sample}.vcf.gz"". Here are the contents of the checkpoints folder for this training: . > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jun 29 01:06 ..; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 1 22:49 .; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 ckpt-14902; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-7451.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-7451.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-14902.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-14902.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 266 Aug 6 22:51 checkpoint. and finally, here are the contents of ckpt-14902: . > total 7.6M; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 1 22:49 ..; > drwxr-s--- 2 haley.arnold proj-pbarc 4.0K Jul 1 22:49 variables; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 .; > -rw-r----- 1 haley.arnold proj-pbarc 6.9M Aug 6 22:51 saved_model.pb; > -rw-r----- 1 haley.arnold proj-pbarc 677K Aug 6 22:51 keras_metadata.pb; > -rw-r----- 1 haley.arnold proj-pbarc 55 Aug 6 22:51 fingerprint.pb; > -rw-r----- 1 haley.arnold proj-pbarc 80 Aug 6 22",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/866:208,load,load,208,,https://github.com/google/deepvariant/issues/866,1,['load'],['load']
Performance,"Hello, after some quite impressive results applying HiSeq-trained DeepVariant on MGISEQ-2000 data, I've been working on achieving even better performance by retraining DeepVariant specifically for the MGISEQ-2000. To do this I've been broadly following the sketch at https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. I now have unshuffled tfrecords for six reference samples, and have some questions about next steps. Because it might be relevant to the questions, my data are structured as follows:; * 6 samples (4xHG001/NA12878, 2xHG005/NA24631), each with:; * 25 tfrecord shards (00000-00024) of chr1, for tuning (perhaps over-optimistically) ; * 247 tfrecord shards (00000-00246) of chr2-19, no downsampling, for training; * 247 tfrecord shards (00000-00246) of chr2-19, 50% downsampling, for training; * 17 tfrecord shards (00000-00016) of chr20-22, for validation. My questions are:; 1. Is it necessary to shuffle the training data? I ask as it's proving to be a bit laborious to set up, and so I'm hoping that I can get around it. Given I have so many shards, if I just shuffle the order of the chr2-19 shards when I supply them to the training loop, will this be almost as good as shuffling the whole dataset?; 2. Is it necessary to shuffle the validation data? The tutorial does this, but I'm not sure why.; 3. How can I supply multiple datasets to the training loop (here effectively 12 datasets: 6 samples x 2 downsampling settings)? In the tutorial, `model_train` is supplied a wildcard path of `validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz`, which seems like it would only work for a single sample, and I'm not sure how this will work with multiple samples.; 4. Have there been any changes to the code base to better support warmstarting, or is the advice at https://github.com/google/deepvariant/issues/185 still the best approach to fine-tuning the model?. DeepVariant is a fantastic tool and I'm very much looking forward to see",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312:142,perform,performance,142,,https://github.com/google/deepvariant/issues/312,1,['perform'],['performance']
Performance,"Hello,. Deepvariant is reported to work well with WGS data from the Element AVITIâ„¢ System.; #623 ; https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data?; Is it possible to use the current WES model or is it still required to update the WES model?. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/703:190,perform,perform,190,,https://github.com/google/deepvariant/issues/703,1,['perform'],['perform']
Performance,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/443:788,cache,cache,788,,https://github.com/google/deepvariant/issues/443,2,"['cache', 'load']","['cache', 'loaded']"
Performance,"Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? ; I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)?. Here's the code I'm running from the Quickstart:; ```; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules; module load singularity; module load cuda-dcgm/2.2.9.1; module load cuda11.4/toolkit; module load cuda11.4/blas; module load cuda11.4/nsight; module load cuda11.4/profiler; module load cuda11.4/fft; source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh; conda activate TensorFlow_GPU. # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. And here's my error:; ```; 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514:324,load,loaded,324,,https://github.com/google/deepvariant/issues/514,9,"['Load', 'load']","['Load', 'load', 'loaded']"
Performance,"Hello,. I'm trying to run Deepvariant using singularity. I just followed the ""Notes on Singularity"" section in quick start test (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md), and I got an error regarding numpy as below. Could you help me resolve this issue? I used deepvariant_1.6.0 image. ```; 2023-12-02 23:23:35.126320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I1202 23:23:41.449015 46912500266816 run_deepvariant.py:519] Re-using the directory for intermediate results in /flashscratch/kimkw/tmp/tmppin2lwy5. ***** Intermediate results will be written to /flashscratch/kimkw/tmp/tmppin2lwy5 in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:439,optimiz,optimized,439,,https://github.com/google/deepvariant/issues/746,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"Hello,. I've been trying to set up the **google/deepvariant:1.6.1-gpu** or **google/deepvariant:latest-gpu** image on a GPU instance, but I've encountered the error message mentioned below when running the **run_deepvariant** or **train** scripts, and despite generating the flags (screenshot) as expected, I believe those incompatible/missing TensorRT libraries are preventing these scripts from using the GPU. **Command used:** ; ` sudo docker run --runtime=nvidia --gpus 1 google/deepvariant:1.6.1-gpu train --help; `. **Error message:**; ```; 2024-05-08 15:11:26.358196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-05-08 15:11:26.358229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; ```. ![image](https://github.com/google/deepvariant/assets/169280348/fd17bf4e-0b6c-46b7-b5e8-74a3525d07a5)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819:662,load,load,662,,https://github.com/google/deepvariant/issues/819,1,['load'],['load']
Performance,"Hello,. I've been using DeepTrio for de novo variant analysis, and it's been performing excellently. However, I noticed from the logs that DeepTrio uses the CPU to prepare data, which is quite time-consuming. In my case, it took 12 hours for one trio analysis, with an additional 4 hours on the GPU. Given that renting GPU servers( usually with less CPU) is more expensive than CPU servers and access to privately owned GPU servers is limited, it seems inefficient to run lengthy CPU processes on a GPU server. It feels like a bit of a waste, and sometimes I half-jokingly feel there might be someone out there with murderous intent because of it!. Would it be possible in future updates to partition the DeepTrio analysis into separate steps? This way, CPU-intensive tasks could be completed on a CPU server, and then the job could be transferred to a GPU server for the remaining tasks. Alternatively, could the data preparation (CPU) and analysis (GPU) be run at the same time? This would help optimize resource usage and reduce costs. Thank you for considering these suggestions.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/873:77,perform,performing,77,,https://github.com/google/deepvariant/issues/873,2,"['optimiz', 'perform']","['optimize', 'performing']"
Performance,"Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:; ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; FAMILY_ID=Case1; PROBAND_ID=Case1_proband; MOTHER_ID=Case1_mother; FATHER_ID=Case1_father; SIBLING_ID=.; PED=$FAMILY_ID.ped. MOTHER_PRESENT=true; FATHER_PRESENT=true; SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam; FATHER_BAM=${FATHER_ID}.sorted.bam; MOTHER_BAM=${MOTHER_ID}.sorted.bam; SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz; FATHER_VCF=${FATHER_ID}.vcf.gz; MOTHER_VCF=${MOTHER_ID}.vcf.gz; SIBL",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:643,Load,Load,643,,https://github.com/google/deepvariant/issues/518,3,"['Load', 'load']","['Load', 'load']"
Performance,"Hello,. There is a way to perform some kind of VQSR or use truth sets for calling on Deepvariant?. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/324:26,perform,perform,26,,https://github.com/google/deepvariant/issues/324,1,['perform'],['perform']
Performance,"Helloï¼ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5.; First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant.; I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp.; Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py.; The final outcome is very good, but I find a detail didn't make sense: as the bed lengthenedï¼Œthe SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below.; ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png); Can you give me a detailed explanation of this detailï¼Ÿ Thank you very muchï¼ ; Finally, thank you very much for developing such a great toolï¼",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/616:736,perform,performed,736,,https://github.com/google/deepvariant/issues/616,1,['perform'],['performed']
Performance,"Hi @pichuan,; I did come across various bits in different documents on how to train custom checkpoint but i don't have a confident on handle on it before I embark. - Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5); - Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?; - Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5); Thank you for guidance!; -Daniel",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/765:392,perform,perform,392,,https://github.com/google/deepvariant/issues/765,1,['perform'],['perform']
Performance,"Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnâ€™t necessary for the Exome dataset (for the provided alignment from Genos). Iâ€™ve re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets w",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:775,perform,performing,775,,https://github.com/google/deepvariant/issues/171,1,['perform'],['performing']
Performance,"Hi Mark (@depristo),. Sorry, I meant to put this together a while ago - regarding https://github.com/google/deepvariant/issues/27#issuecomment-364683474 - but got a bit swamped with a research deadline. In any case, this is purely for intellectual curiosity and discussion. Regarding the first point, where differences in allocated CPUs might be the cause for the timing, that could be remedied by specifying a minimal CPU requirement, as noted here:. https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform. So to control for the variability in the test, the two options are either: a) to set the `--min-cpu-platform` setting to the maximum available (`""Intel Sandy Bridge""`), or b) to keep requesting and canceling instances until the desired one is allocated on which all tests should be performed, thus satisfying consistency. Just as a quick inspection, by looking at the CPU cycles utilization, I just ran a performance analysis of 0.4 and 0.5.1 on `make_examples` - since it displayed the initial discrepancy - and there seem to be some slight increases in `0.5.1`, which might cumulatively affect things. In any case, below is the top of the call-graph of percent utilization by method (per version):. #### DV 0.4. ```; # Samples: 186K of event 'cpu-clock'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--lea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:808,perform,performed,808,,https://github.com/google/deepvariant/issues/50,2,['perform'],"['performance', 'performed']"
Performance,"Hi Team,. I was trying to run below command for call_variants . docker run -it -v /gpfs/projects/bioinfo/najeeb/withKhalid/PMC/:/dv2/PMC01/ -v ${PWD}:/${PWD} -v /gpfs/data_jrnas1/ref_data/Homo_sapiens/GRCh37/Sequences/:/dv2/WholeGenomeSequence/ gcr.io/deepvariant-docker/deepvariant:0.7.0 /opt/deepvariant/bin/call_variants --outfile ""$PWD/PMC01-01_AB082422_S5_L005_R1_001_output.tfrecord.gz"" --examples ""$PWD/PMC01-01_AB082422_S5_L005_R1_001.tfrecord.gz"" --checkpoint ${PWD}/models/model.ckpt. But for some reason , I am getting below error. I1108 10:29:03.150824 140295000123136 call_variants.py:283] Set KMP_BLOCKTIME to 0; 2018-11-08 10:29:03.161879: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 FMA; 2018-11-08 10:29:03.168258: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1108 10:29:03.176211 140295000123136 modeling.py:318] Initializing model with random parameters; W1108 10:29:03.177896 140295000123136 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpkXijQm; I1108 10:29:03.178158 140295000123136 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_session_config': None, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9889d6e750>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmpkXijQm', '_train_distribute': None, '_save_summary_steps': 100}; I1108 10:29:03.178364 140295000123136 call_variants.py:341] Writing calls to /gpfs/project",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/117:946,Tune,Tune,946,,https://github.com/google/deepvariant/issues/117,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"Hi deepvariant developer,; Is DeepVariant suitable for performing variation detection analysis, particularly **calling SNP and indel information**, using **bam formate** alignment results obtained from **vg software pan-genome analysis** and NGS resequencing data as input files?. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/667:55,perform,performing,55,,https://github.com/google/deepvariant/issues/667,1,['perform'],['performing']
Performance,"Hi i have deepvariant 1.5.0 version singularity SIF file,; I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below; #nextflow.config; ```; singularity {; process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'; cacheDir = ""/data/shared/clinical/LongRead/cache/""; singularity.enabled = true; singularity.autoMounts = true; SINGULARITY_BINDPATH = ""/data""; }. conda; {; enabled = true; cacheDir = ""/data/shared/clinical/LongRead/Programs/""; }; params; {; path=""/data/shared/clinical/LongRead/""; ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""; pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi""; at=18; st=6; data_input=""/data/shared/clinical/LongRead/Data/""; }; ```; #deepvariant.nf; ```; process pbc_varicall {; publishDir ""/data/shared/clinical/LongRead/Data/resources/""; container 'docker://google/deepvariant:1.5.0'. input:; path 'fa'; output:; file ""*""; path 'm84011_220902_175841_NF_sif.vcf.gz'. script:; """"""; run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40; """"""; }. workflow {; fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""); pbc_varicall(fa); }; ```; after running for several hours i do not get any output, instead during run , i get msg as; `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `; this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659:285,cache,cacheDir,285,,https://github.com/google/deepvariant/issues/659,3,['cache'],"['cache', 'cacheDir']"
Performance,"Hi, . Is it possible to provide the number of forward-strand and reverse-strand reads supporting the reference allele and the alternate allele for each variant locus?. In short, if I want to keep the variant loci supported by both forward and reverse strands, what should I do? Because DeepVariant performs realignment, there may be differences between the alignment of the original BAM file and the after realigned. Best,; Wenfei",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/630:298,perform,performs,298,,https://github.com/google/deepvariant/issues/630,1,['perform'],['performs']
Performance,"Hi, ; I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". ; I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. ; FranÃ§ois",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/779:179,perform,performed,179,,https://github.com/google/deepvariant/issues/779,1,['perform'],['performed']
Performance,"Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/355:270,cache,cache,270,,https://github.com/google/deepvariant/issues/355,1,['cache'],['cache']
Performance,"Hi, I was working on update the deepvariant source code from ubuntu 16.04 to 18.04 and python 3.6 to python 3.8. Now I met a problem in build_release_binaries.shell scripts. . bazel build -c opt \; --output_filter=DONT_MATCH_ANYTHING \; --noshow_loading_progress \; --show_result=0 \; ${DV_COPT_FLAGS} \; --build_python_zip \; :binaries. The error is below:; [1,442 / 1,802] Compiling third_party/nucleus/protos/struct.pb.cc; 1s local ... (128 actions, 48 running); (17:42:57) [1,544 / 1,802] Compiling external/org_tensorflow/tensorflow/core/util/test_log.pb.cc; 6s local ... (128 actions, 47 running); (17:43:03) ERROR: /opt/deepvariant/deepvariant/realigner/python/BUILD:54:1: C++ compilation of rule '//deepvariant/realigner/python:ssw_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python3.8 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; TF_CONFIGURE_IOS=0 \; TF_ENABLE_XLA=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.d '-frandom-seed=bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o' -fPIC -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DHAVE_SYS_UIO_H -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/libssw -iquote bazel-out/k8-opt/bin/external/libssw -iquote external/org_tensorflow -iquote bazel-out/k8-opt/bin/external/org_tensorflow -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441:813,cache,cache,813,,https://github.com/google/deepvariant/issues/441,1,['cache'],['cache']
Performance,"Hi, I'm trying to visualize the pileup images generated by DeepVariant. The images for SNP sites and deletions seem to be straightforward, but I found those for insertions are rather confusing. The reference lines for insertion sites are still continuous, and at the point where the insertion happens, the bases on the sequenced reads are set to 0. Here's part of an example of a homozygous ""A->AATAAAAT"" variant, the top 5 lines are the reference lines. 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250. The problem is these images are not presenting detailed infomation for the inserted sequence, and on sites where multiple insertions happen, the ""supports variant"" channel might become the only useful infomation to distinguish them.; Also, on the ""base quality"" channel, the qualities for these 0-bases are not zeros, how are these values determined?. I'm wondering if other structures of pileup images on these sites can achieve better performance, like adding 0s on the reference lines?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/306:1244,perform,performance,1244,,https://github.com/google/deepvariant/issues/306,1,['perform'],['performance']
Performance,"Hi, for a 168GB bam the make_examples step produces a total of 48 GB of tf records (sharded, so this is split over 64 different files). Does this seem like a reasonable intermediate output load to pass to call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/153:189,load,load,189,,https://github.com/google/deepvariant/issues/153,1,['load'],['load']
Performance,"Hi, is it possible that we can get access to the `graph.pbtxt` file that was generated during your training? Or can we extract it somehow from the model.ckpt.meta file? I need this to load the model in https://github.com/tensorflow/lucid . I already tried adding `tf.train.write_graph(sess.graph_def, ""/tmp"", ""graph.pbtxt"", True)` during variant calling, and I get a valid graph def. But I suspect that the graph def that I get during variant calling is not the same as the one during training time, and I think I need the latter. Thank you very much.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/84:184,load,load,184,,https://github.com/google/deepvariant/issues/84,1,['load'],['load']
Performance,"Hi,. A few questions about RNA-seq pre-processing before DeepVariant?. - Do you recommend read trimming before alignment using tools such as fastp?; - I can see that this is the repository for the human reference genome, which DeepVariant recommends. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/ . But when I perform read alignment using STAR, can I use GCA_000001405.15_GRCh38_full_analysis_set.fna instead of GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz? GCA_000001405.15_GRCh38_full_analysis_set.fna has the GTF file, which is needed for creating the STAR index.; - Or can I perform the alignment using any GRCh38 reference genome? Does it have to be the same one, GCA_000001405.15_GRCh38_no_alt_analysis_set, used in the RNA-seq tutorial of DeepVariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/791:384,perform,perform,384,,https://github.com/google/deepvariant/issues/791,2,['perform'],['perform']
Performance,"Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```; singularity run -B /scratch \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --customized_model=${MODEL_DIR}/model.ckpt \; --ref=""${FASTA_DIR}""/genome.fa \; --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** ; ```; Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```; INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html); INFO: Using cached SIF image; 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; --ref is required.; Pass --helpshort or --helpfull to see help on flags.; /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory; /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found; ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/783:1130,cache,cached,1130,,https://github.com/google/deepvariant/issues/783,3,"['cache', 'optimiz', 'perform']","['cached', 'optimized', 'performance-critical']"
Performance,"Hi,. I got the following error: . I'm using Docker version 1.1.0; gpu NVIDIA GeForce RTX 3090. Any suggestion or advice?. Thanks in advance.; Amin. `2021-05-06 16:56:50.765879: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1; I0506 16:56:52.008759 140393620989696 call_variants.py:338] Shape of input examples: [100, 221, 6]; 2021-05-06 16:56:52.013998: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-05-06 16:56:52.046181: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz; 2021-05-06 16:56:52.053674: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47507d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-05-06 16:56:52.053727: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-05-06 16:56:52.058754: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1; 2021-05-06 16:56:52.188018: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47b9240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:; 2021-05-06 16:56:52.188089: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6; 2021-05-06 16:56:52.191811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: ; pciBusID: 0000:20:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6; coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s; 2021-05-06 16:56:52.191885: I tensorflow/st",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452:503,optimiz,optimized,503,,https://github.com/google/deepvariant/issues/452,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"Hi,. I have a big load of data to genotype and I did some tests using subsets of my data. My question is if will work use DV default parameter to a Plant genome. I have no gold set for training model. There is any suggestions?. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/357:18,load,load,18,,https://github.com/google/deepvariant/issues/357,1,['load'],['load']
Performance,"Hi,. I tested DeepVariant 1.5.0 on PACBIO data (HG002, chr20), using PACBIO model and got following error in call_variants.py script:. ```; 2023-04-07 15:47:02.393512: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0407 15:47:04.372007 140275094697792 call_variants.py:317] From ./examples.tfrecord-00000-of-00032.gz.example_info.json: Shape of input examples: [100, 221, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0407 15:47:04.374644 140275094697792 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; Traceback (most recent call last):; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/628:247,optimiz,optimized,247,,https://github.com/google/deepvariant/issues/628,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"Hi,. I want to load pre-trained DeepVariant model (DeepVariant-inception_v3-0.7.0+data-wgs_standard).; However, loading model.ckpt.meta file produced some error.; Analyzing environment was google collaboratory (python2, GPU). My purpose is to use the pre-trained model in Keras. I used model.ckpt.meta file as follows:. `import tensorflow as tf`; `pretrian_model_path='/content/drive/My Drive/DeepVariant-inception_v3-0.7.0+data-wgs_standard'`; `saver = tf.train.import_meta_graph(pretrian_model_path + '/model.ckpt.meta', clear_devices=True)`. This produced:. > ValueErrorTraceback (most recent call last); <ipython-input-9-8883daf94bd3> in <module>(); 1 with tf.Session() as sess:; ----> 2 saver = tf.train.import_meta_graph(pretrian_model_path + '/model.ckpt.meta', clear_devices=True); >; >/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs); 1672 """""" # pylint: disable=g-doc-exception; 1673 return _import_meta_graph_with_return_elements(; -> 1674 meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]; 1675 ; 1676 ; >; >/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc in _import_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs); 1694 import_scope=import_scope,; 1695 return_elements=return_elements,; -> 1696 **kwargs)); 1697 ; 1698 saver = _create_saver_from_imported_meta_graph(; >; >/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.pyc in import_scoped_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate, return_elements); 804 input_map=input_map,; 805 producer_op_list=producer_op_list,; --> 806 return_elements=return_elements); 807 ; 808 # Restores all the other collections.; >; >/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/127:15,load,load,15,,https://github.com/google/deepvariant/issues/127,2,['load'],"['load', 'loading']"
Performance,"Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam; samtools index ./N006942-20231016.bam. However, when I tried to â€œRun DeepVariant on chromosome 20 alignmentsâ€ as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta; #PBS -l ncpus=48; #PBS -l ngpus=4; #PBS -l mem=384GB. module load singularity; module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'; INPUT_BAM='/data/N006942-20231016.bam'; OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant; ulimit -u 100000; singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \; --ref ${REF_FA} \; --in-bam ${INPUT_BAM} \; --out-variants ${OUTPUT_VCF} \; --run-partition \; --num-cpu-threads-per-stream 12 \; --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting.; [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue?. Cheers,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/798:776,load,load,776,,https://github.com/google/deepvariant/issues/798,2,['load'],['load']
Performance,"Hi,; I have a question about the information used by DeepVariant (v0.9.0) with the model `PACBIO`: does this model rely on/benefit from the additional quality information contained in ""PacBio-native"" BAM files? In other words, are the variant calls identical for a dataset that is processed (i) using the PacBio-native BAM as input, requiring the alignment to be done with pbmm2 to keep said information intact; and (ii), using FASTQ as input (w/o the additional quality information), and the alignment is performed with minimap2 (since pbmm2 is essentially just a wrapper around minimap2, let's assume the resulting alignments are identical)? Thanks for the clarification.; Best,; Peter",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/275:506,perform,performed,506,,https://github.com/google/deepvariant/issues/275,1,['perform'],['performed']
Performance,"Hi,; Will DeepVariant perform well with PacBio CCS smaller insert reads? (i.e. < 5 kb); Thanks,; Gilad",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/293:22,perform,perform,22,,https://github.com/google/deepvariant/issues/293,1,['perform'],['perform']
Performance,"Hi,; Will DeepVariant still perform on very low coverage (1x, 2x, 3x) highly accurate PacBio CCS reads? Or would GATK be better for lower coverage libraries?. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/294:28,perform,perform,28,,https://github.com/google/deepvariant/issues/294,1,['perform'],['perform']
Performance,"I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/208:178,perform,performed,178,,https://github.com/google/deepvariant/issues/208,2,"['optimiz', 'perform']","['optimized', 'performed']"
Performance,"I am following the [Building from sources](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-build-test.md) tutorial. I cloned the repository, then started building and got the following errors:. ```; viniws@woese:~/Code/deepvariant$ ./build-prereq.sh ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Install the runtime packages' starting ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Misc setup' starting ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Update package list' starting ; E: The repository 'http://apt.postgresql.org/pub/repos/apt YOUR_UBUNTU_VERSION_HERE-pgdg Release' does not have a Release file. ; E: The repository 'http://ppa.launchpad.net/gnome-terminator/ppa/ubuntu bionic Release' does not have a Release file ; ```. And after:. ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ DV_BAZEL_VERSION=0.15.0; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98:280,Load,Load,280,,https://github.com/google/deepvariant/issues/98,2,['Load'],['Load']
Performance,"I am following the instructions under:; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. 1. start GCE image : Ubuntu 16.04 with 100GB. git clone https://github.com/google/deepvariant; cd deepvariant; ./build-prereq.sh; ./build_and_test.sh; ```; ...; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (17:54:59) INFO: Current date is 2017-12-22; (17:55:18) ERROR: /home/<mypath>/0fcc5a420905d68918d80793ee59fab4/external/com_goo; glesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start; with either '//', ':', or '@'. Us; e --incompatible_load_argument_is_label=false to temporarily disable this check. ... (17:55:26) ERROR: Analysis of target '//deepvariant/testing:gunit_extras' failed; build aborted: Loading failed; (17:55:26) INFO: Elapsed time: 27.289s; (17:55:26) FAILED: Build did NOT complete successfully (50 packages loaded); (17:55:26) ERROR: Couldn't start the build. Unable to run tests; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/20:1106,load,load,1106,,https://github.com/google/deepvariant/issues/20,3,"['Load', 'load']","['Loading', 'load', 'loaded']"
Performance,"I am running DeepVariant on google cloud following the wiki (Cost-optimized configuration): https://cloud.google.com/genomics/deepvariant, and it works for one of our sample. . Now I have several bam files for several samples. I can run them one by one. But I wonder is there some options to set a list of bam files? Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/33:66,optimiz,optimized,66,,https://github.com/google/deepvariant/issues/33,1,['optimiz'],['optimized']
Performance,"I am trying to run this PacBio use case and encountered following error: ; https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```; INFO: Using cached SIF image; I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****; time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7; I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader; I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs; 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7; I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader; I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs; 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignor",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/559:222,cache,cached,222,,https://github.com/google/deepvariant/issues/559,1,['cache'],['cached']
Performance,"I launched a training run, but the evaluation run wasn't launched concurrently. When I launch it, it simply evaluates the final checkpoint, not all the checkpoints in between. Is there an option force evaluation of all checkpoints in model_eval?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/378:66,concurren,concurrently,66,,https://github.com/google/deepvariant/issues/378,1,['concurren'],['concurrently']
Performance,"I noticed a mention of VG giraffe evaluation in the 1.5 changelog. . Has your team evaluated the impact of performing indel realignment prior to variant calling VG giraffe-generated bamfiles? This is what was done in the vg giraffe-DeepVariant paper. . In my hands, the indel realignment step significantly adds to run time. If your team does not think it meaningfully improves accuracy beyond make_example's built in realignment algorithm, then I could remove the step from my pipeline. That would be welcome news!. -Joe Lalli",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/629:107,perform,performing,107,,https://github.com/google/deepvariant/issues/629,1,['perform'],['performing']
Performance,"I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions!. **Setup**; - Operating system: linux; - DeepVariant version: 1.5.0 (in Deeptrio as well); - Installation method: Singularity version; - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704:343,perform,perform,343,,https://github.com/google/deepvariant/issues/704,1,['perform'],['perform']
Performance,"I ran training for a number of epochs and obtained a model checkpoint (lets call it `checkpoint-first`) with accuracy > 0.99 (F1/All). Then I launched training again with `--start_from_checkpoint=checkpoint-first`. I expected `model-ckpt-0` for the second training run to show the same high accuracy as `checkpoint-first`. But it shows very low accuracy instead (F1/All is 0.85 or so). However the next checkpoint after `model-ckpt-0` (lets call it `model-ckpt-N`) shows high accuracy. Does this mean `model-ckpt-0` is dumped before loading parameters from `checkpoint-first`, and `model-ckpt-N` is the first checkpoint I should be looking at for meaningful results for the second training run?. I used Google Cloud TPU for both training runs. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/383:533,load,loading,533,,https://github.com/google/deepvariant/issues/383,1,['load'],['loading']
Performance,"I tried to build deepvariant on a local ubuntu server.; With GCP support turned off, so far I am stuck with an error after build_and_test.sh . `(13:58:12) ERROR: /root/deepvariant/deepvariant/core/python/BUILD:174:1: CLIF wrapping deepvariant/core/python/hts_verbose.clif failed (Exit 4): pyclif failed: error execut; ing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/genomics && \; exec env - \; bazel-out/host/bin/external/clif/pyclif --modname deepvariant.core.python.hts_verbose -c bazel-out/k8-opt/genfiles/deepvariant/core/python/hts_verbose.cc -g bazel-out/k8-; opt/genfiles/deepvariant/core/python/hts_verbose.h -i bazel-out/k8-opt/genfiles/deepvariant/core/python/hts_verbose_init.cc --prepend /root/opt/clif/python/types.h -Iextern; al/protobuf_archive -Ibazel-out/k8-opt/genfiles -Ibazel-out/k8-opt/genfiles/external/local_config_python -Iexternal/htslib -Ibazel-out/k8-opt/genfiles/external/htslib -I. -; Iexternal/bazel_tools -Ibazel-out/k8-opt/genfiles/external/bazel_tools -Iexternal/htslib/htslib/htslib_1_6 -Ibazel-out/k8-opt/genfiles/external/htslib/htslib/htslib_1_6 -Ie; xternal/bazel_tools/tools/cpp/gcc3 -Iexternal/clif -Ibazel-out/k8-opt/genfiles/external/clif -Iexternal/local_config_python -Ibazel-out/k8-opt/genfiles/external/protobuf_ar; chive -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/genfiles/external/local_config_python/python_include -Iexternal/protobuf_archive/src -Ibazel-out/k8-o; pt/genfiles/external/protobuf_archive/src '-f-Iexternal/protobuf_archive -Ibazel-out/k8-opt/genfiles -Ibazel-out/k8-opt/genfiles/external/local_config_python -Iexternal/hts; lib -Ibazel-out/k8-opt/genfiles/external/htslib -I. -Iexternal/bazel_tools -Ibazel-out/k8-opt/genfiles/external/bazel_tools -Iexternal/htslib/htslib/htslib_1_6 -Ibazel-out/; k8-opt/genfiles/external/htslib/htslib/htslib_1_6 -Iexternal/bazel_tools/tools/cpp/gcc3 -Iexternal/clif -Ibazel-out/k8-opt/genfiles/external/clif -Iexternal/local_config_py;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12:343,cache,cache,343,,https://github.com/google/deepvariant/issues/12,1,['cache'],['cache']
Performance,"I understand that PRs are not performed on github. So, I just wanted to recommend/discuss some potential changes for the shuffle_tfrecords_beam.py script to enable running it with SparkRunner (PortableRunner). Without these changes the script works only in LOOPBACK mode (which is a testing mode where the actual work is performed on the submitting host).",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365:30,perform,performed,30,,https://github.com/google/deepvariant/pull/365,2,['perform'],['performed']
Performance,"I was trying to follow the quick start guide. While running the run-prereq.sh file, I got; `========== Load config settings.`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Misc setup' starting`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Update package list' starting`; `sudo: apt-get: command not found`; Then, I realize it is because I am running it on mac. Is there any quick fix to this problem?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/44:103,Load,Load,103,,https://github.com/google/deepvariant/issues/44,1,['Load'],['Load']
Performance,"I was wondering if how DeepVariant performs on short tandem repeats and small inversions has been characterized? . I imagine small inversions aren't a problem, but short tandem repeats might be. Also, how does it spell del in variants (where the sequence has been deleted and a new sequence has been inserted)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/180:35,perform,performs,35,,https://github.com/google/deepvariant/issues/180,1,['perform'],['performs']
Performance,"I would like to clarify about the performance improvements re:--call_variants_extra_args ""use_openvino=true"". I am running the Docker Desktop container of the DeepVariant with the openvino feature on Windows 10. I am not sure what version deepvariant is at moment. Below is the output and how can I know if the performance boost is actually working. It seems to be taking just as long. the data is from WGS illumina. . docker stats. ```; CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS; 81c4c886a344 lucid_brown 499.35% 3.191GiB / 44.34GiB 7.20% 3.06kB / 0B 197MB / 0B 20; ```. ```; I0122 00:06:38.538676 140590877263616 make_examples.py:535] Task 2/5: 1501 candidates (1558 examples) [521.15s elapsed]; I0122 00:13:25.409308 139769488508672 make_examples.py:535] Task 1/5: 2300 candidates (2377 examples) [2030.76s elapsed]; I0122 00:14:08.258885 140590877263616 make_examples.py:535] Task 2/5: 1600 candidates (1665 examples) [449.72s elapsed]; I0122 00:17:39.217728 139769488508672 make_examples.py:535] Task 1/5: 2400 candidates (2477 examples) [253.81s elapsed]; I0122 00:24:45.306580 139769488508672 make_examples.py:535] Task 1/5: 2500 candidates (2579 examples) [426.09s elapsed]; I0122 00:33:59.351311 139769488508672 make_examples.py:535] Task 1/5: 2600 candidates (2685 examples) [554.04s elapsed]; I0122 00:36:39.138627 139769488508672 make_examples.py:535] Task 1/5: 2702 candidates (2796 examples) [159.79s elapsed]; I0122 00:44:51.534385 140120745805568 make_examples.py:535] Task 3/5: 1900 candidates (1966 examples) [4131.79s elapsed]; I0122 00:51:27.674227 140590877263616 make_examples.py:535] Task 2/5: 1700 candidates (1765 examples) [2239.42s elapsed]; I0122 01:07:23.046070 139769488508672 make_examples.py:535] Task 1/5: 2808 candidates (2902 examples) [1843.91s elapsed]; I0122 01:14:33.716036 140590877263616 make_examples.py:535] Task 2/5: 1800 candidates (1872 examples) [1386.04s elapsed]; I0122 01:20:36.708237 139769488508672 make_examples.py:535]",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/408:34,perform,performance,34,,https://github.com/google/deepvariant/issues/408,2,['perform'],['performance']
Performance,"I'm trying to build a scatter gather implementation of make_calls -> call_variants -> post_processing, without using GNU parallel, and since multiple shards of call_variants, even with num_readers set to 1, increases the system load well beyond the number of cores, I'd like to try to limit this to a 1:1 ratio where one shard produces a system load of 1. Is this possible?. This is essentially what my pipeline looks like today: https://github.com/oskarvid/wdl_deepvariant/blob/master/deepvar-simple-SG.wdl; I say essentially because I've made insignificant changes, like added --num_readers for example. . One way would be to try to combine all tfrecord files into one, because wdl cannot use your method of using all output files from make_calls as input files for call_variants, inputFile@#shards.gz doesn't compute for wdl, and using wdl's normal way of handling multiple input files, i.e ""--examples ${sep="" --examples "" InputFile}"" doesn't work either since call_variants only takes the last ""--examples"" as input when there are many ""--examples"" in the command. Regarding combining the tfrecord files before they're used as input for call_variants, I'm not familiar enough with tensorflow to know if it's at all possible, and a quick google search didn't return anything fruitful. Is it possible to combine many tfrecord files into one?. Is it easier to try to limit the number of threads per process instead of trying to combine the tfrecord files? Or is there a third method that solves this problem better?. And thanks for a great tool!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49:228,load,load,228,,https://github.com/google/deepvariant/issues/49,2,['load'],['load']
Performance,"I'm trying to build deepvariant on debian_version stretch/sid, 3.10.0-327.3.1.el7.x86_64 #1 SMP Wed Dec 9 14:09:15 UTC 2015, and the build_and_test script is failing. The build-prereq.sh command runs successfully; however, build_and_test.sh throws this error:; In file included from external/htslib/hts.c:45:0:; external/htslib/hts_internal.h:31:32: fatal error: textutils_internal.h: No such file or directory. Yet the file is found here:; .cache/bazel/_bazel_root/5b3dfb1a5a17f553ec98d93bc2cea6e8/execroot/com_google_deepvariant/external/htslib/textutils_internal.h. I get the same error if I try to build on CentOS 7.2.1511 (Core). Please advise.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/134:442,cache,cache,442,,https://github.com/google/deepvariant/issues/134,1,['cache'],['cache']
Performance,"I'm trying to fine tune the original DeepVariant model with some extra data.; However, during the fine tuning process, the model suddenly losses all its predictive power in the first 10000 or 20000 steps. The call-variants output of these models are like all sites have homo-alt variants with a same qual value, 6.8 for example.; The sudden change in the model happens in the first step of fine tuning, as the saved model.ckpt-0 in the begining already gives the above output.; I find this result quite confusing, as the loss should increase dramatically. Is that a normal output of the fine tuning process?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185:19,tune,tune,19,,https://github.com/google/deepvariant/issues/185,1,['tune'],['tune']
Performance,"I'm trying to train a deepvariant model with a very simple topology.; After a few thousands of training steps, the logged training loss starts to vibrate around a rather high value. However the performance of saved models still keeps improving on my validation data set.; Why does this happen?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/194:194,perform,performance,194,,https://github.com/google/deepvariant/issues/194,1,['perform'],['performance']
Performance,"I'm using google/deepvariant-0.10.0 through docker on AVX-512 instruction capable Intel skylake processor. But the docker image ""google/deepvariant-0.10.0"" binaries are not built for AVX-512. Here are the warnings for the same:. ` I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA `. Where can I get deepvariant or tensorflow binaries with AVX-512 optimzation? I tried to build deepvariant from source, but couldn't due to lot of dependencies. . Please let me know if there is any way to get AVX-512 optimized binaries for deepvariant/tensorflow.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/301:604,optimiz,optimized,604,,https://github.com/google/deepvariant/issues/301,1,['optimiz'],['optimized']
Performance,"I've had success following the **Getting started guide** with both CPU and GPU on the example datasets and now I'm trying to run the CPU version on my own data, _C. elegans_, but am getting an error:. ## Submission script for example. ```; #!/bin/bash; #SBATCH --job-name=example_DV; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:655,load,load,655,,https://github.com/google/deepvariant/issues/292,1,['load'],['load']
Performance,"IDENTIFICATION = ""en_US.UTF-8"",; 	LC_TELEPHONE = ""en_US.UTF-8"",; 	LC_MEASUREMENT = ""en_US.UTF-8"",; 	LC_CTYPE = ""C.UTF-8"",; 	LC_TIME = ""en_US.UTF-8"",; 	LC_NUMERIC = ""en_US.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = ""en_US:en"",; 	LC_ALL = (unset),; 	LC_TIME = ""en_US.UTF-8"",; 	LC_MONETARY = ""en_US.UTF-8"",; 	LC_CTYPE = ""C.UTF-8"",; 	LC_ADDRESS = ""en_US.UTF-8"",; 	LC_TELEPHONE = ""en_US.UTF-8"",; 	LC_NAME = ""en_US.UTF-8"",; 	LC_MEASUREMENT = ""en_US.UTF-8"",; 	LC_IDENTIFICATION = ""en_US.UTF-8"",; 	LC_NUMERIC = ""en_US.UTF-8"",; 	LC_PAPER = ""en_US.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2024-02-17 23:32:31.107126: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.108506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.006781: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.007601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, ple",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:6099,load,load,6099,,https://github.com/google/deepvariant/issues/774,1,['load'],['load']
Performance,"Is there a way to redirect the DeepVariant output to another program? For example, to any annotation tool. I tried this command:; ```; sudo -S docker run -v ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/seq1/bowtie2/"":""/ref"" -v ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/Ð ÐµÐ·/Ð½Ð¾Ð²Ð°Ñ_Ð¿Ð°Ð¿ÐºÐ°/SRR062634.filt/"":""/trg"" \; > google/deepvariant /opt/deepvariant/bin/run_deepvariant \; > --num_shards=4 --model_type=WGS \; > --ref=/ref/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz \; > --reads=/trg/SRR062634.filt_srtd.bam |; > sudo docker run -a stdin -v $HOME/vep_data:/opt/vep/.vep -v ""$HOME/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/Ð ÐµÐ·/Ð½Ð¾Ð²Ð°Ñ_Ð¿Ð°Ð¿ÐºÐ°/SRR062634.filt/"":""/SRR062634_filt"" \; > ensemblorg/ensembl-vep ./vep \; > --tab --quiet --no_stats --offline --cache --dir_cache /opt/vep/.vep/ \; > -o /SRR062634_filt/SRR062634.filt_ann.tsv; ```. Then an error message appears:; `FATAL Flags parsing error: flag --output_vcf=None: Flag --output_vcf must have a value other than None.`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/253:709,cache,cache,709,,https://github.com/google/deepvariant/issues/253,1,['cache'],['cache']
Performance,"Is this issue related to TF version?; Any help to fix this issue? Thanks.; ```; (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0""; (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20; INFO: Using cached SIF image; 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>; _ll.load_library(_main_dir); File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library; py_tf.TF_LoadLibrary(lib); tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/555:501,cache,cached,501,,https://github.com/google/deepvariant/issues/555,1,['cache'],['cached']
Performance,Logged training loss does not decrease while performance improves,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/194:45,perform,performance,45,,https://github.com/google/deepvariant/issues/194,1,['perform'],['performance']
Performance,MEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_ele,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:8361,load,load,8361,,https://github.com/google/deepvariant/issues/292,1,['load'],['load']
Performance,"Not sure what is causing the issue but upon reaching this step DeepVariant failed. Any thoughts on how to fix? I tired to run it in a python2.7 environment and still it somehow is pulling from python 3.6 it seems. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp9_28zx5u/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp9_28zx5u/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I0424 15:59:50.266534 139872277903104 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-04-24 15:59:50.321136: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-04-24 15:59:50.376605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz; 2020-04-24 15:59:50.378224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56a1fd0 executing computations on platform Host. Devices:; 2020-04-24 15:59:50.378283: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-04-24 15:59:50.380979: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0424 15:59:50.447775 139872277903104 modeling.py:563] Initializing model with random parameters; W0424 15:59:50.449538 139872277903104 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp3bl4tsmc; I0424 15:59:50.450443 139872277903104 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp3bl4tsmc', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_cou",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304:641,optimiz,optimized,641,,https://github.com/google/deepvariant/issues/304,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"OPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (08:09:38) INFO: Current date is 2017-12-08; (08:09:38) WARNING: /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/core/BUILD:1806:1: in includes attribute of cc_library rule @org_tensorflow//tensorflow/core:framework_headers_lib: '../../../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'external/org_tensorflow/tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/tensorflow.bzl:1100:30; (08:09:38) INFO: Analysed 241 targets (0 packages loaded).; (08:09:38) INFO: Found 185 targets and 56 test targets...; (08:09:38) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1: //deepvariant/core/protos:core_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1 1 input file(s) do not exist; (08:09:38) INFO: Elapsed time: 0.334s, Critical Path: 0.00s; (08:09:38) FAILED: Build did NOT complete successfully; //deepvariant:allelecounter_test NO STATUS; //deepvariant:call_variants_test NO STATUS; //deepvariant:data_providers_test NO STATUS; //deepvariant:make_examples_test NO STATUS; //deepvariant:model_eval_test NO STATUS; //deepvariant:model_train_test NO STATUS; //deepvariant:modeling_test NO STATUS; //deepvariant:pileup_ima",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6:3003,cache,cache,3003,,https://github.com/google/deepvariant/issues/6,1,['cache'],['cache']
Performance,"On Ubuntu 16.04 LTS, when I tried to build it from source with Python 3.6.2, it failed to compile `bazel-out/k8-py3-opt/genfiles/deepvariant/core/python/hts_verbose.cc`, and the error was `hts_verbose.cc:134:143: error: 'Py_InitModule3' was not declared in this scope`. After some investigations and it seems to be an incompatible issue with Python 3. . The relatively full stack is here:. (14:05:26) ERROR: xx/git/deepvariant/deepvariant/core/python/BUILD:174:1: C++ compilation of rule '//deepvariant/core/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd xx/.cache/bazel/xx/7e4d04a878642732d9b8bb40a634229e/execroot/genomics && \; exec env - \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=xx/anaconda/envs/Python36/bin/python \; PYTHON_LIB_PATH=xx/anaconda/envs/Python36/lib/python3.6/site-packages \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 '-std=c++0x' -MD -MF bazel-out/k8-py3-opt/bin/deepvariant/core/python/_objs/hts_verbose_cclib/deepvariant/core/python/hts_verbose.d '-frandom-seed=bazel-out/k8-py3-opt/bin/deepvariant/core/python/_objs/hts_verbose_cclib/deepvariant/core/python/hts_verbose.o' -iquote . -iquote bazel-out/k8-py3-opt/genfiles -iquote external/htslib -iquote bazel-out/k8-py3-opt/genfiles/external/htslib -iquote external/bazel_tools -iquote bazel-out/k8-py3-opt/genfiles/external/bazel_tools -iquote external/clif -iquote bazel-out/k8-py3-opt/genfiles/external/clif -iquote external/local_config_python -iquote bazel-out/k8-py3-opt/genfiles/external/local_config_python -iquote external/protobuf_archive -iquote bazel-out/k8-py3-opt/genfiles/external/protobuf_archive -isystem external/htslib/htslib/hts",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/31:597,cache,cache,597,,https://github.com/google/deepvariant/issues/31,1,['cache'],['cache']
Performance,"PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013.; [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14; [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092.; [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s; user	579m29.953s; sys	11m32.825s; [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND; -------; mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; ; bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; ; tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; ; rm -rf /cromwell_root/pepper_output/pepper_snp/; ; echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; ; zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq; -------; CONTIGS FOUND IN PEPPER SNP VCF:; chr10; chr14; [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND; -------; time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;; mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; ; samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; -------; Running OpenMP with 64 threads.; > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json; > Parsed 346237 HET VCF entrie",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:6179,cache,cacheCopy,6179,,https://github.com/google/deepvariant/issues/491,1,['cache'],['cacheCopy']
Performance,"PU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; ..................; (09:27:04) INFO: Current date is 2017-12-21; (09:27:04) Loading: ; (09:27:04) Loading: 0 packages loaded; (09:27:05) Loading: 0 packages loaded; (09:27:06) Loading: 7 packages loaded; currently loading: deepvariant/core/genomics ... (6 packages); (09:27:07) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:08) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:09) Analyzing: 242 targets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:3642,Load,Loading,3642,,https://github.com/google/deepvariant/issues/19,3,"['Load', 'load']","['Loading', 'loaded', 'loading']"
Performance,Performance issues in deepvariant/data_providers.py (by P3),MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/479:0,Perform,Performance,0,,https://github.com/google/deepvariant/issues/479,1,['Perform'],['Performance']
Performance,Performance on short tandem repeats and inversions,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/180:0,Perform,Performance,0,,https://github.com/google/deepvariant/issues/180,1,['Perform'],['Performance']
Performance,"RNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-01-05 15:52:57.864310: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-01-05 15:53:10.688853: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:10.692890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:26.990784: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:53:27.004992 140619855705920 run_deepvariant.py:519] Re-using the directory for intermediate results in /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9. ***** Intermediate results will be written to /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9 in docker. ****; ***** Running the command:*****; time seq 0 1 | parallel -q --halt 2 --line-bu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:2496,load,load,2496,,https://github.com/google/deepvariant/issues/761,1,['load'],['load']
Performance,Somatic calls (and performance in general),MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/241:19,perform,performance,19,,https://github.com/google/deepvariant/issues/241,1,['perform'],['performance']
Performance,"TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format.; WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object re",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:1943,load,loading,1943,,https://github.com/google/deepvariant/issues/722,1,['load'],['loading']
Performance,"The errors (part of them). + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (00:49:22) INFO: Current date is 2018-01-27; (00:49:22) Loading:; (00:49:22) Loading: 0 packages loaded; (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:96:; 1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=fals; e to temporarily disable this check.; (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:98:; 1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:100; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:102; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:104; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:106; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:108; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:110; :1: name 're2_test' is not defined (did you mean 'ios_test'?)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/43:192,Load,Loading,192,,https://github.com/google/deepvariant/issues/43,12,"['Load', 'cache', 'load']","['Loading', 'cache', 'load', 'loaded']"
